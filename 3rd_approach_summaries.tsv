introduction future work re evolutionary algorithms devang agrawal devang18 kaushik ram sadagopan kaushik7 fatma tlili ftlili the actor critic architecture references results conclusions evolutionary strategies 1 richard s sutton and andrew g barto reinforcement learning an introduction 1998 2 khadka shauharda and kagan tumer evolutionary reinforcement learning arxiv preprint arxiv 1805 07917 2018 3 houthooft rein et al evolved policy gradients arxiv preprint arxiv 1802 04821 2018 policy gradient architecture which outputs predictions of each action value gradient architecture using a single hidden layer motivation policy gradient methods in reinforcement learning face the issue of lack of exploration evolutionary strategies is a black box optimization algorithm to overcome local optima which suffer from low exploitation of the environment feedback signals objective our project is aimed at developing a hybrid evolutionary reinforcement learning algorithm erl and apply it to a classic control problem to prove its superiority over the standalone algorithms we implement a policy gradient algorithm advantage actor critic a2c and an evolutionary algorithm es for the cartpole problem on openai gym subsequently we combine a2c with es for the cartpole problem to show that it performs better than the standalone algorithms environment a pole is attached to a cart which moves along a frictionless track the system is controlled by moving the cart right or left the pole starts upright and the goal is to prevent it from falling over the objective of this task is to keep the cartpole upright continuously for 200 timesteps which corresponds to a reward of 200 schematic of the actor critic setup the neural network architecture for the policy gradient and the value gradient functions are described the policy gradient outputs a probability distribution for the policy from which actions are sampled and hence the actor the value gradient computes the advantage of taking a particular action given an observed state and hence the critic figure 3 vanilla e a2c we combine es and a2c iteratively in a sequence each iteration of algorithm spawns parameters and makes an update by choosing the best candidate the weights updated by the es is passed on to the policy gradient function of the a2c algorithm which performs a gradient descent update figure 1 cartpole problem on openai gym figure 2 evolutionary strategies es spawns moving the parameters to a global optimum figure 4 evolutionary a2c es spawns a population of parameters and a2c updates each member of the population by performing a series of gradient descent updates finally es chooses the best parameter vector based on the rewards obtained in the population and injects noise onto this parameter vector to generate the new population implement evolutionary dqn for the mountain car environment to show the exploration capability enhanced by es figure 6 plots of rewards obtained in each epoch for a2c es vanilla e a2c and evolutionary a2c algorithms respectively observations the training converges when the average reward reaches 200 consistently a2c reaches this state at around 75 epochs 5 episodes each but it has a lot of variation due to the stochasticity in the selection of actions es has a lot of variation at the beginning but stabilizes after 150 epochs vanilla e a2c reaches this state after 125 epochs the evolutionary a2c is clearly superior to the other three algorithms in the sense that it is the quickest to converge and the variations in the reward are minimal after reaching this state evolutionary actor critic figure 5 in the first plot the candidate parameters of the population are weighted by their corresponding rewards in the second plot only the candidate which corresponds to the maximum reward is chosen the plots shows that taking the best candidate parameters performs better than the weighted combination of the candidates weighted combination of the candidate parameters maximum candidate parameters	a2c reaches this state at around 75 epochs the code for each of these algorithms is attached herewith https drive google com open id 19ouh2 rywr97rslumfc t labirb ytey5 a2c trains for 5 episodes in each epoch we have plotted the rewards for we combine es and a2c iteratively in a sequence this paper similarly in the work of finally the objective of this task is to keep the cartpole upright continuously for 200 timesteps which corresponds to a reward of 200 we implement an advantage actor critic a2c policy gradient algorithm for the cartpole problem in the first plot for the policy gradient we output a policy to take an action given a specific set of states we extended our algorithm to the mountain car environment the pole starts upright and the goal is to prevent it from falling over each iteration of algorithm spawns parameters and makes an update by choosing the best candidate a pole is attached to a cart which moves along a frictionless track the advantage is calculated by finding the difference between an estimated average future reward and the average current value of the state while es algorithms are more stable than drl and can explore the feature space better they suffer from low exploitation of the environment feedback signals and tend to have poor performances in most applications due to their high sample complexity new values are calculated by using a discounted monte carlo simulation which will place importance on short term reward rather than long term reward using a discount factor 
stockagent application of rl from lunar lander to stock price prediction stockagent application of rl from lunar lander to stock price prediction caitlin stanton stanton1 stanford edu and beite zhu jupiterz stanford edu cs229 machine learning problem statement reinforcement learning has been used to learn to all sorts of games from chess to arcade games we first trained an agent to play the arcade game lunar lander using both policy gradient descent and deep q learning then using very similar techniques to the ones implemented on lunar lander we train an agent to learn how to invest in stocks data used lunary lander environment provided by openai gym data for stock prediction came from iex s api provided 5 years worth of apple and google stock data we reformatted our stock data to have one entry for each week instead of each day and consolidated the number of features see section on states below for details figure 1 formatted stock data as inputted into our neural network states actions and rewards lunar lander 4 states 8 dimensional vector including information such as the lander s position and orientation in space and amount of fuel used actions do nothing fire right engine fire left engine fire main engine rewards rewards for landing with feet down and penalties for wasting time landing far away from the pad and wasting fuel stock market states volatility standard deviation mean from past 3 weeks weekly change in stock price from past 3 weeks current price of stock current cash level current owned stock value actions hard sell soft sell do nothing soft buy hard buy for our purposes we initially set hard to mean buying selling 100 worth of stock and soft to mean buying selling 10 worth of stock rewards change in portfolio for that week figure 2 the plot of cost against epochs the spike is probably due to the explo ration in the model figure 3 the plot of reward against epochs the plummet is probably due to the exploration in the model figure 4 lunar lander environment model parameters lunar lander fully connected neural network of shape input 8 8 5 exploration rate starts at 0 then increments by 0 001 with max of 0 9 stock market fully connected neural network of shape input 8 8 5 default exploration rate 0 05 train for 1000 epochs where each epoch plays through 200 weeks of stock data input state q value of action 1 q value of action 2 q value of action 3 q value of action 4 q value of action 5 action figure 5 graph of the stockagent using deep q learning results for stock prediction model took about 200 epochs to converge adjusting soft buy sell action size here is a table containing the information of the final portfolio value network goog aapl soft 1 66 61 235 42 soft 5 448 15 342 22 soft 10 50 77 943 07 soft 20 120 14 19 247 soft 50 5716 30 8566 56 table 1 final portfolio value under different soft buy sell value adjusting the exploration rate exploration rate goog aapl 0 249 63 1905 90 0 01 458 92 1713 68 0 05 592 98 476 66 0 1 116 36 264 31 0 2 30 76 129 55 table 2 final portfolio value under different exploration rates loss functions loss q q 2 where we generate the target q by q s a r s a max a q s a for s the state from s via action a and the discount factor in lunar lander we sample some size of memory to do replay to generate q in stockagent the whole training is recorded and served as memory discussion future directions all the parameter choices we tested performed worse than just always choosing hard buy each week with such a policy we have final portfolio value of about 12 000 for apple we are currently unsure of why our model actions skew so heavily in favor of soft buy instead of hard buy except possibly that our model is quite risk averse one thing we would like to try in future training episodes would be to also train and test on stocks that are not very successful i e do not trend upwards in the long run at the moment each of our training simulations last for 200 weeks while our test dataset consists of 30 consecutive weeks in the future we would like to use a longer test set to mimic the more long term gains that our model is being taught to achieve if our goal is to precisely model predict apple stock we might add additional features such when new iphone or macbook models are being released we had a number of spikes in our apple data that we didn t see in our google data and we think that these might correlate with such events acknowledgements we would like to thank our mentor mario srouji for his guidance throughout this project references 1 gabriel garza deep reinforcement learning policy gradients lunar lander medium 2 wolski filip dhariwal prafulla radford alec klimov oleg schulman john proximal policy optimization algorithms corr 2017	all other experiments are conducted around the above base model there are rewards for landing with feet down and penalties for wasting time landing far away from the pad and wasting fuel more volatile stocks could lead to an unprofitable agent in the experiment as in figure viii on the stock of aapl the actions consist primarily of hard buy and sell one can see that unlike the box2d games having a reasonable exploration rate like 5 percent actually negatively influences the training the code for this project is available at https github com zhubeite cs 229 rl project making this number closer to 0 will make the agent more short sighted thus taking bolder actions the final portfolio value is around 12 000 dollars with a bigger probability of taking hard actions the agent will see more of the benefits of hard actions the result can be seen in figure vii note that when 0 and ehc 0 2 we have the baseline model the exploration hard chance or ehc puts a different weight on hard actions when sampling for an action in the exploration part of training here is what we have tried i in experiments we found that lowering or increasing and ehc will indeed encourage more hard actions our base line model as in exploration rate tweaking having random exploration is important in learning a game like lunar lander so that the agent can experience different states of the game and handle various situations in other words the agent is risk averse we used iexs api to download 5 years worth of stock data from apple and google on the other hand it makes the agent highly unstable with respect to different stocks this included the daily price opening closing high and low of the stock 
poster deep cue learning a reinforcement learning agent for pool summary the goal of this project is to apply reinforcement learning to the game of pool the environment is formulated as an mdp and solved with q table dqn and a3c algorithms with two balls on the table q table learns the best but a3c with discrete action space has the best performance considering all trade offs problem formulation markov decision process mdp state s list of x y positions for each ball white ball first action a angle force reward r s a 5 for each ball pocketed 1 if no ball collides with white ball 0 otherwise algorithms q table implements q learning with discretized states and actions uses a lookup table for each s a pair to represent the q function deep q network dqn 1 uses dnn to approximate the q function with continuous state values as input and the q values for each discrete action as output asynchronous advantage actor critic a3c 2 estimates both the value function and policy policy can be updated more intelligently with the value estimate multiple agents learn asynchronously on different threads to speed up the overall training two ball environment q table best performance learns the exact steps to hit the ball in from the starting position table size large limited to two ball environment training time significantly longer dqn good performance but training unstable model learns only two or three good actions that tend to get better total rewards but does not do well in the short term a3c continuous action good performance but longer convergence time space efficient generalizable to larger environments since it predicts mean and variance of the normal distributions for actions it is hard for the values to settle in the right range a3c discrete action better performance than with continuous action sacrifices some accuracy space and time but classification training is more effective than predicting bounded continuous values discussion four ball environment both a3c with continuous and discrete action perform poorly when state space is enlarged in continuous action values tend to be saturated and clipped at 0 or 1 in discrete action a single angle value tends to be favored references 1 mnih volodymyr et al human level control through deep reinforcement learning nature 518 7540 2015 529 2 mnih volodymyr et al asynchronous methods for deep reinforcement learning international conference on machine learning 2016 future work inspect the value saturation problem in a3c look for improvements in environments with more balls compete the ai with human player for more comprehensive evaluation nkatz565 cs229 pool experimental results training results over 1000 episodes q table state 50 buckets angle 18 buckets force 5 buckets dqn a3c continuous action a3c discrete action angle 360 buckets force always max average reward 6 4 21 3 training time 136 min model size 1 12 gb evaluation results over 100 episodes training statistics 27 min 162 kb 13 min 8 kb 17 min 149 kb 19 44 18 46 a3c continuous action a3c discrete action 2 balls episode length 25 4 balls episode length 50 28 min 11 kb 50 0 52 min 152 kb 24 86 peiyu liao pyliao stanford edu department of computer science nick landy nlandy stanford edu department of electrical engineering noah katz nkatz3 stanford edu department of electrical engineering	each episode is automatically concluded when all balls have been pocketed or the maximum number of trials have been reached in this project four different reinforcement learning rl methods are implemented on the game of pool including q the problem is formulated as an mdp with the following set of state action and reward definitions where m is the number of balls x 1 and y 1 are the x and y positions of the white ball x i and y i are those of the i th ball r is the angle of the cue within range 0 1 f r is the force applied to the cue within range 0 1 is the relative reward weight between hitting no ball and pocketing one ball and numballs s returns the number of balls still in play at state s s 2 m is the list of elements in s other than the first element i e each algorithm is trained for 1000 episodes each episode allowing a maximum of 25 hits dqn and a3c to stabilize training he is also the major writer of the report all team members worked together on the report the training results are shown in two ball environment the moving average rewards received over the training period of all five algorithms is shown in the dimension of the 2 hidden layers are 64 and 256 each the following modifications are made to fit this project 1 one of the most notable areas of research in rl is in building agents to play games created an interface to modify game parameters the game can further be made more challenging by enlarging the table size adding rules etc created an interface for the rl algorithm to interact with traditional ai techniques used include search 2 
combining ppo and evolutionary strategies for better policy optimization combining ppo and evolutionary strategies for better policy optimization jennifer she computer science stanford university jenshe stanford edu objecive propose and implement hybrid policy optimization methods inspired by proximal policy optimization ppo and natural evolutionary strategies es in order to leverage their individual strengths compare hybrid methods against ppo and es in two openai environments cartpole and bipedalwalker background under the reinforcement learning rl framework agent environment at rt st 1 the goal of policy optimization is to find a policy s a 0 1 defining pr at a st s that maximizes the expected return j e p rt ppo updates via an approximation of j pro it uses gradient information to guide its updates which helps it to zero in on potential solutions con it may get stuck at a local optima as a result es parameterizes with n 0 i which it updates by sampling 1 k weighted by their return 1 k ki 1 t rt p i 1 pro it incorporates stochasticity in the space of for better exploration of con it treats the rl problem as a black box the goal is then to build hybrid methods that both leverage gra dient information and are stochastic in methods es ppo i ppo ppo ppo update 1 2 k 1 2 k sample i as in es but instead run ppo with these as initializations to obtain i update by 1 with modified perturbations t 1 i max ppo run es ppo as above but directly set to i with the highest return argmaxi rt p i alt ppo run es every j ppo iterations we compare these methods to es and ppo environments a cartpole b bipedalwalker cartpole v0 cp s r4 a 0 1 objective move cart to keep pole upright rewards 1 every timestep for a max of 200 termination pole falls cart goes off screen or episode reaches max of 200 timesteps bipedalwalker v2 bw s r24 a 1 1 4 objective maneuver walker to right most side of environment target without falling rewards for moving forward for a total of 300 on agent reaching target 100 for falling termination walker reaches target or falls architecture details es a s 1 a f s where f is a fully connected neural network 1 fc dim s 100 relu 2 fc 100 dim a 3 sigmoid 1 cp or tanh bw ppo hybrids a s bernoulli g s cp a s n g s bw where g is a fully connected neural network 1 fc dim s 100 relu 2 fc 100 100 relu 3 fc 100 dim a 4 sigmoid cp or tanh bw results es cp es bw ppo cp ppo bw es ppo cp es ppo bw max ppo cp max ppo bw alt ppo cp alt ppo bw figure episode returns over training across 5 trials each return training time es 200 0 60 59 ppo 200 0 53 74 es ppo 200 0 515 03 max ppo 200 0 363 52 alt ppo 200 0 131 24 table final results from cp averaged across 5 trials discussion ppo and es performed well on both tasks ppo training instability bw likely a result of reusing samples from old es evaluating i is slow without leveraging large scale parallel compute extending es ppo and max ppo from es exponentiated this problem and forced us to choose max sample size k 5 for bw es ppo ppo calls may drive i far from thus a weighted average of returns at i may no longer be a good predictor of return at weighted average of i misleading update signals max ppo mitigates averaging problem of es ppo but may lead away from a good solution when all neighbouring i have low returns high variance alt ppo mitigates high computation cost of es ppo and max ppo but its stochasticity may lead away from a good solution when neighbour i have low but different returns future directions investigate trade offs in sample efficiency and variance in the case of ppo investigate ways to leverage high compute in the case of es ppo and max ppo investigate stochasticity with adaptive variance using gradient information to avoid moving away from good solutions investigate more complex environments where es and ppo fail acknowledgements we thank mario srouji ta for the project idea and help during the project jenshe stanford edu	we use the same setting as ppo in the case of cp except we increase h to 2048 and batch size to 64 and make use of the entropy term with c ent 0 0001 we choose k 5 among 5 10 20 as it seems to be sufficient for cp and results in the fastest training time the agent receives for moving forward for a total of 300 on the walker reaching its destination we also set 2 0 1 among 0 1 0 001 0 0001 with learning rate 0 001 among 0 0001 0 0025 0 001 the code for this project is available at https github com jshe cs229project git the agent also receives 100 for falling the episode ends when the walker reaches its destination or falls we then use mini batch samples of these pairs s t a t of to update we thank mario srouji for the project idea and help during the project we alternate between es and ppo iterations by running es every j iterations with in order to inject stochasticity s r 24 and a 1 1 4 represent the various states and actions of the walker and its components hips knees etc for es in this setting we again represent by 9 where f is a fully connected neural network fc 24 100 relu fc 100 4 tanh however they make use of less information and thus require more time and samples to perform well however they are said to face a lack of exploration in the space of policies due the greediness of their updates v is updated along with using an additional lossan entropy termcan also be optionally added to the objective to encourage exploration in a t combining 
overview simple explicit measure of contextual word importance supports tiny contexts 10 sentences uses document word vector cloud properties contextually significant words define meaning weighted bag of words model substantially outperforms state of the art for subjectivity analysis and paraphrase detection comparable to sota for other transfer learning tests applications a better sentence vector baseline easy sentence document summarizer via pathfinding contextual stop word identification improved and context aware cosine distance current implementation limitations long short term memory lstm networks limited because short term document specific context overfitting tf idf sentence embedding vector baseline rarer words are more important essentially sum of tf idf weighted word vectors requires large document no handling of out of context words stratified for rare words ignores word similarity state of the art global context approaches context vectors deep structures etc black boxes unsupervised barely outperform tf idf baseline knowing what you re reading affects interpretation tf idf baseline requires a large context dataset to work but people don t need a ton of text to establish context newspaper articles short stories currently no simple baseline for global context motivation context is everything finding meaning statistically in semantic spaces words clauses stanford sentiment diverse context 9k examples 300 dimensional pretrained glove 42b cc no out of vocabulary keys sentences senteval train dev set variety of transfer learning contexts fasttext vecs 600b token cc out of vocab support replacing tf idf mahalanobis distance normalizes for stdev and covariance distance from document word vector cloud needs only document word vector covariance and average works with tiny data since word vec dimensions are normal better and context aware cosine distance vec cardinal vec red wikipedia page for green vec cardinal vec red article about stanford d 0 909 d 0 943 unified clause word vector space glove space including both two word clauses and words importance relates clause vecs and constituent words sigmoidal sentence embeddings calculate document average word vector and covariance for sentence calculate each word s importance divide by double the sentence average opt ignore words in closest 20 of doc importances for sentence average corresponds closely to stop words weight by sigmoid of relative importances algorithms word vector clouds algorithms sentence embeddings algorithms sentence unembeddings meaning subtraction vec sentence w vecword n vecword n given a sentence vector and one subsentence vector can calculate other subsentence vector assume w vecother is the avg distance solve for vec repeat takes 3 5 iterations to converge to several decimal places path finding for meaning extraction calculate the remaining subsentence vector if within m cosine distance radius return sentence find the new words closest to the subsentence vector enqueue the sentence with the closest words appended ideal params ignore stop words and case harmonic mean erf stretched vertically for range between 0 27 to 0 73 horizontally by 4 2 the sky is bluem d is ta nc e the sky is blue m ea ni ng word proportion of clausal meaning vs importance in document proportion of total general context importance pr op or tio n of c la us al m ea ni ng datasets and evaluation visualization of improved cosine distance conclusion and future directions this technique should replace the tf idf baseline can global context help generate word vectors implications for how we process information appears to suggest we overvalue slightly more salient information when combining meanings linguistic implications where does syntax come into play can a rule based system restricting the subset of closest words that can be chosen as the next word generate grammatical sentences with the unembedding neurological implications can we measure the importance salience of words and sentences and relate them	is it the envisioning of a task in one s mind over and over again or perhaps is it incremental improvement over repeated practice the action is a t w c z t h t b c for controller c at timestamp t we experimented with varying the complexity of this policy but found that it made it more difficult to update from new levels for this particular challenge your agent must achieve a score of 3000 to consider the level solved and our agent received an average score of 3600 across many levels it was also amusingly a fan of spectacle choosing to watch an explosion instead of proceeding what is the nature of excellence perhaps the most compelling future improvement is a stochastic update to the the lstm and vae with every new experiment alongside the per epoch training we introduced the world models paper cma es for covariance matrix adaptation evolution strategy think of how an athlete trains daily while also envisioning her future success the consequence was that the anticipator often did nothing when it didn t think it could contribute the loss used for the generator was partially inspired by the loss used in a vae accounting for both the likelihood of the predicted next frame 2 and an adversarial loss corresponding to how effectively it tricked the discriminator the vae is used to encode and decode frames we updated alertness to only activate if the controller performed badly because the cma es generates samples from a gaussian distribution one can vary the sigma value to get a larger variety of outcomes depending on your performance needs 
poster final predicting nyc taxi fares trip distance and activityomnitaxis authors boxiao pan varun nambiar paul jolly bxpan vnambiar pjolly94 stanford edu stanford university cs229 machine learning december 11th 2018 introduction for a new york city taxi driver being in the right place at the right time is often what makes or breaks a day one may naively assume that the right spot corresponds to a place simply where demand or activity is high however taxi drivers might find it more lucrative to be in a slightly lower activity location where people are demanding shorter trips that are worth more to assist drivers in this decision we explored different models to predict the activity fare amount and trip distance given input features location the day of the week and the time of the day raw data new york city taxi and limousine commission tlc provides a large amount of trip data from 2014 to 2018 including the following information a date and time of trip b pickup location mid 2014 mid 2016 latitudes and longitudes mid 2016 mid 2018 location ids c fare amount d trip distance data pre processing label bucketing instead of using exact values for the labels activity fare and trip distance we discretized them by creating buckets this was done by inspecting the distribution of the labels over the data and selecting realistic bucket ranges table 1 not only did this enhance the model performance but more importantly it proves more useful in application since we are presenting estimated ranges i e buckets to the driver as opposed to exact numbers which is what drives care more about creating location clusters with k means to increase the granularity of the newer trip data i e post mid 2016 we created clusters using the latitude longitude data from the older dataset i e pre mid 2016 and distributed the newer data into cluster ids based on the distribution within each location id obtained from k means figure 1 data and features conclusion and future work experimental results models our models can pick up the relative differences between different neighborhoods but does not perform well when trying to predict the exact numbers this may be a result of using a single model to predict for the entire new york area here are possible ways forward focus our model to manhattan island and the surrounding airports only include areas where yellow taxis officially serve add more data for the lstm model our sparse sampling may be causing issues further tune network hyper parameters model activity fare trip distance rfc 51 02 45 71 30 08 fcnn 35 67 45 72 23 16 lstm 26 65 27 72 23 10 references sklearn cluster kmeans kmeans scikit learn 0 19 2 documentation online available https scikit learn org stable modules generated sklearn cluster kmeans html accessed 11 dec 2018 bucket id activity trips fare trip distance miles 0 2 0 0 5 1 2 5 0 5 0 5 1 0 2 5 7 5 10 1 0 1 5 3 7 10 10 15 1 5 2 0 4 10 15 15 25 2 0 3 0 5 15 25 25 50 3 0 5 0 6 25 35 50 60 5 0 10 0 7 35 45 60 10 0 8 45 location id 261 world trade center manhattan location id 256 williamsburg south side brooklyn before clustering after clustering figure 1 two examples of location clustering based on trip data through k means the top row shows trips before clustering and the bottom row shows trips classified into clusters table 1 label classes created through bucketing random forest classification rfc loss function gini loss 1 fully connected neural network fcnn loss function cross entropy loss log 4 hidden layers with 6 10 6 12 neurons respectively and all with relu activation function 1 output layer with softmax activation function long short term memory lstm network loss function cross entropy loss log the model parameters are for the case of activity prediction subset description data set split 90 5 5 training set fare and trip distance 1 8 million activity 0 27 million validation and test set fare and trip distance 0 2 million activity 0 03 million for all three labels random forest performs the best or nearly among the models while all these three models perform poorly on this task we think this should mainly be attributed to that the prediction task is too hard given the features we have figure 2 shows an example heat map output comparing predicted activity with the ground truth while the exact numbers do not really match our model captures the relative activity quite well table 2 quantitative experimental results figure 2 heat map representation of ground truth activity left versus predicted activity right the lighter the color the heavier the activity despite not being able to predict the exact magnitude of activity well our model is able to capture the relative activity between different locations	however the regions the commission chose were quite large and we wanted to reduce the area to a few blocks patterns of 5 boroughs may be infeasible we go over the methodologies in this section 5 sample more data example outputs are shown in this allows us to roughly estimate activity in a few blocks rather than a large neighborhood however the results turned out to be approximately the same as with cluster id other than this the data is consistent over the years these weights could be determined by the distance between the actual classification and the predicted classification for data prior to july 2016 referred to as older data the pickup and drop off locations are reported as latitude and longitude while for data july 2016 onward referred to as newer data they are reported as location ids ids that correspond to larger geographic regions however some misclassfications may be better than others for an nyc taxi driver being at the right place at the right time is often what makes or breaks a day 2 currently our loss function penalizes all misclassifications equally k means algorithm works as follows 1 randomly assign cluster centroids in a given location id we tuned these hyperparameters on the validation set a few groups have tried to predict nyc taxi activity and fares with a variety of features a weighted loss function could help improve our model s accuracy some of the approaches used include lgbm the problem we are attempting to solve does not have much literature and most of the approaches we used have not been tested before however this may not necessarily be the best place to be for each of the models we used a 90 5 5 split for training validation and testing respectively 
poster apply reinforcement learning in ads bidding optimization ying chen scpd ychen107 online display advertising is a marketing paradigm utilizing the internet to show advertisements to targeted audience and drive user engagement since around 2009 real time bidding rtb has become popular in online display advertising rtb allows an advertiser to use computer algorithms to bid in real time for each individual opportunity to show ads these bids can be based on the impression level features such as user ads and context information with its fine grained user targeting and auction mechanism rtb has significantly improved the campaign return on investment roi looking at the process we can see that bidding optimization is one of the most critical problems for the advertiser which aims to set right bidding price for each auctioned impression to maximize key performance indicator kpi such as click most of existing work only focused on finding optimal bid price but ignored an important part of optimal bidding strategy pacing algorithm pacing algorithm is essentially for budget allocation which aims to smooth budget spending across time according to traffic intensity fluctuation background we propose to use reinforcement learning algorithms to find the optimal bidding strategy the major difference of our algorithm from previous work is that our algorithm try to generate bid and pacing signal at the same time this requires the algorithm to not only learn to adapt to the changes in bidding environment but also the interaction between bid price and pacing signals we first focus on intra day bidding optimization given a certain amount of budget and unknown traffic distribution throughout the day we want to find out a bidding and pacing strategy that maximize the total number of clicks this is equivalent to minimizing cpc cost per click here we trained dqn model in 50 200 500 2000 and 5000 iterations here is one of the 500 iter results we can see the pacing rate is relatively smooth and the spending is relatively better 3 ddpg we also tried ddpg comparing with dqn its action space is continuous the results look very random and the budget some times overspends since we generate a random amount of impressions in each time slot to mimic the actual traffic the environment input is entirely different every time we do not use the quantitative metric to measure the performance of different algorithms instead we compare the budget spending pacing rate smoothness and stables among different algorithm in a qualitative way overall reinforcement learning dqn on pacing can spend most of the budget without overspending and has a relatively smooth pacing rate it shows better performance compared with the baseline algorithm there are also some weakness in ddqn for now 1 the training model is not very stable 2 sometimes it spends too much money so that the remaining daily budget is negative at the end these results give us the confidence to apply reinforcement learning algorithm in bidding optimization in the ads industry in the future we will modify neural network architecture refine cost functions and tune the parameters to mitigate the disadvantages we are also going to explore reinforcement learning in intra day bidding challenge methods experiment result conclusion and future work methods state space the state of the campaign is characterized by its remaining budget remaining delivery time current value of cpc to simplify the discussion we only consider the case when cpc optimization goal is set which is the most common case we define state as s t budget t cpc t hour t action space there are two action signals in our setup the pacing signal p t and the bid adjustment signal a t ideally the pacing signal should be continuous to reduce algorithm complexity we discretize this signal in q learning pacing granularity could be a concern but for the first version it suffices to discretize 0 1 by 2 interval so we end up with 50 possible values the bid adjustment signal a t can be 1 2 5 10 20 50 and 100 on top of previous bid state transition we model the bidding process as a markov decision process given a previous bid b t bid adjustment signal at and pacing signal p t the bidding agent join auctions and observe the outcome then update the state s t note that we don t know the state transition dynamics the optimal control policy will be learned using reinforcement learning reward function the reward function is composed of two parts the discounted reward and the regularization the instant reward r t could be the number of clicks in given time interval t and a regularization is put on the bid signal to encourage its smoothness 1 baseline we use fixed bidding price and pacing signal as a baseline it s a simple feedback loop control we can see the pacing rate has changed a lot during the procedure and the spending looks reasonable 2 dqn q learning is a straightforward off policy learning algorithm it basically builds a q table which gives the reward function for state action pairs and update it while exploring the environment deep q learning is just a neural network version of q learning which uses dnn cnn to approximate the q function experiment result email yingchen107 gmail com pacing rate in na ve bidding remaining budget in na ve bidding pacing rate in dqn remaining budget in dqn pacing rate in ddpg remaining budget in ddpg	for pacing it s hard to qualitatively compare the performance between two different algorithms the simulator is implemented approximately following the convention in gym package it can be regarded as a combination of dpg and intra day budget spending could be another interesting topic for us to explore code path https github com yingchen cs229 the dqn is implemented with experience replay for the discounted cost we set discount factor to 0 99 and we used td 0 to compute the loss for optimization we used adam with traffic ups and downs it makes the environment varies in different time steps overall we explored the possibility of applying reinforcement learning to ads budget pacing problem since the company data is confidential and there is no public dataset available for ads auction simulation the link of our previous choice ipinyou dataset it not only achieves the goal of budget delivery but also maintains the pacing rate relatively smooth the algorithm of dqn with experience replay can be found in 1 so it s omitted here in the interest of space ddpg deep deterministic policy gradient is a model free off policy actor critic algorithm results shown in pacing is a strategy to ensure that the budget spends evenly over the schedule of advertiser s ad set it s a closed loop control system similar to that of the optimal policy on the other hand can be generated by taking greedy actions at each state according to q table 3 try alternative cost function and regularization deep q learning q learning is a straightforward off policy learning algorithm it basically builds a q table which gives the reward function for state action pairs and update it while exploring the environment 
improving product categorization from label clustering alexander friedman alexandra porter alexander rickman cs229 machine learning class project ajfriedman amporter arickman stanford edu motivation in a massive online store an intractably large set of key words to describe books can easily be acquired by either seller input or automatic searching of the text our goal is to organize a massive set of labels applied to a set of books to use for categorization we implement an algo rithmic and application based project to analyze data from amazon web crawl data of books and their categorizations we embed labels into a feature space and apply clustering approaches to find interesting features such as redundan cies hierarchies and anomalies methods node2vec the node2vec algorithm 1 samples a set of random walks and then performs stochastic gradient de scent on the feature representation of the vertices the loss function is the similarity of the pairs of representa tions given that the vertices appear together v d parameter matrix for u v ns u v is neighborhood with sam pling strategy s maximize objective function max f u v logzu ni ns u f ni f u clustering once we have node2vec representations of the network we cluster with k means 3 based on subjective observation and testing on the data set we specified the number of clusters as 6 1 procedure k means k pointset 2 while centers change do 3 clustercenters k random points 4 for p pointset do 5 center p argmin c clustercenters dist p c 6 for c clustercenters do 7 c mean p center p c references 1 aditya grover and jure leskovec node2vec scalable feature learning for networks in proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining pages 855 864 acm 2016 2 f pedregosa g varoquaux a gramfort v michel b thirion o grisel m blondel p prettenhofer r weiss v dubourg j vanderplas a passos d cournapeau m brucher m perrot and e duchesnay scikit learn machine learning in python journal of machine learning research 12 2825 2830 2011 3 saedsayad com k means clustering 2018 dataset the amazon dataset contains metadata on 350 000 books including the categories labels to which each book be longs the graph dataset which we input into node2vec was created by using labels as nodes and generating edges between nodes whenever a book belonged to multiple la bels labels in the original amazon dataset can be de scribed as a forest these labels can often be redundant so our model aims to detect these redundancies so they can be replaced with a cleaner labeling scheme framework and implementation data pre processing select books select top sales ranked labels as nodes edge weights as books embed node2vec output label features pairs cluster k means clustering output point cluster pairs output cluster centers output pca plot top 2 components output kmeans clusters output point center distances analysis option 1 find outliers remove from graph option 2 find clusters create graph of each cluster figure 1 framework as seen above we iterate through a workflow of embed cluster plot analyze and repeat in this process we ad just parameters of both the node2vec and clustering models we can use this system to detect remove outliers before optionally re embedding we can also select a cluster from the initial run then re embed and re cluster that cluster repeating numerous times in order to collect redundant cat egories and analyze label hierarchies after analysis we se lect an induced subgraph of the original graph to re embed and continue the cyclic process we use the scikit learn package to cluster and plot 2 results anomaly detection and removal a b figure 2 a original clustering 6 clusters b anoma lies removed from graph and re embedded before another clustering nested label associations a a initial embedding and clustering 6 clusters b se lecting the subgraph induced by nodes in magenta cluster ing re embedding and re clustering c same as b for the cyan cluster b c figure 3 creating nested clusters analysis anomalies we use euclidean distances of points from k means centroids to detect outliers as seen in the table below we can directly remove these outliers from the plots but we hypothesized that removing outliers from the graph and re embedding before re plotting would produce more cohesive clusters as seen in figure 2 b removing anomalies results in less clearly defined clus ters likely due to the cluster structure being primarily defined by the anomalies we hypothesize that the graph induced by non anomalous nodes is relatively uniform and thus lacks structure for our method to identify distance from center label 1 9203707947953235 subjects 1000 1 9626659852879147 instruction 11811 2 0156069220765436 books 283155 2 0276880269223216 poetry 9966 2 1297687095091673 foreign languages 11773 2 2177811099675195 dictionaries thesauruses 11475 2 4396388017039103 general 725800 label organization after two iterations of embedding and clustering we see that groups are mostly made up of labels which are redundant or closely related below are 3 examples of label sets strings as they appear in the data found in a cluster shown in figure 3 b c cluster 1 cluster 2 cluster 3 regions 17228 computer arts regions 640504 computers camera states 17263 design categories 493964 states 640538 digital collections united internet 768564 collections united programming 3839 general 2050 project photo software photo specialty photographers 229534 photography photography 2020 172282 future work additional parameter optimization node2vec search strate gies depth vs breadth kmeans clusters outlier thresh old determine necessary number of nested label clustering steps to find all redundancy additional applications other product categorizations financial transaction networks telecommunications net works pharmaceutical co prescription data	before discussing the setup in algorithm 1 we provide the pseudo code for the k means algorithm k means runs with o n k t where n is the number of iterations k the cluster number and t the number of data points for example one book in the original dataset belongs to two trees with labels 1 1 2 1 graph clustering code can be found at https github com aporter468 embedandcluster note that the necessary node2vec library is not included it can be found at https github com aditya grover node2vec tree master src b appendix contributions friedman experimented with different combinations of dbscan k means pca and colors to create visualizations f is v d parameter matrix for u v n s u v is neighborhood with sampling strategy s maximize objective function max f u v log pr n s u f u books subjects arts photography photography photo essays and 2 we use pca to select two dimensions for plotting as seen in liu et al recent work on labeled graph clustering includes using node identifiers and community prior for graph based classification zhou et al ran node2vec embeddings and analyzed results amazon web store categories camera photo photography books photo essaysthese categories are somewhat redundant and one of the goals of our model will be to detect categories which can be merged or used to provide additional recommendations to a user since we do not have a ground truth into how labels should be interpreted our tool is designed to present options for improving the label set to a user who would not be able to parse through the massive label set any other way 
logistic regression linear svm svm with rbf kernel we build ml models to detect fraudulent activity in payment systems used pca for data visualization build binary classifiers using logistic regression linear svm svm with rbf kernel developed approach to detect fraud with high accuracy and low number of false positives achieved max recall 99 on transfer dataset in a fraud detection system it s more critical to correctly detect fraud transactions and acceptable to misclassify certain number of non fraud transactions penalize misclassification of fraud transactions more than non fraud transactions assign higher weights to fraud class to obtain high recall on that class and counter data imbalance ensure no more than 1 false positives fraud detection using machine learning aditya oza aditya19 stanford edu we obtain very high auprc values for transfer test set for all three methods with 0 98 highest value for svm with rbf kernel expected from pca decomposition results as this category of transactions is linearly separable relatively lower recall precision auprc scores for cash out test set further improvement on cash out by setting higher threshold for false positives 1 decision trees random forests to leverage categorical features time series based analysis for in context detection customized user specific models based on user s past transactional activity cs 229 autumn 2018 introduction dataset and analysis paysim a kaggle dataset for fraud detection 6 million mobile payment transactions 6 different categories of transactions 8312 fraudulent transactions numerical and categorical features pca on two categories transfer and cash out models class weight based approach results discussion future work precision recall curve and statistics test traintrain test transfer transactions recall precision f1 score auprc logistic regression 0 9958 0 4452 0 6153 0 9204 linear svm 0 9958 0 4431 0 6133 0 9121 svm rbf kernel 0 9958 0 6035 0 7515 0 9895 cash out transactions recall precision f1 score auprc logistic regression 0 9847 0 1541 0 2664 0 7564 linear svm 0 9361 0 1245 0 2199 0 7063 svm rbf kernel 0 9875 0 1355 0 2383 0 7631 transfer transactions recall precision f1 score auprc logistic regression 0 9951 0 4444 0 6144 0 9063 linear svm 0 9951 0 4516 0 6213 0 8949 svm rbf kernel 0 9886 0 5823 0 7329 0 9873 cash out transactions recall precision f1 score auprc logistic regression 0 9886 0 1521 0 2636 0 7403 linear svm 0 9411 0 1246 0 2201 0 6893 svm rbf kernel 0 9789 0 1321 0 2327 0 7271 precision recall trend for cash out for lr during training tuned class weights by measuring recall precision f1 score on validation set weights tuning references 1 a survey of credit card fraud detection sorournejad zojah atani et al 2 support vector machines and malware detection t singh m stamp et al 3 paysim a synthetic financial dataset for fraud detection https www kaggle com ntnu testimon paysim1	details of the splits are mentioned in tables ii and iii this approach seemed to give good recall but also resulted in very high number of false positives 1 percent especially for cash out we employed class weight based approach as described in previous section to train our each of our models github code link https github com aadityaoza cs 229 project viii we assign different weights to samples belonging fraud and non fraud classes for each of the three techniques respectively we have chosen class weights such that we do not have more than around 1 percent of false positives on cv set all software was developed using scikit learn 7 ml library in particular i would like to thank my project mentor fantine for her valuable insights and guidance during the course of this project hence we did not use this approach and instead tuned our a false positive in our case is when a non fraud transaction is misclassified as a fraudulent one in the results section we ll see that this is indeed the case svm with rbf gives a higher recall but with lower precision on average for this set of transactions each model was trained multiple times using increasing weights for fraud class samples we also discuss their performance on train and test sets in our experiments we used increasing weights for fraud samples several ml and non ml based approaches have been applied to the problem of payments fraud detection in this section we describe our dataset split strategy and training validation and testing processes that we have implemented in this work all of these we believe can be very effective in improving our classification quality on this dataset the paper paysim dataset can also be interpreted as time series 
stanford poster templates 36x24 1 poster title poster subtitle first1 last1 1 first2 last2 1 first3 last31 2 1example lab department name stanford university 2example lab department name2 other university motivation explore co clustering on job applications qingyun wan qywan stanford edu data in online job serving platform like linkedin indeed and etc job recommendations are usually generated based on matching users and job posting features which are complicated as feature space can be very large if users forget to update their profiles online we even need to infer features based on other information in this case it is desirable to detect innate groupings of users and jobs based on more direct and truthful information job applications which can improve recommendation quality ultimately the goal of this project is to explore the effectiveness of co clustering users and jobs and compare with one way clustering on users based on job applications kaggle job recommendation challenge 1 6m unique job applications 360k unique jobs 320k unique job applicants preprocess constructed 0 1 user job matrices based on job application with 2 different densities split job applications for each density into 5 partitions for 5 fold cross validation discussion future compared to one way clustering co clustering is more stable with higher f1 score and more accurate with much less false positive from the visual comparison co clustering has more balanced cluster size than k means which is more ideal for job recommendation by supplying more focused pools of jobs to recommend unlike co clustering k means matches users more strictly limiting potential jobs that might be suitable for users both co clustering methods are slow because they both leverage matrix decomposition it s beneficial to explore more scalable co clustering methods so that we can co cluster efficiently on more sparse dataset 1 ding c li t peng w et al orthogonal nonnegative matrix t factorizations for clustering c proceedings of the 12th acm sigkdd international conference on knowledge discovery and data mining acm 2006 126 135 2 dhillon i s co clustering documents and words using bipartite spectral graph partitioning c proceedings of the seventh acm sigkdd international conference on knowledge discovery and data mining acm 2001 269 274 3 ungar l h foster d p clustering methods for collaborative filtering c aaai workshop on recommendation systems 1998 1 114 129 4 long b zhang z m yu p s co clustering by block value decomposition c proceedings of the eleventh acm sigkdd international conference on knowledge discovery in data mining acm 2005 635 640 methodology experiments since the labels for job seekers are unknown but only job applications to validate the effectiveness of different clustering methods i used the job applications in the testing set and computed recall accuracy and f1 score against trained clusters to find the optimal number of clusters use the silhouette method to find the number that yields to the maximum silhouette score baseline k means nonnegative matrix tri factorization nmtf spectral co clustering result density 1 lower density density 2 higher density recall accuracy f1 score recall accuracy f1 score nmtf 0 984 0 975 0 131 0 989 0 968 0 237 spectral co clustering 0 985 0 985 0 141 0 948 0 969 0 213 baseline k means 0 971 0 570 0 009 0 754 0 525 0 016 minimum of job applications per job density of rows users of column s jobs of non zero entries applicatio ns 75 0 32 29957 823 79666 100 0 73 16449 271 32873 given the user job matrix x nmtf does nonnegative 3 factor decomposition of it and the objective function derived from bi orthogonal nonnegative matrix factorization is where f is the cluster indicator matrix of clustering rows and g is the indicator matrix of clustering columns it corresponds to simultaneously clustering rows and columns of x given the 0 1 user job matrix convert it to a undirected bipartite graph which has two sets of vertices representing users and jobs respectively an edge exists if a user has an application of a job lower density higher density apply one way clustering on users using k means each training data point is a user represented in one hot encoded vector based on the applied jobs co clustering cluster size analysis with 5 fold cross validation example of k means example of spectral co clustering example of nmtf effectiveness comparision a cut between two clusters is defined by the total number of edges between them the objective is partitioning the graph to achieve the minimum of a normalized version of all cuts which corresponds to the best co clustering presentation video link https www youtube com watch v 3zpd qipanw	let s f t xg update rules then the cluster membership of rows and columns are extracted from f and g for the second one which is smaller and more dense the optimal number is 50 this method is from it is proved in the data set is from careerbuilder s competition https www kaggle com c job recommendatio n data the sizes of training and testing set for each density is in so i also filtered out jobs whose job has less than certain number of job applications to reduce the sparsity the source code is in https github com qingyunwan cs229 project 3 create two data sets of job applications from the original data set by filtering on different number 75 and 100 of job applications per job all is done by myself some of them are representative while the others more or less extend their ideas as they use different methodologies but acheive the the goal run k means of rows to obtain row cluster centroids as f it contains job applications lasting for 13 weeks match the jobs by running 5 fold cross validation and computing silhouette scores on different number of clusters for two data sets with different densities the result is by observing the coordinates where the silhouette scores start to drop for the first data set which is larger and more sparse the optimal number of cluster is 80 
predict optimized treatment for depression minakshi mukherjee adaboost stanford edu suvasis mukherjee suvasism stanford edu cs229 fall 2018 objective for the past 60 years the anxiety and depression medications are prescribed to patients based on the hamilton depression rating scale hdrs and social and functioning assessment scale sofas these scores are very subjective as they are determined by clinicians based on patient interview without incorporating scientific evidence based on brain fmri data objec tive of this project is to identify whether hdrs score and sofas scores are representative of the three antidepressants prescribed based on fmri data of 5 brain attributes data set and features dataset consists of 128 patients fmri data obtained from williams pan lab precision psychiatry and translational neuroscience stanford medicine ispot d project there are 11 features age gender education 3 antidepres sants sertraline venlafaxine escitalopram 5 fmri brain scan data from brain region amygdala insula and nucleus accumbens along with hdrs and sofas scores for all the patients our project analyzes both supervised and unsupervised methods all methods are carried out independently both for hdrs and sofas scores in supervised model hdrs sofas score is a dependent variable and models are fit using different combinations of brain data as feature variables in unsupervised model we studied 8 features 3 an tidepressants and 5 brain scan data to understand the association between medicine and brain attributes models we selected 20 patients randomly out of 128 as test set and use k fold cross validation k 10 on 108 patient data to train and validate our models the following 6 supervised and unsupervised models are considered for the project logistic regression linear regression bayesian linear regression with laplace prior factor analysis k means clustering svm auc for logistic regression based on average test set misclassification error in logistic regression to predict the sofas logistic outcome sensitivity and specificity of the roc receiver operating characteristic curve and auc area under the curve are used to understand the model performance based on moderately high value for auc we conclude that there is statistical significance between sofas logistic score and brain scan data algorithm supervised algorithm bayesian linear regression with laplace prior elastic net we choose a laplace prior for the parameter as laplace distribution is sym metric around zero and it is more strongly peaked as grows the map estimator is the sparse lasso solution this is useful to pinpoint the exact brain attribute to hdrs sofas score which will establish the func tional connectivity between antidepressants and the specific brain region because some of the s goes to zero laplace prior p 2 exp dataset s x i y i mi 1 y i tx i i epsilon i n 2 we search for a choice of that minimizes the objective function j 1 2 m i 1 tx i y i 2 the output of bayesian linear regression on a new test point x is the posterior predictive distribution p y x s p y x p s d parameter posterior p s p i p y i x i p i p y i x i d we compared elastic net with ridge regression and lasso and discussed it in the results section unsupervised algorithm factor analysis factor analysis works on small dataset where it helps to capture the corre lations in the data p x i z i p x i z i p z i z n 0 i n 0 and z are independent x z x i has the covariance noise z is the k dimensional affine subspace ofrn given the guesses for z that the e step finds m step estimates the unknown linearity and captures the covariance x i z i for the posterior distribution p x i z i we declare the convergence when the increase in likelihood l in successive iterations is smaller than the tolerance parameter we choose the maximum of l out of all obtained by k fold cv references 1 estimation of gaussian mixtures in small sample studies using l1 penaliza tion https arxiv org pdf 0901 4752 pdf 2 fmri preprocessing classification and pattern recognition https arxiv org abs 1804 10167 plots elastic net lasso ridge regression and gmm for brain data results factor analysis in factor analysis we transform the current set of variables into an equal number of variables such that each new variable is a combination of the current ones through some transformation here data gets transformed in the direction of each eigenvector and represent all the new variables or factors using the eigenvalues an eigenvalue more than 1 means that the new factor explains more variance than the original variable output of our factor loadings shows that all 11 feature variables 3 antidepres sants age gender education 5 brain scan attributes adequately represent the factor categories for this medical data set plots conclusion based on the rmse values and the plots above for supervised learning ridge regression performs the best hence hdrs and sofas scores statistically connect antidepressants to brain scan data factor analysis output also conforms to the same result that all 11 feature variables are important to represent the interdependent relationship among the feature variables to get more insight we fit gaussian mixture model using hdrs score and amygdala clus 1 2 brain data as well as hdrs score and nac clus 1 2 brain data however based on the above plot the representation seems unintelligible and requires further analysis future work we would like to enhance our gaussian mixture model with regtession and sparsity as follows instead of estimating the k for k 1 k we would estimate only the coefficients of a sparse linear combinations of the xis for all the data belonging to the same cluster using a sparsity enforcing penalty like l1 norm of the coefficients the main difficulty with such an approach might be to choose the right sample vector representing each cluster a priori we would like to use lasso as one of the potential approach to solve that problem 1 link to video presentation https vimeo com 305777481 https vimeo com 305777481	4 we choose the maximum of l out of all obtained by k fold cv auc for radial kernel 0 4840278 we choose a laplace prior for the parameter x z x i has the covariance noise z is the k dimensional af f ine subspace of r n this might seem obvious because it is very likely to expect non linearity among hdrs sofas all social attributes like age gender education and 5 brain attributes in higher dimensions an eigenvalue more than 1 means that the new factor explains more variance than the original variable healthcare professional prescribes antidepressant medications to patients based on two scores hdrs 1 given the guesses for z that the e step finds m step estimates the unknown linearity relating the x s and z s 2 in the final m step update for it captures the covariance x i z i for the posterior distribution p x i z i radial kernel worked the best among all three kernels we use kfold cross validation with k 10 1 the paper regression shrinkage and selection via the lasso by robert tibshirani assumption 1 2 is known 2 all s are independent with laplace density 3 with this prior the map estimator is the same as the lasso solution this sparse solution is useful because we have five feature variables for brain structure and we would like to establish the functional connectivity between antidepressants and brain structure so we would like to have some of the s zero 4 we search for a choice of that minimizes the objective function5 the output of bayesian linear regression on a new test point x is the posterior predictive distribution factor analysis works on small dataset where it helps to captures the correlations in the data 
poster cs229 uplift modeling predicting incremental gains akshay kumar rishabh kumar cs229 stanford university 12 11 2018 introduction and motivation uplift modelling predictive response modelling technique which models the incremental e ect of a treatment on a target group traditional response modelling techniques just look at treatment group p purchase treatment p purchase no treatment in this project we model the uplift modelling for certain email campaign for an online retailer i e what additiona purchases an email campaign brings in for the product dataset and features hillstrom email dataset email campaign related data for 64k customers with some purchase in past 12 months overall population divided into three di erent groups of equal size received a mail featuring men s merchandize received a mail featuring women s merchandize received no advertizing mail each record contains total 9 features indicator variables indicating visit conversion and spend feature embedding algorithm used dataset has categorical features like segment history segment channel etc inspired from word embedding in nlp created one hot vector representation for each of these feature learn di erent weights for each enum value tackled the problem from two di erent perspective predictive response modelling also did ablative analysis uplift modelling predictive response modelling experimented with the following configurations logistic regression model fc followed by sigmoid activation 3 layer neural net fc followed by relu followed by fc followed by relu followed by fc followed by sigmoid activation logistic regression with bagging same as first but with bagging decision trees since many feature were based on enum values training config adam optimizer gave better results than gradient descent optimizer loss function cross entropy mini batch gradient descent with batch size of 32 trained the model for 5 epochs also performed ablative analysis to get the most influential feature uplift modelling modelling incremental ad e ectiveness problem one individual training data a user either sees an email campaign or do not see it solution two di erent models when no email campaign was seen when an email campaign was seen probability of purchase di erence of the two models predictions uplift modelling evaluation test data consists of points which either saw an email campaign or didn t see an email campaign problem no definite labels for test data a single test data can not have both seen the email campaign and not seen the email campaign as well solution bucketization group test data with similar features into a single bucket actual average uplift rate based on ground truth of labels for test data in the same bucket compare actual average uplift rate v s predicted uplift rate evaluation metrics qini curve area under uplift curve does not model negative uplift problem results and analysis predictive response modelling we split the whole data into 80 training and 20 test data since we have a class imbalance problem we have to use a metric that is not biased towards the majority class therefore we have chosen to use f score model f score train f score test lr 0 753 0 7313 bblr 0 7689 0 749 3nn 0 801 0 79 decision tree 0 7129 0 6366 roc curve for training and test data ablative analysis results evaluated importance of various features in logistic regression model by mea suring drop in accuracy by dropping individual features did this on 3nn results recency 82 40 history segment 81 29 history 80 28 mens 78 05 womens 84 56 zip code 84 71 newbie 84 62 channel 85 14 most powerful signal men s mechanize purchase in past 12 months uplift modelling results qini curve gain chart for uplift extension of roc curve conclusion we experimented with 4 di erent models for predictive response and neural network gave best f score out of 4 models decision tree overfits the training data and predict poorly on test set during uplift modelling we can clearly see from qini curve uplift increases as we increase the treatment but decrease thereafter implying the possibility of negative e ect on certain groups results of uplift modelling illustrates the possibility of achieving more incre mental e ect by targeting a smaller group acknowledgement we are thankful to prof andrew ng prof ron dror and the entire course sta of cs229	we split the whole data into a 60 20 20 split both of us worked together collaboratively on almost all aspects unfortunately this is not optimum strategy as there are some customers who would do the sale regardless of the campaign and there are some who will be annoyed by the campaign 60 for training 20 for validation and the remaining 20 for testing so there is no clear distinction in the contributions all the feature were either real values or enums we did this by plotting tpr true positive rate against fpr false positive rate in this section we will talk about introduction to uplift modelling please refer to 3 below for getting visual representation of uplift modelling with 2 models both of models would output the probability of e g visit here treatment implies watching the ad and no treatment implies not watching the ad for enums we decided two different approaches directly encode it as an ordinal corresponding to each of the enum because the output variables are not 0 1 indicator variables to get the incremental effect we take the difference of the two probabilities p r visit is driven by ad p r purchase ad p r purchase no ad as such there can be a dip in the graph as well it can not both see and not see the ad to tackle this we will create two different models one for computing probabilities when a user was not exposed to an ad campaign and the other when the user was exposed to the ad campaign the result is a loss of a sale or even a complete loss of the customer we also plotted roc curve for the neural net based models lr bblr and 3nn for visit as shown in 
end mark prediction eric mark martin jonathan zwiebel ericmarkmartin stanford edu jzwiebel stanford edu cs 229 machine learning autumn 2018 data introduction problem definition models and results analysis future work references the goal of our project is to be able to correctly punctuate variable length english sentences with one of three end marks periods question marks or exclamation marks denoted period qmark expoint in this poster we want to punctuate sentences drawn from the distribution of english sentences so we did not reweight our data to have equal proportions of each punctuation mark for our baseline we used proportional guessing a model that would randomly guess an end mark using proportions taken from the training set for our oracle we used human level assessment giving a number of people sentences from our test set and asking them to predict the end mark we evaluated five different classes of models logistic regression naive bayes svm random forests and fully connected neural networks and compared their performance each model was evaluated over matching 90 5 5 train dev test splits and scored using standard classification metrics we drew data from 10 of the top english books available at project gutenberg available for free use we found that all of our best models in class were able to outperform our baseline but none were able to beat our oracle as measured by macro averaged f1 score in order from best to worst model we have random forests logistic regression naive bayes and svms still we found that their differences in macro averaged f1 score were minimal and could easily have been the result of poorly tuned hyperparameters additionally we found that all of the models did a better job with precision on qmarks than on expoints this result is in line with the intuitive understanding that questions can be found by looking for specific questions words ex who will when while exclamatory sentences are often structurally similar and indistinguishable to sentences with periods we wanted to ensure that we could extract usable examples even from complex grammar structures such as dialogue and clauses additionally to maximize the number of qmark and expoint samples we needed to ensure that we could extract standalone sentences within quotations ex how are you said frankenstein our definition for a sentence was a sequence of space separated words beginning with a capital word ending with an end mark and unbroken by any single or double quotation marks additionally we added five special use tokens number comma semicolon proper unknown to handle important cases not counted in our dictionary 1 a christmas carol by charles dickens 6 a modest proposal by jonathan swift 2 pride and prejudice by jane austen 7 moby dick by herman melville 3 frankenstein by mary wollstonecraft shelley 8 dracula by bram stoker 4 a tale of two cities by charles dickens 9 alice s adventures in wonderland by lewis carol 5 heart of darkness by joseph conrad 10 the adventures of sherlock holmes by sir arthur conan doyle practically all of our mobile devices use some form of autocorrect predictive typing or diction to complete our sentences for us yet if you open your phone and type a sentence your device will almost certainly punctuate it if at all with a period whether it s talk to you later come over or you up our project creates and compares a number of models based on techniques learned in cs 229 to predict the end mark of a english sentence proportional guessing baseline an estimated distribution of labels is found using the training set predictions are draw as random samples from this distribution p q e p 1568 140 190 q 174 16 8 e 192 19 29 prec rec f1 supp p 0 81 0 81 0 81 1932 q 0 08 0 08 0 08 198 e 0 13 0 12 0 12 240 macro 0 34 0 34 0 34 2370 logistic regression 2 models a standard multiclass logistic regression was run with 20005 features we tested both binary and bag of words feature vectors and found binary feature vectors to be our strongest logistic regression model p q e p 1874 26 32 q 116 72 10 e 166 13 61 prec rec f1 supp p 0 87 0 97 0 92 1932 q 0 65 0 36 0 47 198 e 0 59 0 25 0 36 240 macro 0 70 0 53 0 58 2370l2 loss 100 iterations trained with stochastic average gradient naive bayes 3 models a standard naive bayes model designed for multiclass applications we tested multinomial with bag of words bernoulli with binary vectors and gaussian with bag of words distributions and found bernoulli to perform the best p q e p 1606 23 303 q 76 47 75 e 73 4 163 prec rec f1 supp p 0 92 0 83 0 87 1932 q 0 64 0 24 0 35 198 e 0 30 0 68 0 42 240 macro 0 62 0 58 0 54 2370bernoulli distribution uniform prior support vector machine 2 models a standard svm using a radial basis function rbf kernel we tried both a support vector classifier svc and a stochastic gradient descent classifier sgd we also attempted a tfidf vectorizer for both types of svms we found sgd with a bag of words to work best p q e p 1903 17 12 q 128 69 1 e 182 16 42 prec rec f1 supp p 0 86 0 98 0 92 1932 q 0 68 0 35 0 46 198 e 0 76 0 17 0 28 240 macro 0 77 0 50 0 55 2370random forests 4 models best a standard random forest model aggregating 100 decision trees trained on bag of words feature vectors each tree was allowed to train until each leaf was pure we also attempted stopping the constituent trees at depths 5 10 and 50 but the model model highly over classified periods as shown by the cross validation metrics bagging prevented the model from overfitting even without a limit on tree depth p q e p 1909 11 22 q 133 56 9 e 170 11 62 prec rec f1 supp p 0 87 0 98 0 92 1932 q 0 79 0 34 0 47 198 e 0 68 0 26 0 37 240 macro 0 78 0 53 0 59 2370 hinge loss linear kernel neural network using the remaining time for this project we plan on developing a fully connected deep neural network with an extended feature vector that contains a one hot for the first word one hot for the last word a binary vector for words that start clauses and a binary vector for the length of the sentence we were quite impressed with our models and would like to experiment with deep learning techniques particularly sequence based models some additional models we would like to try fully connected neural network with expanded feature vector rnn with one hot vector rnn with word vector from existing embeddings we would also like to try a model that uses surrounding sentences as context to assist in end mark prediction for this we would need a bidirectional rnn gini loss 100 estimators trained to unlimited depth human level oracle over a reduced test set we asked humans to classify sentences using the same tokenization scheme as given to the learned models p q e p 151 1 10 q 3 7 1 e 12 2 5 prec rec f1 supp p 0 91 0 93 0 92 162 q 0 70 0 64 0 67 11 e 0 31 0 26 0 29 19 macro 0 64 0 61 0 62 192 baron d shriberg e stolcke a 2002 automatic punctuation and disfluency detection in multi party meetings using prosodic and lexical cues in seventh international conference on spoken language processing christensen h gotoh y renals s 2001 punctuation annotation using statistical prosody models in isca tutorial and research workshop itrw on prosody in speech recognition and understanding maas a l daly r e pham p t huang d ng a y potts c 2011 june learning word vectors for sentiment analysis in proceedings of the 49th annual meeting of the association for computational linguistics human language technologies volume 1 pp 142 150 association for computational linguistics makhoul j baron a bulyko i nguyen l ramshaw l stallard d xiang b 2005 the effects of speech recognition and punctuation on information extraction performance in ninth european conference on speech communication and technology ueffing n bisani m vozila p 2013 improved models for automatic punctuation prediction for spoken and written text in interspeech pp 3097 3101 i came i saw i conquered do you have number pickles proper stop now	for instance we don t think of what as some subject that is existing is as some object that we increased the max depth to 5 10 and then 50 to try to get better classifications but even with a depth of 50 the forest still only predicted periods to set a lower bound macro f1 we used proportional guessing without the knowledge that this is a question however it can be much more difficult for a computer to determine this information the macro average is helpful as the fact that it evenly weights each class regardless of number of examples exacts a significant penalty on models that over predict the period class eric worked on the na ve bayes random forest na ve bayes and oracle models we found that this last model performed best in class for svms the question of end marks also poses a simple problem given a sentence classify it as ending with either a period question mark or exclamation point we would also like to acknowledge project gutenberg for providing us with a completely free to use corpus of text and google for providing the free to use trillion word corpus two identical text segments may be punctuated differently when written by different authors almost all of the paper ran into the issue that text corpuses extracted from novels and online sources are written by human authors who often deviate from standard punctuation rules each model was evaluated over matching 90 5 5 train dev test splits and scored using standard classification metrics because this is a relatively unexplored problem there isn t a well established baseline for a na ve algorithm in the question what is that he also worked on the vectorizer and tokenizer or you up 
generating regulatory sequence to produce target expression computer science nic fishman 1 georgi marinov 1 2 anshul kundaje1 2 1department of computer science stanford university 2department of genetics stanford university abstract predicting prediction resultsmodels the abundance of high quality gene expression data afforded by the recent development of massively parallel reporter assays mpra has created an abundance of data for developing a deeper understanding of transcription factor tf binding here we show that convolutional neural networks are capable of learning the motifs that underlie tf binding and predicting expression using these motifs at various amino acid concentrations aa using this result we develop a generative adversarial network that can build segments of regulatory sequence to produce specified gene expression at varying aa we find that a combination of the mse between the predicted expression and the target expression and the standard wgan gp loss give the best results for learning to produce sequence given target expression levels there is still more work to be done especially in tuning the combination of the losses and in trying to resolve the diminishing return on longer training that is found for all values of lambda when generating sequence generation results predicting expression targeted sequence generation predicting protein expression from regulatory sequences given a target expression level generate sequence predicted to produce that expression level data and data processing the data comes from mpra experiments 2 which produce the dna sequence acgt and corresponding protein expression each row is a sequence and the corresponding mean expression under several conditions a t c g t 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 the only feature work is to one hot encode the alphabetic sequences 1 predicting expression targeted sequence generation evaluating generated sequences random architecture search train several regressors select best based on integrated gradients ratio of motif importance over total importance wgan gp loss 3 regressor loss overall loss loo accuracy 1 nn in learned feature space 4 predicting expression via ensemble of regressors motif identification and frequency analysis results overview predicting expression genetic background figure from 1 mpra regulatory sequence instructions for how much protein to make encoded in discrete strings of basepairs called motifs complexity comes from motif interactions which depend on number and position of motifs mpra allows testing expression of thousands of regulatory sequences at once combinatorially combining motifs in regulatory sequence and get associated expression allows decoding of the lexical grammar governing expression 2 predicting expression targeted sequence generation trained for 1000 epochs with early stopping trained for 5000 epochs motif importance scoring discussion the regression model is definitely successfully learning to identify motifs which is a great sign the generated sequence from the wgan is relatively similar to the real distribution and backpropagating on the predicted expression leads to much better accuracy on achieving target loss this fully validates the pipeline in training the gan there is an issue where training stops leading to improvements in the 1 nn and motif count metrics after the first few thousand epochs it would be nice to try to understand why this happens there is a tradeoff in gan training between hitting the given target expression and producing realistic sequence it would be good to try annealing the lambda term to see if this tradeoff can be resolved citations 1 m rajiv et al deciphering regulatory dna sequences and noncoding genetic variants using neural network models of massively parallel reporter assays biorxiv preprint biorxiv 393926 2018 2 d van dijk et al large scale mapping of gene regulatory logic reveals context dependent repression by transcriptional activators genome research 3 gulrajani ishaan et al improved training of wasserstein gans advances in neural information processing systems 2017 4 borji ali pros and cons of gan evaluation measures arxiv preprint arxiv 1802 03446 2018 replicated from 2 https files slack com files pri t7sav7lad fawjj54te 3 m2 png	these losses are weighed with a tunable term as follows 3 5 the relevant motif indeed tend to be the most highly scoring sequences with two examples shown in evaluating generated sequences optimal real value generated 1 nn loo 0 5 0 89 predicted expression mse 0 0 64324 5176 ation based on the promise such approaches have shown in the language generation literature throughout training we used the default hyperparameters for the adam optimizer the default hyperparameters for the wgan gp algorithm we used two mpra datasets in this study formally this loss is calculated as 1 where we update the discriminator by maximizing with respect to l d throughout this paper we examine this pipeline in the con we also log transformed the regression targets so we could use the relu non linearity as the output of our regression networks 
