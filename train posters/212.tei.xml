<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/lopez/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5-SNAPSHOT" ident="GROBID" when="2019-03-20T09:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Latent variables are strongly correlated with the most physically important parameters (thicknesses of high- refractive index layers), indicating that the network automatically learns the important physical parameters of the model. Generated device examples Modeling and Optimization of Optical Devices using a Variational Autoencoder</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Roberts</surname></persName>
							<email>johnr3@stanford.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Wang</surname></persName>
							<email>wangevan@stanford.edu</email>
						</author>
						<title level="a" type="main">Latent variables are strongly correlated with the most physically important parameters (thicknesses of high- refractive index layers), indicating that the network automatically learns the important physical parameters of the model. Generated device examples Modeling and Optimization of Optical Devices using a Variational Autoencoder</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Our data consists of the parameters (layer thicknesses h n ) of a thin film optical device and its discretized transmission spectrum (T j ). 100,000 devices are randomly generated and the transmission spectra are found using transfer matrix simulations. We generate an additional 1,000 for the testing set.</p><p>The features consist of the five layer thicknesses and the 101-point discretized transmission spectrum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VAE Network Architecture Discussion</head><p>References</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Because the VAE is robust to noise, we attempt to find optimized devices by reconstructing the target spectrum with a random device. . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Device</head><formula xml:id="formula_0">Hidden Low-dimensional latent space T' 1 T' n h' 1 h' 5 .</formula><p>. .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reconstructed device</head><p>Reconstructed spectrum</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>Optical thin film systems are structures composed of stacked layers of different materials. They find applications in areas such as:</p><p>•</p><note type="other">Solar cell design • Ellipsometry and metrology • Radiative cooling • Dielectric mirrors</note><p>The main property of interest is the transmission spectrum, which has a complicated dependence on the parameters of the thin film stack. This makes thin films a good model system for the investigation of machine learning techniques in optical device design <ref type="bibr" target="#b0">[1]</ref>.</p><p>We use a variational autoencoder (VAE), which encodes a representation of data in a latent space using neural networks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, to study thin film optical devices. VAEs can learn physics of thin film devices, generate new devices, and show potential for designing devices with arbitrary spectral responses.</p><p>Our network is based on a PyTorch example by D. Kingma and C. Botha <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>04 .80 .10 -.24 .16 z 3 -.84 .02 -.06 -.11 -.49 Latent z correlation with decoded thickness Output device Output T Actual T TM simulation Compute accuracy (MSE) Device generation Randomly sample latent space Decoder z 1 z 2 z 3 Decoded h 2 in latent space</figDesc><table>Encoding: 80 hidden neurons + ReLU 
Latent Space: 3 dimensions 
Decoding: 80 hidden neurons + sigmoid 
Loss: Reconstructed MSE + KL divergence 

Model 

Recon. 
MSE 

Accuracy 
MSE 

PCA 
4.89 
17.87 

VAE 
0.164 
1.17 

Partial to full reconstruction is 
possible after compression to the 3-
dimensional latent space. New 
devices and their predicted spectra 
can be generated by randomly 
sampling the latent space. 

Future work: Continue to tune model parameters for 
improved accuracy, extend VAE model to more 
complicated optical devices 

VAE 

Random 

Random 

Random 

Latent space 
representation of test set 

. 
. 
. 

. 
. 
. 

Example reconstructed spectra 

Latent ~(0, ) 

h 1 
h 2 
h 3 
h 4 
h 5 

z 1 
-.23 .33 .34 .82 -.22 

z 2 
-.Output device 

Output T 

Actual T 

TM simulation 

Compute accuracy (MSE) 

Device generation 

Randomly 
sample latent 
space 

Decoder 

z 1 
z 2 

z 3 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Training Deep Neural Networks for the Inverse Design of Nanophotonic Structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS Photonics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1365" to="1369" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<idno>arXiv: 6 6 9 8v</idno>
		<title level="m">Tutorial on Variational Autoencoders</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>stat ML</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gómez-Bombarelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS Central Science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="268" to="276" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Basic VAE Example</title>
		<ptr target="https://githubcom/pytorch/examples/tree/master/vae" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Variational Autoencoder in PyTorch, commented and annotated</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Botha</surname></persName>
		</author>
		<ptr target="https://vxlabs.com/2017/12/08/variational-autoencoder-in-pytorch-commented-and-annotated/" />
		<imprint>
			<date type="published" when="2018-11-20" />
		</imprint>
	</monogr>
	<note>Accessed</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
