<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/lopez/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5-SNAPSHOT" ident="GROBID" when="2019-03-19T09:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Assigning Style Grade and Providing Style Feedback by k-means Clustering and Softmax Regression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Homero</roleName><forename type="first">Roman</forename><surname>Roman</surname></persName>
							<email>homero@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Assigning Style Grade and Providing Style Feedback by k-means Clustering and Softmax Regression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Abstract</head><p>While identifying functionality mistakes in a program can be done with near certainty for a finite number of final requirements, identifying style mistakes is subject to subtle conventions and an infinite number of possible programs. This poses a problem for students trying to get feedback on their program style while they are still working on the assignment. Also it makes it harder for teacher assistants (TA's) to agree on a certain style grade. Therefore, the goal of this research is to figure out how to automatically assign a style grade to a program and provide style feedback. More specifically, the procedures employed are, first, k-means clustering of the data according to one of three different strategies, then, from each cluster fitting a logistic regression or naive Bayes model for classifying functions into those well decomposed and another for well formatted functions, and, finally, fitting a softmax multi-class classification model to assign a program style grade.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Introduction</head><p>Developing good programming style is a vital part of the CS106A class and to enforce this practice all assignments are graded not only on functionality but also on style. But what exactly constitutes "good" style is not very clear. As such, students who are just learning about programming struggle to develop this "good" style. Unlike functionality which they can test themselves by running their program on their computer, style is not given feedback until the program has been graded by the TA by which time it is too late to fix. During the developing stage of the program writing, the student's program is not necessarily functionally complete but we still wish to provide feedback on the style and format of the parts already written so that the student may back trace if necessary and rethink his current implementation. By providing both style and functionality feedback at every stage, the aim is to have the student improve both at the same time rather than pushing style as an afterthought for the end and as a result make bugs harder to debug.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hypothesis</head><p>Since we want to classify into more than one grade bucket a multi-class classification model seems appropriate for the situation. Intuitively, the three most important factors when considering whether a program has good style are whether it is well decomposed, well formatted and whether it works. But since a single program may have several possible correct solutions it may be the case that different combination of these characteristics produce different style grades for each solution. Therefore, it is of interest to explore whether running Softmax only on those programs that have an implementation similar to the test program has more accuracy than simply fitting Softmax to all the training data. And since we also want to provide feedback for non-working programs we explore the possibility of determining the features of the Softmax model by running logistic regression or naive Bayes on the functions alone to decide whether each function is well decomposed and well formatted and finally take an average for an entire program to determine whether the program is well decomposed and well formatted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pre-processing</head><p>First, 125 programs were collected from the Karel midpoint finding assignment from the CS106A class. 70 of them working programs. 30 non-working and 5 from each grading bucket category (plus, check plus, check, check minus, minus). Initially, the plan was to obtain hundreds of programs from professor Chris Piech but due to student privacy issues, he was only able to provide a couple. As such, most of the programs were personally written making sure to include all major possible solutions and collected from friends. Then, after obtaining the programs, the training programs were pre-processed to obtain feature vectors describing each program and each function in each program. For the training data, the function vectors also contained a label on whether it is well decomposed and well formatted and the program vectors contained the actual grade of a program. As for the testing data, no program vectors were given.</p><p>Then, for clustering, 3 different strategies were explored.  Then for the third strategy, I tried something more clever. Namely, to run kmeans twice. Once to find the the average of the coordinates within a single program and the second time to cluster these averages into k clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Clustering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Clustering Results</head><p>Clustering by counts of primitives is very susceptible to unnecessary command calls. Also it does not distinguish between call times. Clustering by pouring all coordinates may be susceptible to outlying programs that tend to spend too much time at a certain place. Double k-means overall seems to address the issues above and can even be implemented for variable times. However, it breaks down with infinite loops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Logistic Regression or Naive Bayes for function classification</head><p>With the training programs clustered, we proceed to fit a logistic regression or a Naive Bayes model for all the function is each program cluster. For this </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Feedback</head><p>After fitting the cluster functions from our training data we can use them to provide feedback to a student on his functions as he is still working on the program. All we need to do is take his program and for each function decide whether it is decomposed or not and point to whether we found it to have appropriate length and non-repeating code as for format we can report whether we believe each function to be well formatted and point to whether it has correct indentation, no blank lines, and comments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Softmax regression</head><p>Once we have decided whether each function is well decomposed and well formatted we take an average over all functions in the test program and test whether the program is functionally correct to come up with the program description vector [Program decomposed; program well formatted; Program Works]. Then we fit a Softmax regression model for each program cluster in the training data where y ( i) ∈ {1, 2, ..., k}. More specifically, we do gradient descent on the following cost function <ref type="bibr">[1]</ref> including a weight decay term to ensure the cost function J(θ) is convex for any λ &gt; 0</p><formula xml:id="formula_0">J(θ) = − 1 m [ m i=1 k j=1 1{y (i) = j}log e θ T j x (i) k l=1 e θ T l x (i) ]+ λ 2 k i=1 n j=0 θ 2 ij</formula><p>and taking the gradient gives:</p><formula xml:id="formula_1">∇ θj J(θ) = − 1 m m i=1 [x (i) (1{y (i) = j}− e θ T j x (i) k l=1 e θ T l x (i) )]+λθ j</formula><p>then plugging into the stochastic gradient descent rule we get:</p><formula xml:id="formula_2">θ j := θ j − α∇ θj J(θ)</formula><p>for each value j = 1, ..., k where we set α := 0.001 λ := 1 m m := Number of Train Programs and k := 5 Finally we use this model to assign a bucket style grade to a test program.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Softmax Results</head><p>The following MINUS 86% 75% 80% 69% As we can see from the table, clustering before hand seems to perform better. Also running logistic regression on each function seems to have higher accuracy than logistic regression. This can be easily explained if we consider that the characteristics in the function vector are not necessarily independent. However, while our softmax model performed at 87% with logistic and clustering for the plus bucket, it performed poorly for the check plus and check minus buckets. On retrospect, we can explain this polarizing behavior by realizing that when we ran logistic regression on each function we only identified between well decomposed/formatted or not at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Future Work</head><p>A future possible fix would be to run Softmax on each function and take that average. However, that approach would be very susceptible to mal-formed functions and would likely require to increase the size of the feature function vector. Also we worked strictly within a Karel-world setting and as such it would be interesting to see whether these results also hold for purely java programs. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 := 3 Figure 2 :</head><label>132</label><figDesc>strategy, I parsed the 100 train set programs into the basic command primitives Karel could understand ( move(), putBeeper(), pickBeeper(), turnLeft(), turnRight(), turnAround() ) and counted how many of each were in the program. Then I ran k-means to cluster them in 6-dimensional space. Below is a 2D cross section of the two dominant fea- tures move() and turnLeft():2 Figure 1: move() vs turnLeft() 2D cross section for k = 3 Figure 2: Coordinate clustering for k = 34.2.2 Strategy 2: Cluster by pour- ing all coordinatesFor the second strategy, I kept track of the coordinates Karel was in dur- ing each time step in a program. This produced a set of [time, x-coordinate, y-coordinate] vectors for each program and a set of vector sets for all 100 train programs. Then I proceeded to run k- means by pouring all coordinates to- gether.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3</head><label>3</label><figDesc>Figure 3: k-means clustering for k = 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Logistic function regression we use collected properties of each func- tion. For decomposition we use the fol- lowing feature vector: [Is the function between 2-10 lines?; does it have ap- propriate line length?;Is there no re- peating code?;] And for function for- mat we use the following: [Does it have correct indentation?; Is there no blank lines?; Is it commented?]. Shown above is an example of logistic regression for the functions in the train set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>table presents the accu- racies obtained for each test set in each bucket category (consisting of 5 pro- grams each as noted earlier.) 4 Figure 5: Softmax regression with lo- gistic and clustering for training data.</figDesc><table>BUCKET Softmax 
with 
logistic 
and 
clustering 

Softmax 
with 
logistic 
without 
clustering 

Softmax 
with 
Naive 
Bayes 
and 
clustering 

Softmax 
with 
Naive 
Bayes 
without 
clustering 
PLUS 
87% 
75% 
81% 
70% 
CHECK 
PLUS 

66% 
61% 
60% 
55% 

CHECK 
70% 
65% 
64% 
60% 
CHECK 
MINUS 

64% 
60% 
59% 
55% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Softmax Regression" http : //uf ldl.stanf ord.edu/wiki/index.php/ Sof tmax R egression</figDesc><table>7 References 

References 

[1] Unsupervised 
Feature 
Learn-
ing 
and 
Deep 
Learning. 
"</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
