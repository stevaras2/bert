<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/lopez/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5-SNAPSHOT" ident="GROBID" when="2019-03-19T09:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Playing Chinese Checkers with Reinforcement Learning CS 229 Spring 2016 Project Final Report</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijun</forename><surname>He</surname></persName>
							<email>[sijunhe@stanford.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Hu</surname></persName>
							<email>huwenjie@stanford.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yin</surname></persName>
						</author>
						<title level="a" type="main">Playing Chinese Checkers with Reinforcement Learning CS 229 Spring 2016 Project Final Report</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-We built an AI for Chinese checkers using reinforcement learning. The value of each board state is determined via minimaxation of a tree of depth k, while the value of each leaf is approximated by weights and features extracted from the board. Weights are tuned via function approximation. The performance of our modified minimax strategy with tuned weights stands out among all the other strategies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Chinese checkers is a game played on a hexagram-shaped board that can be played by two to six players individually or as a team. The objective is to be the first to move all ten pieces across the board into the opposite starting corners. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the allowed moves include rolling and hopping. Rolling means simply moving one step in any direction to an adjacent empty space. Hopping stands for jumping over an adjacent piece into a vacant space. Multiple continuous hops are allowed in one move. A more detailed introduction of the Chinese checkers can be seen in Wikipedia.</p><p>The reinforcement learning is an area of machine learning typically formulated as Markov decision process (MDP). The model consists of states, actions, transitions, etc., which is suitable for decision making in board game like Chinese checkers. The objective of our project is to use reinforcement learning to build an AI agent for Chinese checkers, and to explore the effectiveness and efficiency of the AI.</p><p>To simplify the problem, our AI only solves the one vs one mode of Chinese checkers. Dif- ferent from classic reinforcement learning where at each state the player solves a simple maximization problem, in our AI for Chinese checkers, it is a adversarial zero-sum game. Therefore, each player needs to consider not only his/her own strategy, but also the opponent's responding strategy. Therefore, a better way to depict such procedure is minimaxation, which is elaborated in Section 3.</p><p>The biggest challenge in applying reinforcement learning to our AI is how to learn the weights in function approximation. We modified the standard algorithm for function approximation so that it fits our minimax setting, and we adopted a diminishing learning rate to stabilize the update. Our simulation results showed that this learning procedure is effective, in that the difference between the weights before and after an iteration is small. Simulation results also showede the robustness of our update, in that our learning procedure with different initializations will end up with very close weights.</p><p>We tested the performance of different strategies by playing against a random look-ahead greedy player. Simulation results showed that the minimax strategy with tuned weights significantly outperforms the minimax strategy with initial weights. Moreover, we further modified our strategy such that it divides the game into three stages and applies different strategies thereon. Simulation results showed that this modified strategy outperforms the basic minimax strategy.</p><p>The rest of this report is organized as the follows. We first talk about how we implement the board in Section 2. Then we introduce the basic methodology of our AI in Section 3, and point out the difficulties in implementation as well as our solution in Section 4. We cover our modified strategy in Section 5. Simulation results are shown in Section 6. <ref type="figure" target="#fig_1">Figure 2</ref> is the starting board where we worked on in minimax searching, weights tuning, and simulations. Each o stands for a vacant spot, 1 stands for a spot occupied by player 1's piece, and 2 stands for a spot occupied by player 2's piece. Note that this board is smaller than the original board of Chinese checkers. The state-space complexity of Chinese Checker 10 23 is high <ref type="bibr" target="#b0">[1]</ref>, thus building and testing the AI for the full game board is computationally intensive. Thus we adopted a smaller board of 6 pieces for each player during development and simulation. Furthermore, the hexagram-shaped board was modified into a heuristic diamond-shaped board, which is reasonable for one vs one mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BOARD REPRESENTATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>We adopted the classical approach for game playing AI, game search tree, which best mimics the behavior of a human player while demonstrates super-human performance by taking advantage of the computing power. With each node representing a board state of the game, and each edge representing one possible move from one board state to a subsequent board state, the game search tree can emulate the thinking process of a human player. Chinese checkers is a two-player zero-sum game, thus an objective "value" is needed to evaluate the situation on the board. Player 1's goal is to maximize the "value", while Player 2 minimizes it. The logical approach is the minimax tree, which is a decision tree that minimizes the possible loss for a worst case scenario resulted from the opponent's optimal move.</p><p>Due to the large state-space complexity of Chinese checker, it is unrealistic to build a top down game search tree. Instead, a shallow k-depth minimax game tree that searches only the tip of the tree is built. At each node, the "value" is taken as the "minimax score" which is computed by the minimax algorithm of depth k. When the search has reached the bottom of the k-depth search tree, a.k.a. the leaves, the score is approximated by a raw score, which is a linear evaluation function based on the features of the board state. We exploited 6 features that are based on the positions of pieces on the board, which are described as the following:</p><p>• A i : the squared sum of the distances to the destination corner for all pieces of player i; • B i : the squared sum of distances to the vertical central line for all pieces of player i; • C i : the sum of maximum vertical advance for all pieces of player i;</p><p>with i = 1, 2. Note that we used the square of the distances to penalize the trailing pieces, which would motivate each player to make the pieces cohesively and thus promote hopping. Besides, the 6 features are extracted from a larger amount of possible features, so that the overfitting of the model was avoided. The other features we have explored and found unnecessary included:</p><p>• the horizontal variance (how scattered) of pieces of each player; • the vertical variance of pieces of each player;</p><p>• The maximum vertical advance for a single piece of each player; The evaluation function is the form of</p><formula xml:id="formula_0">V = w 1 (A 2 − A 1 ) + w 2 (B 2 − B 1 ) + w 3 (C 1 − C 2 )</formula><p>where the weights w = (w 1 , w 2 , w 3 ) T would be trained via function approximation, which would be described in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CHALLENGES AND SOLUTIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Weights tuning and function approximation</head><p>The performance of the AI is highly dependent on how well the weights in the evaluation function is tuned. The objective of weights tuning is to allow the evaluation function to consistently approximate the minimax value through the depth-k tree search. The challenge is to develop an algorithm to improve and stabilize the weights within certain iterations.</p><p>The following Algorithm 1 based on function approximation is our solution to perform weights tuning. An introduction of function approximation can be seen in <ref type="bibr" target="#b1">[2]</ref>. The basic idea of the algorithm is to conduct value iteration after each game played by both AI players. A new weight is computed by performing least squares on the recorded feature vectors at each turn and their corresponding minimax scores. A diminishing learning rate α is imposed at each iteration, in order to stabilize the update. The iteration is repeated until the weights value are stabilized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Run-time complexity and alpha-beta pruning</head><p>Another challenge is the run-time complexity of the algorithm. Due to the nature of Chinese checkers, players usually have around 20 -100 feasible Play one game with both player following Minimax-rule using weights w, record the feature vectors at each turn in a matrix Φ and the corresponding minimax scores in a vectorṽ; <ref type="bibr">4:</ref> w new ← LeastSquare(Φ,ṽ);</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>w ← w + α(w new − w) 6: until stabilized moves at a typical turn. The worst-case number of board states evaluated in a minimax tree of search depth 4 is on the scale of 10 8 . The large branching factor, combined with any non-trivial search depth, can easily result in impractical run-time for realtime game play.</p><p>We address this problem by adopting alpha-beta pruning. A detailed description of this technique can be seen in <ref type="bibr" target="#b2">[3]</ref>. Furthermore, in order to expedite the alpha-beta pruning, we enqueued all the feasible moves of the player in a priority queue where the priority is in the order of the weight-calculated score of the resulting board if the corresponding move is taken. The effectiveness of the alpha-beta pruning is shown below in <ref type="table">Table I</ref>, where we listed the time used as well as the number of nodes visited for the starting board to compute the minimax value. It shows that the time used as well as the total number of nodes visited is significantly reduced. V. FURTHER DEVELOPMENT</p><p>In the strategy described above, we adopted minimax tree search at each turn of playing. Note that there is a waste of computing power in this strategy at the beginning and end of game. At the beginning of game, the pieces of two players have not interact with each other, thus each player's move does not interfere the other's. Therefore, it is unnecessary to consider the opponent's strategy, thus the minimax strategy can be simplified as a pure maximizing strategy. This is also true at the endgame, when the pieces of two players are split up, thus each player only needs to consider how to end the game as soon as possible, without considering the opponent's strategy.</p><p>In the light of this knowledge, we modified our strategy above in the following way. In the start game, the player only consider his/her feasible moves and chooses the one that gives the maximal weights-calculated value. In the midgame when two players' pieces intersect, we search our optimal move via the minimax procedure. In the endgame, the player would take a move that achieves maximal vertical advance.</p><p>We will test the performance of this strategy and compare it with the basic strategy in Section 6.B(3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Convergence of weight tuning</head><p>With Algorithm 1, we were able to tune and stabilize the weights. The criterion for stabilization is w new − w. As shown in the plot, the difference of weights diminished as more training games were played. While the algorithm didn't necessarily converge, it did improve the effectiveness of the AI dramatically after weights were tuned, which will be shown in Section 6.B <ref type="bibr" target="#b0">(1)</ref>.</p><p>Furthermore, the algorithm is also robust to initializations. Two different unit-length initial weights were attempted,  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Benchmarking</head><p>We measured the performance of our algorithms by simulating 200 games against a benchmark strategy. The benchmark is a greedy random look-ahead algorithm that takes the move that gives the most combined vertical advance in 2 steps. Tiebreaker is preference to trailing pieces and further ties are broken by random selection. The result was measured by winning steps, which is the number of steps needed for the losing player to finish the game.</p><p>1) Effects of weights: <ref type="figure" target="#fig_5">Figure 4</ref> demonstrates the game results of AI players with tuned weights and untuned weights. The untuned weights are the initial weights [0.577, 0.577, 0.577] T . The results show that the AI with tuned weights performs significantly better than one with untuned weights, which is expected.</p><p>2) Effects of search depth: <ref type="figure" target="#fig_6">Figure 5</ref> shows the game results of AI players with search depth 2 and 4. As expected, the AI with search depth 4 outperforms the same strategy with search depth 2. It is worth noting that the worst case runtime for depth 2 strategy is under 5 seconds, while the worst case runtime for depth 4 strategy is over 900 seconds.</p><p>3) Effects of modified strategy: <ref type="figure" target="#fig_7">Figure 6</ref> compares the game results of the basic minimax strategy and the modified strategy. The modified minimax strategy improves the performance tremendously for depth 2, while there's no significant improvement for depth 4. The reason is that the endgame algo-  rithm in the modified strategy is comparable to the minimax algorithm with a search depth 4. Though there's no improvement in terms of winning steps, the modified strategy is orders-of-magnitude faster in terms of time complexity, reducing the runtime for an average of 400 seconds to less than 10 seconds. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Chinese checkers rules (downloaded from website)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Board representation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>[0.577, 0.577, 0.577] T and [0.990, 0.099, 0.099] T , and both of them converged to the similar values after weight tuning. The tuned weights from different search depth are shown below:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Convergence of weight tuning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>Effects of weights</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Effects of search depth</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Effects of modified strategy</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The shortest game of chinese checkers and related problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">I</forename><surname>Bell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Integers</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="39" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="http://web.stanford.edu/class/cs221/lectures/mdp2.pdf" />
		<title level="m">Lecture 8: MDPs II</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="http://web.stanford.edu/class/cs221/lectures/games1.pdf" />
	</analytic>
	<monogr>
		<title level="j">Lecture</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
