introduction future work re evolutionary algorithms devang agrawal devang18 kaushik ram sadagopan kaushik7 fatma tlili ftlili the actor critic architecture references results conclusions evolutionary strategies 1 richard s sutton and andrew g barto reinforcement learning an introduction 1998 2 khadka shauharda and kagan tumer evolutionary reinforcement learning arxiv preprint arxiv 1805 07917 2018 3 houthooft rein et al evolved policy gradients arxiv preprint arxiv 1802 04821 2018 policy gradient architecture which outputs predictions of each action value gradient architecture using a single hidden layer motivation policy gradient methods in reinforcement learning face the issue of lack of exploration evolutionary strategies is a black box optimization algorithm to overcome local optima which suffer from low exploitation of the environment feedback signals objective our project is aimed at developing a hybrid evolutionary reinforcement learning algorithm erl and apply it to a classic control problem to prove its superiority over the standalone algorithms we implement a policy gradient algorithm advantage actor critic a2c and an evolutionary algorithm es for the cartpole problem on openai gym subsequently we combine a2c with es for the cartpole problem to show that it performs better than the standalone algorithms environment a pole is attached to a cart which moves along a frictionless track the system is controlled by moving the cart right or left the pole starts upright and the goal is to prevent it from falling over the objective of this task is to keep the cartpole upright continuously for 200 timesteps which corresponds to a reward of 200 schematic of the actor critic setup the neural network architecture for the policy gradient and the value gradient functions are described the policy gradient outputs a probability distribution for the policy from which actions are sampled and hence the actor the value gradient computes the advantage of taking a particular action given an observed state and hence the critic figure 3 vanilla e a2c we combine es and a2c iteratively in a sequence each iteration of algorithm spawns parameters and makes an update by choosing the best candidate the weights updated by the es is passed on to the policy gradient function of the a2c algorithm which performs a gradient descent update figure 1 cartpole problem on openai gym figure 2 evolutionary strategies es spawns moving the parameters to a global optimum figure 4 evolutionary a2c es spawns a population of parameters and a2c updates each member of the population by performing a series of gradient descent updates finally es chooses the best parameter vector based on the rewards obtained in the population and injects noise onto this parameter vector to generate the new population implement evolutionary dqn for the mountain car environment to show the exploration capability enhanced by es figure 6 plots of rewards obtained in each epoch for a2c es vanilla e a2c and evolutionary a2c algorithms respectively observations the training converges when the average reward reaches 200 consistently a2c reaches this state at around 75 epochs 5 episodes each but it has a lot of variation due to the stochasticity in the selection of actions es has a lot of variation at the beginning but stabilizes after 150 epochs vanilla e a2c reaches this state after 125 epochs the evolutionary a2c is clearly superior to the other three algorithms in the sense that it is the quickest to converge and the variations in the reward are minimal after reaching this state evolutionary actor critic figure 5 in the first plot the candidate parameters of the population are weighted by their corresponding rewards in the second plot only the candidate which corresponds to the maximum reward is chosen the plots shows that taking the best candidate parameters performs better than the weighted combination of the candidates weighted combination of the candidate parameters maximum candidate parameters	we implement a policy gradient algorithm advantage actor critic a2c and an evolutionary algorithm es for the cartpole problem on openai gym  subsequently we combine a2c with es for the cartpole problem to show that it performs better than the standalone algorithms  evolutionary strategies tend to overcome these challenges by probabilistically selecting promising candidate out of a population of candidates thus allowing more exploration  this paper similarly in the work of finally a pole is attached to a cart which moves along a frictionless track  the system is controlled by moving the cart right or left  the pole starts upright and the goal is to prevent it from falling over  the objective of this task is to keep the cartpole upright continuously for 200 timesteps which corresponds to a reward of 200  we implement an advantage actor critic a2c policy gradient algorithm for the cartpole problem  we get a probability distribution for the actions for each state and we choose actions sampled from that distribution  the advantage is calculated by finding the difference between an estimated average future reward and the average current value of the state  these advantages are used to scale our current predictions directly into our policy gradient figure 2 actor critic architecture for the policy gradient we output a policy to take an action given a specific set of states  this process is done using tensorflow and the loss is minimized using adamoptimizer for backpropagation  we spawn instances of the parameters which are jittered by random noise  one episode of cartpole is run with each parameter instantiation and the total reward at the end of the episode is calculated  in the first plot we combine es and a2c iteratively in a sequence  each iteration of algorithm spawns parameters and makes an update by choosing the best candidate we expect the es algorithm to take the longest time to reach a reward of 200 and expectedly so it takes 1160 episodes to reach a reward of 200 averaged over 100 different training sequences  the a2c algorithm takes 266 episodes to reach a reward of 200 averaged over 100 different training sequences  a2c trains for 5 episodes in each epoch we have plotted the rewards for  we extended our algorithm to the mountain car environment the training converges when the average reward reaches 200 consistently  a2c reaches this state at around 75 epochs the code for each of these algorithms is attached herewith https drive google com open id 19ouh2 rywr97rslumfc t labirb ytey5 we could explore combinations of proximal policy optimization with a similar evolutionary strategy to solve tasks on mujoco  literature study of previous work on combining policy gradient algorithms with es fatma implementing a standalone a2c kaushik ram implementing a standalone es devang vanilla combination of a2c and es devang and fatma final combination of a2c and es devang and kaushik ram extending combined algorithm to the mountain car problem devang writing the report kaushik ram and fatma 
stockagent application of rl from lunar lander to stock price prediction stockagent application of rl from lunar lander to stock price prediction caitlin stanton stanton1 stanford edu and beite zhu jupiterz stanford edu cs229 machine learning problem statement reinforcement learning has been used to learn to all sorts of games from chess to arcade games we first trained an agent to play the arcade game lunar lander using both policy gradient descent and deep q learning then using very similar techniques to the ones implemented on lunar lander we train an agent to learn how to invest in stocks data used lunary lander environment provided by openai gym data for stock prediction came from iex s api provided 5 years worth of apple and google stock data we reformatted our stock data to have one entry for each week instead of each day and consolidated the number of features see section on states below for details figure 1 formatted stock data as inputted into our neural network states actions and rewards lunar lander 4 states 8 dimensional vector including information such as the lander s position and orientation in space and amount of fuel used actions do nothing fire right engine fire left engine fire main engine rewards rewards for landing with feet down and penalties for wasting time landing far away from the pad and wasting fuel stock market states volatility standard deviation mean from past 3 weeks weekly change in stock price from past 3 weeks current price of stock current cash level current owned stock value actions hard sell soft sell do nothing soft buy hard buy for our purposes we initially set hard to mean buying selling 100 worth of stock and soft to mean buying selling 10 worth of stock rewards change in portfolio for that week figure 2 the plot of cost against epochs the spike is probably due to the explo ration in the model figure 3 the plot of reward against epochs the plummet is probably due to the exploration in the model figure 4 lunar lander environment model parameters lunar lander fully connected neural network of shape input 8 8 5 exploration rate starts at 0 then increments by 0 001 with max of 0 9 stock market fully connected neural network of shape input 8 8 5 default exploration rate 0 05 train for 1000 epochs where each epoch plays through 200 weeks of stock data input state q value of action 1 q value of action 2 q value of action 3 q value of action 4 q value of action 5 action figure 5 graph of the stockagent using deep q learning results for stock prediction model took about 200 epochs to converge adjusting soft buy sell action size here is a table containing the information of the final portfolio value network goog aapl soft 1 66 61 235 42 soft 5 448 15 342 22 soft 10 50 77 943 07 soft 20 120 14 19 247 soft 50 5716 30 8566 56 table 1 final portfolio value under different soft buy sell value adjusting the exploration rate exploration rate goog aapl 0 249 63 1905 90 0 01 458 92 1713 68 0 05 592 98 476 66 0 1 116 36 264 31 0 2 30 76 129 55 table 2 final portfolio value under different exploration rates loss functions loss q q 2 where we generate the target q by q s a r s a max a q s a for s the state from s via action a and the discount factor in lunar lander we sample some size of memory to do replay to generate q in stockagent the whole training is recorded and served as memory discussion future directions all the parameter choices we tested performed worse than just always choosing hard buy each week with such a policy we have final portfolio value of about 12 000 for apple we are currently unsure of why our model actions skew so heavily in favor of soft buy instead of hard buy except possibly that our model is quite risk averse one thing we would like to try in future training episodes would be to also train and test on stocks that are not very successful i e do not trend upwards in the long run at the moment each of our training simulations last for 200 weeks while our test dataset consists of 30 consecutive weeks in the future we would like to use a longer test set to mimic the more long term gains that our model is being taught to achieve if our goal is to precisely model predict apple stock we might add additional features such when new iphone or macbook models are being released we had a number of spikes in our apple data that we didn t see in our google data and we think that these might correlate with such events acknowledgements we would like to thank our mentor mario srouji for his guidance throughout this project references 1 gabriel garza deep reinforcement learning policy gradients lunar lander medium 2 wolski filip dhariwal prafulla radford alec klimov oleg schulman john proximal policy optimization algorithms corr 2017	reinforcement learning has been at the core of many of the most exciting recent developments in ai  for example while computers have been relatively successful at playing chess for many years notably the computer deep blue was able to defeat the reigning world chess champion garry kasparov in 1996 the game of go was considered much harder it wasn t until reinforcement learning techniques were used in 2015 that the program alphago was finally able to beat a professional human go player here we use deep q learning to train an agent to learn the arcade game lunar lander a game where the goal is to steer a landing module and successfully land it on the surface of the moon  reinforcement learning techniques have already been applied to the lunar lander game  there is even a competition this work was not supported by any organization 1 department of mathematics stanford university sunetid stanton1 2 department of mathematics stanford university sunetid jupiterz released on kaggle starting september 25 2018 and ending january 8 2019 by the company two sigma related to stock prediction this seems to be a common feature for current applications of machine learning to the stock market most models wish to predict stock prices and not directly tell us an investment strategy  for instance in the lunar lander environment was provided by openai gym https gym openai com there are four actions allowed at any given point in the game firing the main engine firing the right engine firing the left engine or doing nothing  there are rewards for landing with feet down and penalties for wasting time landing far away from the pad and wasting fuel  we used iexs api to download 5 years worth of stock data from apple and google  this included the daily price opening closing high and low of the stock  we also wanted to predict stock prices on a slightly longer time scale so we restricted to looking at stock prices each week choosing the closing price on the previous friday as the stock s price at the beginning of the following week we did however save some data about how the stock changed in the lead up to a given week  we added a feature corresponding to the volatility of the stock price in the previous three weeks  more precisely for week n the volatility feature for that week is equal to the standard deviation of the daily stock prices from weeks n 3 through n 1 divided by the mean of the stock price during this range  so in total our data each row corresponding to one week in our 5 year span contains 7 features the volatility of the stock delta1 delta2 delta3 the price of that stock at the beginning of the week i e  the closing cost from the previous friday our current cash on hand and the value of the stock we own  specifically at any given moment in time we are allowed to do a hard buy or sell a soft buy or sell or do nothing our default values of hard and soft were 100 and 10 respectively though we did test different soft values later while keeping hard fixed see section v c for details  by letting hard and soft correspond to the amount we invest in the stock market on a given week and not to the number of shares we ensure that our model is independent of the average price of the stock our model just cares about how the stock price fluctuates from week to week and not the absolute price of the stock  notice also that for simplicity of our model we allow negative cash and negative stock value in our portfolio  some of this even makes sense for instance negative cash could correspond to taking out a loan so as to invest more in the stock market our reward function is just the change in portfolio value each week  for both lunar lander and the stock market we used deep q learning to train our agent  the goal here is to learn the q function which gives our total maximum expected reward if we start at state s and take action a  since q corresponds to net reward in order to implement a trained network at state s we would take action a which corresponds to the largest entry in our output from the neural network  in order to train our model we need a loss function  given a state s and an action a our loss is just the difference between what our model predicts and what the optimal bellman equation should give  in other words if we letq be our target function generated vi our loss is then given by  for the lunar lander model we used a fully connected neural network of shape input  our base line model as in all other experiments are conducted around the above base model  here is what we have tried i  soft buy sell tweaking as we listed out the actions from our base model after training we realized that our model has a very strong preference for soft actions  in other words the agent is risk averse  ii  less competitive stock testing when training on apple and google stock we noticed that the agent chose soft buy the vast majority of the time  part of the issue here might be that google and apple are relatively stable stocks and have an upward long term trend in price  in order to really exhibit the predictive capabilities of our model it made sense to apply it to stocks that are more volatile and which maybe don t have such an obvious long term trend  staying in the tech realm three additional stocks we looked at were facebook twitter and nvidia  we don t currently know what is causing these discrepancies but we think it s likely that our model is too risk averse  for example for nvidia a highly volatile stock our model is usually doing nothing or performing a soft buy  iii  exploration rate tweaking having random exploration is important in learning a game like lunar lander so that the agent can experience different states of the game and handle various situations  in the lunar lander game our agent in fact grows more curious and explores more as time goes on  however we felt this is unnecessary in a stock prediction situation and thus we have experimented with various fixed exploration rates  the result can be seen in figure vii  one can see that unlike the box2d games having a reasonable exploration rate like 5 percent actually negatively influences the training  this is probably due to the fact that the process of stock prediction is much more formulated and deterministic  unlike other games where various actions could lead to different states that could still achieve optimal outcome if the price is going up a hard buy is just the absolute best action  thus for the best training outcome one should stick to a minimal exploration rate exploration rate goog aapl iv  sadqnbold the risk rewarding experiment and tuning as we observed our agent is highly riskaverse  even though it can forecast stock behaviour in the long run it will still choose the soft action instead of the hard one  to encourage our agent to take risks we have implemented a new model called sadqnbold  sadqnbold has the extra two parameters the volatility weight is a variable that currently associates reward with volatility using the formula reward profit 1 volatility thus making a riskier profit more rewarding  the exploration hard chance or ehc puts a different weight on hard actions when sampling for an action in the exploration part of training  with a bigger probability of taking hard actions the agent will see more of the benefits of hard actions  note that when 0 and ehc 0 2 we have the baseline model  another related parameter is the discount factor in the bellman equation for generating ourq  making this number closer to 0 will make the agent more short sighted thus taking bolder actions  in experiments we found that lowering or increasing and ehc will indeed encourage more hard actions  on the other hand it makes the agent highly unstable with respect to different stocks  more volatile stocks could lead to an unprofitable agent in the experiment as in figure viii on the stock of aapl the actions consist primarily of hard buy and sell  the final portfolio value is around 12 000 dollars  vi  conclusion and future directions conclusion by drawing connections between the game of lunar lander and stock investment we have established a baseline structure of a stock predicting agent using the model of deep q learning  the model is demonstrated to be rather risk averse but can master long term investment strategy with reasonably volatile stocks future directions shot term vs long term in our data split there is a mismatch as we are training on approximately 5 years of data and trying to test the agent on 10 weeks of data  however what the agent picks up from the base model is a long term strategy and is not optimal on a 10 week basis  thus we tried to implement stockagentdqnshort  this agent instead of training the whole 5 year period trains on several episodes of 10 consecutive weeks or whatever the test data length randomly selected from the training data  but as one can observe from figure ix the randomness we introduced is giving the model a hard time converging and thus the reward graph is highly fluctuating  one explanation why this approach failed is that though we match the data with train and test this is not the traditional way humans predict stock  another solution which is closer to human prediction is to include real world events to help short term prediction the code for this project is available at https github com zhubeite cs 229 rl project  we would like to thank our mentor mario srouji for his guidance throughout this project  contributions caitlin contributed code for pre processing data transition reward functions and downloading stock data contributed to write up jupiter wrote tensorflow code to train our neural networks created overall code architecture tested hyperparameters contributed to write up 
poster deep cue learning a reinforcement learning agent for pool summary the goal of this project is to apply reinforcement learning to the game of pool the environment is formulated as an mdp and solved with q table dqn and a3c algorithms with two balls on the table q table learns the best but a3c with discrete action space has the best performance considering all trade offs problem formulation markov decision process mdp state s list of x y positions for each ball white ball first action a angle force reward r s a 5 for each ball pocketed 1 if no ball collides with white ball 0 otherwise algorithms q table implements q learning with discretized states and actions uses a lookup table for each s a pair to represent the q function deep q network dqn 1 uses dnn to approximate the q function with continuous state values as input and the q values for each discrete action as output asynchronous advantage actor critic a3c 2 estimates both the value function and policy policy can be updated more intelligently with the value estimate multiple agents learn asynchronously on different threads to speed up the overall training two ball environment q table best performance learns the exact steps to hit the ball in from the starting position table size large limited to two ball environment training time significantly longer dqn good performance but training unstable model learns only two or three good actions that tend to get better total rewards but does not do well in the short term a3c continuous action good performance but longer convergence time space efficient generalizable to larger environments since it predicts mean and variance of the normal distributions for actions it is hard for the values to settle in the right range a3c discrete action better performance than with continuous action sacrifices some accuracy space and time but classification training is more effective than predicting bounded continuous values discussion four ball environment both a3c with continuous and discrete action perform poorly when state space is enlarged in continuous action values tend to be saturated and clipped at 0 or 1 in discrete action a single angle value tends to be favored references 1 mnih volodymyr et al human level control through deep reinforcement learning nature 518 7540 2015 529 2 mnih volodymyr et al asynchronous methods for deep reinforcement learning international conference on machine learning 2016 future work inspect the value saturation problem in a3c look for improvements in environments with more balls compete the ai with human player for more comprehensive evaluation nkatz565 cs229 pool experimental results training results over 1000 episodes q table state 50 buckets angle 18 buckets force 5 buckets dqn a3c continuous action a3c discrete action angle 360 buckets force always max average reward 6 4 21 3 training time 136 min model size 1 12 gb evaluation results over 100 episodes training statistics 27 min 162 kb 13 min 8 kb 17 min 149 kb 19 44 18 46 a3c continuous action a3c discrete action 2 balls episode length 25 4 balls episode length 50 28 min 11 kb 50 0 52 min 152 kb 24 86 peiyu liao pyliao stanford edu department of computer science nick landy nlandy stanford edu department of electrical engineering noah katz nkatz3 stanford edu department of electrical engineering	in this project four different reinforcement learning rl methods are implemented on the game of pool including q  it is an interesting topic to explore in that when considering a hit we may not simply want to hit a ball into the pocket but also want the next ball position to be convenient for future hits the problem is formulated as a markov decision process mdp and solved with four different rl methods including q one of the team members noah katz completed this project for cs229 and cs229a  rl was not taught in cs229a however the applied use of neural networks and the skills needed to understand and debug issues with neural networks were covered in the coursework of 229a and have been helpful in this project the code for the project can be found on github 1  one of the most notable areas of research in rl is in building agents to play games  games provide a simplified environment that agents can quickly interact with and train on  in the case of pool plenty of work has been done in applying rl or other ai techniques to the game  traditional ai techniques used include search the problem is formulated as an mdp with the following set of state action and reward definitions where m is the number of balls x 1 and y 1 are the x and y positions of the white ball x i and y i are those of the i th ball r is the angle of the cue within range 0 1 f r is the force applied to the cue within range 0 1 is the relative reward weight between hitting no ball and pocketing one ball and numballs s returns the number of balls still in play at state s s 2 m is the list of elements in s other than the first element i e  the positions of all balls other than the white ball  in other words negative reward is assigned if no balls are hit zero reward is assigned if the white ball makes contact but does not produce a pocketed ball and a positive reward is assigned if some balls are pocketed  in this project is set to 5 normalization is applied to state and reward in the deep methods to be introduced below i e  dqn and a3c to stabilize training  the game simulation engine is modified from an open source pool game implemented in python 2  the following modifications are made to fit this project 1  created an interface to modify game parameters  2  created an interface for the rl algorithm to interact with  3  set game graphics up so that they can be turned off for faster training or turned on for visualization of the training process  4  optimized the game engine by removing unnecessary functions there is only one player our software agent in this game  in addition instead of applying the complete pool game rule the experiments are conducted in a simplified setting with a small number of balls and the goal is simply to pocket the balls disregarding the order  the four ball scenario would prove that the model can have some extra understanding of how the balls interact with additional balls that may be in the way in the learning an episode is a game with a set maximum number of trials  each episode is automatically concluded when all balls have been pocketed or the maximum number of trials have been reached  in order to solve the mdp three algorithms are implemented including q table deep q networks dqn and asynchronous advantage actor critic a3c  for a3c it is implemented with both continuous action and discrete action  whereq s a is the current estimate of the q value s is the current state a is the current action r is the reward received by taking action a at state s s is the transitioned next state is the learning rate and is the discount factor of the rewards q table implements q learning using a look up table to keep the q value for each discrete state action pair we use the epsilon greedy method as our exploration strategy where at each time step there is a probability of selecting a random action and probability 1 of selecting the optimal action from the current estimate of the optimal q function  q tables work well on small number of states and actions but for continuous states and actions that need to be discretized much information is lost and the learning becomes inefficient  the parameters of the q network are then updated using optimization algorithms such as stochastic gradient descent and backpropagation in the dqn implementation a neural network with 2 hidden layers is used with continuous state values as input and a discrete action as output  the dimension of the 2 hidden layers are 64 and 256 each  actions are then sampled from this probability distribution a replay buffer is used to hold all experiences obtained in an episode which are then shuffled for training  the target network is updated every several iterations as the weighted sum of the parameters of the two networks with the use of a target network the q values will not be chasing a moving target the network parameters are trained with the following equations where w is the parameter of the model is the learning rate l is the mse loss s i is the state of the i th experience in the batch f is the output of the original network and f tar is the output of the target network  a3c consists of a global network that approximates both the value function and the policy and several workers that interacts with the environment asynchronously to gain independent experiences and send them to the global network for a global update every few actions in this algorithm the benefits of both policy iteration and value iteration are combined and the policy can be updated more intelligently with the value estimate  in addition multiple agents learning asynchronously on different threads speeds up the overall training  the variance itself serves as the exploration factor discrete action the same as in dqn discrete actions are chosen based on the q values predicted for each action  four algorithms q table dqn a3c with continuous action and a3c with discrete action are first evaluated in the simplest environment with two balls i e  one white ball hitting another ball  each algorithm is trained for 1000 episodes each episode allowing a maximum of 25 hits  while other settings remain the same the maximum hits allowed are increased to 50 in q table the state is discretized into 50 buckets for both x and y positions the angle into 18 buckets and the force into 5 buckets  in both dqn and a3c with discrete actions the angle is discretized into 360 buckets while the maximum force is always chosen to interpret the numerical values of the rewards note that the minimum reward is 1 max hits per episode for not hitting any ball at all during the episode and the maximum is 5 num balls for pocketing all balls a random policy is also evaluated and serves as the baseline for other algorithms all experiments are conducted on a machine with 16 gb of memory and an 8 core 6th gen intel i7 processor running at 2 60 ghz  two ball environment the moving average rewards received over the training period of all five algorithms is shown in the training results are shown in overall a3c with discrete action is considered the more ideal choice for this problem considering all trade offs  it is scalable with state space the training is stable and efficient and the performance is acceptable  however in an environment with simpler settings and with potentially unlimited resources q table has the advantage of being the simplest implementation and having the best performance q table has produced a particularly interesting result in that at the end of its learning it repeatedly executed an exact set of moves that will complete an episode in 6 moves for a total reward of 4  this is an acceptable solution given the problem but it would be better if it learned how to pocket the ball in one hit  this might be solved with a lower gamma value to discount the non immediate rewards more harshly or by more random exploration from the experiments several additional observations have been made 1  sparse rewards may affect training efficiency  in the design of reward model positive rewards are only given when a ball is pocketed which is difficult to achieve at the beginning of training  more timesteps are required for the model to learn a good action which makes the training inefficient  2  the game of pool has been formulated into a mdp and solved with four different algorithms q q in the future the poor scalability of the models in an environment with more balls should be addressed first  the game can further be made more challenging by enlarging the table size adding rules etc  finally the model can be integrated with hardware as a pool robot for entertainment and educational purpose  he also implemented the worker class in the a3c method peiyu liao created the environment wrapper for mdp and implemented the q nick landy worked on implementing and refining dqn  he also conducted all the experiments and analysis related to the model  he also worked on optimizing the simulator to improve model training speed  he is also the major writer of the report all team members worked together on the report  
combining ppo and evolutionary strategies for better policy optimization combining ppo and evolutionary strategies for better policy optimization jennifer she computer science stanford university jenshe stanford edu objecive propose and implement hybrid policy optimization methods inspired by proximal policy optimization ppo and natural evolutionary strategies es in order to leverage their individual strengths compare hybrid methods against ppo and es in two openai environments cartpole and bipedalwalker background under the reinforcement learning rl framework agent environment at rt st 1 the goal of policy optimization is to find a policy s a 0 1 defining pr at a st s that maximizes the expected return j e p rt ppo updates via an approximation of j pro it uses gradient information to guide its updates which helps it to zero in on potential solutions con it may get stuck at a local optima as a result es parameterizes with n 0 i which it updates by sampling 1 k weighted by their return 1 k ki 1 t rt p i 1 pro it incorporates stochasticity in the space of for better exploration of con it treats the rl problem as a black box the goal is then to build hybrid methods that both leverage gra dient information and are stochastic in methods es ppo i ppo ppo ppo update 1 2 k 1 2 k sample i as in es but instead run ppo with these as initializations to obtain i update by 1 with modified perturbations t 1 i max ppo run es ppo as above but directly set to i with the highest return argmaxi rt p i alt ppo run es every j ppo iterations we compare these methods to es and ppo environments a cartpole b bipedalwalker cartpole v0 cp s r4 a 0 1 objective move cart to keep pole upright rewards 1 every timestep for a max of 200 termination pole falls cart goes off screen or episode reaches max of 200 timesteps bipedalwalker v2 bw s r24 a 1 1 4 objective maneuver walker to right most side of environment target without falling rewards for moving forward for a total of 300 on agent reaching target 100 for falling termination walker reaches target or falls architecture details es a s 1 a f s where f is a fully connected neural network 1 fc dim s 100 relu 2 fc 100 dim a 3 sigmoid 1 cp or tanh bw ppo hybrids a s bernoulli g s cp a s n g s bw where g is a fully connected neural network 1 fc dim s 100 relu 2 fc 100 100 relu 3 fc 100 dim a 4 sigmoid cp or tanh bw results es cp es bw ppo cp ppo bw es ppo cp es ppo bw max ppo cp max ppo bw alt ppo cp alt ppo bw figure episode returns over training across 5 trials each return training time es 200 0 60 59 ppo 200 0 53 74 es ppo 200 0 515 03 max ppo 200 0 363 52 alt ppo 200 0 131 24 table final results from cp averaged across 5 trials discussion ppo and es performed well on both tasks ppo training instability bw likely a result of reusing samples from old es evaluating i is slow without leveraging large scale parallel compute extending es ppo and max ppo from es exponentiated this problem and forced us to choose max sample size k 5 for bw es ppo ppo calls may drive i far from thus a weighted average of returns at i may no longer be a good predictor of return at weighted average of i misleading update signals max ppo mitigates averaging problem of es ppo but may lead away from a good solution when all neighbouring i have low returns high variance alt ppo mitigates high computation cost of es ppo and max ppo but its stochasticity may lead away from a good solution when neighbour i have low but different returns future directions investigate trade offs in sample efficiency and variance in the case of ppo investigate ways to leverage high compute in the case of es ppo and max ppo investigate stochasticity with adaptive variance using gradient information to avoid moving away from good solutions investigate more complex environments where es and ppo fail acknowledgements we thank mario srouji ta for the project idea and help during the project jenshe stanford edu	a good policy search algorithm needs to strike a balance between being able to explore candidate policies and being able to zero in on good ones  we compare these methods against ppo and es in two openai environments cartpole and bipedalwalker  the standard reinforcement learning framework is modelled by a markov decision process m s a p r where at each time step t the agent takes an action a t a at a state s t s and as a result transitions to a new state s t 1 according to p and receives a reward r t according to r the objective of policy search is to determine a policy s a 0 1 parameterized by in our case that specifies how the agent should act at each state s ie  however they are said to face a lack of exploration in the space of policies due the greediness of their updates  however they make use of less information and thus require more time and samples to perform well  we test out 3 hybrid methods combining ppo and es that make use of the gradient and involve stochastic sampling of  we compare these methods to the original ppo and es in cartpole cp and bipedalwalker bw  the code for this project is available at https github com jshe cs229project git  at each iteration trajectories are sampled under old for a total of h state action pairs  we then use mini batch samples of these pairs s t a t of to update  is updated using a modification of 3 where log a t s t is replaced by a ratio at st old at st to allow for this type of sampling and the ratio is clipped if it falls outside of some range 1 1 to increase stability  this results in the objectivewhere is sampled using old the advantage function t r t v s t where r t t t t t t r t is a modification of r calculated using a learned value function v  v is updated along with using an additional lossan entropy termcan also be optionally added to the objective to encourage exploration in a t  combining in evolutionary strategies or derivative free policy optimization the function j is treated as a black box the general framework of evolutionary strategies involves at each step sampling candidate parameters 1 k from some distribution and using these i s based on their performance in terms of j to update a recent variant called natural evolutionary strategies where is updated using an approximation of the gradient e j derived by rewriting it as e n 0 i log p j and approximating this using the samples instead of sampling i naively as in es we propose running ppo with each of these samples as initializations to obtain new samples i  we then update by 8 with returns from these new samples and modified perturbationsthe general idea of this algorithm is summarized in instead of using the update 8 as in es ppo we directly set to i with the highest returnwe conjecture that this method would work well despite its apparent greediness because i are likely to be decent solutions as a result of the ppo updates  we alternate between es and ppo iterations by running es every j iterations with in order to inject stochasticity  the objective of cp see for es in this setting we represent bywhere f is a fully connected neural network fc 4 100 relu fc 100 1 sigmoid 1  for all other methods we usewhere g is a fully connected neural network fc 4 100 relu fc 100 100 relu fc 100 1 sigmoid  we also parameterize v by fc 4 100 relu fc 100 100 relu fc 100 1 where the first fullyconnected layer is tied with g  we perform hyperparameter search for each method to obtain the best configuration and describe the details below  the results are shown in we choose k 5 among 5 10 20 as it seems to be sufficient for cp and results in the fastest training time  we also set 2 0 1 among 0 1 0 001 0 0001 with learning rate 0 001 among 0 0001 0 0025 0 001  the objective of bw is to maneuver the walker to the rightmost side of the environment without falling  the agent receives for moving forward for a total of 300 on the walker reaching its destination  the agent also receives 100 for falling  the episode ends when the walker reaches its destination or falls  s r 24 and a 1 1 4 represent the various states and actions of the walker and its components hips knees etc for es in this setting we again represent by 9 where f is a fully connected neural network fc 24 100 relu fc 100 4 tanh  for all other methods we usewhere g is a fully connected neural network fc 24 100 relu fc 100 100 relu fc 100 4 tanh  we also parameterize v by fc 24 100 relu fc 100 100 relu fc 100 1 without tied layers this time because we need more parameters in a more complex setting  we perform hyperparameter search for each method to obtain the best configuration under the constraints of our compute which we detail below  the results are shown in we use the same setting as ppo in the case of cp except we increase h to 2048 and batch size to 64 and make use of the entropy term with c ent 0 0001  we thank mario srouji for the project idea and help during the project  
overview simple explicit measure of contextual word importance supports tiny contexts 10 sentences uses document word vector cloud properties contextually significant words define meaning weighted bag of words model substantially outperforms state of the art for subjectivity analysis and paraphrase detection comparable to sota for other transfer learning tests applications a better sentence vector baseline easy sentence document summarizer via pathfinding contextual stop word identification improved and context aware cosine distance current implementation limitations long short term memory lstm networks limited because short term document specific context overfitting tf idf sentence embedding vector baseline rarer words are more important essentially sum of tf idf weighted word vectors requires large document no handling of out of context words stratified for rare words ignores word similarity state of the art global context approaches context vectors deep structures etc black boxes unsupervised barely outperform tf idf baseline knowing what you re reading affects interpretation tf idf baseline requires a large context dataset to work but people don t need a ton of text to establish context newspaper articles short stories currently no simple baseline for global context motivation context is everything finding meaning statistically in semantic spaces words clauses stanford sentiment diverse context 9k examples 300 dimensional pretrained glove 42b cc no out of vocabulary keys sentences senteval train dev set variety of transfer learning contexts fasttext vecs 600b token cc out of vocab support replacing tf idf mahalanobis distance normalizes for stdev and covariance distance from document word vector cloud needs only document word vector covariance and average works with tiny data since word vec dimensions are normal better and context aware cosine distance vec cardinal vec red wikipedia page for green vec cardinal vec red article about stanford d 0 909 d 0 943 unified clause word vector space glove space including both two word clauses and words importance relates clause vecs and constituent words sigmoidal sentence embeddings calculate document average word vector and covariance for sentence calculate each word s importance divide by double the sentence average opt ignore words in closest 20 of doc importances for sentence average corresponds closely to stop words weight by sigmoid of relative importances algorithms word vector clouds algorithms sentence embeddings algorithms sentence unembeddings meaning subtraction vec sentence w vecword n vecword n given a sentence vector and one subsentence vector can calculate other subsentence vector assume w vecother is the avg distance solve for vec repeat takes 3 5 iterations to converge to several decimal places path finding for meaning extraction calculate the remaining subsentence vector if within m cosine distance radius return sentence find the new words closest to the subsentence vector enqueue the sentence with the closest words appended ideal params ignore stop words and case harmonic mean erf stretched vertically for range between 0 27 to 0 73 horizontally by 4 2 the sky is bluem d is ta nc e the sky is blue m ea ni ng word proportion of clausal meaning vs importance in document proportion of total general context importance pr op or tio n of c la us al m ea ni ng datasets and evaluation visualization of improved cosine distance conclusion and future directions this technique should replace the tf idf baseline can global context help generate word vectors implications for how we process information appears to suggest we overvalue slightly more salient information when combining meanings linguistic implications where does syntax come into play can a rule based system restricting the subset of closest words that can be chosen as the next word generate grammatical sentences with the unembedding neurological implications can we measure the importance salience of words and sentences and relate them	the world models paper what is the nature of excellence  is it the envisioning of a task in one s mind over and over again or perhaps is it incremental improvement over repeated practice  think of how an athlete trains daily while also envisioning her future success  in this paper we propose our adversarial learning framework amore to help ascertain whether these ideas can be applied to a reinforcement learning framework  the world models paper we initially conducted tests using openai s racecar setup because it has a simple world with a clear distinction between reward and penalty areas  1  perform initial rollout using the jerk algorithm just enough retained knowledge 6  because the cma es generates samples from a gaussian distribution one can vary the sigma value to get a larger variety of outcomes depending on your performance needs  the vae is used to encode and decode frames  the loss used for the generator was partially inspired by the loss used in a vae accounting for both the likelihood of the predicted next frame 2 and an adversarial loss corresponding to how effectively it tricked the discriminator  the action is a t w c z t h t b c for controller c at timestamp t we experimented with varying the complexity of this policy but found that it made it more difficult to update from new levels  it is likely that a joint training model that is one which trains on all the levels at the same time would be more compatible with this approach  cma es for covariance matrix adaptation evolution strategy  one interesting side effect of this was the tendency for sonic to take the long route around levels exploring dead ends and sometimes finding very non obvious rewards due to it like hidden ledges that required jumping while going down a waterfall in labyrinthzone act3 of the first sonic  it was also amusingly a fan of spectacle choosing to watch an explosion instead of proceeding  this is reminiscent of initially an interesting issue came up we were at the time letting the anticipator choose moves every time it ran since it went through all of the possible actions but often not moving had the same reward as the controller making its move since the controller would eventually accomplish whatever the best action accomplished  the consequence was that the anticipator often did nothing when it didn t think it could contribute  we solved this first by discounting future rewards with an impatience term an exponential drop off in the value of future rewards  one accidental effect was that the anticipator accidentally created short sighted loops if in alert mode like repeatedly hitting a 10 point bumper  we updated alertness to only activate if the controller performed badly  the iterative architecture then uses policies generated from the previous experimental round as rollouts for the next experimental round which should improve the training of both the vae and the gan as they will be training on frames that score higher balanced with some exploration  we started with the code provided by openai for their world models experiment but have since modified it to implement our own algorithm  we rewrote the procedure to be iterative and to reuse previous policies during rollouts as well as adding gan capacity and some features to action selection in the code  for this particular challenge your agent must achieve a score of 3000 to consider the level solved and our agent received an average score of 3600 across many levels  upon inspection the agent adapted novel behaviors such as one instance where the agent jumped on a button over and over as the optimal action  our goal for this paper was to improve upon the original world models architecture and in that sense we think we succeeded  for one there are certainly more cognitive principles that could further inspire our algorithm s architecture  perhaps the most compelling future improvement is a stochastic update to the the lstm and vae with every new experiment alongside the per epoch training we introduced  
poster final predicting nyc taxi fares trip distance and activityomnitaxis authors boxiao pan varun nambiar paul jolly bxpan vnambiar pjolly94 stanford edu stanford university cs229 machine learning december 11th 2018 introduction for a new york city taxi driver being in the right place at the right time is often what makes or breaks a day one may naively assume that the right spot corresponds to a place simply where demand or activity is high however taxi drivers might find it more lucrative to be in a slightly lower activity location where people are demanding shorter trips that are worth more to assist drivers in this decision we explored different models to predict the activity fare amount and trip distance given input features location the day of the week and the time of the day raw data new york city taxi and limousine commission tlc provides a large amount of trip data from 2014 to 2018 including the following information a date and time of trip b pickup location mid 2014 mid 2016 latitudes and longitudes mid 2016 mid 2018 location ids c fare amount d trip distance data pre processing label bucketing instead of using exact values for the labels activity fare and trip distance we discretized them by creating buckets this was done by inspecting the distribution of the labels over the data and selecting realistic bucket ranges table 1 not only did this enhance the model performance but more importantly it proves more useful in application since we are presenting estimated ranges i e buckets to the driver as opposed to exact numbers which is what drives care more about creating location clusters with k means to increase the granularity of the newer trip data i e post mid 2016 we created clusters using the latitude longitude data from the older dataset i e pre mid 2016 and distributed the newer data into cluster ids based on the distribution within each location id obtained from k means figure 1 data and features conclusion and future work experimental results models our models can pick up the relative differences between different neighborhoods but does not perform well when trying to predict the exact numbers this may be a result of using a single model to predict for the entire new york area here are possible ways forward focus our model to manhattan island and the surrounding airports only include areas where yellow taxis officially serve add more data for the lstm model our sparse sampling may be causing issues further tune network hyper parameters model activity fare trip distance rfc 51 02 45 71 30 08 fcnn 35 67 45 72 23 16 lstm 26 65 27 72 23 10 references sklearn cluster kmeans kmeans scikit learn 0 19 2 documentation online available https scikit learn org stable modules generated sklearn cluster kmeans html accessed 11 dec 2018 bucket id activity trips fare trip distance miles 0 2 0 0 5 1 2 5 0 5 0 5 1 0 2 5 7 5 10 1 0 1 5 3 7 10 10 15 1 5 2 0 4 10 15 15 25 2 0 3 0 5 15 25 25 50 3 0 5 0 6 25 35 50 60 5 0 10 0 7 35 45 60 10 0 8 45 location id 261 world trade center manhattan location id 256 williamsburg south side brooklyn before clustering after clustering figure 1 two examples of location clustering based on trip data through k means the top row shows trips before clustering and the bottom row shows trips classified into clusters table 1 label classes created through bucketing random forest classification rfc loss function gini loss 1 fully connected neural network fcnn loss function cross entropy loss log 4 hidden layers with 6 10 6 12 neurons respectively and all with relu activation function 1 output layer with softmax activation function long short term memory lstm network loss function cross entropy loss log the model parameters are for the case of activity prediction subset description data set split 90 5 5 training set fare and trip distance 1 8 million activity 0 27 million validation and test set fare and trip distance 0 2 million activity 0 03 million for all three labels random forest performs the best or nearly among the models while all these three models perform poorly on this task we think this should mainly be attributed to that the prediction task is too hard given the features we have figure 2 shows an example heat map output comparing predicted activity with the ground truth while the exact numbers do not really match our model captures the relative activity quite well table 2 quantitative experimental results figure 2 heat map representation of ground truth activity left versus predicted activity right the lighter the color the heavier the activity despite not being able to predict the exact magnitude of activity well our model is able to capture the relative activity between different locations	for an nyc taxi driver being at the right place at the right time is often what makes or breaks a day  one may naively assume that the right spot corresponds to a place where demand is high for instance the new york financial district at the end of a work day  however this may not necessarily be the best place to be  taxi drivers might find themselves spending the whole day stuck in traffic traveling far away from the high activity zone thereby reducing overall profits for the day  sometimes a better option might be for the driver to go to an area that is slightly less popular where people are making many short local trips which would accumulate to a handsome sum over the course of the day to assist drivers in deciding where to be we used nyc yellow taxi data provided by the nyc taxi and limousine commission tlc in this paper we discuss the design and results for each of these approaches and outline the next steps that would lead to a successful tool  a few groups have tried to predict nyc taxi activity and fares with a variety of features  some of the approaches used include lgbm the problem we are attempting to solve does not have much literature and most of the approaches we used have not been tested before  the nyc taxi and limousine commission tlc provides a large amount of trip data covering over a billion trips from the years 2009 to 2018  each trip sample contains a lot of information from which we chose pickup date and time pickup location drop off date and time drop off location fare amount and trip distance  for data prior to july 2016 referred to as older data the pickup and drop off locations are reported as latitude and longitude while for data july 2016 onward referred to as newer data they are reported as location ids ids that correspond to larger geographic regions  other than this the data is consistent over the years  due to computational and storage constraints we used a sample of the full data set from july 2014 to june 2018 which amounted to 2 million trips  for each of the models we used a 90 5 5 split for training validation and testing respectively  this did not practically make sense since it would be more useful to a driver to split some of the larger clusters into smaller buckets  instead we used the k means clusters as a guide to discretizing the labels into bins  the corresponding output label for that example is the label fares activity or trip distance of the next one data piece right after all seq len pieces  our work employed various data pre processing and modeling techniques  we go over the methodologies in this section  we cluster our location ids into 10 smaller clusters to increase the granularity of the model  in the newer data set the pickup drop off locations are specified by large location ids defined by the ny taxi commission  however the regions the commission chose were quite large and we wanted to reduce the area to a few blocks  k means algorithm works as follows 1 randomly assign cluster centroids in a given location id  this approach works well for our case because we are attempting to clump pickups by their spatial locations  this allows us to roughly estimate activity in a few blocks rather than a large neighborhood  example outputs are shown in the output size for all lstm layers is 128  like the fcnn we used cross entropy loss as the loss function equation 4  a hyperparameter tuning 1 random forests we tuned the hyperparamaters of our model with the validation set  some of the parameters we tweaked include number of trees tree depth and loss criterion  in our final models we used 40 trees the gini criterion and tree depths of two for activity prediction and three for fare and distance predictions  we tuned these hyperparameters on the validation set  c accuracy with the predictions we were able to produce heatmaps for each of the models to visualize how they vary with region for a given day of week and time interval  we split 5 of the older data into a validation set and compared the cluster ids assigned to them using location id and non uniform distributions versus their ground truth cluster ids we got this by directly mapping their lat long to cluster id  after running tests the accuracy of our k means algorithm was only 11  in order to test the effect of this poor performance on our prediction model we further trained the model using only newer data i e location id rather than cluster id  however the results turned out to be approximately the same as with cluster id  this suggests that our prediction model inherently doesn t do well  we attribute the poor performance of our prediction model mainly to two aspects  first our models only use three features to tackle the problem and this might not be enough  this sparse distribution may be causing models such as lstm to not capture the temporal pattern in the data  2 currently our loss function penalizes all misclassifications equally  however some misclassfications may be better than others  a weighted loss function could help improve our model s accuracy  these weights could be determined by the distance between the actual classification and the predicted classification  3 even though yellow taxis are allowed to operate in all the boroughs of new york there are regions where the taxis do not frequently pickup customers from  that way our model has plenty of data for a given location id  patterns of 5 boroughs may be infeasible  we are making an assumption that the behaviors of people across all the boroughs can be predicted using one model  a possible future implementation could focus a single model on one of the boroughs  this may improve performance  5 sample more data  
poster apply reinforcement learning in ads bidding optimization ying chen scpd ychen107 online display advertising is a marketing paradigm utilizing the internet to show advertisements to targeted audience and drive user engagement since around 2009 real time bidding rtb has become popular in online display advertising rtb allows an advertiser to use computer algorithms to bid in real time for each individual opportunity to show ads these bids can be based on the impression level features such as user ads and context information with its fine grained user targeting and auction mechanism rtb has significantly improved the campaign return on investment roi looking at the process we can see that bidding optimization is one of the most critical problems for the advertiser which aims to set right bidding price for each auctioned impression to maximize key performance indicator kpi such as click most of existing work only focused on finding optimal bid price but ignored an important part of optimal bidding strategy pacing algorithm pacing algorithm is essentially for budget allocation which aims to smooth budget spending across time according to traffic intensity fluctuation background we propose to use reinforcement learning algorithms to find the optimal bidding strategy the major difference of our algorithm from previous work is that our algorithm try to generate bid and pacing signal at the same time this requires the algorithm to not only learn to adapt to the changes in bidding environment but also the interaction between bid price and pacing signals we first focus on intra day bidding optimization given a certain amount of budget and unknown traffic distribution throughout the day we want to find out a bidding and pacing strategy that maximize the total number of clicks this is equivalent to minimizing cpc cost per click here we trained dqn model in 50 200 500 2000 and 5000 iterations here is one of the 500 iter results we can see the pacing rate is relatively smooth and the spending is relatively better 3 ddpg we also tried ddpg comparing with dqn its action space is continuous the results look very random and the budget some times overspends since we generate a random amount of impressions in each time slot to mimic the actual traffic the environment input is entirely different every time we do not use the quantitative metric to measure the performance of different algorithms instead we compare the budget spending pacing rate smoothness and stables among different algorithm in a qualitative way overall reinforcement learning dqn on pacing can spend most of the budget without overspending and has a relatively smooth pacing rate it shows better performance compared with the baseline algorithm there are also some weakness in ddqn for now 1 the training model is not very stable 2 sometimes it spends too much money so that the remaining daily budget is negative at the end these results give us the confidence to apply reinforcement learning algorithm in bidding optimization in the ads industry in the future we will modify neural network architecture refine cost functions and tune the parameters to mitigate the disadvantages we are also going to explore reinforcement learning in intra day bidding challenge methods experiment result conclusion and future work methods state space the state of the campaign is characterized by its remaining budget remaining delivery time current value of cpc to simplify the discussion we only consider the case when cpc optimization goal is set which is the most common case we define state as s t budget t cpc t hour t action space there are two action signals in our setup the pacing signal p t and the bid adjustment signal a t ideally the pacing signal should be continuous to reduce algorithm complexity we discretize this signal in q learning pacing granularity could be a concern but for the first version it suffices to discretize 0 1 by 2 interval so we end up with 50 possible values the bid adjustment signal a t can be 1 2 5 10 20 50 and 100 on top of previous bid state transition we model the bidding process as a markov decision process given a previous bid b t bid adjustment signal at and pacing signal p t the bidding agent join auctions and observe the outcome then update the state s t note that we don t know the state transition dynamics the optimal control policy will be learned using reinforcement learning reward function the reward function is composed of two parts the discounted reward and the regularization the instant reward r t could be the number of clicks in given time interval t and a regularization is put on the bid signal to encourage its smoothness 1 baseline we use fixed bidding price and pacing signal as a baseline it s a simple feedback loop control we can see the pacing rate has changed a lot during the procedure and the spending looks reasonable 2 dqn q learning is a straightforward off policy learning algorithm it basically builds a q table which gives the reward function for state action pairs and update it while exploring the environment deep q learning is just a neural network version of q learning which uses dnn cnn to approximate the q function experiment result email yingchen107 gmail com pacing rate in na ve bidding remaining budget in na ve bidding pacing rate in dqn remaining budget in dqn pacing rate in ddpg remaining budget in ddpg	online display advertising is a marketing paradigm utilizing the internet to show advertisements to the targeted audience and drive user engagement  pacing is a strategy to ensure that the budget spends evenly over the schedule of advertiser s ad set  here we present two reinforcement learning approaches dqn and ddpg to smooth the daily budget spending  since 2009 real time bidding rtb has become popular in online display advertising 1 when a user visits an ad supported site each ad placement triggers an ad request to the ad exchange  reinforcement learning as a framework for sequential decision making has attracted the attention of researchers since many years ago since the company data is confidential and there is no public dataset available for ads auction simulation the link of our previous choice ipinyou dataset we implemented a simulator that simulates the ads auction environment  to be more similar to the real world we use time of day pattern to simulate the traffic  with traffic ups and downs it makes the environment varies in different time steps  it also brings difficulty in algorithm training which is good  the simulator is implemented approximately following the convention in gym package it is well known that optimal bidding can be formulated as an mdp problem deep q learning q learning is a straightforward off policy learning algorithm it basically builds a q table which gives the reward function for state action pairs and update it while exploring the environment  the optimal policy on the other hand can be generated by taking greedy actions at each state according to q table  the algorithm of dqn with experience replay can be found in 1 so it s omitted here in the interest of space ddpg deep deterministic policy gradient is a model free off policy actor critic algorithm  it can be regarded as a combination of dpg since we generate a random amount of impressions in each time slot to mimic the actual traffic the environment input is entirely different every time  for pacing it s hard to qualitatively compare the performance between two different algorithms  we look at two quantitative criterions a the overspending or underspending should be small at the end of a day b the pacing signal should be as smooth as possible  we could alternatively use the cost function in section 4 1 to compare rl based algorithm and baseline which is defined to quantify the above heuristic  it s a closed loop control system similar to that of the action space is discretized into a finite number of bins we experimented with 10 12 15 and 20 then found 10 performs best  the dqn is implemented with experience replay for the discounted cost we set discount factor to 0 99 and we used td 0 to compute the loss for optimization we used adam  to avoid overfitting we simplified the architecture of the neural network in code using only one hidden layer with 16 neurons  results shown in for ddpg the action space is continuous so that we won t see the stairs like pacing signal curve as in dqn  however our algorithm still needs improvement because the training is not stable sometimes the network can generate results like in overall we explored the possibility of applying reinforcement learning to ads budget pacing problem  it not only achieves the goal of budget delivery but also maintains the pacing rate relatively smooth  in the future we ll also consider the following directions for performance improvement 1 adding more features e g features extracted from time series 2 explore other network structures  3 try alternative cost function and regularization  regarding the project we can also have the training on some company data if the data security team grants permission  and intra day budget spending could be another interesting topic for us to explore code path https github com yingchen cs229 
improving product categorization from label clustering alexander friedman alexandra porter alexander rickman cs229 machine learning class project ajfriedman amporter arickman stanford edu motivation in a massive online store an intractably large set of key words to describe books can easily be acquired by either seller input or automatic searching of the text our goal is to organize a massive set of labels applied to a set of books to use for categorization we implement an algo rithmic and application based project to analyze data from amazon web crawl data of books and their categorizations we embed labels into a feature space and apply clustering approaches to find interesting features such as redundan cies hierarchies and anomalies methods node2vec the node2vec algorithm 1 samples a set of random walks and then performs stochastic gradient de scent on the feature representation of the vertices the loss function is the similarity of the pairs of representa tions given that the vertices appear together v d parameter matrix for u v ns u v is neighborhood with sam pling strategy s maximize objective function max f u v logzu ni ns u f ni f u clustering once we have node2vec representations of the network we cluster with k means 3 based on subjective observation and testing on the data set we specified the number of clusters as 6 1 procedure k means k pointset 2 while centers change do 3 clustercenters k random points 4 for p pointset do 5 center p argmin c clustercenters dist p c 6 for c clustercenters do 7 c mean p center p c references 1 aditya grover and jure leskovec node2vec scalable feature learning for networks in proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining pages 855 864 acm 2016 2 f pedregosa g varoquaux a gramfort v michel b thirion o grisel m blondel p prettenhofer r weiss v dubourg j vanderplas a passos d cournapeau m brucher m perrot and e duchesnay scikit learn machine learning in python journal of machine learning research 12 2825 2830 2011 3 saedsayad com k means clustering 2018 dataset the amazon dataset contains metadata on 350 000 books including the categories labels to which each book be longs the graph dataset which we input into node2vec was created by using labels as nodes and generating edges between nodes whenever a book belonged to multiple la bels labels in the original amazon dataset can be de scribed as a forest these labels can often be redundant so our model aims to detect these redundancies so they can be replaced with a cleaner labeling scheme framework and implementation data pre processing select books select top sales ranked labels as nodes edge weights as books embed node2vec output label features pairs cluster k means clustering output point cluster pairs output cluster centers output pca plot top 2 components output kmeans clusters output point center distances analysis option 1 find outliers remove from graph option 2 find clusters create graph of each cluster figure 1 framework as seen above we iterate through a workflow of embed cluster plot analyze and repeat in this process we ad just parameters of both the node2vec and clustering models we can use this system to detect remove outliers before optionally re embedding we can also select a cluster from the initial run then re embed and re cluster that cluster repeating numerous times in order to collect redundant cat egories and analyze label hierarchies after analysis we se lect an induced subgraph of the original graph to re embed and continue the cyclic process we use the scikit learn package to cluster and plot 2 results anomaly detection and removal a b figure 2 a original clustering 6 clusters b anoma lies removed from graph and re embedded before another clustering nested label associations a a initial embedding and clustering 6 clusters b se lecting the subgraph induced by nodes in magenta cluster ing re embedding and re clustering c same as b for the cyan cluster b c figure 3 creating nested clusters analysis anomalies we use euclidean distances of points from k means centroids to detect outliers as seen in the table below we can directly remove these outliers from the plots but we hypothesized that removing outliers from the graph and re embedding before re plotting would produce more cohesive clusters as seen in figure 2 b removing anomalies results in less clearly defined clus ters likely due to the cluster structure being primarily defined by the anomalies we hypothesize that the graph induced by non anomalous nodes is relatively uniform and thus lacks structure for our method to identify distance from center label 1 9203707947953235 subjects 1000 1 9626659852879147 instruction 11811 2 0156069220765436 books 283155 2 0276880269223216 poetry 9966 2 1297687095091673 foreign languages 11773 2 2177811099675195 dictionaries thesauruses 11475 2 4396388017039103 general 725800 label organization after two iterations of embedding and clustering we see that groups are mostly made up of labels which are redundant or closely related below are 3 examples of label sets strings as they appear in the data found in a cluster shown in figure 3 b c cluster 1 cluster 2 cluster 3 regions 17228 computer arts regions 640504 computers camera states 17263 design categories 493964 states 640538 digital collections united internet 768564 collections united programming 3839 general 2050 project photo software photo specialty photographers 229534 photography photography 2020 172282 future work additional parameter optimization node2vec search strate gies depth vs breadth kmeans clusters outlier thresh old determine necessary number of nested label clustering steps to find all redundancy additional applications other product categorizations financial transaction networks telecommunications net works pharmaceutical co prescription data	we concluding by discussing findings involving anomaly detection identification of redundant or closely associated labels and label hierarchical organization  in a massive online store such as amazon keywords to describe books can easily be acquired by either seller input or automatic searching of the text  here we take a novel approach of applying clustering techniques to achieve our goal  1 2 1 graph clustering  recent work on labeled graph clustering includes using node identifiers and community prior for graph based classification zhou et al  liu et al  other works applying node2vec include node2vec in telco an application of the novel feature learning method for predictions in call networks extensions of node2vec include metapath2vec thus by analyzing multiple embeddings by applying clustering we can learn which labels are similar in both role and actual meaning  labels in the original amazon dataset can be described as a forest and there are multiple trees to which a book may belong  for example one book in the original dataset belongs to two trees with labels 1  books subjects arts photography photography photo essays and 2  amazon web store categories camera photo photography books photo essaysthese categories are somewhat redundant and one of the goals of our model will be to detect categories which can be merged or used to provide additional recommendations to a user  we first describe how embedding is set up as a stochastic gradient descent method  let f v r d be mapping to features representation i e  f is v d parameter matrix  for u v n s u v is neighborhood with sampling strategy s maximize objective function max f u v log pr n s u f u  thus the maximum function simplifies tonode2vec allows for random walks to be selected between depth first search and breadth first search strategies  this is accomplished by using parameters which weight the probability of a in this section we dive deeper into the clustering algorithms implemented  in each iteration as discussed above we use node2vec and then k means to determine optimal modules  before discussing the setup in algorithm 1 we provide the pseudo code for the k means algorithm  k means runs with o n k t where n is the number of iterations k the cluster number and t the number of data points  throughout the process the clusters of smallest radii indicate groups of labels which may be similar enough to combine into one category  outliers indicate labels which are not closely related to any others in practice these are labels which do not need to be included in a user facing system or simply the lowest level i e  most specific labels  since we do not have a ground truth into how labels should be interpreted our tool is designed to present options for improving the label set to a user who would not be able to parse through the massive label set any other way  while we use the full dimensionality of the embedding to cluster and identify outliers numerically by distance from cluster center our system is also useful as a user facing tool  we use pca to select two dimensions for plotting  we compared this to 3 dimensional plots and plots of other dimensions besides the principal ones but the 2 dimensional pca plots conveyed the full structure of the point set while being much more concise  we present visualizations of the label space created using two variations of our workflow  we use euclidean distances of points from k means centroids to detect outliers as seen in the table below  we can directly remove these outliers from the plots but we hypothesized that removing outliers from the graph and re embedding before re plotting would produce more cohesive clusters  as seen in the first pass on embedding and clustering show in europe united states regions professional science medicine professional technical guidebook guidebook series  and finally similar methods could be applied to financial transaction networks telecommunication networks and healthcare data  code can be found at https github com aporter468 embedandcluster note that the necessary node2vec library is not included it can be found at https github com aditya grover node2vec tree master src b appendix contributions friedman experimented with different combinations of dbscan k means pca and colors to create visualizations  ran node2vec embeddings and analyzed results  performed literature review  optimized clustering parameters to improve performance and visualization  
logistic regression linear svm svm with rbf kernel we build ml models to detect fraudulent activity in payment systems used pca for data visualization build binary classifiers using logistic regression linear svm svm with rbf kernel developed approach to detect fraud with high accuracy and low number of false positives achieved max recall 99 on transfer dataset in a fraud detection system it s more critical to correctly detect fraud transactions and acceptable to misclassify certain number of non fraud transactions penalize misclassification of fraud transactions more than non fraud transactions assign higher weights to fraud class to obtain high recall on that class and counter data imbalance ensure no more than 1 false positives fraud detection using machine learning aditya oza aditya19 stanford edu we obtain very high auprc values for transfer test set for all three methods with 0 98 highest value for svm with rbf kernel expected from pca decomposition results as this category of transactions is linearly separable relatively lower recall precision auprc scores for cash out test set further improvement on cash out by setting higher threshold for false positives 1 decision trees random forests to leverage categorical features time series based analysis for in context detection customized user specific models based on user s past transactional activity cs 229 autumn 2018 introduction dataset and analysis paysim a kaggle dataset for fraud detection 6 million mobile payment transactions 6 different categories of transactions 8312 fraudulent transactions numerical and categorical features pca on two categories transfer and cash out models class weight based approach results discussion future work precision recall curve and statistics test traintrain test transfer transactions recall precision f1 score auprc logistic regression 0 9958 0 4452 0 6153 0 9204 linear svm 0 9958 0 4431 0 6133 0 9121 svm rbf kernel 0 9958 0 6035 0 7515 0 9895 cash out transactions recall precision f1 score auprc logistic regression 0 9847 0 1541 0 2664 0 7564 linear svm 0 9361 0 1245 0 2199 0 7063 svm rbf kernel 0 9875 0 1355 0 2383 0 7631 transfer transactions recall precision f1 score auprc logistic regression 0 9951 0 4444 0 6144 0 9063 linear svm 0 9951 0 4516 0 6213 0 8949 svm rbf kernel 0 9886 0 5823 0 7329 0 9873 cash out transactions recall precision f1 score auprc logistic regression 0 9886 0 1521 0 2636 0 7403 linear svm 0 9411 0 1246 0 2201 0 6893 svm rbf kernel 0 9789 0 1321 0 2327 0 7271 precision recall trend for cash out for lr during training tuned class weights by measuring recall precision f1 score on validation set weights tuning references 1 a survey of credit card fraud detection sorournejad zojah atani et al 2 support vector machines and malware detection t singh m stamp et al 3 paysim a synthetic financial dataset for fraud detection https www kaggle com ntnu testimon paysim1	we are living in a world which is rapidly adopting digital payments systems  in many available datasets majority of transactions are genuine with an extremely small percentage of fraudulent ones  we compare the effectiveness of these approaches in detecting fraud transactions  several ml and non ml based approaches have been applied to the problem of payments fraud detection  the paper in the results section we ll see that this is indeed the case  we have built binary classifiers using logistic regression linear svm and svm with rbf kernels for transfer and cash out sets respectively  logistic regression is a technique used to find a linear decision boundary for a binary classifier  for a given input feature vector x a logistic regression model with parameter classifies the input x using the following hypothesis h x g t x 1 1 e t x where g is known as sigmoid function  for a binary classification problem the output h x can be interpreted as a probability of x as belonging to class 1  the logistic loss function with respect to parameters can be given as the rbf kernel function on two vectors x and z in the input space can be defined as we assign different weights to samples belonging fraud and non fraud classes for each of the three techniques respectively  we have chosen class weights such that we do not have more than around 1 percent of false positives on cv set  this design trade off enables us to establish a balance between detecting fraud transactions with high accuracy and preventing large number of false positives  a false positive in our case is when a non fraud transaction is misclassified as a fraudulent one  in this section we describe our dataset split strategy and training validation and testing processes that we have implemented in this work  all software was developed using scikit learn 7 ml library  we divided our dataset based on different transaction types described in dataset section  in particular we use trans fer and cash out transactions for our experiments since they contain fraud transactions  for both types we divided respective datasets into three splits 70 percent for training 15 percent for cv and 15 percent for testing purposes  stratified sampling allows us to maintain the same proportion of each class in a split as in the original dataset  details of the splits are mentioned in tables ii and iii  we employed class weight based approach as described in previous section to train our each of our models  each model was trained multiple times using increasing weights for fraud class samples  for each model we chose the class weights which gave us highest recall on fraud class with not more than 1 percent false positives finally we used the models trained using chosen set of class weights to make predictions on our test dataset split  in the next section we elaborate on our choice of class weights based on their performance on cv set  we also discuss their performance on train and test sets  in this section we discuss results obtained in training validation and testing phases  we evaluated performance of our models by computing metrics like recall precision f1 score area under precision recall curve auprc  in our experiments we used increasing weights for fraud samples  we initially considered making class weights equal to imbalance ratio in our dataset  this approach seemed to give good recall but also resulted in very high number of false positives 1 percent especially for cash out  hence we did not use this approach and instead tuned our in this section we discuss results on train and test sets using chosen class weights  similarly logistic regression and linear svm have similar performance and hence similar linear decision boundaries and pr curves  svm with rbf gives a higher recall but with lower precision on average for this set of transactions  a possible reason for this outcome could be non linear decision boundary computed using rbf kernel function  however for all three algorithms we can obtain high recall scores if we are more tolerant to false positives  in the real world this is purely a design business decision and depends on how many false positives is a payments company willing to tolerate  with more tolerance to false positives we can see that it can perform well on cash out transactions as well  fraud detection often involves a trade off between correctly detecting fraudulent samples and not misclassifying many non fraud samples  paysim dataset can also be interpreted as time series  all of these we believe can be very effective in improving our classification quality on this dataset  github code link https github com aadityaoza cs 229 project viii  acknowledgement i would like to thank professor ng and entire teaching staff for a very well organized and taught class  in particular i would like to thank my project mentor fantine for her valuable insights and guidance during the course of this project  
stanford poster templates 36x24 1 poster title poster subtitle first1 last1 1 first2 last2 1 first3 last31 2 1example lab department name stanford university 2example lab department name2 other university motivation explore co clustering on job applications qingyun wan qywan stanford edu data in online job serving platform like linkedin indeed and etc job recommendations are usually generated based on matching users and job posting features which are complicated as feature space can be very large if users forget to update their profiles online we even need to infer features based on other information in this case it is desirable to detect innate groupings of users and jobs based on more direct and truthful information job applications which can improve recommendation quality ultimately the goal of this project is to explore the effectiveness of co clustering users and jobs and compare with one way clustering on users based on job applications kaggle job recommendation challenge 1 6m unique job applications 360k unique jobs 320k unique job applicants preprocess constructed 0 1 user job matrices based on job application with 2 different densities split job applications for each density into 5 partitions for 5 fold cross validation discussion future compared to one way clustering co clustering is more stable with higher f1 score and more accurate with much less false positive from the visual comparison co clustering has more balanced cluster size than k means which is more ideal for job recommendation by supplying more focused pools of jobs to recommend unlike co clustering k means matches users more strictly limiting potential jobs that might be suitable for users both co clustering methods are slow because they both leverage matrix decomposition it s beneficial to explore more scalable co clustering methods so that we can co cluster efficiently on more sparse dataset 1 ding c li t peng w et al orthogonal nonnegative matrix t factorizations for clustering c proceedings of the 12th acm sigkdd international conference on knowledge discovery and data mining acm 2006 126 135 2 dhillon i s co clustering documents and words using bipartite spectral graph partitioning c proceedings of the seventh acm sigkdd international conference on knowledge discovery and data mining acm 2001 269 274 3 ungar l h foster d p clustering methods for collaborative filtering c aaai workshop on recommendation systems 1998 1 114 129 4 long b zhang z m yu p s co clustering by block value decomposition c proceedings of the eleventh acm sigkdd international conference on knowledge discovery in data mining acm 2005 635 640 methodology experiments since the labels for job seekers are unknown but only job applications to validate the effectiveness of different clustering methods i used the job applications in the testing set and computed recall accuracy and f1 score against trained clusters to find the optimal number of clusters use the silhouette method to find the number that yields to the maximum silhouette score baseline k means nonnegative matrix tri factorization nmtf spectral co clustering result density 1 lower density density 2 higher density recall accuracy f1 score recall accuracy f1 score nmtf 0 984 0 975 0 131 0 989 0 968 0 237 spectral co clustering 0 985 0 985 0 141 0 948 0 969 0 213 baseline k means 0 971 0 570 0 009 0 754 0 525 0 016 minimum of job applications per job density of rows users of column s jobs of non zero entries applicatio ns 75 0 32 29957 823 79666 100 0 73 16449 271 32873 given the user job matrix x nmtf does nonnegative 3 factor decomposition of it and the objective function derived from bi orthogonal nonnegative matrix factorization is where f is the cluster indicator matrix of clustering rows and g is the indicator matrix of clustering columns it corresponds to simultaneously clustering rows and columns of x given the 0 1 user job matrix convert it to a undirected bipartite graph which has two sets of vertices representing users and jobs respectively an edge exists if a user has an application of a job lower density higher density apply one way clustering on users using k means each training data point is a user represented in one hot encoded vector based on the applied jobs co clustering cluster size analysis with 5 fold cross validation example of k means example of spectral co clustering example of nmtf effectiveness comparision a cut between two clusters is defined by the total number of edges between them the objective is partitioning the graph to achieve the minimum of a normalized version of all cuts which corresponds to the best co clustering presentation video link https www youtube com watch v 3zpd qipanw	in the job marketplace the supply side represents the job postings posted by job posters and the demand side presents job seekers who would like to apply for suitable jobs  consider the job marketplace  one approach is based on content where job seekers preferences are predicted depending on how their explicit skills titles industries and etc  match the jobs  it involves extensive data from job seekers and jobs so that features can be complicated  another approach is consider jobs preferred by similar job seekers via collaborative filtering which leverages less data  in collaborative filtering one way to obtain similar job seekers is through clustering in the follow sections first i will briefly discuss related work on co clustering and the data set to use in this project  second i will introduce two co clustering methods i experimented to deal with job applications  third i will discuss the validation process compare the performances of co clustering methods with the baseline algorithm k means and demonstrate visual comparision  at the end is the conclusion  there are many co clustering approaches that simultaneously clustering rows and columns of a given matrix experimented on clustering documents and words  some of them are representative while the others more or less extend their ideas as they use different methodologies but acheive the the goal  to explore co clustering jobs and job seekers via job application in this project intuitively  the data set is from careerbuilder s competition https www kaggle com c job recommendatio n data  it contains job applications lasting for 13 weeks  for each application record each row in the original data set contains userid applicationdate and jobid  so i also filtered out jobs whose job has less than certain number of job applications to reduce the sparsity  so the general preprocessing steps are 1  create two data sets of job applications from the original data set by filtering on different number 75 and 100 of job applications per job  2  for each data set generated above split into 5 partitions and create 5 pairs of training set and testing set for 5 fold cross validation 3  for each training set convert it into 0 1 user job matrix for clustering  two data sets yield user job matrices with different densities  the sizes of training and testing set for each density is in the baseline clustering method is one way clustering on users using k means  each user is represented by a vector u where u i 1 if the user applies to the ith job 0 otherwise k means will cluster these user vectors based on the euclidean distance  this method is proposed in if imposing orthogonality on both f and g the objective is as shown in where f is the cluster indicator matrix of row clusters and g is the cluster indicator matrix of column clusters because as described in since our user job matrices contain 0 valued entries when the corresponding user row doesn t apply to the corresponding job column i replaced 0 with a small fractional number 0 01 which can be distinctive from the 1 valued entries before running this method then implemented the following algorithm to minimize the objective function and obtain f and g algorithm nmtf initialization 1  run k means of columns to obtain column cluster centroids as g 2  run k means of rows to obtain row cluster centroids as f  3  let s f t xg update rules then the cluster membership of rows and columns are extracted from f and g  this method is from it is proved in since there are no labels for either jobs or users the way to validate the trained clusters and generate usefuly metrics is to verify job applications in the testing set against the trained clusters  since we shouldn t verify the same job applications in the testing set as the training set and are unable to verify jobs and users in the testing set that are unseen in the training set i preprocessed the testing test to extract unseen job applications whose corresponding jobs and users were involved in the training set  the recall accuracy and f1 score are computed on different number of clusters for two data sets with different densities  the results are illustrated in we can see although the recall of k means is not bad since the jobs are overlapping in different clusters it can provide many true positives  by running 5 fold cross validation and computing silhouette scores on different number of clusters for two data sets with different densities the result is by observing the coordinates where the silhouette scores start to drop for the first data set which is larger and more sparse the optimal number of cluster is 80  for the second one which is smaller and more dense the optimal number is 50  all is done by myself  the source code is in https github com qingyunwan cs229 project  
predict optimized treatment for depression minakshi mukherjee adaboost stanford edu suvasis mukherjee suvasism stanford edu cs229 fall 2018 objective for the past 60 years the anxiety and depression medications are prescribed to patients based on the hamilton depression rating scale hdrs and social and functioning assessment scale sofas these scores are very subjective as they are determined by clinicians based on patient interview without incorporating scientific evidence based on brain fmri data objec tive of this project is to identify whether hdrs score and sofas scores are representative of the three antidepressants prescribed based on fmri data of 5 brain attributes data set and features dataset consists of 128 patients fmri data obtained from williams pan lab precision psychiatry and translational neuroscience stanford medicine ispot d project there are 11 features age gender education 3 antidepres sants sertraline venlafaxine escitalopram 5 fmri brain scan data from brain region amygdala insula and nucleus accumbens along with hdrs and sofas scores for all the patients our project analyzes both supervised and unsupervised methods all methods are carried out independently both for hdrs and sofas scores in supervised model hdrs sofas score is a dependent variable and models are fit using different combinations of brain data as feature variables in unsupervised model we studied 8 features 3 an tidepressants and 5 brain scan data to understand the association between medicine and brain attributes models we selected 20 patients randomly out of 128 as test set and use k fold cross validation k 10 on 108 patient data to train and validate our models the following 6 supervised and unsupervised models are considered for the project logistic regression linear regression bayesian linear regression with laplace prior factor analysis k means clustering svm auc for logistic regression based on average test set misclassification error in logistic regression to predict the sofas logistic outcome sensitivity and specificity of the roc receiver operating characteristic curve and auc area under the curve are used to understand the model performance based on moderately high value for auc we conclude that there is statistical significance between sofas logistic score and brain scan data algorithm supervised algorithm bayesian linear regression with laplace prior elastic net we choose a laplace prior for the parameter as laplace distribution is sym metric around zero and it is more strongly peaked as grows the map estimator is the sparse lasso solution this is useful to pinpoint the exact brain attribute to hdrs sofas score which will establish the func tional connectivity between antidepressants and the specific brain region because some of the s goes to zero laplace prior p 2 exp dataset s x i y i mi 1 y i tx i i epsilon i n 2 we search for a choice of that minimizes the objective function j 1 2 m i 1 tx i y i 2 the output of bayesian linear regression on a new test point x is the posterior predictive distribution p y x s p y x p s d parameter posterior p s p i p y i x i p i p y i x i d we compared elastic net with ridge regression and lasso and discussed it in the results section unsupervised algorithm factor analysis factor analysis works on small dataset where it helps to capture the corre lations in the data p x i z i p x i z i p z i z n 0 i n 0 and z are independent x z x i has the covariance noise z is the k dimensional affine subspace ofrn given the guesses for z that the e step finds m step estimates the unknown linearity and captures the covariance x i z i for the posterior distribution p x i z i we declare the convergence when the increase in likelihood l in successive iterations is smaller than the tolerance parameter we choose the maximum of l out of all obtained by k fold cv references 1 estimation of gaussian mixtures in small sample studies using l1 penaliza tion https arxiv org pdf 0901 4752 pdf 2 fmri preprocessing classification and pattern recognition https arxiv org abs 1804 10167 plots elastic net lasso ridge regression and gmm for brain data results factor analysis in factor analysis we transform the current set of variables into an equal number of variables such that each new variable is a combination of the current ones through some transformation here data gets transformed in the direction of each eigenvector and represent all the new variables or factors using the eigenvalues an eigenvalue more than 1 means that the new factor explains more variance than the original variable output of our factor loadings shows that all 11 feature variables 3 antidepres sants age gender education 5 brain scan attributes adequately represent the factor categories for this medical data set plots conclusion based on the rmse values and the plots above for supervised learning ridge regression performs the best hence hdrs and sofas scores statistically connect antidepressants to brain scan data factor analysis output also conforms to the same result that all 11 feature variables are important to represent the interdependent relationship among the feature variables to get more insight we fit gaussian mixture model using hdrs score and amygdala clus 1 2 brain data as well as hdrs score and nac clus 1 2 brain data however based on the above plot the representation seems unintelligible and requires further analysis future work we would like to enhance our gaussian mixture model with regtession and sparsity as follows instead of estimating the k for k 1 k we would estimate only the coefficients of a sparse linear combinations of the xis for all the data belonging to the same cluster using a sparsity enforcing penalty like l1 norm of the coefficients the main difficulty with such an approach might be to choose the right sample vector representing each cluster a priori we would like to use lasso as one of the potential approach to solve that problem 1 link to video presentation https vimeo com 305777481 https vimeo com 305777481	for the past 60 years the anxiety and depression medications are prescribed to patients based on the hamilton depression rating scale hdrs  healthcare professional prescribes antidepressant medications to patients based on two scores hdrs the paper regression shrinkage and selection via the lasso by robert tibshirani unsupervised model will help to understand the similarity among the brain attributes obtained from mri images and we can use this prior information to build supervised model in order to associate connections between antidepressants and 5 brain attributes our medical data is scarce so we need a method to make sure the model trained on this dataset will predict with similar accuracy on new patients we split the dataset as follows we kept 20 percent dataset aside for test and 80 percent for training and validation set  we use kfold cross validation with k 10 1  randomly split dataset s into k disjoint subsets of m k data in each mixture model in order to understand the association between hdrs sofas score and the structure of each of the brain scan data we deep dive further using mixture models here we assume f 1 f 2 f k follow gaussian  symbolically we ran different gaussian mixture models using hdrs sofas baseline score and brain data  we like to model the dataset with a joint distribution p and z are independent  x z x i has the covariance noise z is the k dimensional af f ine subspace of r n  1 given the guesses for z that the e step finds m step estimates the unknown linearity relating the x s and z s 2 in the final m step update for it captures the covariance x i z i for the posterior distribution p x i z i  3 we declare the convergence when the increase in likelihood l in successive iterations is smaller than the tolerance parameter  4 we choose the maximum of l out of all obtained by k fold cv  we choose a laplace prior for the parameter  the idea behind choosing laplace prior is that laplace distribution is symmetric around zero and it is more strongly peaked as grows  assumption 1 2 is known 2 all s are independent with laplace density 3 with this prior the map estimator is the same as the lasso solution this sparse solution is useful because we have five feature variables for brain structure and we would like to establish the functional connectivity between antidepressants and brain structure so we would like to have some of the s zero 4 we search for a choice of that minimizes the objective function5 the output of bayesian linear regression on a new test point x is the posterior predictive distribution lowest misclassification error on validation set 0 3379138 and on test set we get misclassification error of 41  hence hdrs and sofas scores statistically connect antidepressants to brain scan data to get more insight we fit gaussian mixture model using hdrs score and amygdala clus 1 2 brain data as well as hdrs score and nac clus 1 2 brain data however based on the plot below the representation seems unintelligible and requires further analysis  here data gets transformed in the direction of each eigenvector and represent all the new variables or factors using the eigenvalues  an eigenvalue more than 1 means that the new factor explains more variance than the original variable  output of our factor loadings shows that all 11 feature variables 3 antidepressants age gender education 5 brain scan attributes adequately represent the factor categories for this medical data set svm classifier is tested using three different kernels  here are the test errors for different types of kernels  linear kernel 0 3745 radial kernel 0 34125 p olynomial kernel of degree2 0 37825 usage of kernel depends on the data set  the linear kernel works best if the dataset is linearly separable but if there is non linearity then radial or polynomial kernel will produce better results  radial kernel worked the best among all three kernels  this might seem obvious because it is very likely to expect non linearity among hdrs sofas all social attributes like age gender education and 5 brain attributes in higher dimensions  auc for radial kernel 0 4840278 we would like to enhance our gaussian mixture model with regression and sparsity 
poster cs229 uplift modeling predicting incremental gains akshay kumar rishabh kumar cs229 stanford university 12 11 2018 introduction and motivation uplift modelling predictive response modelling technique which models the incremental e ect of a treatment on a target group traditional response modelling techniques just look at treatment group p purchase treatment p purchase no treatment in this project we model the uplift modelling for certain email campaign for an online retailer i e what additiona purchases an email campaign brings in for the product dataset and features hillstrom email dataset email campaign related data for 64k customers with some purchase in past 12 months overall population divided into three di erent groups of equal size received a mail featuring men s merchandize received a mail featuring women s merchandize received no advertizing mail each record contains total 9 features indicator variables indicating visit conversion and spend feature embedding algorithm used dataset has categorical features like segment history segment channel etc inspired from word embedding in nlp created one hot vector representation for each of these feature learn di erent weights for each enum value tackled the problem from two di erent perspective predictive response modelling also did ablative analysis uplift modelling predictive response modelling experimented with the following configurations logistic regression model fc followed by sigmoid activation 3 layer neural net fc followed by relu followed by fc followed by relu followed by fc followed by sigmoid activation logistic regression with bagging same as first but with bagging decision trees since many feature were based on enum values training config adam optimizer gave better results than gradient descent optimizer loss function cross entropy mini batch gradient descent with batch size of 32 trained the model for 5 epochs also performed ablative analysis to get the most influential feature uplift modelling modelling incremental ad e ectiveness problem one individual training data a user either sees an email campaign or do not see it solution two di erent models when no email campaign was seen when an email campaign was seen probability of purchase di erence of the two models predictions uplift modelling evaluation test data consists of points which either saw an email campaign or didn t see an email campaign problem no definite labels for test data a single test data can not have both seen the email campaign and not seen the email campaign as well solution bucketization group test data with similar features into a single bucket actual average uplift rate based on ground truth of labels for test data in the same bucket compare actual average uplift rate v s predicted uplift rate evaluation metrics qini curve area under uplift curve does not model negative uplift problem results and analysis predictive response modelling we split the whole data into 80 training and 20 test data since we have a class imbalance problem we have to use a metric that is not biased towards the majority class therefore we have chosen to use f score model f score train f score test lr 0 753 0 7313 bblr 0 7689 0 749 3nn 0 801 0 79 decision tree 0 7129 0 6366 roc curve for training and test data ablative analysis results evaluated importance of various features in logistic regression model by mea suring drop in accuracy by dropping individual features did this on 3nn results recency 82 40 history segment 81 29 history 80 28 mens 78 05 womens 84 56 zip code 84 71 newbie 84 62 channel 85 14 most powerful signal men s mechanize purchase in past 12 months uplift modelling results qini curve gain chart for uplift extension of roc curve conclusion we experimented with 4 di erent models for predictive response and neural network gave best f score out of 4 models decision tree overfits the training data and predict poorly on test set during uplift modelling we can clearly see from qini curve uplift increases as we increase the treatment but decrease thereafter implying the possibility of negative e ect on certain groups results of uplift modelling illustrates the possibility of achieving more incre mental e ect by targeting a smaller group acknowledgement we are thankful to prof andrew ng prof ron dror and the entire course sta of cs229	marketing strategies directed to random customers often generate huge costs and a weak response  sometime such campaigns tend to unnecessarily annoy customers and make them less likely to react to any communication  uplift modelling has wide applications in customer relationship management for up sell cross sell and retention modelling  it has also been applied to political election and personalized medicine  in this section we will talk about introduction to uplift modelling  in such techniques we will figure out which customers are most likely to buy after the campaign and they will be selected as target  unfortunately this is not optimum strategy as there are some customers who would do the sale regardless of the campaign and there are some who will be annoyed by the campaign  targeting them results in unnecessary costs  the result is a loss of a sale or even a complete loss of the customer  uplift modeling techniques provides a solution to this problem implying that we should only target customers who will buy because of the campaign i e those who are likely to buy if targeted but unlikely to buy otherwise  this measures the incremental gains of a particular treatment on a population  more precisely to measure uplift effect we divide the population into two groups control and exposed  exposed group is exposed to the treatment whereas control group is suppressed from the treatment  the difference in their responses is used to gauge the uplift effect also as said earlier uplift modelling is unique in the sense that it s only concerned with incremental effect i e p r purchase treatment p r purchase no treatment  here treatment implies watching the ad and no treatment implies not watching the ad  there has not been many machine learning papers studying similar problems but learning effectiveness of marketing campaign along with identification of target group has always been a hot topic  traditional response modelling techniques we used hillstrom email dataset 1 3 were randomly chosen to receive an email campaign featuring women merchandise 1 3 were randomly chosen to not receive an e mail campaign each record in the dataset can be described as in conversion 1 customer purchased merchandise in the following two weeks spend 1 customer dollars spent in the following two weeks we realized that dataset has categorical features like segment history segment channel etc  data preprocessing is explained in later section  we tackled this problem from two different perspective predictive response modelling and uplift modelling  response modelling tries to predict the probability of purchase or a visit or conversion in our example based on the input features  here input also includes the email campaigns  uplift modelling on the other hand models the incremental probability of purchase visit or conversion respectively based on exposure to the email campaign before discussing the algorithms used we will briefly talk about data sanitization step  all the feature were either real values or enums  for enums we decided two different approaches directly encode it as an ordinal corresponding to each of the enum  encode it as a one hot vector  when using the one hot representation each training data was encoded as a 20 dimensional vector  we ran the algorithm for 5 epochs the increment in accuracy after 5 epochs was neglible and the model was beginning to overfit  we will now discuss how to model incremental ad effectiveness  it can not both see and not see the ad to tackle this we will create two different models one for computing probabilities when a user was not exposed to an ad campaign and the other when the user was exposed to the ad campaign  both of models would output the probability of e g visit  please refer to 3 below for getting visual representation of uplift modelling with 2 models  to get the incremental effect we take the difference of the two probabilities p r visit is driven by ad p r purchase ad p r purchase no ad  evaluation also suffers from the same problem one single test data can not both see and not see the ad  roc curve is a plot of tpr true positive rate versus false positive rate fpr  note that in this case we can not compute normal logistic regression metrics like sensitivity recall f score etc  because the output variables are not 0 1 indicator variables  instead they are uplift probabilities  we will extend roc curve and define a variant of roc curve qini like curve which plots tpr and fpr for both the cases where an incremental visit happened due to ad and existing visit stopped due to ad perhaps because the ads are annoying qini curve sorts these buckets in descending order based on predicted uplift rate and plots it against number of users targeted  in this case there is a chance that some users who actually intended to visit might actually get annoyed and end up not visiting after seeing the ad  as such there can be a dip in the graph as well  see in coming sections we will focus on results of both predictive response and uplift modelling we split the whole data into a 60 20 20 split  60 for training 20 for validation and the remaining 20 for testing  overall three layer neural network achieved better results than both decision tree and logistic regressions model  we first experimented with encoding enum based feature as a single dimensional discrete feature  however the accuracy on validation set in this approach was only 60  the accuracy shot up to 85 using one hot vector representation the detailed results are in refer we also plotted roc curve for the neural net based models lr bblr and 3nn for visit  we did this by plotting tpr true positive rate against fpr false positive rate  as shown in we tried to evaluate the importance of various feature by using logistic regression by looking at drop in accuracy by selectively removing each feature  as described in section 5 3 1 we look at a variant of qini curve by discretizing the predicted uplift rates for email campaign  our experiments confirm the usefulness of uplift modeling in ad campaigns  here are important observations which we noted we experimented with 4 different models for predictive response and neural network gave best f score out of 4 models  decision tree overfits the training data and predicts poorly on test set  these user are annoyed by the email campaign and might end of not purchasing the merchandize when they could have otherwise  both of us worked together collaboratively on almost all aspects  so there is no clear distinction in the contributions  
end mark prediction eric mark martin jonathan zwiebel ericmarkmartin stanford edu jzwiebel stanford edu cs 229 machine learning autumn 2018 data introduction problem definition models and results analysis future work references the goal of our project is to be able to correctly punctuate variable length english sentences with one of three end marks periods question marks or exclamation marks denoted period qmark expoint in this poster we want to punctuate sentences drawn from the distribution of english sentences so we did not reweight our data to have equal proportions of each punctuation mark for our baseline we used proportional guessing a model that would randomly guess an end mark using proportions taken from the training set for our oracle we used human level assessment giving a number of people sentences from our test set and asking them to predict the end mark we evaluated five different classes of models logistic regression naive bayes svm random forests and fully connected neural networks and compared their performance each model was evaluated over matching 90 5 5 train dev test splits and scored using standard classification metrics we drew data from 10 of the top english books available at project gutenberg available for free use we found that all of our best models in class were able to outperform our baseline but none were able to beat our oracle as measured by macro averaged f1 score in order from best to worst model we have random forests logistic regression naive bayes and svms still we found that their differences in macro averaged f1 score were minimal and could easily have been the result of poorly tuned hyperparameters additionally we found that all of the models did a better job with precision on qmarks than on expoints this result is in line with the intuitive understanding that questions can be found by looking for specific questions words ex who will when while exclamatory sentences are often structurally similar and indistinguishable to sentences with periods we wanted to ensure that we could extract usable examples even from complex grammar structures such as dialogue and clauses additionally to maximize the number of qmark and expoint samples we needed to ensure that we could extract standalone sentences within quotations ex how are you said frankenstein our definition for a sentence was a sequence of space separated words beginning with a capital word ending with an end mark and unbroken by any single or double quotation marks additionally we added five special use tokens number comma semicolon proper unknown to handle important cases not counted in our dictionary 1 a christmas carol by charles dickens 6 a modest proposal by jonathan swift 2 pride and prejudice by jane austen 7 moby dick by herman melville 3 frankenstein by mary wollstonecraft shelley 8 dracula by bram stoker 4 a tale of two cities by charles dickens 9 alice s adventures in wonderland by lewis carol 5 heart of darkness by joseph conrad 10 the adventures of sherlock holmes by sir arthur conan doyle practically all of our mobile devices use some form of autocorrect predictive typing or diction to complete our sentences for us yet if you open your phone and type a sentence your device will almost certainly punctuate it if at all with a period whether it s talk to you later come over or you up our project creates and compares a number of models based on techniques learned in cs 229 to predict the end mark of a english sentence proportional guessing baseline an estimated distribution of labels is found using the training set predictions are draw as random samples from this distribution p q e p 1568 140 190 q 174 16 8 e 192 19 29 prec rec f1 supp p 0 81 0 81 0 81 1932 q 0 08 0 08 0 08 198 e 0 13 0 12 0 12 240 macro 0 34 0 34 0 34 2370 logistic regression 2 models a standard multiclass logistic regression was run with 20005 features we tested both binary and bag of words feature vectors and found binary feature vectors to be our strongest logistic regression model p q e p 1874 26 32 q 116 72 10 e 166 13 61 prec rec f1 supp p 0 87 0 97 0 92 1932 q 0 65 0 36 0 47 198 e 0 59 0 25 0 36 240 macro 0 70 0 53 0 58 2370l2 loss 100 iterations trained with stochastic average gradient naive bayes 3 models a standard naive bayes model designed for multiclass applications we tested multinomial with bag of words bernoulli with binary vectors and gaussian with bag of words distributions and found bernoulli to perform the best p q e p 1606 23 303 q 76 47 75 e 73 4 163 prec rec f1 supp p 0 92 0 83 0 87 1932 q 0 64 0 24 0 35 198 e 0 30 0 68 0 42 240 macro 0 62 0 58 0 54 2370bernoulli distribution uniform prior support vector machine 2 models a standard svm using a radial basis function rbf kernel we tried both a support vector classifier svc and a stochastic gradient descent classifier sgd we also attempted a tfidf vectorizer for both types of svms we found sgd with a bag of words to work best p q e p 1903 17 12 q 128 69 1 e 182 16 42 prec rec f1 supp p 0 86 0 98 0 92 1932 q 0 68 0 35 0 46 198 e 0 76 0 17 0 28 240 macro 0 77 0 50 0 55 2370random forests 4 models best a standard random forest model aggregating 100 decision trees trained on bag of words feature vectors each tree was allowed to train until each leaf was pure we also attempted stopping the constituent trees at depths 5 10 and 50 but the model model highly over classified periods as shown by the cross validation metrics bagging prevented the model from overfitting even without a limit on tree depth p q e p 1909 11 22 q 133 56 9 e 170 11 62 prec rec f1 supp p 0 87 0 98 0 92 1932 q 0 79 0 34 0 47 198 e 0 68 0 26 0 37 240 macro 0 78 0 53 0 59 2370 hinge loss linear kernel neural network using the remaining time for this project we plan on developing a fully connected deep neural network with an extended feature vector that contains a one hot for the first word one hot for the last word a binary vector for words that start clauses and a binary vector for the length of the sentence we were quite impressed with our models and would like to experiment with deep learning techniques particularly sequence based models some additional models we would like to try fully connected neural network with expanded feature vector rnn with one hot vector rnn with word vector from existing embeddings we would also like to try a model that uses surrounding sentences as context to assist in end mark prediction for this we would need a bidirectional rnn gini loss 100 estimators trained to unlimited depth human level oracle over a reduced test set we asked humans to classify sentences using the same tokenization scheme as given to the learned models p q e p 151 1 10 q 3 7 1 e 12 2 5 prec rec f1 supp p 0 91 0 93 0 92 162 q 0 70 0 64 0 67 11 e 0 31 0 26 0 29 19 macro 0 64 0 61 0 62 192 baron d shriberg e stolcke a 2002 automatic punctuation and disfluency detection in multi party meetings using prosodic and lexical cues in seventh international conference on spoken language processing christensen h gotoh y renals s 2001 punctuation annotation using statistical prosody models in isca tutorial and research workshop itrw on prosody in speech recognition and understanding maas a l daly r e pham p t huang d ng a y potts c 2011 june learning word vectors for sentiment analysis in proceedings of the 49th annual meeting of the association for computational linguistics human language technologies volume 1 pp 142 150 association for computational linguistics makhoul j baron a bulyko i nguyen l ramshaw l stallard d xiang b 2005 the effects of speech recognition and punctuation on information extraction performance in ninth european conference on speech communication and technology ueffing n bisani m vozila p 2013 improved models for automatic punctuation prediction for spoken and written text in interspeech pp 3097 3101 i came i saw i conquered do you have number pickles proper stop now	our project developed models for the task of assigning one of three end marksperiods question marks or exclamation points to variable length english sentences  our models were trained with a labeled data set extracted from english novels and scored using labeled examples from the same set  we tested logistic regression na ve bayes random forests and svms and found them all to be more effective than our baseline proportional guessing and less effective than our oracle human level  we found that random forests had the strongest performance of the developed models although the similarities in performance make it impossible to conclude that they outperform the other models for this task  practically all of our mobile devices use some form of autocorrect predictive typing or dictation to complete our sentences for us  yet if you open your phone and type a sentence your device will almost certainly punctuate it if at all with a period whether it s talk to you later  come over  or you up  we present results and analyses of our experimentation with several different methods for predicting the terminal punctuation of a sentence the terminal punctuation or end mark of a sentence provides important semantic and syntactical information about a sentence  in the question what is that  for instance we don t think of what as some subject that is existing is as some object that  without the knowledge that this is a question however it can be much more difficult for a computer to determine this information  the question of end marks also poses a simple problem given a sentence classify it as ending with either a period question mark or exclamation point  we want to punctuate sentences drawn from the distribution of english sentences so we did not re weight our data to have equal proportions of each punctuation mark deep learning for punctuation restoration in medical reports by wael salloum greg finley erik edwards mark miller and david suendermann oeft it is clear that the state of the art for a problem like this is an rnn  it is also clear from reading these papers that this is a difficult problem even for humans given both the complexity and subjectivity of assigning punctuation  almost all of the paper ran into the issue that text corpuses extracted from novels and online sources are written by human authors who often deviate from standard punctuation rules  two identical text segments may be punctuated differently when written by different authors  this in turn suggests that algorithms may do better when trained on a single source which is promising for applications such as smart keyboards and speech to text engines  our goal was to be able to correctly punctuate variable length english sentences with one of three end marks periods question marks or exclamation marks denoted period qmark expoint in this poster  additionally to maximize the number of question mark and exclamation point samples we needed to ensure that we could extract standalone sentences within quotations eg how are you  our definition for a sentence was a sequence of space separated words starting with a capital word ending with an end mark and unbroken by any single or double quotation marks  because this is a relatively unexplored problem there isn t a well established baseline for a na ve algorithm  random guessing would given an unfairly low benchmark as the vast majority of the data belongs to the period class we started by looking at the results of guessing all periods one metric it scored very poorly on however was macro averaged even weighted f1 score macro f1  the macro average is helpful as the fact that it evenly weights each class regardless of number of examples exacts a significant penalty on models that over predict the period class  using f1 score ensures that we are taking both precision and accuracy into account the 0 30 macro f1 from guessing all periods doesn t serve as a good benchmark to evaluate our models against  to set a lower bound macro f1 we used proportional guessing we evaluated five different classes of models logistic regression na ve bayes svm random forests and fully connected neural networks and compared their performance  each model was evaluated over matching 90 5 5 train dev test splits and scored using standard classification metrics  a standard multiclass logistic regression was run with 20005 features  see we trained two multinomial na ve bayes models one trained on bag of words vectors and the other on term frequency inverse document frequency tfidf transformed bag of word vectors one bernoulli model on binary vectors and one gaussian model on bag of word vectors  of the four the bernoulli model achieved the highest macro f1  see we started by training a random forest with aggregation by averaging of 100 decision tree estimators using gini loss on bag of words vectors with a max tree depth of 5  while this model trained very quickly it predicted that everything was a period  we increased the max depth to 5 10 and then 50 to try to get better classifications but even with a depth of 50 the forest still only predicted periods  finally we tried allowing the constituent trees to branch until total leaf purity  on the contrary the bagging kept the model from overfitting and it performed very well on macro f1 score  overall this random forest was the highest performing model statistical model we tested on the test set each of the support vector machines svms were trained on bag of words vectors  we also tested a stochastic gradient descent sgd classifier with a linear kernel  we found that this last model performed best in class for svms though the random forest classifier out performed the other statistical models on the test set it did so by only a few percentage points of macro average f1 score  we see this in the confusion matrix for our oracle we also found that logistic regression and na ve bayes seemed to overpredict period while random forests and svms were more willing to predict qmarks and expoints  eric worked on the na ve bayes random forest na ve bayes and oracle models  he also set up the development environment and preprocessing scripts  jonathan worked on the logistic regression and baseline models  he also worked on the vectorizer and tokenizer  we would like to acknowledge professor ng and the cs 229 course staff for teaching us the material necessary to create these models and for looking over providing feedback on our work  we would also like to acknowledge project gutenberg for providing us with a completely free to use corpus of text and google for providing the free to use trillion word corpus  
generating regulatory sequence to produce target expression computer science nic fishman 1 georgi marinov 1 2 anshul kundaje1 2 1department of computer science stanford university 2department of genetics stanford university abstract predicting prediction resultsmodels the abundance of high quality gene expression data afforded by the recent development of massively parallel reporter assays mpra has created an abundance of data for developing a deeper understanding of transcription factor tf binding here we show that convolutional neural networks are capable of learning the motifs that underlie tf binding and predicting expression using these motifs at various amino acid concentrations aa using this result we develop a generative adversarial network that can build segments of regulatory sequence to produce specified gene expression at varying aa we find that a combination of the mse between the predicted expression and the target expression and the standard wgan gp loss give the best results for learning to produce sequence given target expression levels there is still more work to be done especially in tuning the combination of the losses and in trying to resolve the diminishing return on longer training that is found for all values of lambda when generating sequence generation results predicting expression targeted sequence generation predicting protein expression from regulatory sequences given a target expression level generate sequence predicted to produce that expression level data and data processing the data comes from mpra experiments 2 which produce the dna sequence acgt and corresponding protein expression each row is a sequence and the corresponding mean expression under several conditions a t c g t 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 the only feature work is to one hot encode the alphabetic sequences 1 predicting expression targeted sequence generation evaluating generated sequences random architecture search train several regressors select best based on integrated gradients ratio of motif importance over total importance wgan gp loss 3 regressor loss overall loss loo accuracy 1 nn in learned feature space 4 predicting expression via ensemble of regressors motif identification and frequency analysis results overview predicting expression genetic background figure from 1 mpra regulatory sequence instructions for how much protein to make encoded in discrete strings of basepairs called motifs complexity comes from motif interactions which depend on number and position of motifs mpra allows testing expression of thousands of regulatory sequences at once combinatorially combining motifs in regulatory sequence and get associated expression allows decoding of the lexical grammar governing expression 2 predicting expression targeted sequence generation trained for 1000 epochs with early stopping trained for 5000 epochs motif importance scoring discussion the regression model is definitely successfully learning to identify motifs which is a great sign the generated sequence from the wgan is relatively similar to the real distribution and backpropagating on the predicted expression leads to much better accuracy on achieving target loss this fully validates the pipeline in training the gan there is an issue where training stops leading to improvements in the 1 nn and motif count metrics after the first few thousand epochs it would be nice to try to understand why this happens there is a tradeoff in gan training between hitting the given target expression and producing realistic sequence it would be good to try annealing the lambda term to see if this tradeoff can be resolved citations 1 m rajiv et al deciphering regulatory dna sequences and noncoding genetic variants using neural network models of massively parallel reporter assays biorxiv preprint biorxiv 393926 2018 2 d van dijk et al large scale mapping of gene regulatory logic reveals context dependent repression by transcriptional activators genome research 3 gulrajani ishaan et al improved training of wasserstein gans advances in neural information processing systems 2017 4 borji ali pros and cons of gan evaluation measures arxiv preprint arxiv 1802 03446 2018 replicated from 2 https files slack com files pri t7sav7lad fawjj54te 3 m2 png	we then build on our cnn models and develop generative adversarial networks gans that can produce novel regulatory sequences with particular gene expression activities  initial work in this area has shown the ability of generative deep learning methods to generate dna sequence de novo here we present a novel framework for end to end generation of sequences to minimize any developer defined loss function on a set of dna sequences with associated scores for some property s of interest  throughout this paper we examine this pipeline in the con  we used two mpra datasets in this study  the first one 11 was carried out in the budding yeast saccharomyces cerevisiae and includes testing the activity of 5 000 synthetic sequences containing defined numbers of tfbss for factors known to be important in regulating gene expression upon changes in nutrient availability  regulatory activity was measured under a range of six different increasing amino acid concentrations  we also log transformed the regression targets so we could use the relu non linearity as the output of our regression networks  the deeplift framework 9 was used to assign feature importance scores of mpra prediction models to each nucleotide in input sequences  formally this loss is calculated as 1 where we update the discriminator by maximizing with respect to l d  for the generator we feed the generated sequence into both the discriminator and the regressor from the previous phase and the generator is updated by minimizing with respect to a weighted average of the two losses  these losses are weighed with a tunable term as follows 3 5  evaluating generated sequences the best method for evaluating the quality of a generated distribution given an actual distribution is to do calculate the loo leave one out accuracy of a 1 nn algorithm in a learned feature space  throughout training we used the default hyperparameters for the adam optimizer the default hyperparameters for the wgan gp algorithm  after random architecture search on the yeast mpra dataset we arrived at an optimal model with a test mse of 0 0266 to confirm that the neural net s models are not just learning to predict expression but are also identifying the key regulatory sequences driving we applied deeplift featue importance scoring 9 on the input sequences  the relevant motif indeed tend to be the most highly scoring sequences with two examples shown in optimal real value generated 1 nn loo 0 5 0 89 predicted expression mse 0 0 64324 5176 ation based on the promise such approaches have shown in the language generation literature  
