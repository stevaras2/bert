<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/lopez/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5-SNAPSHOT" ident="GROBID" when="2019-03-20T09:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PROBLEM STATEMENT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">PROBLEM STATEMENT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The corporate world deals with task management in a variety of ways all having some form of triaging process to correctly assign tickets to developers. Automation of this task has proven elusive with less than 60% accuracy of latest ML solutions even for Web and SaaS companies that handle high volumes of tickets in the form of exceptions, support requests, user-reported bugs, and crash reports. Effective automation is essential to improve productivity and obviate the tedious work of manually triaging tickets.</p><p>Project aims to reduce this overhead by deploying a deep neural network classifier to assign tickets to a developer. The DNN model is trained by using the final assignee listed in the JIRA ticket as the label and predicts a previously seen developer on new tickets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DATASET &amp; FEATURES</head><p>The project utilized the generated jumble.expium.com dataset for developing the algorithm and then applies the architecture to train and test on the private LinkedIn Foundation team support dataset. The implementation uses a bag of words multinomial event model to vectorize the JIRA ticket. The text components of JIRA ticket namely: subject, body and comments, are concatenated and featurized with a frequency threshold of &gt;= 5.</p><p>A JIRA ticket has the following JSON structure: { "summary": "Update success. 3.0 USB Card", "description": "OR memory.dmp folder, ....", "priority": "Minor", "reporter": "chantal.colman", "labels": [ "Communication" ], "worklogs": [], "status": "Open", "issueType": "Epic", "created": "2018-09-13T14:22:15-07:00", "updated": "2018-11-16T16:00:00-08:00", " Crashes only hardware problems. Even something or removing the buzz is supposed to do anything.", "author": "sarah.clark", "created": "2018-10-11T17:00:00-07:00" }, "customFieldValues": [ { "fieldName": "Epic Name", "fieldType": "com.opper.jira:gh-epic-label", "value": "Update success. 3.0 USB Card" } ],</p><p>"history": [ { "author": "chantal.colman", "created": "2018-11-06T16:00:00-08:00", "items": [ { "fieldType": "jira", "field": "status", "from": 1, "fromString": "Open", "to": 3, "toString": " </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural Network Design</head><p>The Neural network parameters were chosen by searching through parameters using the Expium dataset. The dimensions investigated in the images on the left are: The architecture chosen is: a 3 wide, 16 high, ReLu activated network with a learning rate of 0.005 and 1,000 backpropagation iterations.</p><formula xml:id="formula_0">•</formula><p>• Hidden layers: <ref type="bibr">[8,</ref><ref type="bibr">16,</ref><ref type="bibr">32]</ref> • Depth : <ref type="bibr">[3,</ref><ref type="bibr">5]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FEATURES, MODELS &amp; RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features and Vectorization</head><p>The input data described in the data set section was parsed into a bag of words, multinomial event model representation. The words are assumed to be independent and chosen with separate distributions at create time. During the preprocessing step we capture the list of frequent words at a threshold of &gt;= 5 when building multinomial vector. While developing our solution we make the following assumptions: /&gt; A ticket ! " in a the set of tickets {! $ , … , ! |(| } assigned to a developer {* $ , … , * |+| } is generated by a unique distribution modelled by ,: -t " ,) = ∑ 23$</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>|4|</head><p>-t " d 2 , , -t 2 ,)</p><p>/&gt; Tickets are composed of independently and identically distributed words chosen at random (naive Bayes assumption) and from a multinomial distribution: -! " * 2 , , = ∏ "3$ 7 8 w : |! 2 , , ;&lt;&lt;=&gt;?7&lt;@</p><p>The labels are developers we have seen before represented as integers. We then create a matrix a multinomial input vector and an integer class label representing assignee to train and test. NB and SVM are not investigated by this paper but have been added only for reference. The shape of the datasets after vectorizing were: LinkedIn m x n = &lt;559, 2936&gt; and Expium dataset m x n = &lt;5435, 1970&gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Model</head><p>We utilize a DNN to approximate , in order to predict a designated assignee. As per the neural network design section, we use a 3x16 tanh neural network with a 0.005 learning rate and 1,000 backprop iterations. The final label (developer) selection is done via softmax followed by a cross entropy loss for backpropagation. The network is trained and tested using a kFold cross validation similar to the work by Bettenburg et al.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Blow are the results of the experiments conducted on March 2018 tickets, larger compute/optimization is required to run full dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remarks</head><p>-The highly non-linear nature of DNN has allowed almost perfect 5Fold mean accuracy on over 3,000 LinkedIn tickets and 5,500 generated tickets, that same benefit has heavily overfit the data as observed by the dismal &lt;7% test prediction accuracy.</p><p>-Computational challenges with vectorizing text has prevented us from significantly scaling the number of trained samples, working on a batching solution to count words efficiently, store to a datastore and fetch on batched training and prediction stages.</p><p>-A developer that has never been assigned a ticket will not be considered during prediction, this is normal in triaging.</p><p>-The SVM classifier was only run with splitting up data once instead of the bagging like kFold cross validation completed for the others</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion Remarks</head><p>Conclusions: The initial results of our experiments resulted in poor accuracy on the test set. The DNN overfit the small dataset and we hope that after running on full set (~9x more tickets) the test accuracy will dramatically increase, currently experiments have only been applied to a set of 5,000 tickets Future Work: The most important work is to implement dropout to fix overfit. Additional features from JIRA ticket fields leaves a lot of room for improvement • Take advantage of NLP concepts to improve test accuracy, implementing stemming, lemmatization, Word2Vec and word embeddings.</p><p>• Utilize additional features such as: watchers, labels, reporter, hashed exceptions and so on.</p><p>• Make the vectorization process cacheable to handle entire dataset. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Murphy, G., and D. Cubranic. "Automatic bug triage using text categorization." Proceedings of the Sixteenth International Conference on Software Engineering &amp; Knowledge Engineering. 2004. [2] -Srivastava, Nitish, et al. "Dropout: a simple way to prevent neural networks from overfitting." The Journal of Machine Learning Research 15.1 (2014): 1929-1958. APA [3] -Domingos, P., Pazzani, M., 1996. Beyond Independence: Conditions for the Optimality of the Simple Bayesian Classifier. In: Machine Learning, Morgan Kaufmann, pp. 105-112. [4] -Bettenburg, N., Premraj, R., Zimmermann, T., Kim, S., 2008. Duplicate Bug Reports Considered Harmful. . .Really? In: ICSM. [5] -WUYUNTANA, D. and WANG, S. (2018). Distributed Representations of Mongolian Words and Its Efficient Estimation. DEStech Transactions on Computer Science and Engineering, (iceit).</figDesc><table>In Progress" 
} 
] 
}, 
] 

References 

[1] </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>* The architecture is chosen due to the fast and smooth descent and relative simplicity of the network compared to the higher train but lower test accuracy architectures below.</figDesc><table>Model 

Linkedin Dataset (559/43483) 
Expium Generated Dataset (~5500) 

Train Accuracy 
Test Accuracy 
Train Accuracy 
Test Performance 

SVM Classifier 
(linear kernel, hinge loss) 

1.0 w/ 70% train* 
0.29166 w/o 30% test 
1.0 w/ 70% train* 
0.223175 w/ 30% test 

Naive Bayes Classifier 
5Fold Mean: 0.65251 
5Fold Mean: 0.11613 
5Fold Mean: 0.77097 5Fold Mean: 0.208463 

Deep Neural Network Classifier 5Fold Mean: .996420 
5Fold Mean: 0.058976833 2Fold Mean : 1.0 
2Fold Mean: 0.071940617 </table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
