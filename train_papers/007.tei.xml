<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/lopez/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5-SNAPSHOT" ident="GROBID" when="2019-03-19T09:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Have You Met The 1? A Machine&apos;s Approach to Human Relationships</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Lou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Yang</surname></persName>
						</author>
						<title level="a" type="main">Have You Met The 1? A Machine&apos;s Approach to Human Relationships</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>We have all pondered about the same thing in a relationship: is he/she the right one for me? Countless articles, quizzes, suggestions, tips, and consultants out there are trying to answer this question for us. They bring out big words to make it logical; they tell us to mind the differences, but also note that some differences matter and some do not; they place their own arbitrary weights on terms such as ambition, core values, intelligence, emotional intelligence, spiritual beliefs, etc. It makes us wonder: if we are already talking about logic in romance, why don't we take a step further? What if, instead of reading these sources and still remaining puzzled about the relationship, we can tell you with a high confidence level exactly how likely you are going to be with your significant other for the rest of your life with the help of a machine learning algorithm? Thanks to How Couples Meet and Stay Together dataset (HCMST) by Rosenfeld, Michael J., Reuben J. Thomas, and Maja Falcon, it is now possible, based on your everyday habit, your usage of Internet, your marriage status, and other miscellaneous information about your life, to predict if you will stay with him/her for at least the next few years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>The HCMST dataset has been utilized in various ways concerning different social study fields, yet most of the papers are based on a few arbitrarily chosen measurements, and no MachineLearning-related work is found. Early work entails the Internet-related data to argue increasing Internet coverage has increased chance of meeting partners for GLB (Gay, Lesbian, Bisexual) individuals, and Internet as a social intermediary has partly replaced traditional dating spaces <ref type="bibr" target="#b1">[2]</ref>. On a similar note, data on the respondents' sexualities and their relationship longevity has been used to support the assertion that same-sex couples' break-up rates are comparable to heterosexual couples' <ref type="bibr" target="#b3">[3]</ref>.</p><p>More recent working papers discuss the topic of relationship stability as a whole, following the main motives behind the HCMST project. However, restricted by the limits of human, these papers tend to focus on selected areas of the whole dataset, drawing conclusions from partial observations and features, thus indirectly putting arbitrary weights on the subject matters. One such example chooses to investigate the relation between relative earnings in the household and the relationship stability <ref type="bibr" target="#b4">[4]</ref>, while the others select subjects of gender and marital status in order to explore the break-up rates of heterosexual couples <ref type="bibr" target="#b5">[5]</ref>.</p><p>Potentially due to the novelty of the data and the fact that it has only finished its budgeted five waves very recently in 2015, until now there exists minimal efforts to attempt bringing all parameters together under the roof of machine learning algorithms. We are here to pioneer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset and</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. Data Elimination</head><p>Since this project aims at predicting the future relationship status based on current info, we believe that at this point only the data collected in the first wave is relevant to our purpose. We have hence dropped data collected from wave 2 to wave 5, keeping only the survey results on their relationship status at each milestone.</p><p>The survey assumed that some couples who didn't respond to follow-up surveys still stayed together. For the sake of precision, however, we only kept data of respondents with partners in the beginning of the timespan who continuously responded to the surveys in following periods until wave 5 or breakup. Based on the results from the 4 follow-up surveys, we generated feature "final_relationship_status" (Boolean) to indicate the final status of couples after 6 years. After deleting all the redundant features and observations, we are left with 1569 respondents with 269 features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. Preprocessing Non-Standard Data Values</head><p>The initial data contains many features with a substantial amount of missing values. While some bear minimal relevance to our goal (e.g. gender of the 15th member in your family) and can be dropped without significant impact, other missing values are indicative of important information and dropping them will result in high bias. Therefore, for those question answers that have already been processed, we only kept their corresponding feature. For example, for question 34 "how would you describe the quality of your relationship?", we dropped feature "q34" and kept the corresponding processed feature "relationship_quality". This will prevent problems generated by singular matrix in future prediction models. Secondly, some of the missing values are results of branching questions. For example, if question 12b is only required for people who answered "yes" for question 12a, then feature "q12b" will contain lots of missing values. For these features, we integrated them into the main branch question by generating more categorical classes in features like "q12a". Thirdly, we dropped clearly unrelated features with high level of missing values or nonnumerical and non-categorical values. These operations leave us with 148 features to work with.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. Feature Selection</head><p>Considering that the size of our datasets after processing stays around 1600, we decided to include fewer features to avoid potential overfitting. Therefore, we have implemented the forward-based sequential feature selection based on Logistic Regression model with crossvalidation of 10-folds. Features were selected based on misclassification rate using Logistic Regression model and feature selection terminates after the misclassification error no longer improves. This leaves us with 47 features, with 10 most important features: Any output of logistic regression is in the range {0,1}, where output smaller than 0.5 will be categorized as 0 and the rest categorized as 1. This method generates misclassification error of 11.1% for train set and 12.39% for test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. Support Vector Machine</head><p>We implemented the the SVM without kernel:</p><formula xml:id="formula_0">1 2 B + + 6 +01 + B + + ≥ 1 − + + ≥ 0, = 1, … ,</formula><p>Then we integrated the Gaussian kernel into SVM, which didn't improve the result. Observe that the train error using Gaussian kernel is substantially smaller than normal SVM without kernel. Because the number of observations is relatively small, we believe that using kernel would further complicate the method and therefore result in overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. Decision Tree</head><p>Many of the survey questions have sequential correlation with each other and some features' existence are entirely based on others' (i.e., only respondents who have answered "yes" to question "have your religion changed since 16?" will be asked to answer "what is your religion at 16?"), therefore we believe that the decision tree model would be a proper representation of the set of if-then choices and would replicate the design logic behind the survey.</p><p>We generated the top-down binary decision tree by examining the optimal statistical improvement brought about by each feature at each split. To capture the optimal improvement, we ordered both the categorical and the continuous attributes from the smallest to the largest and measured the improvement in misclassification error by dividing at each consecutive pairs. When a missing value is encountered, we used surrogate split because many alternative features with high variance can be found.</p><p>In actual prediction, we first generated the binary decision tree with minimum branch size of 10 observations and unlimited depth, and this results in misclassification error of 4.07% for train set and 22.67% for test set after 10-fold crossvalidation. As certain level of overfitting is shown, we decided to pre-prune the tree by adding the maximum number of splits in our decision tree. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. Naive Bayes</head><p>Our final dataset is left with fewer than ten numeric values: the distance from the respondent home to the current home, how long ago they first met and how long ago the respondent first lives together with partner, etc. In order to apply Naive Bayes model, we have discretized the data by converting these numeric features into several categorical classes. Because the assumption of the Naive Bayes is that every conditional probability is independent of each other, we also calculated the covariance matrix between all the features and dropped features with a covariance over 0.3. We then applied the Naive Bayes model trying to maximize</p><formula xml:id="formula_1">= ( + , + ) = / +01 ( ( 4 + | )) ( + ) 6 U 401 / +01</formula><p>Because some of the survey questions have relatively small amount of responses or unbalanced results, we also added Laplace smoothing to ensure at least one data point per feature per class. Using the obtained probabilities W|X01 , W|X0Y , X we then cross validated by partitioning the data in 10-folds and obtained the averaged misclassification error 22.27% for train set and 23.19% for test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Analysis</head><p>At this step, we have graphed the test error and train error regarding each method after crossvalidation. As the graph have shown, the errors generated by all the methods range from 13% to around 24%.</p><p>As the graphs have shown, generally logistic regression produces the best result while Naive Bayes performs the worst. Decision tree and support vector machine have very close performance in both test and train dataset. One possible reason that Naive Bayes doesn't generate good precision is that some of the input features are not completely mutually independent. Despite that we have used sequential feature selection and later removed features with correlation greater than 0.3 when using NB, some features left are still vaguely related with each other and this violates the basic independent assumption of Naive Bayes. The correlation between variables, however, helped to boost the precision in Decision Tree Model because the missing values can be replaced with their correlated alternatives using surrogate splits.</p><p>We believe that one explanation for the logistic regression to generate better result than decision tree is the high dimensionality of the dataset compared to the number of observations. As trees always tends to overfit in presence of high dimensionality since it has high freedom degree, we had to limit the number of splits to prevent overfitting. However, some important information are lost in this process and bias is sacrificed to obtain lower variance. Logistic Regression, though very simple, does draw information from the basis of the entire set of features and therefore could perform better than tree-based model.</p><p>Note that the false positive rate is almost about 3 times as high as the false negative rate in logistic regression, SVM and decision tree. This is partially because the initial dataset is imbalanced with the ratio between positive data and negative data being 2:1. We then decided to rebalance the dataset by adding a weight vector to assign more weights to negative classes. The table below shows false positive and false negative results generated using the re-balanced dataset. Notice that the the precision rate doesn't stay stable for all the four models and false-positive and falsenegative rates are more balanced than before for logistic regression, SVM and decision tree. However, the false-negative rate from NB is still substantially higher than the other three, meaning that Naive Bayes model is very pessimistic about the couple's relationship: it tends the believe a couple would break up even indeed they will very likely not. We also tried to apply the logistic regression and decision model to predict the data points that have been deleted from our dataset due to (1) missing values; (2) unknown labels. These observations were first removed from our samples because the respondents stopped to respond to the survey from the second, third or fourth round. Interestingly, the pattern demonstrated by our prediction shows that the earlier the couple stops to respond to the survey, the more likely they will get a "0" in prediction. In other words, for couples that stop to respond to the survey since round 2, our model predicts that very large portion of them will break up in 6 years. This verifies our guess: people don't just quit in the middle of the survey for random reasons; the absence of a couple's voices in later surveys might already indicates a deceased romance, and the pain and embarrassment to admit this usually makes people shun away.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>While our models express &gt;10% test errors, it is rather reasonable given that relationships are still indeed based on one of the most complex systems in the known universe: human mind. Our results provide insights of what otherwise remains mysterious and unquantified, and will potentially help sociologists and everyday individuals alike.</p><p>In process of fitting samples through the models, we have also discovered that adding more samples don't always result in higher accuracy. This is likely rooted in the nature of relationships and romance: it's the surprise and unexpected turn of events that highlight their beauty. When consensus deems long-distance relationships hard to maintain, there are always outliers who prove it wrong, and same goes for other difficulties in love. For this exact reason, while increasing data size from additional surveys will theoretically improve our prediction, we believe that it may not be necessary.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Feature Selections I. Initial Data HCMST conducted 5 sequential rounds of surveys in 2009, 2010, 2011, 2013 and 2015,respectively. The initial dataset consists of two parts: the respondents' answers to the original questions (e.g., recorded answers to question "How old are you?"), and features generated from the collected raw data (e.g., the categorization of cases based upon age division). The dataset includes 4002 respondents with 370 features, supplemented by additional 62 features from wave 4 survey and 78 features from wave 5 survey results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>After trial and error, 13 splits generate the lowest test error and achieves a relative good balance between the test set and the train set, with misclassification error of 14.96% and 17.75%, respectively. For the convenience of representation, we have pruned the tree to have at most 13 splits, as shown below.</figDesc><table>Despite decision tree helps to visualize the 
primary features at each split, surrogate variables 
may never be used in actual splitting. Therefore 

we also calculated the variable importance trying 
to capture all the highly important variables by 
measuring the improvement attributable to each 
variable either as a primary or a surrogate splitter. 
Below is a graph showing the importance of 
features with non-zero importance value. Note 
that the 2nd to the 5th variable doesn't appear in 
splitting at all but might serve as surrogates for 
"Coresident". Also, despite that there are over 40 
features, the variable importance table shows that 
only very few of them are decisive features and 
actually have an effect in splitting. 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">How Couples Meet and Stay Together</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Reuben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Falcon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">3</biblScope>
			<pubPlace>Stanford, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Stanford University Libraries</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Reuben</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Searching for a Mate: The Rise of the Internet as a Social Intermediary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Sociological Review</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="523" to="570" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Couple Longevity in the Era of Same-Sex Marriage in the United States</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Marriage and Family</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="905" to="918" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Earnings Equality and Relationship Stability for SameSex and Heterosexual Couples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Weisshaar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Working paper</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Who wants the Breakup? Gender and Breakup in Heterosexual Couples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Rosenfeld</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Working paper</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
