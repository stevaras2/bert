<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/lopez/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5-SNAPSHOT" ident="GROBID" when="2019-03-19T09:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Analysis of Code Submissions in Competitive Programming Contests</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenli</forename><surname>Looi</surname></persName>
							<email>wlooi@stanford.edu</email>
						</author>
						<title level="a" type="main">Analysis of Code Submissions in Competitive Programming Contests</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Algorithmic programming contests provide an opportunity to gain insights into coding techiques. This paper analyzes contest submissions on Codeforces, a popular competitive programming website where participants solve about 5 to 10 algorithmic problems in a typical contest. We attempt to predict a user's rank and country based on a single C++ source code submission. Features were generated by running source code through the Clang C++ compiler and extracting bigrams from the tokens and traversal of the abstract syntax tree (AST). Out of several models, the neural network model achieved the highest accuracies of 77.2% in predicting rank (within one rank) and 72.5% in predicting country. Despite not achieving the highest accuracy, the GDA model was easier to interpret and allowed us to find specific differences in coding styles between programmers of different ranks and countries.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Codeforces <ref type="bibr" target="#b0">[1]</ref> is one of many competitive programming platforms that provide an opportunity to gain insights into coding techniques. In a contest, participants solve about 5 to 10 well-defined algorithmic programming problems by writing short stand-alone solutions in a programming language such as C++, Java, or Python. Each solution is generally written by only one person (user). It is known whether the solution passed or failed. In addition, the user's skill level (rating/rank) and declared country are known. Most problems on Codeforces have hundreds or thousands of passing submissions. Unlike many other platforms, all submissions on Codeforces are publicly viewable, making it an ideal candidate for analysis.</p><p>On Codeforces, users are assigned a numerical rating based on their performance in past contests. Users are then assigned one of ten ranks based on their rating, ranging from Newbie to Legendary Grandmaster. These ranks are shown in the Data Set section in <ref type="table" target="#tab_0">Table II</ref>.</p><p>The goal of this project is to predict a user's rank (within one rank) and country based solely on a single passing source code submission. As well, some interpretation of the learned models is done to find differences in coding styles between skill levels and countries. Since only passing submissions are considered, predictions are based only on coding style and not whether the code works or not (all code works).</p><p>Analysis from this project will not only highlight the coding techniques of competitive programmers, but may also be relevant for code written in industry or academia. While code written in programming contests differs from real-world code, some coding best practices may apply to both, and the analysis techniques used here may also be applicable to other code bases. Code from programming contests, however, is easier to analyze than most code for the reasons described above (e.g. written by exactly one person).</p><p>II. RELATED WORK <ref type="bibr" target="#b1">Allamanis et al. (2018)</ref>  <ref type="bibr" target="#b1">[2]</ref> is a survey of machine learning techniques used to analyze source code. There has been significant past work on applying techniques from natural language processing (NLP) and analyzing the abstract syntax tree (AST), both of which are techniques used in this project. Machine learning has been applied to many problems, such as autocompletion, inferring coding conventions, finding defects, translating code, and program synthesis. <ref type="bibr" target="#b2">Burrows et al. (2007)</ref>  <ref type="bibr" target="#b2">[3]</ref> attempted to determine the author of C source code by finding N-grams in the tokens, similar to what is done here. They classified a code sample by finding the closest code sample in the corpus as measured by some similarity measure. On a collection of 1640 documents written by 100 authors, they were able to identify the author with 67% accuracy using 6-grams. A disadvantage of this approach is that searching through the entire training corpus may have scalability issues as it grows.</p><p>A study by <ref type="bibr" target="#b3">Ugurel et al. (2002)</ref>  <ref type="bibr" target="#b3">[4]</ref> attempted to classify C/C++ source code archives into various categories, like Database, Network, and Word Processor. For features, they used single tokens from the source code as well as bigrams and lexical phrases from README files and comments. A support vector machine (SVM) was then trained to perform the classification. They achieved an accuracy of around 40-70% depending on the features and data set.</p><p>More recently, recurrent neural networks (RNNs), such as long short-term memory (LSTM) based networks, have been used to classify source code. <ref type="bibr" target="#b4">Alsulami et al. (2017)</ref>  <ref type="bibr" target="#b4">[5]</ref> used an LSTM-based network to determine the author of Python and C++ source code. They fed a traversal of the AST into the RNN, which inspired the traversal-based method used in this paper. The RNN included an embedding layer to convert AST nodes into a fixed length vector. Their best-performing model, a bidirectional LSTM, achieved 85% accuracy on a C++ dataset with 10 authors and 88.86% accuracy on a Python dataset with 70 authors.</p><p>Techniques similar to doc2vec (Le et al., 2014) <ref type="bibr" target="#b5">[6]</ref>, where entire documents are converted to an embedding space, have also been used to classify programs. <ref type="bibr" target="#b6">Piech et al. (2015)</ref>  <ref type="bibr" target="#b6">[7]</ref> encoded programs as a linear map between a precondition space and postcondition space. They used the linear maps with an RNN to predict feedback that an educator would provide for a piece of code, achieving 90% precision with recall of 13-48% depending on the coding problem.</p><p>Recurrent neural networks may not be better than N-gram based methods, however. <ref type="bibr" target="#b7">Hellendoorn et al. (2017)</ref>  <ref type="bibr" target="#b7">[8]</ref> found that carefully tuned N-gram based models are more effective than RNN and LSTM-based networks in modeling source code. They had higher modeling performance (entropy) and were able to provide more accurate code suggestions. This paper uses an approach based on N-grams and AST traversal with machine learning methods taught in CS 229.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DATA SET</head><p>The data set currently consists of 10 contests on Codeforces from Aug to Nov 2018. All of the contests are "combined division" contests, open to users of all ranks. Each contest has~6k submissions for a total of~60k, with the data format shown in <ref type="table" target="#tab_0">Table I</ref>. Not all contest submissions are in the data set. Only contestants with a declared country and who participated in at least one previous contest are considered. For each problem, only the latest passing submission for each user is considered (if any). Only C++ solutions are considered. C++ is the most popular language, with about 90% of total submissions being C++ in the contests used here.</p><p>The number of submissions for each rank in the data set is shown in <ref type="table" target="#tab_0">Table II</ref>. A user's rating may change based on their performance in the contest. We only consider the user's rating before the contest.</p><p>For the country analysis, we used a subset of the data consisting only of the users in the 10 most common countries. These countries cover about 70% of the full data set. A summary of this data set is shown in <ref type="table" target="#tab_0">Table III</ref>.</p><p>Both data sets have a significant class imbalance. Various techniques were needed to handle this, as described later.</p><p>We implemented a custom scraper for Codeforces in Python using lxml <ref type="bibr" target="#b9">[9]</ref> to parse the HTML.  IV. PREPROCESSING Before applying machine learning algorithms, the C++ source code is preprocessed as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>Source code is first converted to a sequence of strings. It is run through the Clang C++ compiler to produce a list of tokens and an abstract syntax tree (AST). Comments are removed as our focus is coding style. To help the learning algorithms generalize better, all tokens equal to a variable or function name in the AST, or representing a string or character literal, are replaced with special tokens !!VAR, !!FUN, !!STR, and !!CHR respectively. The AST is converted to a list of strings as a pre-order traversal, where additionally a special token endblock is added when all of a node's children have been visited. To simplify later processing, the processed tokens are concatenated with the AST traversal to produce a single sequence of strings. The sequence of strings is then processed further. Bigrams (and unigrams) with at least 1% frequency in the training set are counted to produce features. To help prevent the learning algorithms from favoring shorter or longer solutions, each count vector is normalized by the L2 norm. (TF-IDF <ref type="bibr" target="#b10">[10]</ref> was briefly tested, but L2 normalization seemed to work better.) The features are then scaled to have zero mean and unit variance. Without the normalization and scaling, the GDA model had a much lower accuracy, and the logistic regression model failed to converge in training (probably because the gradients were ill-behaved).</p><p>In the data set, the average number of tokens in a program is 428, the average length of the AST traversal is 627, and the average length of the concatenated sequence is 1055.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. METHODS</head><p>Several learning algorithms are used to predict a user's rating and country based on their code. Here, m denotes the number of training examples, x (i) denotes the feature vector for example i, and y (i) denotes the label for example i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Linear regression</head><p>Linear regression is used to predict the user's rating and the rank is inferred from the rating using </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Raw tokens</head><p>"int", "n", ";", "int", "main", "(",")", "{", "scanf", "(", "\"%d\"", ",", "&amp;", "n", ")", ";", ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Processed tokens</head><p>"int", "!!VAR", ";", "int", "main", "(",")", "{", "scanf", "(", "!!STR", ",", "&amp;", "!!VAR", ")", ";", ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AST traversal</head><p>"TranslationUnit", "VarDecl", "endblock", "FunctionDecl", "CompoundStmt", "CallExpr", ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>libclang C++ parser</head><p>Concatenated sequence "int", "!!VAR", ";", ..., "TranslationUnit", "VarDecl", "endblock", ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bigram extraction (min. 1% freq)</head><p>Normalization and scaling Approx. 2k features the weight w (i) is the inverse of the number of users in the training set with the same rank:</p><formula xml:id="formula_0">min θ 1 2 m i=1 w (i) θ T x (i) − y (i) 2</formula><p>All of the other methods are classification algorithms, rather than regression, as they seemed to work better (see results).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Gaussian discriminant analysis (GDA)</head><p>The maximum likelihood estimators of each class mean µ k and the covariance matrix Σ are computed, where again the weight w (i) is the inverse of the class size:</p><formula xml:id="formula_1">µ k = m i=1 1{y (i) = k}x (i) m i=1 1{y (i) = k} Σ = m i=1 w (i) x (i) − µ y (i) x (i) − µ y (i) T m i=1 w (i)</formula><p>Prediction assumes a uniform prior due to class imbalance:</p><formula xml:id="formula_2">p(y = k) = 1/(# classes) (forced uniform) p(x|y = k) ∼ N (µ k , Σ)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Logistic regression</head><p>For country prediction, we use softmax regression with a weighted cross-entropy loss. Again, the weight w (i) is the inverse of the class size.ŷ (i) k denotes the predicted probability that example i is in class k.</p><formula xml:id="formula_3">y (i) k = p y (i) = k|x (i) = exp θ T k x (i) j exp θ T j x (i) max θ m i=1 w (i) logŷ y (i)</formula><p>For rank prediction, since the goal is to predict within one rank of the actual rank, we trained a separate logistic regression model for each rank. Each training example of rank r is considered to be a positive example in the models for ranks within 1 rank of r.</p><formula xml:id="formula_4">y (i) k = 1 1 + exp −θ T k x (i) max θ m i=1 # ranks j=1 w (i) 1{|j − y (i) | ≤ 1} logŷ j +1{|j − y (i) | &gt; 1} log(1 −ŷ j )</formula><p>All examples are considered positive examples in three ranks, except for examples of the lowest and highest ranks, which are only considered positive in two ranks. Therefore, their weight is multiplied by 3/2 here. Empirically, this results in a model where the classification accuracy for each rank is more even.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Neural network</head><p>The neural network model is similar to logistic regression and uses the same loss functions. A single fully-connected rectified linear layer with 100 units is inserted between the input and output layers as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. Adding even more hidden units seemed to increase the accuracy but this was not fully tested due to time constraints. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTS</head><p>All experiments were conducted using 10-fold cross validation. For each type of model, we trained 10 models where each model is trained on 9 contests (~54k examples) and tested on 1 contest (~6k examples). The values reported here are averages over the 10 models. With this methodology, the models are tested on problems never seen in training. This ensures that the models are not learning specific features about the problems in the training set.</p><p>Due to the class imbalance described before, accuracy is defined as the weighted accuracy where the weight w (i) of each example is the inverse of the class size in the test set. For rank, we allow the predicted rank to be within one rank of the actual rank. If y (i) is the actual label andŷ (i) is the predicted label for example i:</p><formula xml:id="formula_5">Accuracy (Country) = m i=1 w (i) 1{y (i) =ŷ (i) } m i=1 w (i) Accuracy (Rank±1) = m i=1 w (i) 1{|y (i) −ŷ (i) | ≤ 1} m i=1 w (i)</formula><p>The weighted accuracy shows how well the model can predict all classes and not just the majority. A model that strongly favors larger classes would achieve a high unweighted accuracy but low weighted accuracy.</p><p>For the linear regression model, we also report the weighted root mean-squared error (RMSE) for the predicted rating: <ref type="bibr" target="#b11">[11]</ref> is used to train the linear regression and GDA models, while TensorFlow <ref type="bibr" target="#b12">[12]</ref> is used to train the logistic regression and neural network models. Models were trained with the entire training set as a single batch. For logistic regression, we used gradient descent with a 0.1 learning rate, while for the neural network, we used the Adam algorithm <ref type="bibr" target="#b13">[13]</ref> with a 0.0001 learning rate. These learning rates were experimentally found to converge. 50% dropout is used for the hidden layer, meaning that on every iteration, 50% of the hidden nodes are inactive. This helps prevent the network from overfitting and was found to increase the test accuracy.</p><formula xml:id="formula_6">RMSE = m i=1 w (i) y (i) −ŷ (i) 2 m i=1 w (i) Scikit-learn</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RESULTS AND DISCUSSION</head><p>The accuracies obtained for each model are shown in <ref type="table" target="#tab_0">Table  IV</ref>. For reference, the accuracy of a model that outputs a random or constant output is shown in the first row. A model that outputs a constant or random rank, except for the highest and lowest rank, would achieve 30% accuracy because there are 3 ranks within 1 rank of the chosen rank. For country, however, we require that the model classifies the exact country, and there are 10 countries in the data set.</p><p>Classification was found to work better than regression when predicting the rank. This may be because classification optimizes what we actually care about, which is predicting the correct rank, rather than the rating. The linear regression model had a weighted RMSE (as previously defined) of 545 when predicting a user's rating in the test set. Given that ranks have a rating range of~200, this is a fairly large error. GDA worked surprisingly well, achieving accuracies that are almost as high as logistic regression. While GDA assumes that p(x|y) is multivariate Gaussian, logistic regression does not make that assumption and is capable of modeling a large variety of other distributions. Since the accuracies are similar, this indicates that p(x|y) is Gaussian to some degree.</p><p>Out of all the algorithms, the neural network had the highest accuracies. The neural network was probably able to learn more complex relationships between the features compared to the other algorithms. Perhaps some combination of several bigrams is highly indicative of rank or country. Interpretation of the neural network is out of scope of this project, however.</p><p>The high training accuracies, compared to test accuracies, may indicate overfitting. In the neural network, dropout helped reduce overfitting (as described before), but no other regularization techniques were used. We briefly tried using principal component analysis (PCA) to reduce the number of features, and L2 regularization on the parameters, but these techniques decreased the test accuracy. More data helped reduce overfitting, as the accuracy values are about 5% higher than initial tests performed with 5 contests instead of 10.</p><p>For each actual rank and country, the neural network test accuracies are shown in <ref type="figure">Fig. 3 and 4</ref>. The model seems to be able to predict all ranks with similar accuracy. For country, the model is able to predict the more common countries with higher accuracy despite the weighted loss function used. This may be because there is significantly more training data for the more common countries.  From this analysis, we can see that both tokens, like cin &gt;&gt;, and AST nodes, like FunctionTemplate, are important to the model. As well, both unigrams and bigrams are important, although they are often related.</p><p>High skilled competitors appear to use #ifdef significantly, perhaps to change the code's behavior at compile time by defining macros in the compiler flags. Also, they appear to use assertions and C++ function templates.</p><p>Low skilled programmers appear to use cin and cout for input. This makes sense since scanf and printf are faster input methods and often preferred by experienced competitors.</p><p>It is interesting to see TranslationUnit as a strong indicator of low skill level. TranslationUnit is the root of the AST and appears exactly once per per program, but since the count is normalized by the L2 norm of the count vector, its value will be higher in shorter programs. Thus, it appears that GDA has learned to associate smaller programs with lower skill levels, despite having the L2 normalization to try to prevent this. It makes sense that a long program would likely indicate a hard problem and a high skilled competitor.</p><p>Tables VII and VIII show the features with the highest class means for Chinese and American competitors respectively. It seems that Chinese competitors often use getchar to read single characters from standard input, and import C input libraries like cstdio. American competitors seem to often spell out std in their code (like std::cout &lt;&lt; std::endl) instead of importing the entire namespace with using namespace std, and use ld which is a commonly used alias for long double.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. CONCLUSION AND FUTURE WORK</head><p>In this paper, we studied the application of machine learning techniques in predicting the rank and country of a Codeforces competitor based on a single source code submission. The neural network model achieved the highest accuracy of 77.2% accuracy in predicting rank (within one rank) and 72.5% in predicting country. Despite not achieving the highest accuracy, the GDA model was easier to interpret and we were able to find unigrams and bigrams that were the strongest indicators of certain skill levels and countries.</p><p>Future work may include testing RNN or LSTM based models, as discussed in Related Work. Acquiring more data may help reduce overfitting. Token processing could be improved, for example by replacing class and macro names with special tokens in addition to variable and function names. N-grams with N &gt; 2 could be tested as only unigrams and bigrams were considered here. More hidden units or layers could be added to the neural network. Interpretation of the logistic regression or neural network model could be attempted.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Data preprocessing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Neural network architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Neural network test accuracy for rank (±1) by actual rank India China Russian Bangladesh Vietnam Ukraine Poland Egypt United States Iran 0 0.25 0.5 0.75 1 Fig. 4. Neural network test accuracy for country by actual countryVIII. INTERPRETATION OF THE GDA MODEL While the GDA model did not achieve the highest accuracy, its simplicity makes it possible to interpret the learned model more easily. For this analysis, we randomly chose one of the models from the 10-fold cross validation. To determine the unigrams and bigrams that were the strongest indicators of high and low skill level, we compared the class means µ k for the International Grandmaster and Pupil ranks and found the features where the class means had the largest (positive) and smallest (negative) absolute difference. These features are shown in Tables V and VI. The features are ordered in decreasing strength from left to right and top to bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>TABLE I DATA</head><label>I</label><figDesc>FORMAT FOR EACH SUBMISSION</figDesc><table>Country Rating 
Source Code 
RU 
2193 
#include &lt;iostream&gt;\n#include... 
US 
1747 
#include &lt;bits/stdc++.h&gt;\nusing... 
· · · 
· · · 
· · · 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>TABLE II DATA</head><label>II</label><figDesc>SET USED FOR RANK PREDICTION</figDesc><table>Codeforces Rank 
Rating 
# in 
% of 
Bounds 
Data 
Data 
Legendary Grandmaster 
3000+ 
378 
0.63% 
International Grandmaster 2600-2999 
1319 
2.20% 
Grandmaster 
2400-2599 
1781 
2.97% 
International Master 
2300-2399 
1733 
2.89% 
Master 
2100-2299 
6375 
10.64% 
Candidate Master 
1900-2099 
8756 
14.61% 
Expert 
1600-1899 
17824 29.75% 
Specialist 
1400-1599 
11445 19.10% 
Pupil 
1200-1399 
7405 
12.36% 
Newbie 
0-1199 
2896 
4.83% 
Total 
59912 100.00% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>TABLE III DATA</head><label>III</label><figDesc>SET USED FOR COUNTRY PREDICTION</figDesc><table>Country 
# in 
% of 
Data 
Data 
India 
12331 
29.31% 
China 
8818 
20.96% 
Russia 
6761 
16.07% 
Bangladesh 
4536 
10.78% 
Vietnam 
1753 
4.17% 
Ukraine 
1694 
4.03% 
Poland 
1664 
3.95% 
Egypt 
1662 
3.95% 
United States 1450 
3.45% 
Iran 
1406 
3.34% 
Total 
42075 
100.00% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table II .</head><label>II</label><figDesc>Due to class imbalance, we used a weighted least squares loss where</figDesc><table>Source code 
int n; // my var 
int main() { 
scanf("%d", &amp;n); 
... 

Abstract syntax tree 
TranslationUnit 
VarDecl n 
FunctionDecl main 
CompoundStmt 
CallExpr scanf ... 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table>ACCURACY FOR EACH MODEL (10-FOLD CROSS VALIDATION) 

Model 
Accuracy 
(Rank±1) 

Accuracy 
(Country) 
Train 
Test 
Train 
Test 
Random/constant 
30.0% 
30.0% 
10.0% 10.0% 
Linear regresion 
69.6% 
60.1% 
N/A 
N/A 
GDA 
75.7% 
67.2% 
75.0% 65.0% 
Logistic regression 86.1% 
71.6% 
92.2% 68.4% 
Neural network 
94.4% 
77.2% 
97.0% 72.5% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>TABLE V STRONGEST</head><label>V</label><figDesc>INDICATORS OF HIGH SKILL LEVEL ifdef # ifdef assert endif # endif assert ( ( ... FunctionTemplate TemplateTypeParameter __VA_ARGS__ FunctionTemplate ifdef LOCAL LOCAL TABLE VI STRONGEST INDICATORS OF LOW SKILL LEVEL</figDesc><table>ifdef 
# ifdef 
assert 
endif 
# endif 
assert ( ( ... 
FunctionTemplate TemplateTypeParameter 
__VA_ARGS__ 
FunctionTemplate 
ifdef LOCAL 
LOCAL 

cin &gt;&gt; 
cin 
&gt;&gt; !!VAR 
&gt;&gt; 
cout &lt;&lt; cout 
TranslationUnit InclusionDirective 
TranslationUnit 
std ; 
IfStmt BinaryOperator main 
main ( 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>TABLE VII STRONGEST</head><label>VII</label><figDesc>INDICATORS OF A CHINESE COMPETITOR</figDesc><table>= getchar 
getchar 
getchar ( 
char !!VAR 
; char 
cstdio 
cstdio &gt; 
&lt; cstdio 
cstring 
cstring &gt; &lt; cstring 
&gt; !!CHR 
&lt; !!CHR 
{ scanf 
UnexposedExpr CharacterLiteral 
&gt;= !!CHR 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>TABLE VIII STRONGEST</head><label>VIII</label><figDesc>INDICATORS OF AN AMERICAN COMPETITOR</figDesc><table>( std 
&lt;&lt; std ld &gt; 
&gt; struct 
| ( 
:: 
&lt; ld 
std :: 
| 
, std 
struct 
; template 
ClassTemplate 
endblock ClassTemplate 
os &lt;&lt; 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirzayanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Codeforces</surname></persName>
		</author>
		<ptr target="http://codeforces.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey of machine learning for big code and naturalness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Devanbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<idno type="doi">10.1145/3212695</idno>
		<ptr target="http://doi.acm.org/10.1145/3212695" />
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="81" />
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Source code authorship attribution using n-grams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Burrows</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Tahaghoghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelth Australasian Document Computing Symposium</title>
		<meeting>the Twelth Australasian Document Computing Symposium<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="32" to="39" />
		</imprint>
		<respStmt>
			<orgName>RMIT University. Citeseer</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">What&apos;s the code?: automatic classification of source code archives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ugurel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krovetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="632" to="638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Source code authorship attribution using long short-term memory based networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alsulami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Harang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mancoridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Greenstadt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Symposium on Research in Computer Security</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="65" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning program embeddings to propagate feedback on student code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Piech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Phulsuksombati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sahami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3045118.3045235" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32Nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32Nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1093" to="1102" />
		</imprint>
	</monogr>
	<note>ser. ICML&apos;15. JMLR.org</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Are deep neural networks the best choice for modeling source code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J</forename><surname>Hellendoorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Devanbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering, ser. ESEC/FSE 2017</title>
		<meeting>the 2017 11th Joint Meeting on Foundations of Software Engineering, ser. ESEC/FSE 2017<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="763" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<idno type="doi">10.1145/3106237.3106290</idno>
		<ptr target="http://doi.acm.org/10.1145/3106237.3106290" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">lxml: Xml and html with python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Faassen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bicking</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Introduction to modern information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Mcgill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>software available from tensorflow.org. [Online</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
