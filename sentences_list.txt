sentence
basketball is an invasion sport which means that players move freely during every second of the game 
first sentence
in order for us to evaluate a player it is highly beneficial to consider the movements of all the players on the basketball court 
introduction
a more complete understanding of a player s performance can be achieved by taking into account the spatio temporal considerations of movement and player interactions with basketball a lucrative and competitive sport in the us teams have incentives to accurately project player quality 
the nba competition has become so fierce that minor details about players could have a dramatic impact on game results 
for example taking shots under pressure from different angles and at different times of the game are very hard to accurately assess qualitatively 
using statistical methods to assess these shots would help us identify the quality of players and would provide us with the ideal situations of taking shots 
to do so one would need to analyze every player at different points in time and luckily we could do so by using the spatio temporal data acquired by special techniques recently developed we sought to create a probability shot model in a basketball game 
we started by getting a single nba data set which breaks down every second into 25 moments in which every moment consists of the player s and the ball s locations 
based on this data we created criteria such as the distance between the player with the ball and the closest defender the ball s velocity acceleration maximal height along with many others features to determine whether a shot was taken or not 
once we knew whether a shot was taken or not we trained our model on 70 of the shots attempted in 632 basketball games and created a probability shot model that we tested on the remaining 30 of attempted shots 
to focus on our analysis we only considered jump shots and excluded other shots such as alley oops and lay ups 
last sentence
a few papers have been published on this field mainly dealing with analyzing specific players or the team as a whole 
a common technique called network analyses turns teammates into nodes and passes into paths or archs thus creating a flow chart 
related work
using these flow charts one could analyze the most frequent paths that the ball went through 
based on this model one could mathematically justify why the triangle offense works and why the winning team tends to have more entropy 
topological depth entropy price of anarchy and power law distributions are assigned to each player to assess outcome classification 
other techniques used are known as intensity matrices and maps which transform the playing area into polar space and induce subdivisions in the space 
this common technique uses matrix factorization on the intensity matrices to produce a compact low rank representation 
it thus models shooting behaviors with the insight that similar types of players shoot from similar locations and then maps each type to an area within the court 
other papers were written on tactical group movement and how they affect the play 
special techniques were used to identify formations such as clustering coefficients and different forms of centrality 
the data were obtained from public github account that had scraped the publicly available sportvu player tracking data in basketball 
this data contains the xy coordinate of the 10 players 5 for each team that are on the court as well as the xyz coordinates of the ball 25 times per second 
data and processing
we transformed this raw data into a csv file with each row containing the location of the ball and the location and identities of the 10 players on the court we then worked on processing this data into our response variable the shots that were taken and whether they were successful 
in order to identify when a shot was taken we relied on the physics of the ball s flight through the air 
we computed the position velocity and acceleration of the ball 
we identified shots as moments in which the ball traveled through the air with no x acceleration or y accerleration and z acceleration only due to gravity with the ball s flight ending at the rim to determine whether a shot was successful or not we check whether in the fifth of a second immediately following the ball being in the area of the rim the ball passes through the area immediately below the basketball hoop 
if so we mark the shot as a make through manual checking of this procedure against the actual play by play of several games we know that the process successfully captures a large majority of shots and successfully classifies a large majority of the shots 
there are nonetheless some errors however with about 10 error in each of the steps 
this is because we attempt to identify shots on the basis of a definition approach 
it would perhaps be more effective to manually label a certain number of games and extrapolate from these labels with a machine learning approach over the 632 games for which we had data this approach identified 42 034 shots of which 29 6 were classified as successful next we extracted a feature set from the data 
these features are the distance of the shooter to the hoop the angle of the shot whether the shooter played for the home or away team the distance to the nearest defender the length of time the shooter had the ball and whether the shooter had dribbled since receiving the ball 
model selection we randomly selected 70 percent of the shots to train logistic regression and svm 
we then tested the models on the remaining 30 percent 
technical approach
when we implemented support vector machine we got 20 percent accuracy 
on the other hand when we used logistic regression we got 50 percent accuracy finally we used boosting and got 64 percent accuracy 
to implement boosting we evaluated using resampling the effect of model tuning parameters on performance 
we then chose the optimal model across these parameters and then estimated the model performance 
we used repeated training test splits 4 with a 60 percent partition on the data 
this algorithm predicted whether the shot would be successful with 64 percent accuracy 
after training our data on logistic regression we found that logistic regression performed at a mediocre rate 
the following figure gives the distribution of projected shot probabilities on our test set 
current results and analysis
the following figure indicates the relative influence of our various features on the boosting algorithm 
from the results we conclude that adaptive boosting works best with such a data set 
since knowing the location of the player his nearest defender angle to the basket along with the many other features only provides us with a slightly better guess than random it makes total sense why boosting might out perform logistic regression and svm 
conclusion
from boosting s relative influence or weights assigned per feature we could see that the major components to predicting a shot are the angle to the basket closest distance to the defender and the ball s distance to the basket 
determining whether the player dribbled before or not was not of much importance 
this project could be improved and extended in several ways 
first it would be highly beneficial to apply machine learning to the identification of shots and the classification of their success 
where to go from here
this would require labeling a certain number of shots and then using those labels to identify other shots we could also improve our model by adding additional features 
these other features include defender angle as well as defender distance and angle for the second nearest defender 
a new feature that we think would be valuable is the velocity speed and angle of the shooter at the moment at which they take their shot 
these are features that we learned are used by second spectrum a sports analytics company focused on analysis of sportvu data another area of expansion is into other elements of the game of basketball such as rebounding and passing 
predicting the outcome of increasingly many elements of the game of basketball would build into a more comprehensive model of the game that could be used to thoroughly evaluate basketball performance finally this type of analysis can be applied to more sports 
the nfl has tracking chips in players shoulder pads many european soccer leagues have optical player tracking the nhl has experimented with player and puck tracking technology 
the methods used in analyzing basket ball player tracking data could also be applied to other sports to more thoroughly understand player performance 8 references
feedforward convolutional neural networks have achieved some success when applied to problems of image recognition and object discrimination 
as it stands convolutional neural networks have made impressive gains in traditional image recognition tasks such as telling objects apart and classifying images based on familiar features from training data however several problems persist with convolutional neural networks on more general image recognition tasks 
i introduction and related work
firstly convolutional neural networks are computationally expensive to train and they require large amounts of manually labeled data to train on usually involving tens of thousands of manually labeled images and several gpu cores to train to convergence 
however models of human visual attention from neuroscience literature suggest that human eyes scan and glimpse throughout an image recurrent neural networks can be used to implement this attention model by iterating through the internal network states in a fashion parallel to image processing in the brain when humans initially encounter and recognize images 
in particular the problem of object counting has mostly been approached from the perspective of convolutional neural networks and it presents a different set of challenges than traditional image recognition 
we build upon previous recurrent attention model ram efforts and apply the architecture equipped with reinforcement learning to object counting 
we believe that sequentially incorporating data from multiple glimpses could outperform a feedforward approach in cases where objects are sparse or clustered or otherwise unevenly distributed 
our goals are to compare the efficacy of two different approaches to the counting problem and analyze the performance of the attention model in more detail 
in particular we evaluate neural networks abilities to closely replicate the pattern of retinal glimpses human eyes take upon encountering images with multiple objects 
training and testing images were generated using simcep
a data and general design
first we implement a feedforward network with convolution layers 
the network has two spatial convolution layers with max pooling to reduce the dimensionality of the input followed by three standard linear hidden layers with nonlinear activation functions 
b convolutional feedforward network
the network is representative of feedforward networks commonly deployed for many applications 
second we train a model that sequentially chooses a predetermined number of glimpse locations within each image and uses the images resulting from these fixations as the sequential inputs to a recurrent neural network 
c recurrent attention model
we model visual focus by giving the network access to several small concentric windows that progressively decrease in size and increase in resolution 
sequential inputs take the form of 3 different 64 x 64 images layered on each other see the area of high focus is most helpful for counting while the low resolution windows allow the program to only make judgments about where to place the focus in the next time step of the network processing 
1 simulating visual focus 
this replicates the parallel processing of input to the retina while discerning and counting objects in a scene being vaguely aware of objects in the background while sharply focused on discrete objects in a given location 
2 controlling focus trajectory we implement an algorithm that can choose along what path the algorithm glimpses to recognize discrete objects within the attention window 
previous approaches have included learning policies using reinforcement learning to plot glimpse paths 3 integrating information from sequential perspectives we choose the recurrent neural network as the base architecture because of its ability to accept arbitrarily long sequences of input 
traditional feedforward networks lack this capability 
in our training model we input the images in certain sequences 
the neural network accepts these sequences and incorporates them into a global visual understanding of the image from multiple glimpses rather than taking in all the data at once 
the aforementioned considerations deciding where to focus and how to integrate the information obtained from these focus points are integrated into a single network architecture 
the model has two central components the processing component that mixes the input image sequence and the glimpse location and the recurrent component 
d unifying the model
the recurrent component mixes the image input and the internal network representation at each iteration updating the internal network representation at each iteration 
the network sequentially integrates the information from each iteration into a final prediction for the object count of an image 
preliminary testing yielded a 95 4 percent accuracy rate after eight minutes of stochastic gradient descent
a convolutional feedforward network
after training the attention model using three concentric resolution windows and seven total glimpses for eight minutes and obtained a final accuracy on the test set of 66 6 percent 
this value is a reflection of numerous successful trials 
b attention model
however it should be noted that on some trials the recurrent model s accuracy remained flat at 20 percent no better than random 
this is a result of the ram glimpsing at locations from which it can glean no substantial information 
an issue that has received relatively little treatment in the literature is the extent to which the recurrent attention model successfully learns to focus on the most salient areas of the image 
our decision to apply the model to object counting arose in part because this task lends itself well to rigorous evaluation of the algorithm s decisions about where to focus 
iv evaluating the focus mechanism
we tested this behavior as follows 
first we trained the network to estimate counts after seven glimpses 
during the testing phase however we limited it to n glimpses with n ranging from 1 to 7 inclusive as a hyperparameter for the network 
in doing so we could analyze the model s intermediate estimates of the object count after only a few glimpses and evaluate how each additional glimpse affects predictive power for each count class after training and evaluation the network produces the clear result that images with fewer objects e g 
one or two were correctly classified even when the model was restricted to fewer glimpses 
images with more objects required more glimpses to be classified accurately 
there exists a close correlation between the number of glimpses a network is allowed and the number of objects that it must count in the image 
this suggests that the network succeeds in choosing glimpse locations that closely correspond to an object in the image as a control to demonstrate that this effect was in fact do to intelligent glimpse decisions we repeated this test but restricting the input to the network to only the smallest highest resolution window 
in other words we restricted the model s field of view removing its access to low resolution information about the entire image 
in this implementation performance even on low count images improved with more glimpses indicating that the glimpse locations were being decided more or less randomly and therefore the program benefited from being allowed as many as possible 
this result indicates that the model s access to a low resolution version of parts of the image outside its focus window was allowing it to choose salient points to focus on 
the ram has advantages that make it worth studying further 
once trained it can classify images more rapidly since it processes less data 
furthermore it more accurately models human perception making it a valuable model for cognitive scientists 
such a model has applications outside of object counting as the concept of integrating a series of incomplete snapshots could in principle be applied to processing of text speech or a number of other tasks even with our current model however we successfully demonstrated the viability of a recurrent model for object counting 
most significantly we have shown the effectiveness of the attention mechanism proving that the success of these attention models is in fact due to correct glimpse behavior not simply good predictive capacity given limited information 
we believe this is an important result as it allows one to aim for symbiotic training of the attention mechanism and prediction mechanism 
though the interrelated nature of these two parts of the algorithm gives the model its power it also has pitfalls 
we believe that the occasional nobetter than random accuracy of the model after training was due to a chicken and egg effect where the model could not learn how to use glimpse information before learning how to glimpse properly and vice versa 
introducing some stochasticity into the process could alleviate this issue but more study is needed 
we identified the largest source of inconsistency in the network s output as dependent upon the choice of glimpse hyperparameter 
the current architecture of the network depends upon a preset fixed number of glimpses for the network to take before it stops trying to improve predictive accuracy 
vi directions for future research
our specific improvement in the model is to implement a network that trains itself on how many glimpses to take for each image first we will insert a mechanism that if the ram glimpses at a location with no information then it randomly glimpses somewhere else and restarts the iterative process second we would thus modify the reward function in the reinforcement learning to not only reward the network for correctly identifying object counts but also continuing glimpsing until the network is able to within a threshold of accuracy identify this correct object count 
this reward function would take the gradient of the probability given to the correct count class as the network keeps glimpsing and stop the network from further glimpsing once the gradient falls below a certain threshold meaning that the network has become confident in the correct count for an image 
this more closely replicates the motion of the human retina third we would apply our network to non image data to demonstrate the versatility of a recurrent attention approach to other sensory input such as potentially sound tactile input natural language understanding and translation 
we would like to thank professor james mcclelland and steven hansen in stanford s department of psychology for the initial inspiration behind the research and their tremendous support and guidance in giving us the resources and references for the work 
we also are grateful to nicholas leonard for providing open source code that aided us in implementing the recurrent attention architecture 
vii acknowledgments
chinese checkers is a game played on a hexagram shaped board that can be played by two to six players individually or as a team 
the objective is to be the first to move all ten pieces across the board into the opposite starting corners 
i introduction
as shown in the reinforcement learning is an area of machine learning typically formulated as markov decision process mdp 
the model consists of states actions transitions etc which is suitable for decision making in board game like chinese checkers 
the objective of our project is to use reinforcement learning to build an ai agent for chinese checkers and to explore the effectiveness and efficiency of the ai to simplify the problem our ai only solves the one vs one mode of chinese checkers 
dif ferent from classic reinforcement learning where at each state the player solves a simple maximization problem in our ai for chinese checkers it is a adversarial zero sum game 
therefore each player needs to consider not only his her own strategy but also the opponent s responding strategy 
therefore a better way to depict such procedure is minimaxation which is elaborated in section 3 the biggest challenge in applying reinforcement learning to our ai is how to learn the weights in function approximation 
we modified the standard algorithm for function approximation so that it fits our minimax setting and we adopted a diminishing learning rate to stabilize the update 
our simulation results showed that this learning procedure is effective in that the difference between the weights before and after an iteration is small 
simulation results also showede the robustness of our update in that our learning procedure with different initializations will end up with very close weights we tested the performance of different strategies by playing against a random look ahead greedy player 
simulation results showed that the minimax strategy with tuned weights significantly outperforms the minimax strategy with initial weights 
moreover we further modified our strategy such that it divides the game into three stages and applies different strategies thereon 
simulation results showed that this modified strategy outperforms the basic minimax strategy the rest of this report is organized as the follows 
we first talk about how we implement the board in section 2 
then we introduce the basic methodology of our ai in section 3 and point out the difficulties in implementation as well as our solution in section 4 
we cover our modified strategy in section 5 
simulation results are shown in section 6 
we adopted the classical approach for game playing ai game search tree which best mimics the behavior of a human player while demonstrates super human performance by taking advantage of the computing power 
with each node representing a board state of the game and each edge representing one possible move from one board state to a subsequent board state the game search tree can emulate the thinking process of a human player 
iii methodology
chinese checkers is a two player zero sum game thus an objective value is needed to evaluate the situation on the board 
player 1 s goal is to maximize the value while player 2 minimizes it 
the logical approach is the minimax tree which is a decision tree that minimizes the possible loss for a worst case scenario resulted from the opponent s optimal move due to the large state space complexity of chinese checker it is unrealistic to build a top down game search tree 
instead a shallow k depth minimax game tree that searches only the tip of the tree is built 
at each node the value is taken as the minimax score which is computed by the minimax algorithm of depth k when the search has reached the bottom of the k depth search tree a k a 
the leaves the score is approximated by a raw score which is a linear evaluation function based on the features of the board state 
we exploited 6 features that are based on the positions of pieces on the board which are described as the following a i the squared sum of the distances to the destination corner for all pieces of player i b i the squared sum of distances to the vertical central line for all pieces of player i c i the sum of maximum vertical advance for all pieces of player i with i 1 2 
note that we used the square of the distances to penalize the trailing pieces which would motivate each player to make the pieces cohesively and thus promote hopping 
besides the 6 features are extracted from a larger amount of possible features so that the overfitting of the model was avoided 
the other features we have explored and found unnecessary included the horizontal variance how scattered of pieces of each player the vertical variance of pieces of each player the maximum vertical advance for a single piece of each player the evaluation function is the form ofwhere the weights w w 1 w 2 w 3 t would be trained via function approximation which would be described in the following section 
the performance of the ai is highly dependent on how well the weights in the evaluation function is tuned 
the objective of weights tuning is to allow the evaluation function to consistently approximate the minimax value through the depth k tree search 
a weights tuning and function approximation
the challenge is to develop an algorithm to improve and stabilize the weights within certain iterations the following algorithm 1 based on function approximation is our solution to perform weights tuning 
an introduction of function approximation can be seen in
another challenge is the run time complexity of the algorithm 
due to the nature of chinese checkers players usually have around 20 100 feasible play one game with both player following minimax rule using weights w record the feature vectors at each turn in a matrix and the corresponding minimax scores in a vector 
b run time complexity and alpha beta pruning
w w w new w 6 until stabilized moves at a typical turn 
the worst case number of board states evaluated in a minimax tree of search depth 4 is on the scale of 10 8 
5 
the large branching factor combined with any non trivial search depth can easily result in impractical run time for realtime game play we address this problem by adopting alpha beta pruning 
a detailed description of this technique can be seen in in the strategy described above we adopted minimax tree search at each turn of playing 
note that there is a waste of computing power in this strategy at the beginning and end of game 
at the beginning of game the pieces of two players have not interact with each other thus each player s move does not interfere the other s 
therefore it is unnecessary to consider the opponent s strategy thus the minimax strategy can be simplified as a pure maximizing strategy 
this is also true at the endgame when the pieces of two players are split up thus each player only needs to consider how to end the game as soon as possible without considering the opponent s strategy in the light of this knowledge we modified our strategy above in the following way 
in the start game the player only consider his her feasible moves and chooses the one that gives the maximal weights calculated value 
in the midgame when two players pieces intersect we search our optimal move via the minimax procedure 
in the endgame the player would take a move that achieves maximal vertical advance we will test the performance of this strategy and compare it with the basic strategy in section 6 b 3 
with algorithm 1 we were able to tune and stabilize the weights 
the criterion for stabilization is w new w as shown in the plot the difference of weights diminished as more training games were played 
a convergence of weight tuning
while the algorithm didn t necessarily converge it did improve the effectiveness of the ai dramatically after weights were tuned which will be shown in section 6 b furthermore the algorithm is also robust to initializations 
two different unit length initial weights were attempted 
we measured the performance of our algorithms by simulating 200 games against a benchmark strategy 
the benchmark is a greedy random look ahead algorithm that takes the move that gives the most combined vertical advance in 2 steps 
b benchmarking
tiebreaker is preference to trailing pieces and further ties are broken by random selection 
the result was measured by winning steps which is the number of steps needed for the losing player to finish the game 1 effects of weights 2 effects of search depth 3 effects of modified strategy 
for the state farm photo classification kaggle challenge we use two different convolutional neural network models to classify pictures of drivers in their cars 
the first is a model trained from scratch on the provided dataset and the other is a model that was first pretrained on the imagenet dataset and then underwent transfer learning on the provided statefarm dataset 
abstract
with the first approach we achieved a validation accuracy of 10 9 which is not much better than random 
however with the second approach we achieved an accuracy of 21 1 
finally we explore ways to make these models better based on past models and training techniques 
this paper will discuss using convolutional neural networks cnns to perform deep learning on an image dataset provided by state farm insurance 
state farm has provided a dataset that contains several pictures of drivers in their cars performing specific actions detailed later in the paper 
our task is to be able to classify these images based on the action that the driver is performing 
many of these images show drivers performing actions that are unsafe while driving a car 
the motivation behind this problem stems from state farm s need to understand what kinds of behaviors drivers are engaging in so that they can set their insurance policies accordingly 
for example if texting and driving is a common cause for accidents which it often is insurance companies can increase their premium for drivers that are more likely to be texting and driving 
by analyzing this data we will not only learn what trends drivers tend to exhibit but also be able to automate the process of pinpointing the actions that lead to accidents 
convolutional neural networks is where machine learning meets computer vision to tackle the problem of object detection 
every month the state of the art in convolutional neural gets pushed further and further 
previous work and background
currently the leading model is the inception v4 model that achieved a 3 08 top error rate on the imagenet dataset the inceptionv4 model has 75 trainable layers 
these kinds of models take about 2 weeks to train along with a substantial amount of computing power 
another top framework in this field was proposed by girshick et al 
this framework divides classification into two steps where the first step performs object detection and the second step performs cnn recognition 
along with the two methods listed above there are several other ways to modify and leverage cnns in order to perform object recognition tasks
the dataset contains about 4 gb of photos of drivers in their cars which translates to approximately 25 000 pictures 
each driver has several images associated with them and the images themselves are divided into 10 classes which leads to about 2 500 images per class 
dataset
the classes are as follows for the purposes of this paper we divide up our dataset into a training set and a validation set 
80 of our dataset will make up our training set and 20 will makeup the validation set 
to ensure that the data is primed for usage in a convolutional neural network we shrink each image to 224x224 pixels 
this size was selected based on the default values that are used in a popular convolutional neural network model known as vggnet 19 
preprocessing
resizing of the images was done by a simple python script 
for performing actual classification there are various types of architectures and training techniques we can use 
we will evaluate each different architecture and compare the results to see which architecture performs best 
classification
also within each architecture technique there are various different hyperparameters we can adjust in order to fine tune the model 
the first architecture involves using a small 10 layer cnn that contains 8 convolutional layers and 2 fully connected layers at the end 
the output of the final layer is an individual probability vector for each image where each entry i in the vector indicates the predicted probability of that image to generate this probability vector we will be applying a softmax activation to the final layer of the network 
approach 1 small network
the probabilistic interpretation of the softmax classifier is as follows p y i x i w e fy i j e fy j with convolutional neural networks there are several different update rules one can use to update each of the weight matrices bias vectors and convolutional filters 
the update rule that has been known to work best in practice is known as the adam update rule which consists of the following the hyperparameters that we set are the following 1 
learning rate 1 10 72 
beta1 decay 0 9 3 
beta2 decay 0 99
the purpose of using this architecture was to provide a baseline for how well a convolutional neural network can perform on this type of dataset 
weight initializer gaussian
this next architecture leverages an existing architecture that has been proven successful in the past known as vggnet 19 
vggnet 19 has been trained on the imagenet dataset and therefore is good at recognizing images in that dataset 
approach 2 transfer learning on vgg network
however we want our model to be able to distinguish the images in our dataset to do this we use a process called transfer learning 
transfer learning is the process of training only some of the layers in a cnn on a new dataset while leaving the majority of the layers untouched 
in our case we can preload the pretrained weights of the vggnet 19 model found at https gist github com baraldilorenzo 07d7802847aaad0a35d3 
then we can train the top layers on our state farm dataset without affecting the rest of the layers 
by doing this we get the advantage of using a pretrained model that can perform basic feature extraction i e edge detection sift descriptors etc 
in the convolutional layers of the network and then the final fully connected layers can use these features to make classifications specific to our problem 
the hyperparameters that we set are the following 1 
learning rate 1 10 92 
beta1 decay 0 93 
a visual of the model is provided below
weight initializer vggnet 19 weights
due to the nature of convolutional neural networks we need a significant amount of computing power and time in order to successfully train a model to the point where it can be used in practice 
for example training the inceptionv4 model took 2 weeks to train on 8 nvidia tesla k40s which are some of the best gpus currently in the market 
technical resources limitations and solutions
for the purposes of this paper we created a gpu instance on amazon aws which had a reasonable amount of processing power and memory to perform the robust calculations required to train a convolutional neural network 
however it was not robust enough to handle a large training set which is why we had to limit the number of training examples to 10 000 
to get around this we trained each model with the first 10 000 pictures for 25 epochs and noted down the results 
then we took that model and trained it on a completely new set of 10 000 pictures for 25 epochs 
then finally we took the remaining 5 000 images and trained the model for 25 epochs 
while this was not ideally how the model would have been trained given the computing resources we had it provided us with a way to expose the model to all of the images in order to make it more generalized 
we measure the quality of our approach based on a loss function the higher the loss the lower the accuracy 
the loss function used for accuracy is as followswhere n is the number of images in the test set m is the number of image class labels and y ij is the true value of example i for attribute j as posted on the kaggle website the loss function above is essentially identical to the categorical cross entropy loss function so it was a straightforward choice to minimize that loss function during gradient descent 
evaluation metric
however intuitively this loss function does not provide an intuitive evaluation of the model 
instead we establish our own criterion for evaluating the model 
as mentioned earlier every training example outputs a vector of the following form because each training example belongs to one class we assign the training example to the class whose probability is the highest and generate a vector of binary values 
then we compare the class assigned by the classifier and its true class to evaluate the accuracy 
the official accuracy metric used acc numcorrectp redictions numexamples
we trained both of the networks described earlier a network trained from scratch and a pretrained network with transfer learning performed on it 
as detailed earlier we had to train our model 3 spurts twice with approximately 10 000 images and the final time with approximately 5 000 images due to technical limitations 
loss and accuracy results
once we trained the whole model on these images we evaluated our model by taking the 100 images that were not used in the training set and computing the categorical cross entropy across all these examples 
the training loss after 25 epochs of training on the first 10 000 examples was loss train 13 985 and the validation loss wasthe graph is as follows while we only trained for 25 epochs it seemed that the loss still had not converged and required several more epochs for it to fully converge 
this makes sense because we are training this model from scratch and so it requires several epochs to actually understand the trends in the data 
after exposing the model to the rest of the training set in chunks we achieved losses of loss train 11 235now with a trained model we can evaluate the accuracy on a dataset of 100 images 
the accuracy was 10 9 
given that there are 10 possible classes for a training example random guessing expects to achieve 10 accuracy 
we explore reasons as to why this was the case in section 5 
we can also observe how the both the training accuracy and validation accuracy change as we train the model 
after performing transfer learning on this model we achieved a training accuracy of 21 1 and a validation accuracy of 24 
clearly this is better than random guessing so our model was able to understand the trends in our data we analyze the reason for this in section 55 
analysis
there are several improvements that can be made to the above approaches 
it would be interesting to explore the effects of different optimizing techniques i e 
further improvements
adagrad adadelta vanilla sgd etc 
to see how the speed of convergence changes and how it affects the accuracy 
also we can try different nonlinearities after each layer of the network leaky relu as opposed to regular relu and see how that affects the above metrics 
one of the biggest drawbacks to using cnns is the amount of time it takes to train one to the point where its usable therefore computational efficiency is of utmost importance 
specifically with the first approach training that network till convergence would have taken much more time and computing resources 
experimenting with fewer layers in this model is a potential way to reduce the amount of time it would take and also reduce the potential for overfitting 
however since the model was nowhere close to convergence it is difficult to evaluate whether the model would have high bias or high variance given more time to train 
specifically for transfer learning on the second approach it would make sense to try training more or fewer layers and see how that affects the rate of convergence and overall accuracy 
most of the time in transfer learning we only train the fully connected layers becaue the convolutional layers act as feature extractors but there have been advantages in also training the final convolutional layer so this would be an avenue worth exploring 
as mentioned earlier the dataset was not only a set of individual pictures but could actually be grouped into sequences of frames from a video 
while convolutional neural networks do not normally work on videos by default there have been some techniques researched by researchers here at stanford that make classification of videos possible 
some of the techniques detailed in the paper are early fusion late fusion and slow fusion
while identifying functionality mistakes in a program can be done with near certainty for a finite number of final requirements identifying style mistakes is subject to subtle conventions and an infinite number of possible programs 
this poses a problem for students trying to get feedback on their program style while they are still working on the assignment 
also it makes it harder for teacher assistants ta s to agree on a certain style grade 
therefore the goal of this research is to figure out how to automatically assign a style grade to a program and provide style feedback 
more specifically the procedures employed are first k means clustering of the data according to one of three different strategies then from each cluster fitting a logistic regression or naive bayes model for classifying functions into those well decomposed and another for well formatted functions and finally fitting a softmax multi class classification model to assign a program style grade 
developing good programming style is a vital part of the cs106a class and to enforce this practice all assignments are graded not only on functionality but also on style 
but what exactly constitutes good style is not very clear 
as such students who are just learning about programming struggle to develop this good style 
unlike functionality which they can test themselves by running their program on their computer style is not given feedback until the program has been graded by the ta by which time it is too late to fix 
during the developing stage of the program writing the student s program is not necessarily functionally complete but we still wish to provide feedback on the style and format of the parts already written so that the student may back trace if necessary and rethink his current implementation 
by providing both style and functionality feedback at every stage the aim is to have the student improve both at the same time rather than pushing style as an afterthought for the end and as a result make bugs harder to debug 
since we want to classify into more than one grade bucket a multi class classification model seems appropriate for the situation 
intuitively the three most important factors when considering whether a program has good style are whether it is well decomposed well formatted and whether it works 
hypothesis
but since a single program may have several possible correct solutions it may be the case that different combination of these characteristics produce different style grades for each solution 
therefore it is of interest to explore whether running softmax only on those programs that have an implementation similar to the test program has more accuracy than simply fitting softmax to all the training data 
and since we also want to provide feedback for non working programs we explore the possibility of determining the features of the softmax model by running logistic regression or naive bayes on the functions alone to decide whether each function is well decomposed and well formatted and finally take an average for an entire program to determine whether the program is well decomposed and well formatted 
first 125 programs were collected from the karel midpoint finding assignment from the cs106a class 
70 of them working programs 
pre processing
30 non working and 5 from each grading bucket category plus check plus check check minus minus 
initially the plan was to obtain hundreds of programs from professor chris piech but due to student privacy issues he was only able to provide a couple 
as such most of the programs were personally written making sure to include all major possible solutions and collected from friends 
then after obtaining the programs the training programs were pre processed to obtain feature vectors describing each program and each function in each program 
for the training data the function vectors also contained a label on whether it is well decomposed and well formatted and the program vectors contained the actual grade of a program 
as for the testing data no program vectors were given then for clustering 3 different strategies were explored 
then for the third strategy i tried something more clever 
namely to run kmeans twice 
once to find the the average of the coordinates within a single program and the second time to cluster these averages into k clusters 
clustering by counts of primitives is very susceptible to unnecessary command calls 
also it does not distinguish between call times 
clustering results
clustering by pouring all coordinates may be susceptible to outlying programs that tend to spend too much time at a certain place 
double k means overall seems to address the issues above and can even be implemented for variable times 
however it breaks down with infinite loops 
with the training programs clustered we proceed to fit a logistic regression or a naive bayes model for all the function is each program cluster 
for this
logistic regression or naive bayes for function classification
after fitting the cluster functions from our training data we can use them to provide feedback to a student on his functions as he is still working on the program 
all we need to do is take his program and for each function decide whether it is decomposed or not and point to whether we found it to have appropriate length and non repeating code as for format we can report whether we believe each function to be well formatted and point to whether it has correct indentation no blank lines and comments 
feedback
once we have decided whether each function is well decomposed and well formatted we take an average over all functions in the test program and test whether the program is functionally correct to come up with the program description vector program decomposed program well formatted program works 
then we fit a softmax regression model for each program cluster in the training data where y i 1 2 k 
softmax regression
more specifically we do gradient descent on the following cost function and taking the gradient gives then plugging into the stochastic gradient descent rule we get for each value j 1 k where we set 0 001 1 m m number of train programs and k 5 finally we use this model to assign a bucket style grade to a test program 
the following minus 86 75 80 69 as we can see from the table clustering before hand seems to perform better 
also running logistic regression on each function seems to have higher accuracy than logistic regression 
softmax results
this can be easily explained if we consider that the characteristics in the function vector are not necessarily independent 
however while our softmax model performed at 87 with logistic and clustering for the plus bucket it performed poorly for the check plus and check minus buckets 
on retrospect we can explain this polarizing behavior by realizing that when we ran logistic regression on each function we only identified between well decomposed formatted or not at all 
a future possible fix would be to run softmax on each function and take that average 
however that approach would be very susceptible to mal formed functions and would likely require to increase the size of the feature function vector 
future work
also we worked strictly within a karel world setting and as such it would be interesting to see whether these results also hold for purely java programs 
we have all pondered about the same thing in a relationship is he she the right one for me 
countless articles quizzes suggestions tips and consultants out there are trying to answer this question for us 
they bring out big words to make it logical they tell us to mind the differences but also note that some differences matter and some do not they place their own arbitrary weights on terms such as ambition core values intelligence emotional intelligence spiritual beliefs etc 
it makes us wonder if we are already talking about logic in romance why don t we take a step further 
what if instead of reading these sources and still remaining puzzled about the relationship we can tell you with a high confidence level exactly how likely you are going to be with your significant other for the rest of your life with the help of a machine learning algorithm 
thanks to how couples meet and stay together dataset hcmst by rosenfeld michael j reuben j thomas and maja falcon it is now possible based on your everyday habit your usage of internet your marriage status and other miscellaneous information about your life to predict if you will stay with him her for at least the next few years 
the hcmst dataset has been utilized in various ways concerning different social study fields yet most of the papers are based on a few arbitrarily chosen measurements and no machinelearning related work is found 
early work entails the internet related data to argue increasing internet coverage has increased chance of meeting partners for glb gay lesbian bisexual individuals and internet as a social intermediary has partly replaced traditional dating spaces more recent working papers discuss the topic of relationship stability as a whole following the main motives behind the hcmst project 
however restricted by the limits of human these papers tend to focus on selected areas of the whole dataset drawing conclusions from partial observations and features thus indirectly putting arbitrary weights on the subject matters 
one such example chooses to investigate the relation between relative earnings in the household and the relationship stability potentially due to the novelty of the data and the fact that it has only finished its budgeted five waves very recently in 2015 until now there exists minimal efforts to attempt bringing all parameters together under the roof of machine learning algorithms 
we are here to pioneer 
since this project aims at predicting the future relationship status based on current info we believe that at this point only the data collected in the first wave is relevant to our purpose 
we have hence dropped data collected from wave 2 to wave 5 keeping only the survey results on their relationship status at each milestone the survey assumed that some couples who didn t respond to follow up surveys still stayed together 
ii data elimination
for the sake of precision however we only kept data of respondents with partners in the beginning of the timespan who continuously responded to the surveys in following periods until wave 5 or breakup 
based on the results from the 4 follow up surveys we generated feature final relationship status boolean to indicate the final status of couples after 6 years 
after deleting all the redundant features and observations we are left with 1569 respondents with 269 features 
the initial data contains many features with a substantial amount of missing values 
while some bear minimal relevance to our goal e g 
iii preprocessing non standard data values
gender of the 15th member in your family and can be dropped without significant impact other missing values are indicative of important information and dropping them will result in high bias 
therefore for those question answers that have already been processed we only kept their corresponding feature 
for example for question 34 how would you describe the quality of your relationship 
we dropped feature q34 and kept the corresponding processed feature relationship quality 
this will prevent problems generated by singular matrix in future prediction models 
secondly some of the missing values are results of branching questions 
for example if question 12b is only required for people who answered yes for question 12a then feature q12b will contain lots of missing values 
for these features we integrated them into the main branch question by generating more categorical classes in features like q12a 
thirdly we dropped clearly unrelated features with high level of missing values or nonnumerical and non categorical values 
these operations leave us with 148 features to work with 
considering that the size of our datasets after processing stays around 1600 we decided to include fewer features to avoid potential overfitting 
therefore we have implemented the forward based sequential feature selection based on logistic regression model with crossvalidation of 10 folds 
iv feature selection
features were selected based on misclassification rate using logistic regression model and feature selection terminates after the misclassification error no longer improves 
this leaves us with 47 features with 10 most important features any output of logistic regression is in the range 0 1 where output smaller than 0 5 will be categorized as 0 and the rest categorized as 1 
this method generates misclassification error of 11 1 for train set and 12 39 for test set 
we implemented the the svm without kernel then we integrated the gaussian kernel into svm which didn t improve the result 
observe that the train error using gaussian kernel is substantially smaller than normal svm without kernel 
ii support vector machine
because the number of observations is relatively small we believe that using kernel would further complicate the method and therefore result in overfitting 
many of the survey questions have sequential correlation with each other and some features existence are entirely based on others i e only respondents who have answered yes to question have your religion changed since 16 
will be asked to answer what is your religion at 16 
iii decision tree
therefore we believe that the decision tree model would be a proper representation of the set of if then choices and would replicate the design logic behind the survey we generated the top down binary decision tree by examining the optimal statistical improvement brought about by each feature at each split 
to capture the optimal improvement we ordered both the categorical and the continuous attributes from the smallest to the largest and measured the improvement in misclassification error by dividing at each consecutive pairs 
when a missing value is encountered we used surrogate split because many alternative features with high variance can be found in actual prediction we first generated the binary decision tree with minimum branch size of 10 observations and unlimited depth and this results in misclassification error of 4 07 for train set and 22 67 for test set after 10 fold crossvalidation 
as certain level of overfitting is shown we decided to pre prune the tree by adding the maximum number of splits in our decision tree 
our final dataset is left with fewer than ten numeric values the distance from the respondent home to the current home how long ago they first met and how long ago the respondent first lives together with partner etc 
in order to apply naive bayes model we have discretized the data by converting these numeric features into several categorical classes 
iv naive bayes
because the assumption of the naive bayes is that every conditional probability is independent of each other we also calculated the covariance matrix between all the features and dropped features with a covariance over 0 3 
we then applied the naive bayes model trying to maximizebecause some of the survey questions have relatively small amount of responses or unbalanced results we also added laplace smoothing to ensure at least one data point per feature per class 
using the obtained probabilities w x01 w x0y x we then cross validated by partitioning the data in 10 folds and obtained the averaged misclassification error 22 27 for train set and 23 19 for test set 
at this step we have graphed the test error and train error regarding each method after crossvalidation 
as the graph have shown the errors generated by all the methods range from 13 to around 24 as the graphs have shown generally logistic regression produces the best result while naive bayes performs the worst 
results and analysis
decision tree and support vector machine have very close performance in both test and train dataset 
one possible reason that naive bayes doesn t generate good precision is that some of the input features are not completely mutually independent 
despite that we have used sequential feature selection and later removed features with correlation greater than 0 3 when using nb some features left are still vaguely related with each other and this violates the basic independent assumption of naive bayes 
the correlation between variables however helped to boost the precision in decision tree model because the missing values can be replaced with their correlated alternatives using surrogate splits we believe that one explanation for the logistic regression to generate better result than decision tree is the high dimensionality of the dataset compared to the number of observations 
as trees always tends to overfit in presence of high dimensionality since it has high freedom degree we had to limit the number of splits to prevent overfitting 
however some important information are lost in this process and bias is sacrificed to obtain lower variance 
logistic regression though very simple does draw information from the basis of the entire set of features and therefore could perform better than tree based model note that the false positive rate is almost about 3 times as high as the false negative rate in logistic regression svm and decision tree 
this is partially because the initial dataset is imbalanced with the ratio between positive data and negative data being 2 1 
we then decided to rebalance the dataset by adding a weight vector to assign more weights to negative classes 
the table below shows false positive and false negative results generated using the re balanced dataset 
notice that the the precision rate doesn t stay stable for all the four models and false positive and falsenegative rates are more balanced than before for logistic regression svm and decision tree 
however the false negative rate from nb is still substantially higher than the other three meaning that naive bayes model is very pessimistic about the couple s relationship it tends the believe a couple would break up even indeed they will very likely not 
we also tried to apply the logistic regression and decision model to predict the data points that have been deleted from our dataset due to 1 missing values 2 unknown labels 
these observations were first removed from our samples because the respondents stopped to respond to the survey from the second third or fourth round 
interestingly the pattern demonstrated by our prediction shows that the earlier the couple stops to respond to the survey the more likely they will get a 0 in prediction 
in other words for couples that stop to respond to the survey since round 2 our model predicts that very large portion of them will break up in 6 years 
this verifies our guess people don t just quit in the middle of the survey for random reasons the absence of a couple s voices in later surveys might already indicates a deceased romance and the pain and embarrassment to admit this usually makes people shun away 
while our models express 10 test errors it is rather reasonable given that relationships are still indeed based on one of the most complex systems in the known universe human mind 
our results provide insights of what otherwise remains mysterious and unquantified and will potentially help sociologists and everyday individuals alike in process of fitting samples through the models we have also discovered that adding more samples don t always result in higher accuracy 
this is likely rooted in the nature of relationships and romance it s the surprise and unexpected turn of events that highlight their beauty 
when consensus deems long distance relationships hard to maintain there are always outliers who prove it wrong and same goes for other difficulties in love 
for this exact reason while increasing data size from additional surveys will theoretically improve our prediction we believe that it may not be necessary 
currently academically motivated parties whether for research industry purposes or casual interest have access to a wealth of scholarly information to meet their information needs 
in fact in the current age with thousands of articles and papers constantly being published in hundreds of journals most conducting research have the opposite problem of having to sift through too much often not incredibly relevant information to find what they are looking for 
thus automated recommendation platforms are becoming increasingly more relevant to uncovering helpful resources and potentially interesting articles that cannot simply be found by following a trail of citations or an unfocused keyword search current article recommendation platforms generally fall under one of two umbrella categories 
content based models collaborative topic modeling is a class of recommendation algorithms that combine topic modeling of articles with implicit feedback from users i e 
information about user s article preferences that is not based on an explicit ranking system or explicit evaluations of articles 
however the majority of such algorithms currently in place tend to be heavily skewed in favor of one model over another 
collaborative topic modeling has been most prominently used in news article recommendations that focus on filtering and user similarity matrices only modeling content for broad keywords and for articles released within the hour 
on the other hand scientific article recommendations tend to heavily rely on topic modeling based on the content of the title and abstract given that these snippets tend to be filled with keywords that give fairly accurate insight into the content of the article in this paper we seek to apply an adaptation of the collaborative topic regression model to make recommendations for humanities research and nonfiction writing based on a combination of implicit network and user feedback and topic modeling 
non scientific academic publications and nonfiction writing have topics that can be modeled somewhat effectively but abstracts and titles tend to be metaphorical in nature and less clustered around a small number of technical terms and clear cut ideas 
further articles tend to be concentrated around specific niches with respect to readership and reader interest 
on the other hand such publications also tend to come with lower readership and citation count and thus any recommendation platform needs to be robust with respect to relatively small amounts of implicit feedback 
thus the different environments of humanities research are not necessarily optimally modeled by either a purely content or filtering based model or the same parameters and combinations effective for high volume article sites or technical stem papers 
finally we will examine if a similar model can be used to effectively simulate users to iteratively update purely content based recommendation platforms 
latent dirichlet allocation lda note also that the probability of generating each document is p ni p wi ni so argmax log p wi argmax i log p ni log p wi ni argmax i log p wi ni as ni is drawn from a poisson distribution independent of and i 
a topic modeling via latent dirichlet allocation
collaborative filtering matrix factorization for recommendations is a latent factor model in which manifest variables relate to latent or nonobservable variables 
in this scenario we only observe the set of all rij which represents the rating that user i gives article j 
b collaborative filtering via matrix factorization
notice that while a high rating is unambiguous a low value can symbolize one of two situations user i has read the article and does not recommend the article to others user i has never seen the article and thus cannot recommend it 
therefore the goal of the ctr method is to change zero entries of the second type into predictions about whether user i would recommend article j 
this distinction is especially critical in light of the fact that the majority of user document pairs would fall under the latter rather than former category we represent both users and items as latent k dimensional vectors ui and vj where k is significantly smaller than the number of users or articles 
we predict new ratings by computing the goal of the algorithm is to minimize the least squared error over all user article pairs 
if u ui i i 1 is the set of all user vectors and v vj j j 1 is the set of all article vectors then the algorithm finds where u and v are regularization parameters 
we use probabilistic matrix factorization pmf to model the generation of user and article vectors as it scales linearly with the number of observations and performs well with large sparse data 
3 the generative process for producing the user and article data is as follows 1 for each user i 1 
the precision parameters cij measure the confidence of the rating rij 
i choose a latent user vector ui where cij is the precision parameter 
as mentioned previously a high rating rij unambiguously represents a positive rating that user i gives to article j therefore cij should be high in magnitude 
however a lower rating rij can symbolize multiple scenarios so cij should be lower in magnitude 
specifically in our algorithm the input data rij consists of only binary values so we assign for hyperparameters 0 a b note however that each u t i vj can be a decimal value 
to find the optimal values of u and v given a set of binary rij we use gradient coordinate ascent on each ui and vj to find u and v in equation 2 
we may then generate a set ofrij u t i vj to use as the prediction ratings ctr on its own however cannot make accurate recommendations for articles that few or no users have seen 
therefore we must complement the ctr method with lda topic modeling in our model 
collaborative topic regression matrix factorizations learns short feature vectors to represent each user and document and predicts the recommendation status of the pair user i and document j asrij u t i vj 
when incorporating content modeling of the documents we continue to predict u t i vj but here vj j j where j is the topic proportion learned via lda and j is a latent error variable to offset j the purely content based proportion that enables the document s latent vector to diverge from j 
iii the regression model
thus as more users rate an article the prediction becomes increasingly dependent on the recommendations of users and less so on the lda model of the document proportions 
the ctr model has very strong similarities to collaborative filtering as can be seen in the generative process that characterizes the model and its assumptions about how these articles and feedback are generated 
assume that we begin with k topics derived from an lda analysis of the documents 1 wherefor the user document pair i j assume the rating can be modeled as rij n u t i vj c 1 ij given this generative model the expectation of rij is similar to collaborative filtering e rij u t i vj with the primary difference in how we model the latent document vector incorporating the content based proportion vj j j where j n 0 1 v ik 
thus learning each of these parameters can be done by finding a maximum a posteriori estimate of ui vj j and rij where he can find the map estimates of u v r using coordinate ascent 
we compute the map estimates by maximizing the log likelihood of the data in particular the overall log likelihood of each of u v r 1 j minimizing the least squared error of our eventual prediction with regularization ll u v 1 j r u v then we can iteratively maximize this function using coordinate ascent by setting the gradient of the log likelihood to 0 and determining the the new optimal values for each user and document latent vector ui and vj as here ri rij j j 1 and rj rij i i 1 
for the moment we will fix j as the original lda proportions and treat the proportion vectors as constants 
for the moment we predict the expected ratingrij u t i vj where vj j if there is no user information about document j 
we will eventually employ an algorithm in the style of
iv making recommendations
collaborative topic regression and similar models have been studied in a wide variety of largely scientific contexts however many of these recommendations are largely if not completely irrelevant to the content of the article 
thus we sought to improve these recommendations by using ctr on a set of simulated recommendations to iteratively update these similar article recommendations 
v empirical study simulating recommendations with escholarship
we simulated users by considering each list of 20 similar articles corresponding to a particular article to be recommendations of a user and then applying ctr on this set of articles and user note that we discard the article from which we generated the simulated user to arrive at a new list of recommendations for this user which hopefully present a better set ranking of similar articles to the original article in order to test this we gathered data from 580 users and 4827 articles from the international journal of comparative psychology 
each user had a total of 20 similar articles from which we isolated the odd numbered ones as user recommendations 
this gave a total of 5800 user item observed pairs 
experimentally we considered a variety of values for the model hyperparameters to optimize precision and recall using a restricted grid search settling on k 100 u 0 01 v 0 1 a 1 b 0 01 
some example topics yielded have top words species patterns california populations habitat public policy states issues economic and expression gene genetic function levels 
some of the hyperparameter search statistics for k are pictured below some sample data for hyperparameter configurations illustrates more completely the relative performance and confidence of our model in predicting training witheld testing and new articles in its top recommendations as desired the learning model predicted all of the userrecommended articles with high ratings and 100 of the time recommended the original article from which the user was created with a rating at least 0 75 or in the top 20 recommendations note that we withheld these articles for testing purposes from the training data set 
additionally 100 of the time the top 3 withheld recommendations were recommended by the ctr algorithm 
overall the recall on the relevant withheld and provided recommendations was 95 
as the data suggest not all of withheld articles were predicted in part because the content based model used by the international journal of comparative psychology often made predictions irrelevant to the bulk of the other recommended articles that were thus discarded 
in this sense our model performed far better in recommending relevant articles to compute the precision quantitatively we took a random sample of 20 of the 580 users and classified each of the original recommendations from the international journal of comparative pyschology as relevant or irrelevant by hand where only articles clearly about a different subject entirely were marked as 
then we saw that in aggregate our model correctly recommended with a rating at least 0 75 or in the top 20 recommendations over 90 of the articles and less than 5 of the articles 
as a this graph shows the proportion of documents that received predicted rating r ij in the intervals depicted on the x axis for k 25 where a higher rating corresponds to a prediction that user i is more likely to like document j 
the blue bars represent training data and the green bars represent withheld user information 
as the data suggest the provided recommendations received high scores as they should given that a user has expressed interest already and many of the withheld documents around 50 occurred as a top 20 prediction for the article 
this graph shows the proportion of documents that received predicted rating r ij in the intervals depicted on the x axis for k 25 including new predictions of articles not originally predicted by the comparative psychology journal model with the yellow bars 
given the sparseness of the training matrix and also the high confidence in predicting original user recommendations there are a large number of new articles predicted especially as the confidence decreases case study consider user 19 created from the recommendations generated for the article the development of juvenile typical patterns of play fighting in juvenile rats does not depend on peer peer play experience in the peri weaning period 
below is a table summarizing the results we obtained as far as the journal recommendations 
our model predicted 8 new articles not included in this set the highest rank of which rank 8 was the original article from which we drew the simulated user data and which included all 7 other articles scored as a like altruism in animal play and human ritual and how studies of wild and captive dolphins contribute to our understanding of individual differences and personality 
thus our final model was able to achieve around 95 precision and relevant recall on the dataset making it a far better article recommendation platform than the existing content based platform employed by the international journal of comparative psychology that recommended at least 30 irrelevant articles for each of the randomly sampled papers 
after simulating user data we implemented our algorithm on user given ratings data 
for this scenario the user article interactions are much sparser and noisier than in the first scenario while the simulated users for escholarship each had at least ten recommendations the average number of recommendations in our citeulike dataset is roughly 6 4 with most users recommending fewer than 5 articles 
vi empirical study humanities research with citeulike
in addition users rarely recommended articles that were all in the same topic we also expanded the diversity of our article corpus by not limiting our articles to one journal while the escholarship articles primarily originated from international journal of comparative psychology our citeulike articles were found in journals ranging from latin american research review to asian theatre journal 
some were even written in foreign languages see more details in the discussion section 
these articles compared to scientific articles collectively had fewer abstracts 
these humanities abstracts also tended to be less summary focused we collected all 2115 of the user profiles whose declared research areas lay in european eastern asian african american or australasian language or literature studies 
of these 223 had at least one article in her personal library collectively the set of users had 1269 articles 
like in the previous empirical study we withheld half of our collected user article interactions ratings to reserve for the test set 
therefore the training set of user article interactions consisted of 715 instances of a user recommending an article as before we used grid search to find the hyperparameter values that maximized our precision and recall 
for each set of hyperparameters we ran lda ctr on the training data and produced recommendations for our set of users 
for this study the hyperparameter values that optimized our precision and recall were k 40 u 0 01 v 100 and cij 1 0 01 where cij 1 when user ui recommended vj and cij 0 01 otherwise as previously mentioned we hide half of the users ratings and use them to evaluate our algorithm s recall performance to calculate recall for each user ui we compute how many articles in the entire dataset that user ui rated positively both in the hidden and training halves of the recommendation data 
we then take an average of our results 
our algorithm then has a 64 recall rate which means that our algorithm predicts at least 28 of the hidden articles note that it is possible for our algorithm to choose to not recommend articles associated with ui supplied in the training set 
though the nominal value is low our algorithm performs relatively well compared to other recommendation systems to consider our algorithm s performance in the context of citeulike s current recommendations we analyze our algorithm s accuracy only for users who have rated at least 20 articles users with accounts can only receive recommendations after adding at least 20 articles to their libraries 
we calculated precision in a similar manner as in the previous empirical study for each user in a random sample we classified the recommendations for which the algorithm provided rating were above 0 75 for that user 
we then manually classified the recommendations as relevant or irrelevant in a similar manner as in the previous experiment 
with this metric 89 of the algorithm s recommendations fell into the category giving us an 89 precision value 
based on our analysis we conclude that composing lda topic modeling with collaborative filtering significantly improves the existing recommendations from escholarship s international journal of comparative psychology 
for each psychology article our algorithm not only adds relevant similar articles but also removes irrelevant articles from the original set of given recommendations 
vii discussion
this means that our lda ctr algorithm can augment escholarship s existing recommendation system 
when we apply our lda ctr model to the citeulike humanities articles database given the sparse and noisy user data we achieve precision and recall results that compare to those of previous recommendation algorithms an interesting observation was that the lda algorithm on the citeulike data categorized words from foreign languages besides english into their own topic 
this phenomenon has a theoretical explanation articles written in spanish french and italian comprised a significant portion of the articles retrieved from the citeulike database and within these documents foreign word tokens appear together 
therefore to effectively assign topics to foreign documents we must employ a machine translation model in the future for our other future progress we are looking to implement our algorithm in practice we are in the process of communicating with both escholarship and citeulike to inform them of our suggest improvements to their recommendation algorithms 
in terms of improving our current model we plan to employ the following changes to our algorithm accounting for documents with the same author as previous recommendations given a user ui and document vj we set a different document v k with the same author as vj to have c ik 0 1 
implementing this change would allow the algorithm to have less sparse data concerning users who do not rate many articles extending the lda to run on introductions rather than only abstracts as many humanities articles lack legitimate abstracts introductions would expand the dataset to include more articles incorporating citation sources into the learning model given a user ui and document vj we set a different document v k that cites or is cited by vj to have c ik 0 1 
thank you to professor duchi for advice on the project and to chong wang and david blei for sample data 
acknowledgment
the exo 200 experiment uses a liquid xenon lxe time projection chamber tpc to search for neutrinoless double beta decay 0 an extremely rare hypothetical decay that would indicate the majorana nature of neutrinos 
events deposit energy in the lxe through both scintillation light 175nm and free ionization charge 
the scintillation light is detected at either end of the exo 200 detector by large area avalanche photodiodes apds 
the ionized charge is drifted along the z axis of the detector where it first passes a shielding induction wire grid v wires and is than collected by a second wire grid of collection wires u wires 
each wire is 3mm in pitch and wires are ganged into groups of three before being readout and saved 
the total charge energy of an event is then calculated by determining the sum amplitude of all channels which collected charge in order to accurately reconstruct the energy of the waveform signals have to be accurately identified as either collection or induction signals 
this is currently done by performing a 2 fit to both a collection and induction signal and than classifying the waveform based on the ratio of these scores 
waveforms classified as induction are flagged and then not included into the sum when determining event energy 
this current technique achieves reasonable efficiency at identifying collection and induction signals but energy deposits spanning multiple channels present a slight challenge because waveforms will contain both collection and induction signals 
in addition the waveforms are shaped before being saved to disk making identification and energy estimation somewhat more complicated 
in this study an alternative technique for reconstruction of event energy in exo 200 using boosted regression trees from sklearn is explored 
the exo 200 monte carlo has been described in detail elsewhere each waveform in the detector consists of 2048 samples taken at a sampling rate of 1mhz with a fixed trigger time at 1024 
in addition standard monte carlo includes added noise sampled from real data into the waveforms 
monte carlo
in order to simplify the first stage of the analysis no noise was added 
the second stage of this analysis included added noise into the waveforms 
this was done by using a database of noise waveforms seen in real data and adding these into the generated waveforms at random an example is shown in
initially a template waveform for both induction and collection was created by simulating an event in the exact center of the detector 
the resulting collection and the average of the induction signals from this event was than used as a template to represent the typical collection and induction waveform 
template generation
the results of these templates in time space are shown in although most induction and collection signals have roughly the same shape there are some differences due to the exact position and energy of the initial deposit 
using these two templates offered a reasonable projection into collection and induction space but this did not fully capture the detailed shaped information 
for the final analysis a more sophisticated template generating algorithm was implemented 
this involved using a k means clustering algorithm to find a larger subset of template waveforms that better samples the waveform space results for template generation using the sklearn k means clustering algorithm with 15 clusters are shown in
the goal of this analysis is to learn a mapping from the space of digitized waveforms to the amount of charge deposited on a particular channel 
early work showed that learning in the full space of 2048 length vectors that comprise the raw waveforms was impractical 
optimal filter
we devised a method for compressing some of the information stored in each pulse into many fewer than 2048 real parameters using optimal filters optimal filters take as inputs a noisy time series v t and a template s t and return the amplitude of s in v under certain assumptions it can be shown that an optimal filter is the best estimator of of the amplitude of s in v the optimal filtering we employed is equivalent to lms fitting of the template to the signal in the frequency domain 
the 2 cost function for a particular amplitude a is defined as followswhere f ands f are the fourier transforms of the signal and the template respectively 
j f is the power spectral density psd which is used as the weight for each frequency in the cost function 
in practice the waveforms are always sampled discretely so that discrete fourier transforms are implemented using ffts and the integral in equation 1 is carried out as a sum 
for waveforms generated without added noise a uniform psd was used but for waveforms with added noise the average psd was estimated using real data as previously stated we initially started by compressing waveforms into 2 component vectors where the first component is the amplitude of the optimal filter with a typical collection pulse shape used as a template and the second component is the amplitude of the optimal filter with a typical induction pulse used as a template 
the results of doing this for a sample of 10000 monte carlo pulses is shown in
the compressed pulse data with 2 components shown in in addition this same procedure was repeated for this same testing and training set using the 15 template wfs generated in clustering instead of the 2 typical templates input by hand 
the results of this method is shown in finally to test the generalization of this method to mc with added noise we repeated the same procedure using the 15 template wfs and 90k training wfs generated with noise and reported the error on a test set including 10k wfs with added noise 
energy estimation
the results are shown in
using the boosted regression tree algorithm from sklearn an initial energy reconstruction algorithm has been implemented to predict the energy associated with u wire collection signals in the exo 200 detector 
the current algorithm uses a set of n 15 templates to represent pulses as vectors in r n using an optimal filter 
conclusions
a boosted regression tree learns the mapping between these vectors and the energy of the pulses 
this resulted in 0 1 error in the energy estimate for wfs with no added noise but 17 error for wfs with realistic noise 
future work by the exo collaboration to improve this analysis will focus on understanding the large error of this estimation 
for this project we investigated the predictive power of newspaper articles on the stock prices of various companies 
using supervised learning we were able to obtain a testing accuracy of up to 61 
we then performed reinforcement learning on this predictor feeding its predictions into a markov decision process mdp which bought and sold shares on a simulation that we programmed 
we compared the mdp s performance on a year s stock prices to that of a random guesser 
overall we conclude that the mdp with our predictor generally outperforms a random guesser 
we treated data from newspaper articles with machine learning tools to predict up or down changes in the stock prices i e 
our response was binary 
in particular we examined four algorithms the naive bayes algorithm svm the perceptron and boosting with weak learners on a bag of words model of newspaper articles 
in addition we optimised these algorithms by stemming the bag ofwords introducing term frequency inverse document frequency tf idf to improve our algorithms grasp of the relevance of words and finally with cross validation for the best parameters after obtaining a base predictor that predicts using newspaper articles whether the stock of a company would go up or go down we also wrote a markov decision process mdp learner that builds on the base predictor using reinforcement learning that is able to buy and sell shares of a company 
in order to do so we needed to make the assumption that this approximates to a markov process 
our final simulation tests if there may be credence in such an assumption despite its limitations 
our literature survey found five works that also used news articles for classifying stock prices 
however almost all of these previous works used a naive bayes classifier on a simple bag of words model which generally performed poorly gidfalvi two such previous works were former cs 224n projects from stanford university ma chen et al 
previous works
the last work is a masters thesis from the norwegian university of science and technology by aase we observe that none of these previous works employed tf idf or cross validation to improve their algorithms which may explain why they mostly concluded that newspaper articles have weak predictive power on changes in stock prices 
in addition none of these previous works used reinforcement learning to improve on their base predictor 
we needed to obtain articles that mention specific companies from reputable newspapers in order to compile our data set and set up our bag of words models 
we decided to use reputable sources instead of publications like twitter buzzfeed etc 
data
because we hypothesize that reputable articles based on fundamental company research and understanding will generally have more predictive power than less reputable articles 
unfortunately the newspapers that we wish to examine such as the new york times and the wall street journal have paywalls that block their websites and make automatic scraping difficult 
instead we relied on the proquest newsstand database to obtain our data 
proquest newsstand archives all articles from these newspapers in a searchable database 
by writing an algorithm to generate federated search urls we were able to automatically obtain the pqms xml tree which contains the urls of the full texts of articles that mention the company in interest from the newspapers that we want the articles from and the year for which we wish to generate the data 
we then wrote an xml tree parser in order to automatically compile a list of all the pqns urls wherein the article full texts are contained unfortunately proquest does not allow these articles to be accessed without a validating url 
hence we also had to write a web scraper specifically tailored to work around the limitations of the proquest database by generating curls so that our automatic article gatherer behaved like a real user s browser 
we then used regular expressions to parse only the full texts and the article publication dates from the webpages to finally obtain our raw text data which could then be treated with the python stemming library for pre processing 
the schematic in next we modeled our input data i e 
news articles as a bagof words model 
for the input matrix x each of the n columns represents a word in our dictionary and each of the m rows represents a news article 
each x ij entry contains the frequency for which the word j appears in the news article i our response variables were the stock movements of individual companies 
we obtained the end of day stock prices for the last twenty years of six companies we chose these particular companies either because their stock prices are volatile or mainstream news publications frequently write about them the stock prices for these six companies were obtained from the quandl database using their api 
for our base predictor we only took into account whether the stock prices moved up or down to obtain a binary response 
our response i e 
stock prices is a binary label where 1 indicates the stock price has gone up and 1 indicates the stock price has gone down 
we computed the label by subtracting the closing price on the day the article came out from the closing price before the day the article came out 
if the article published date falls on a weekend when the markets are closed we used the difference between the monday closing price and the friday closing price we separated our data into a training and testing set where the training set was twice the size of the testing set 
in total we processed 6 776 articles for our dataset over six companies 
we used four supervised learning algorithms to treat our data 
since previous literature mostly used the naive bayes algorithm we included it in our investigation to compare with other algorithms 
supervised learning algorithms
for naive bayes we made what s called the naive bayes assumptionwe assumed that all the x i s are conditionally independent given y therefore now to figure out whether the stock price will go up y 1 or go down y 1 we will calculate the probabilitythen we will calculate the same probability for stock price going down i e 
p y 1 x 
now we can classify whether y 1 or y 1 based on which one has a higher probability in addition we also treated our data using support vector machines svm in the hope to capture the nonlinear relationship in our data 
the support vector machine model was initialised without smoothing on a gaussian kernel then we used the perceptron algorithm a simple linear algorithm that decides whether an input belongs to one class or another 
we first tried the algorithm with a bias term so we made a prediction according tofinally we applied a boosting algorithm that automatically chooses its feature representation 
in this case our boosting algorithm was initialized using simple decision stumps as weak learners our motivation for using these four algorithms was to compare their relative performance in testing accuracy as we applied the various optimisations in subsequent steps 
our first step in dealing with the raw text data was to pre treat it with a stemming library 
this is to ensure that reflects the various forms of a word e g 
optimizations
diversify diversifying diversified as the same token 
our reasoning for this is that the various forms of the word do not differ significantly in semantic content and indicate roughly the same meaning when they occur in newspaper prose so the stemmed forms of the words is a better representation 
we will compare the test accuracy of the various algorithms using stemmed and unstemmed tokens our second step in optimising the four algorithms was to apply term frequency inverse document frequency tf idf weighting on the stemmed text data 
tf idf places the following weights on the tokens as they appear where t refers to the term index d refers to the document index and d the corpus of all documents 
additionally tf t d refers to the number of times the term t appears in the document d and df t refers to the number of documents d the term t appears in 
finally tfidf t d d refers to the final weight placed on the term t this weighting places less weight on words that occur in more articles and that are therefore less able to distinguish between articles 
our motivation for applying tf idf was because we observed that a naive weighting on token frequencies put heavy weights on very common words such as the for etc 
which dominate other words in terms of absolute frequency 
we hypothesized that while such words are on an absolute level more common words they are less able to distinguish between articles than rarer words that are able to distinguish more between articles or better reflect an article s semantic content finally we applied parameter optimisation via cross validation of these four algorithms drawing on the training data for a crossvalidation set 
the final optimised parameters are naive bayes with a laplace smoothing of 1 on the observations svm with a linear kernel without smoothing perceptron without regularisation and boosting on decision trees with arbitrary depth on 100 estimators 
we used the final parameter optimised algorithms to be used on our mdp reinforcement learning simulation 
having obtained a base predictor for stock price movements based on newspaper articles we applied reinforcement by using it to build a markov decision process mdp that simulated the buying and selling of shares 
the base predictor we used to train the mdp is the optimised naive bayes predictor 
reinforcement learning
we placed our mdp learner in a simulation run using real data from the stock price of tesla in the year 2015 and using real newspaper articles from the new york times and the wall street journal all of which were not in the training set used to train the base predictor 
our design of the mdp learner is as follows these are the parameters we used to write our markov decision process 1 states percent return from the starting amount 2 rewards same as the state 
a higher percent return equates to a higher reward 3 discount factor 0 995 4 actions for the most part our algorithm was allowed to buy or sell up to 3 stocks or take no action 
however we placed restrictions on the actions based on the following conditions if we had negative cash the algorithm was only allowed to sell stocks up to 3 if there was no article for a particular day the only allowable action was to do nothing 0 5 transition probabilities before discussing transition probabilities we must first consider the observations we recorded 
we observed every transition from one state to another under an action 
for example if we moved from 3 return to 4 return by buying 3 shares we would record it 
now let the prediction we get from an article on a given day be represented by p and the test accuracy of that prediction be 
let the number of states we have be represented by n 
finally let o sa s be the number of observations from state s to s under the action a 
then the transition probability is given by the following in order to implement mdp however we needed to fundamentally assume that the day to day transitions of a stock buy and sell portfolio satisfy the markov property 
such an assumption is we acknowledge possibly contentious 
while changes in the price of a stock on a day to day basis and the actions of buying and selling a stock are arguably independent because our mdp is a trader it necessarily needs to hold different amounts of stocks in its portfolio from day to day 
this different level of stocks being held or shorted is not reflected in the state which therefore dissatisfies the markov property 
hence we implemented a penalty on holding or shorting too much stock such that the amount of stock held by our mdp s portfolio is usually within 3 to 3 shares 
in that manner it is able to buy or get rid of stocks within a single action and so the transitions between states and actions approximates the markov property 
in other words we are able to implement mdp by making the percent changes in the value of the portfolio and the actions of buying and selling stock a pseudo markov decision process 
then we implemented the value iteration algorithm to compute an optimal mdp policy and its value 
for each day that the stock market is open we compute the maximum q values that a state can obtain by taking all the possible actions where the q value equals the sum of q i for each of the new states s reachable from the current state s andplease see the appendix a for a pseudocode summary of our markov decision process and value iteration implementation 
generally the four algorithms showed incremental improvement with each optimisation that we applied as demonstrated in the upward trend in test accuracy shown in in addition since there were insufficient days in the year to allow the learner to converge the mdp is still not making optimal decisions 
it is unclear however if extending the learning period and increasing the amount of data the learner has access to will lead to convergence since the if we extend the period to more than one year the assumption of the markov property may begin to break down and the transition probabilities arrived at by the mdp for one year may not be useful in the next year or in the long run in general 
results
whether this is a fundamental limitation of using mdp for such a use would require further experimentation 
in summary the four algorithms that we used naive bayes support vector machine perceptron and boosting without any optimisations only produced results no better than random as expected 
however adding several optimisations were able to bring the test accuracy up to 61 stemming helps unify the words that are represented in different forms in our training and test data but have the same semantic meanings 
tf idf helps increase the weights of rare words that are strong indicators of a particular class and decrease the weights of common words that are weak indicators of a particular class 
parameter tuning helps us to figure out what are the best parameters to use in the four algorithms in order to produce the best results there are few interesting takeaways from our work in modeling the stock simulation as a markov decision process and run ning the value iteration algorithm on it 
first we found that modeling the discretised percentage returns as states helps the value iteration algorithm converge quickly 
second we found that using past observations to determine the transitional probabilities gave us an accurate measure of the magnitude with which the stock would change in the future 
third adding a penalty for buying too much stock helps prevent the case where it would take too long to sell the stocks during a negative run of a particular stock 
fourth this penalty also helped us approximate the markov process even when some assumptions about the model have been violated 
lastly we acknowledge that in order to conclusively determine whether or not the mdp simulator consistently outperforms a random guesser more analysis must be performed 
we tested the four algorithms of our base predictor on individual companies stock prices separately 
it remains an open question as to whether such supervised learning methods would be effective in predicting stock price movements in general i e 
if there is a general model that is able to be tested on companies in general 
this would be especially interesting if there is significant predictive power when testing on companies that were never included in the training dataset in the first place it was clear that even after running a year s worth of data on our mdp algorithm it still had not reached convergence after the end of the year 
while we may suggest that the mdp algorithm be simulated over a longer time period than one year there is an open question as to whether a company s stock price over a longer period of time will still allow the mdp learner to approximate a markov decision process 
it is generally acknowledged that stock prices are more variable in the long run hence the short term forces that underlie the mdp learner s transition probabilities may change over the long run 
while investigating the long run future accuracy of the base predictor and the long run soundness of modelling stock purchase and sale on an mdp are interesting they are unfortunately outside the scope of our project wherefore we leave these questions to future work 
task design 
one male rhesus macaque monkey t performed a checkerboard discrimination task 
methods
visualizing neural activity during movement 
to connect neural activity to task behavior we first wanted to visualize the neural activity during the task to gain an intuition for how neural activity was evolving during behavior 
here we focus on neural activity that occurs during the course of the motor choice reaching to the left or right target as previous work has shown that motor choice information is contained in pmd to illustrate this to circumvent this problem and analyze population level activity during movement neuroscientists oftentimes plot trial averaged smoothed firing rates 
this tactic is often extended to population activity by averaging across units in order to reveal any potential differences in the population level neural activity during movement without plotting the firing rates for all neurons we employ a slightly more complex data analysis procedure and decompose neural activity using principal components analysis pca 
specifically given a matrix x x slow r x f ast r x slow l x f ast l where x slow r is an n xt matrix of the average neural firing rates across 100 slow rt right reach trials for n neurons over t time points and x f ast r x slow l x f ast l defined similarly we find the set of vectors u or principal components that maximize u t xx t u 
by projecting the data onto the first 3 principal components we see that the population level neural activity for left reaches diverges from the neural activity for right reaches demonstrating that motor choice is indeed encoded in this data 
note that this divergence occurs similarly for both slow and fast rts indicating that the neural activity evolves in the same way but at different timescales for fast versus slow rts 
average firing rate across all neurons during the reach t 0 refers to movement onset 
dotted lines refer to right reaches 
c pca performed on the neural activity 
black circles correspond to 300 ms prior to movement onset while the blue circles correspond to movement onset decoding movement choice and color percept from neural activity 
now that it is clear that the population level responses vary with motor choice we first wanted to investigate whether we could decode motor choice from neural activityand if so how far in advance of the movement onset classification accuracy was above chance 
to explore this we first divided the time within the trial into 41 time points spanning from 300 ms prior to movement onset to 100 ms after movement onset 
then for each rt and each time point in the trial we create a 200 trial x 101 neuron matrix with an associated 200 element vector indicating left right reaches for each trial 
we then used this data to train and test three separate decoders two discriminative logistic regression and kernelized svm using a radial basis function and one generative naive bayes nb 
for svm we used parameters that matched what we used in problem set 2 as this parameter set seemed to do well for our data 
in all cases we performed 10 fold cross validation where we trained on 90 of the data tested on the remaining 10 and repeated this procedure for all 10 sets of testing data 
we used three decoders for several reasons for all decoders we were able to decode the choice long before the movement had actually started we noted that while classifier accuracy for short rts is comparable across the trial for all three classifiers it appears that nb significantly out performs logistic regression and kernelized svm for long rts 
as the neurons are conditionally independent as they were independently recorded it is perhaps not surprising that nb slightly outperforms logistic regression as discussed in class if the assumptions underlying generative models are true they can outperform the discriminative models 
however we were slightly surprised that svm performed so poorly compared to nb 
upon further investigation we found that the svm training accuracy was 100 for all time points indicating that this model is over fitting prior to movement onset 
we hypothesize then that the svm is learning unimportant features of the data it also has 180 parameters while the other models have 101 that give rise to lower testing accuracy given that pmd encodes the output of the decision making process motor choice does it also encode the variables that lead to the decision output 
to answer this question we turned to the checkerboard epoch to search for color related signals 
while it is possible that color related signals are also present during the target presentation we wanted to specifically investigate whether percepts that influence the decisionmaking process are encoded in pmd 
for our initial analysis we only considered trials during which highest coherence stimuli were presented 
we reason that if a color signal is present in this brain area it should be most robust when a strong stimulus is presented 
we aligned the data to stimulus onset and divided the time within the trial into 41 time points spanning from 100 ms prior to stimulus onset to 300 ms after stimulus onset 
we then used our three classifiers to decode dominant checkerboard color 
interestingly we found that the decoders could not classify red or green checkerboard trials above chance suggesting that this color related signal is not present in pmd during stimulus presentation taken together our decoding analyses suggest that while pmd activity reflects the internal deliberation process immediately prior to movement onset pmd activity does not reflect the stimulus percept needed to make the decision leading to motor output decoding movement choice prior to movement onset using pca 
as discussed in class one common method of preprocessing a dataset before running a supervised learning algorithm is to apply pca to the dataset 
this type of preprocessing can carry computational benefits since the algorithms have fewer features to train on which can be critical for realtime decoding for brain machine interfaces 
further as applying pca can reduce over fitting we investigated whether preprocessing the data by applying pca could increase our classification accuracy to apply pca to our dataset we create a 180 trials x 101 neurons matrix for each set of training data at each time point for each rt 
for each matrix of training data we then find the set of k orthogonal vectors u1 
we choose k 5 10 101 to explore the dependence of classification accuracy on the number of pcs 
u k that maximize u t xx t u where x is the 180 x 101 matrix 
after finding the pcs we then project the training data into this space throughx u t x where u u1 
we also project the testing data onto the same principal components wherextest u t xtest prior to analyzing classification accuracy we exploited the low dimensionality of the projected data to visualize the evolution of neural dynamics over the trial 
u k 
we projected the matrix of training data created 300 ms or 160 ms before the movement during the time of movement onset and 100 ms after movement onset onto its first two components we then investigated classification accuracy of the kernelized svm and logistic regression decoders when they have pca projected inputs 
we did not investigate the classification accuracy of the nb decoder as the inputs now span a wide range of values and no longer directly map onto the classic nb framework 
we found that while decoding accuracy did not change for the logistic regression decoder decoding accuracy increased for the kernelized svm decoder forward feature selection reveals that motor choice can be decoded with very few neurons 
we next investigated how distributed and how robustly motor choice information is encoded across the neural population 
for example it is possible that only a few neurons strongly encode motor choice and thus a decoder using only these neurons can accurately predict reach direction 
on the other hand it is possible that all neurons weakly encode motor choice and thus a decoder needs all neurons to accurately predict reach direction 
finally it is possible that all neurons strongly encode motor choice and thus a decoder only needs a few neurons which can be randomly selected to accurately predict reach direction 
to explore which of these possibilities is correct we used forward feature selection to determine the minimum number of units needed to approach the classification accuracy achieved from a classifier using all neurons 
further we investigated the minimum number of neurons needed at 200 ms prior to movement onset and the minimum number of neurons needed at movement onset 
this will allow us to simultaneously compare whether the most important neurons change over the course of the trial 
we then compared the ten most informative neurons ranked according to the forward feature selection algorithm when decoding 200 ms before movement onset and when decoding at movement onset comparing the x axis of the right side of investigating the stability of the neural representation of motor choice during the task 
the results from the previous section suggest that different populations contribute to encoding motor choice at different times in the trial 
we wondered if the contributions from these units were stable during the time leading up to the movement so we investigated the testing accuracy of a decoder that was trained on neural activity from movement onset termed movement onset decoder in grey applied to neural activity at times throughout the trial one hypothesis is that if neural representations are stable and robust then the performance of this decoder would do just as well as the decoders we previously identified when we had a separate decoder for each time point termed optimal decoder identical to the decoder from we next investigated the testing accuracy of a decoder trained on neural data 200 ms prior to movement onset termed pre movement decoder in red when it was applied to neural activity throughout the trials 
similarly it performed well around the time that the decoder determined but performance around other times such as movement onset was poorer compared to the optimal decoder 
thus a decoder at a given time point movement onset decoder or pre movement decoder does not generalize to other time points 
these findings are consistent with our results from forward feature selection and demonstrate that the representation of motor choice in these units changes dynamically over time 
in this project we aimed to characterize the neural activity of this subpopulation to gain insight into the role of dorsal premotor cortex during a decision making reaching task 
what types of information does this bran area encode when does it encode that information and how stable is this encoding 
summary
we found that at the time of movement onset and up to 100 ms following movement onset the right and left reaches were easily and well decoded with any of the three classifiers we used 
we also wanted to know how long before the hand movement began could we decode the motor choice 
we found that for short reaction times we could decode motor choice 200 ms before movement onset whereas for long reaction times the accuracy at 300 ms before movement onset was still above chance 50 
the naive bayes classifier yielded the best results on long reaction times 
on the other hand when we looked at the stimulus epoch we could not decode the color of the stimulus 
thus under this perceptual decision making task paradigm motor choices are represented in dorsal premotor cortex but perceptual color information is not 
this study extends our basic scientific knowledge of how brain regions may integrate and represent complex variables such as perceptual decisions and action selection we then took a closer look at how motor choices are encoded 
given that data projected onto merely 5 pcs could yield high prediction accuracy we wondered if there might be a small subpopulation of units that are contributing the bulk of the prediction accuracy 
our results from forward feature selection reveal that only 3 4 units were necessary to achieve performance that was comparable to that of the population 
this combined with the knowledge that 5 pcs are enough to decode from suggests that the intrinsic complexity to complete this task is quite low dimensional in our case it is low dimensional because there are few neurons 
designing cooperative multi robot systems has gained much attention of researchers in the robotics community 
such multi robot systems need a communication scheme to appropriately share the information among the team 
while wireless electric communication is commonly used it is not robust to adversarial jamming 
one emerging alternative is motion based communication the idea that a message from the sender is encoded into its own trajectory and the receiver decodes the message by observing it 
some previous work this problem is highly non trivial especially when the receiver s observation model is monocular vision without depth perception because the relative attitude between two robots cannot be directly estimated from one observation 
although recent work thus the present paper addresses a trajectory classification problem for motion based communication between two robots using monocular vision only 
the main controbutions of this study are two fold 
first we formulate the online classification problem in which both of the sender s message encoded in its trajectory and the receiver s relative position to it are sequentially estimated as the receiver moves around the sender 
we provide a recursive bayesian estimation algorithm to handle the multimodal distribution over the joint belief state 
gaussian approximation to the belief and model linearization lead to a multi hypothesis extended kalman filter approach 
similar algorithms to ours can be found in the rest of the paper is organized as follows 
in the next section we formally state the problem and define the robot models 
in section 3 we derive the recursive bayesian update formula and provide the multi hypothesis extended kalman filter algorithm 
we also formulate the active control policy of the receiver in section 4 
simulation results are presented for 3 class classification in section 5 with a comparison of the random and the active control policies 
conclusion with future direction is discussed in section 6 
the sender encodes a message into its pre specified trajectory 
the correnspondence between the true trajectory and the message is known a priori as the trajectory codebook to both of the sender and the receiver if the sender intends to send the message z i 1 
preliminaries
however the sender s trajectory is not necessarily directed to the receiver while the sender is engaed in its own task 
n and performed 
this can bring ambiguity to the message decoding depending on the design of the trajectory codebook because the receiver s monocular vision is unable to determine the relative position between two robots from a single observation hence in order to correctly classify the trajectory and decode the message the receiver is allowed to sequentially move around the sender and observe it repeating one particular class of the trajectories 
this leads to a sequential state estimation and decision making problem 
the bayesian network structure of this problem is depicted in
in this work we assume that the smooth trajectory of the sender can be represented by a set of m points the sender chooses a message z and the corresponding trajectory from the codebook 
when it is executed however the sender s motion is subject to disturbance 
trajectory generation
we assume the resulting complete trajectory has a gaussian distribution 
the receiver s state r specifies its position and attitude with respect to the sender s reference frame which is expressed in the receiver s camera frame gives the exponential coordinates on so note that the receiver will move only after it observes one complete trajectory performed by the sender 
state transition model of the receiver
the details of the pinhole camera model is omitted here for saving space and the reader is referred to 3 bayesian online learning for state estimation
observation model
we are interested in estimating the joint distribution over r and z given the history of observations and control inputs p r k 1 z 1 k 1 u 1 k 
leveraging the bayes s rule and the structure of the bayesian network it can be decoupled and simplified as follows 
recursive bayesian estimation formula
5 implies that we can separately update our belief of r conditional on z and the belief of z itself given the state transition model and the observation model 
this allows us to derive the closed form update formula discussed in the next section 
the problem of estimating the joint distribution over r and z given past observations and control inputs can be viewed as multihypotheses filtering 
thus we will adapt the multi hypothesis extended kalman filter algorithm which is a common parametric filter to handle multimodal distributions with linear model approximation 
multi hypothesis extended kalman filter algorithm
in our problem note that the observation function takes two arguments r and 
therefore the first order taylor expansion around the current estimate e r k 1 1 k u 1 denotingk 1 r 2m 3m we obtain the following linearized observation model in order to derive the extended kalman filter update formula we further assume that the prior belief of r conditional on z before taking an action u k and observing k 1 is a gaussian distribution substituting
the convergence of the extended kalman filter algorithm is sensitive to the accuracy of the prior p r 1 z 1 
fortunately estimating the means 1 can be also found by comparing their residuals assuming the same initial covariance matrix i 1 for all i 1 
beleif initialization
given the current esimate of the state our goal is to control the position and the attitude of the receiver in order to correctly classify the trajectory and decode the message 
one information theoretic approach is to select u k from the control space u so that the expected entropy of z taken over the next obervation k 1 is minimized 
active vision control for entropy minimization
formally the control objective is given by multi hypothesis extended kalman filter algorithm for the trajectory classification problem by substituting the kalman update formulas and extracting the terms dependent on u k only this yields the following formula the first term in 10 follows from the entropy of gaussian distributions 
the second term is the negative entropy of a gaussian mixture and exact solution is not available 
however huber et al 
5 simulation results
in this paper we have presented an online trajectory classification algorithm for motion based communication between two robots with monocular vision 
bayesian update formulas were derived in the form of multi hypothesis extended kalman filter 
we have also derived the active control algorithm and demonstrated in simulations that the proposed framework outperforms the random control policy in future research we intend to concentrate on two main aspects to extend the proposed approach 
first the use of extended kalman filter might not be the best algorithm to implement the bayesian state estimation 
we will also adapt other filtering algorithms such as unscented kalman filter and particle filter 
second we will apply this algorithm not only to 2d trajectories but to 3d trajectories in order to evaluate the general performance of our method in 3d space 
clutter is an effect that degrades the quality of medical ultrasound imaging 
to better understand and subsequently reduce the effects of clutter the underlying mechanisms must be studied 
i abstract
clutter is thought to be highly reliant on the reverberations of acoustic signal due to microscopic subcutaneous tissues this paper presents a method for producing a fourtissue acoustic map to be used in three dimensional nonlinear ultrasound simulations 
a full volumetric map was successfully produced through the application of the k means clustering algorithm to a fat water separated mri volume 
in ultrasonic imaging clutter a phenomenon associated with poor quality images produces a temporallystable obstruction resulting in decreased image contrast and a reduced ability to discern imaging targets 
clutter is frequently a factor when imaging overweight obese and difficult to image patients 
ii motivation a clutter
the effect of clutter is attributed to the interactions of acoustic waves with the subcutaneous tissue layers 
particularly acoustic reverberation off axis scattering and phase aberrations from the subcutaneous layers are significant sources of clutter
the imaging difficulties presented by clutter are a major obstacle for clinical ultrasound imaging 
as such modeling the acoustic wave tissue interactions that produce clutter is of importance to ultrasound researchers 
b tissue classification for modeling
while much previous work has been done in characterizing this phenomenon using twodimensional models the problem must be examined on a three dimensional scale in order to produce simulation results of significant value 
presently the literature lacks three dimensional models of these interactions this project which forms a piece a of an ongoing project that i am conducting in jeremy dahl s ultrasound research lab seeks to produce acoustic maps to be used in three dimensional ultrasound simulations 
an acoustic map in the context of this project is a mapping of tissue characteristics to spatial locations 
these maps will be produced through the processing of 3d magnetic resonance imaging mri scans at this time the primary aim is to take a set of mri slices and assign each image pixel a label corresponding to the tissue displayed in that pixel 
the primary tissues of interest are skin fat muscle and connective tissues 
any pixels corresponding to regions without signal are classified together as background pixels in a fifth cluster 
the mri datasets used in this project were acquired at the lucas center at stanford specifically for this project 
in order to achieve high signal to noise ratio snr at a very high resolution 100 micron isotropic prohibitively long scan times are required 
iii materials
as such a slab of pork belly is used as an ideal ex vivo model 
it is desired that the results achieved on this model be replicated when applied to a non ideal in vivo human scan which will show a significant degradation in snr a tailored mri pulse was used to generate two datasets 
one dataset contains the signals from protons within water based tissues the other contains signal from protons within fat based tissue 
although out of the scope of this project as brief explanation this was achieved by making use of the chemical shift seen in the resonant frequencies of shielded protons 
this allows for the signals from different chemical species here water and fat to be distinguished based on the phase of the received signal for illustrative purposes a two dimensional slice of the dataset is provided in
the k means clustering algorithm is fairly straightforward 
a set of centroids is initially chosen either at random or with some initial estimate of the likely final centroids 
iv methodology a k means algorithm
during each iteration every point in this application pixel is assigned the label corresponding to the centroid closest in euclidean distance in the feature space the feature space will be described in the next section 
once every point has been assigned a label the locations of the centroids are updated to the means of the locations of all points that were assigned the corresponding label in the previous step 
this process is repeated until a convergence criterion has been met mathematically this can be described as presented in the course lecture notes by the following repeat steps 1 and 2 until convergence is achieved in these equations c i denotes the label assigned to data point x i j the location of centroid j and 1 is the indicator function yielding 1 when its argument is true 
the algorithm is implemented using the builtin function provided with matlab for computational efficiency 
in the k means clustering method the feature space can be thought of as a space with dimensions defined by the features chosen to describe the data to be categorized 
the centroids take on values within this space and distances within the space can be calculated between the values taken on by the centroids and the values held by the data a substantial piece of this project involves devising and properly weighting the features that are used to describe each pixel 
b feature space
the dataset can be considered as a three dimensional matrix whose dimensions correspond to the three spatial dimensions x y and z 
each entry in the matrix is a scalar representing the pixel intensity at each spatial location 
as such the only information available is a pixel intensity and its location in cartesian space the results that will be presented in the next section were achieved using a weighting of nine features 
the features were chosen and weighted in order to best accommodate the clustering of the data into the five desired clusters 
1 known signal as the data has already been grouped into fat signal and water signal it is very desirable to make use of this information 
the voxels corresponding to the water based tissues skin muscle and connective tissue should be represented by high signal values in the water signal dataset while the pixels corresponding to the fat signal should of course be represented by high signals in the fat signal dataset 
furthermore the regions corresponding to a lack of signal should have very low pixel values in both datasets this information was included as three binary features 
representative slices of these features are provided in 2 connected regions intuitively neighboring voxels of similar intensity are likely to belong to the same tissue class 
this is a very natural assumption and can be implemented by using a region labeling algorithm 
the algorithm implemented in this work used the matlab function bwlabel 
this function takes a binarized volume and outputs a label for each detected connected region it was found that in three dimensional space the connective tissue fibers link the larger tissue structures to each other as anticipated by prior knowledge of this anatomy 
as such running the region labeling algorithm in three dimensions did not provide useful results 
however by labeling regions in two dimensional space it is possible to separate the connective tissue regions from the larger skin and muscle masses 
a representative slice of this feature is shown in the sizes of each of these connected regions can also be usefully incorporated into the feature space 
as the connective tissue regions the smallest connected regions were found to be the most difficult to classify the region size feature was chosen to emphasize clustering these small regions together 
this was achieved by summing the number of pixels in each region and then thresholding the labeled volume so that only the regions significantly smaller than the mean region size were kept
again referring to experimentation was done with using polynomial weightings of the voxel locations to try to enforce prior knowledge about the expected spatial distributions of the water based tissues 
however a simple logarithm of the vertical position with respect to the orientations of the slices as presented in the figures in this work was found to effectively enforce the expected spatial distributions of tissues while preserving the ability to generalize to future datasets 
4 spatial distribution 
5 image gradients additionally there are spatial variations in pixel intensity throughout each tissue 
as a particular example the connective tissues thin tendrils as seen in 6 feature space weighting the complete feature space can be represented as in eqn 
s total is the complete signal mask s water and s fat the water and fat signal masks r label the connected region labels and r size the sizes of the connected regions 
i i denotes the intensity of the ith pixel y i denotes the normalized and weighted y position vertical of the ith pixel and as before g x i and g y i are the local gradients calculated for each pixel multiplicative weightings were added to the region size and y location features in order to encourage the algorithm to emphasize these features over the rest as these were found to produce better results when weighted 
clustering was run on the water only and fat only volumes 
v results
overall excellent qualitative results were achieved 
as is to be expected some misclassifications must be accepted as any mri acquired at this resolution will have low snr and contrast 
vi discussion and conclusions
the feature space was specifically defined with this in mind in order to produce benign misclassifications whenever possible in medical ultrasound images are constructed based on acoustic reflections 
such reflections occur when a propagating acoustic wave encounters an acoustic impedance mismatch 
larger mismatches produce larger reflections 
this aspect of ultrasound physics was used to guide the definition of benign and malignant misclassifications a benign misclassification was defined to be a classification of either skin or muscle voxels into the connective tissue class 
as long as kept to small numbers these misclassifications will have negligible effect on simulation results there are two types of misclassifications defined as malignant for this application 
the first was defined to be any deep classification of a fat voxel as water based tissue or water based voxel as fat 
here i use deep in the anatomical sense that is an example of this misclassification would be an erroneous voxel in the middle of a piece of muscle labeled as fat 
this type of error was heavily influenced by the rician noise inherent in mri scan data and was nearly eliminated by the addition of the connected regions feature the second malignant misclassification type is the classification of any tissue voxel as background or empty space 
such erroneous voxels would produce extremely large imaging artifacts the impedance mismatch between any tissue and empty space is large 
the preliminary results presented in the milestone suffered greatly from these misclassifications much of the subsequent work involved incorporating features and preprocessing steps to eliminate these errors 
the inclusion of a more sophisticated image registration transform to ensure alignment between the fat and water volumes along with inclusion of the connected region and binary signal features allowed for the almost complete removal of these misclassifications 
the results presented in investigations into the robustness of this algorithm to the presence of these less ideal conditions will need to be performed 
vii future work
people use sound both consciously and unconsciously to understand their surroundings 
as we spend more time in a setting whether in our car or our favorite cafe we gain a sense of the soundscape the aggregate acoustic characteristics in the environment 
our project aims to test whether the acoustic environment in different areas of stanford campus are distinct enough for a machine learning algorithm to localize a user based on the audio alone we limit our localization efforts to seven distinct regions on stanford campus as enumerated in section iii c we characterize the locations as regions because we hope to capture qualitative rather than quantitative descriptions 
for example the huang region includes the outdoor patio area as well as the lawn beside the building 
furthermore we restrict our efforts to daytime hours due to the significant soundscape differences between daytime and nighttime a significant advantage of audio localization is the qualitative characterization on which we focus 
specifically an acoustic environment does not generally linearly vary with position 
for example any point within a large room will likely have common acoustic characteristics 
however we expect a drastic soundscape change just outside the door or in another room and that difference can be of significant value 
however gps may not capture this change for two reasons 1 this change may be below current gps accuracy thresholds typically 10 50 feet 
2 gps only produces lat long data 
an additional layer of information is needed to provide information about the precise boundaries of the building 
furthermore gps fails to distinguish accurate vertical position e g 
floors which may be of special interest in buildings such as malls or department stores 
a previous cs229 course project identified landmarks based on visual features
ii related work
the system hardware consists of an android phone and a pc 
the android phone runs the android 6 0 operating system and uses the hi q mp3 rec free application to record audio 
a hardware and software
the pc uses python with the following open source libraries the system also makes use of a few custom libraries developed specifically for this project 
an audio input goes through our system in the manner below 1 the audio signal is recorded by the android phone 2 the android phone encodes the signal as a wav file 3 the wav file enters the python pipeline as a sample instance 4 a trained classifier instance receives the sample a the sample is broken down into subsamples of 1 second in length b a prediction is made on each subsample c the most frequent subsample prediction is output as the overall prediction 
a graphical illustration of this is shown in
b signal flow
the system is trained to recognize the following 7 locations 1 
rains graduate housing 2 
c locations
circle of death intersection of escondido and lasuen
we collected data using a freely available android application as noted in section iii a 
monophonic audio was recorded without preprocessing and postprocessing at a sample rate of 44 1 khz 
a audio format
data was collected on 7 different days over the course of 2 weeks 
each data collection event followed the following procedure 1 hold the android recording device away from body with no obstructions of the microphone 2 stand in a single location throughout the recording 3 record for 1 minute 4 restart if recording interferes with the environment in some way e g causing a bicycle crash 5 split recording into 10 second long samplesin total we gathered 252 recordings of 1 minute in length for a total of 1507 data samples of 10 seconds in length 
b data collection
even though our system is designed to handle any inputs of length greater than 1 second we standardized our inputs to be 10 seconds for convenience we also attempted to maintain sample balance amongst the 7 locations while also diversifying sample collection temporally 
the distribution of samples by location is in 13 mel frequency cepstral coefficients mfccs we observed best performance using mfcc and spd features for a total of 73 features 
these 2 feature types are described in the subsequent subsections 
mfccs are commonly used to characterize structured audio such as speech and music in the frequency domain often as an alternative to the fourier transform
a mfcc
spd is a method we developed for finding consistent sources of spectral energy over time 
first spd generates a spectrogram using short period ffts obtaining the energy of the signal as a function of both time and frequency 
b spectrogram peak detection spd 
the method then finds the local maxima in frequency as defined by a window size a local maximum is marked 1 and all other elements are zero 
finally this matrix is summed across time to give a histogram of local maxima as a function of frequency 
finally the method bins the results according to a log scale spd finds low signal to noise ratio snr energy sources that produce a coherent signal e g a motor or fan producing a quiet but consistent sum of tones 
since all maxima are weighted equally spd attempts to expose all consistent frequencies regardless of their power 
we show a comparison of spd outputs between the circle and bytes in
we investigated the redundancy in our features by doing a pca on our data set using the above features 
c principal component analysis pca 
svm using gaussian and linear kernelsdescribed in more detail in the next section when picking the hyperparameters to use for each classifier we did a 70 30 split of our training dataset and then searched over a grid of parameters evaluating based on accuracy of classification for logistic regression and svm we also compared the use of one vs one ovo and one vs rest ovr multiclassification schemes 
we found no significant difference in performance for logistic regression and linear svm 
using the mfcc and spd features we investigated the following classifiers 
however ovr gaussian svm exhibited much worse performance than ovo gaussian svm 
as described in section iii b our prediction method offers the following advantage a test sample with single label is made up of multiple subsamples each of which is processed and classified 
the final prediction for the sample is made on a basis of majority vote from each subsample which significantly reduces our test error 
a voting
our original implementation broke voting ties randomly 
when analyzing the predictions of the gaussian kernel svm we noticed that 27 of misclassifications resulted from incorrect tiebreaks and 42 5 of misclassifications occurred with voting margins of at most 1 
we investigated 2 approaches to improving performance in these scenarios our first attempt used the total likelihood produced by the svm predictions across 10 subsamples 
while this approach seemed sound in theory the small training sample size make the likelihood estimates highly inaccurate and this approach did not change overall performance our second approach was to use the gaussian svm logistic ensemble method mentioned in section vi 
previous testing indicated that our gaussian kernel svm was prone to overfitting while the linear logistic classifier tended to have a better balance between training and test error 
the final method we chose was to employ the ensemble only when the voting margin for the svm is no more than 1 
for these close call scenarios the logistic classifier calculates its predictions for all subsamples 
the svm votes are given 1 45x weight to prevent any potential future ties and the highest total is chosen 
this method provided a 2 5 generalization error reduction it is also interesting to note how test error varied as we changed the duration of our test sample effectively changing the number of votes per test sample 
using our ensemble we achieved just under 17 error with 30 second test samples
we distinguished between 2 types of testing errors 1 cross validation error error on the testing set when we split the data set completely randomly 2 generalization error error on the testing set when we split based on random days 
our data has a significant temporal correlation 
b generalization
we discovered that the typical cross validation error was too optimistic because audio samples recorded on the same day can be significantly more correlated to each other than to audio recorded on different days 
we were able to decrease our cross validation error to around 8 using a gaussian svm 
however when we attempt to use this seemingly general classifier on a completely new day s data we discovered it was actually very overfitted with this in mind we were able to reduce our generalization error to a bit less than 20 using a gaussian svm with logistic classifier ensemble as described in vi a 
to calculate generalization error we did a form of 7 fold crossvalidation 
we held out all samples from a single day for testing while using all other days for training and then we repeat for all 7 days during which we had gathered data 
we finally do a weighted combination to calculate the generalization error weighting based on the number of samples in each held out day 
our classifier did relatively well in terms of accuracy to eliminate any effects due to our data collection s minor class imabalance
as the final step in evaluating our system we compared the performance of our classifier to people s ability to localize based on audio clips 
we created a small game that would present the user with a random 10 second audio clip from our dataset 
c classifier evaluation
the user would then choose from which of the 7 locations the audio was taken 
the pool of participants comprised of stanford cs229 students and other attendees of our poster presentation 
a major challenge in this project was data collection 
due to the limited number of audio samples collected our efforts to develop additional relevant features generally significantly increasing our training set may allow exploring additional features 
vii future work and conclusion
in particular we believe hour of day and day of week could be significant additions especially to mitigate the temporal challenge of classification 
as discussed in section vi b we observed a gap between cross validation error and generalization error 
as we utilized more data we observed this gap lessening even with just the current set of features 
we expect that our algorithm s ability to predict new data would continue to improve with additional training data 
finally increasing our training set would make the likelihood estimates of our classifiers more accurate 
thus it may be worthwhile to revisit the use of likelihood estimates in our voting scheme as described in section vi a the student testing we performed as described in section vi c demonstrate the challenges of audio based localization 
users frequently noted that their 10 second clip did not seem to match the typical soundscape of the area they imagine 
given the variability of soundscape at each region between different times and days we are encouraged by our algorithm s performance 
however significant work remains to be done before conclusions can be reached about the feasibility of this method for broader applications 
in particular it is unknown how scaling the number of regions affects prediction accuracy 
it would also be interesting to see our chosen features and techniques applied to very different environments with the same number of regions 
the goal was to give homeowners a prediction on their house s closing price which would be put on the market 
classification and regression methods were used as well as natural language processing on each house s remarks 
the best set of models found used lasso l1 regularization 
prediction performed remarkably better than classification and predicted close prices with 95 
cleaning the data set included eliminating features that were relevant only once the house had been sold since we were only interested in predicting the price that a house should be given when entered into the market 
we then converted categorical features such as the type of flooring of the house into binary ones 
dataset and features
we also created new features such as binary features for the season in which the house was listed on the market and the age of the house 
these modifications to our dataset largely increased the number of features in our training set so that at the end we had 83 features 
once we had cleaned our data set we looked at the shape of the distribution of the target feature the closing price of the house to understand how to best fit the data 
the closing price seemed to follow a normal distribution skewed left and the log nor mal of the distribution looked closer to a normal distribution not skewed in any direction 
we decided to test how our models did predicting the log of the closing price as well as just the closing price 
our training and model selection methods throughout the project were as follows 
we split our dataset into a training and testing set by a 70 30 split 
training methods
throughout the first part of the project we used k fold cross validation with k 10 to approximate a test error 
this proxy test error helped us make decisions about which models to select and what to try next without including bias into the testing phase 
during the testing phase we trained on the whole training set and tested on the test set only with the best model found with the cross validation error as a baseline model we decided to use a naive bayes classifier for our predictions 
in order to use this model we needed to discretize our values including our target variable 
market research on online housing real estate engines such as zillow showed that it is common to split houses into brackets of approximately 40 000 
using this value for our bucketing we were now trying to predict the price of a house within ten possible buckets 
first we implemented two types of naive bayes gaussian naive bayes and multinomial naive bayes 
multinomial naive bayes performed much better probably because a greater majority of our features were binary than continuous 
nonetheless multinomial naive bayes only achieved 28 accuracy even with the log of the target values 
to explain the performance of naive bayes we obtained correlation plots on features conditional on their actual target price to improve our classification we needed to relax the model assumption so we decided to try a multinomial logistic regression or logit classification on the same bucketed dataset 
this model outperformed naive bayes reaching an accuracy of 49 after all variables such as regular finally we decided to compare whether regression had really done better than classification 
we could see that a 0 78 mean squared error was well within the range of 40 000 with which we had bucketed our features before 
yet we had not tried to see if classification did better with smaller bucket sizes 
as such we ran logit classification on a data set with the target values discretize in buckets of 20 000 yielding 20 buckets 
however this model did much worse than with the original bucket size most likely because creating more buckets meant that each bucket had less samples 
therefore we could predict with more granularity the price using regression and with a high accuracy optimizing performance for model variables 1 
time dependence 
our goal was to predict the closing price of houses in the future which meant time was an important factor in our modeling 
the dataset was ordered chronologically by when the house was placed on the market our worry was that if we only trained on houses sold a year ago market changes through time would not be taken into account 
therefore to test the importance of time in our model we trained our models on a chronologically ordered dataset and on a dataset in random order 
for classification it was hard to find a pattern between whether random or chronological did better for the logged target values chronological did better but for the others random did better 
however for regression we could clearly see that random dataset was doing far better than chronological one 
therefore we decided that time did play an important part in our dataset and we wanted to account for it by adding a feature to our training matrix 
using the date in which a house was placed in the market we created a feature which determined how recent the sample was 
for both classification and regression we saw that the time feature improved the performance of our model under cross validation 2 
feature interaction terms 
we had learned from naive bayes that our features had high dependency hence we wanted to use this fact to improve our model 
as such we added interaction features of degree two to our dataset in other words we created 83 choose 2 new features which represented the product of every pair of features 
this brought the number of features in our model up to 3486 which was more than the number of samples in our dataset 
to counteract the significant increase in dimensionality of our model as we will see in the next section we used aggressive regularization 
using the feature interactions we were able to improve our models slightly 
for the logit classifier we obtained 48 2 accuracy without interaction terms and 49 2 accuracy with interaction terms 
for lasso linear regression we obtained 0 81 mspe without interaction terms and 0 78 mspe with interaction terms 3 
optimizing performance with regularization 
for both the logit classifier and lasso linear regression regularization played an important part in model performance 
first for classification we had to select the appropriate c value which was inversely proportional to the intensity of regularization 
we found that for the interaction terms we needed to use strong regularization c 0 0001 while for the dataset without the interaction terms only a small amount of regularization was necessary c 0 6 
this suggests the the interaction terms resulted in overfitting our model but regularization helped counterbalance this 
similarly for regression we had a constant to control the regularization alpha which in this case was proportional to the data 
we see that once again interaction terms require strong regularization alpha 0 4 where the noninteraction terms data set required only alpha 0 0002 
in other words our new interactions regression model had 2000x more regularization 
more interestingly we see that without interaction terms ridge linear regression does better than lasso but the opposite is true for the dataset with the interaction terms further proving that these in 
the dataset also contained descriptions on each of the sample homes and we wanted to understand whether these could help our model predict the closing price 
therefore we ran bag of words on the remarks and then used tf idf to remove the words that appeared more than 5 of the time which would not help us distinguish the comments 
remarks analysis
we used 5 as we noticed in next we wanted to reduce the dimensionality of this matrix to add to our input train matrix and not have too many features 
to do so we used two techniques principal components analysis or pca and the anova f statistic 
for pca we selected the top 10 principal components and for the anova f stat matrix we selected the top 10 of words that gave the most information about the closing price appending these two matrices pca and anova fvalues to our original matrix and running both classification and regression on it had little to no impact on prediction 
for regression both remark matrices seemed to be eliminated by regularization since we obtain the same mspe and for classification pca and f value matrices gave lower accuracy rates suggesting overfitting 
therefore we can conclude that regardless of the dimensionality reducing method the remarks are not useful for predicting closing prices 
below we present a summary of the results for the different model iterations that we have discussed previously 
it is interesting to note that for both classification and regression the best model included interaction terms and the log of the target values 
model comparison
this demonstrate that the relationship between features were very important for prediction as well as having a normal distribution on the target values 
using our test set on the lasso model with interaction terms and the log of our target values i e 
our best model we obtained an mspe of 0 71 which was even lower than what we obtained with cross validation on our training set 
testing and conclusion
because we observed that our model s residuals followed a normal distribution as in from a first glance looking at another model which has great potential in improving prediction due to this method modeling feature dependencies is random forest regressors 
preliminary research showed that this model has high potential with an mspe of 0 5 and 80 r squared value 
further research on the varying factors of random forest regressors could improve the models performance 
expedia users who prefer the same types of hotels presumably share other commonalities i e non hotel commonalities with each other 
with this in mind kaggle challenged developers to recommend hotels to expedia users 
armed with a training set containing data about 37 million expedia users we set out to do just that 
our machine learning algorithms ranged from direct applications of material learned in class to multi part algorithms with novel combinations of recommender system techniques 
kaggle s benchmark for randomly guessing a user s hotel cluster is 0 02260 and the mean average precision k 5 value for na ve recommender systems is 0 05949 
our best combination of machine learning algorithms achieved a figure just over 0 30 
our results provide insight into performing multi class classification on data sets that lack linear structure 
recommending products to consumers is a popular application of machine learning especially when there exists substantial data about the consumers preferences 
kaggle has provided a dataset containing substantial information about 37 million expedia users and has posed the challenge of recommending hotel clusters to these users 
a large amount has already been done and written about item recommendation systems 
for initial data visualization we compressed the dataset into 3 dimensions using basic pca dimensionality reduction 
to reduce the visual clutter of the data we only plotted the 3 most popular hotel clusters as seen in
figure 2 the distribution of the target hotel clusters our first algorithm used the naive bayes conditional independence assumption to rank hotel clusters 
another simple method used was training an svm 
baseline methods
one issue we faced in applying svms to our problem was adapting the typically discriminative svm algorithm to generate probabilities which can then be used to create a ranking of hotel clusters we ended up using a method described by wu et al relying on pairwise coupling of single class predictions learning the class probabilities using cross validation 
our first success came from using a form of ensemble tree boosting algorithm 
this was chosen due to boosting s tendency to intelligently learn the non linear structure of data 
weighted user similarity 4 3 gradient boosting
our approach involved minimizing a softmax objective function to output a list of probabilities one per class for a test user which would then be used to get the top k hotel cluster recommendations 
we did this by using an off the shelf xgboost package which trains several classification and regression trees at each step fixing the previous trees and adding a new one 
we experimented with several regularization terms and stepsizes but found that they had no real impact on our results 
for the number of rounds to train we chose 6 as a reasonable trade off between complexity of the model and time needed to train it our most successful class of algorithms involve a novel combination of clustering kernelized similarity and weighting distance inspired by user similarity algorithms commonly used in recommendation systems 
in measuring user similarity our first attempt with cosine similarity performed relatively poorly as certain features serve as numeric representations of qualitative information 
for example the continent ids for two users may be close together numerically but that does not imply the two continents should be considered similar 
thus we instead opted to represent similarity through jaccard similarity 
thus to this end we opted to utilize the kernel functionwhich counts the number of features shared 
using this kernel we define the kernel matrix to be the vector k where the ith element of k is the result of the kernel function defined above 
we then sort k and extract the top 150 users 
now that we have the sorted top 150 users and their similarity scores for each according to the kernel function we need to assign each hotel cluster a score based on the number of times it appears in these top 150 users and how similar these users are 
lets define v to be a vector with 150 elements containing the hotel clusters in order of the top 150 users 
the first function that was attempted is as follows where tau is some constant 60 worked best for us and v i is the hotel cluster of the top ith user 
the basis of the equation above is to weigh users who are really close to the test user more than the users who are far away but still heavily weigh clusters that appear more often in those top users 
it is based on the equation for locally weighted linear regression and was meant to test the hypothesis that similarity could be used to effectively make recommendations 
as shown in figures 5 and 6 this method outperformed our benchmark by a noticeable amount and made us move towards exploring other similar algorithms and scoring functions one important flaw with the previous scoring algorithm is that it weighs based on placement in the list of top 150 clusters 
it does not account for what the difference between those elements are or how close they actually are to the test user the ith place in the list is always weighed the same 
to remedy is our second attempt with kernel methods used the same kernel function but the scoring function for each hotel cluster was redefined as follows where k is defined to be the sorted k matrix which since we do this for each individual test example can be considered a vector and e is some constant 
the larger e is the more we penalize farther values as the defined kernel function gives values strictly between 0 and 1 
we tested several values and found 5 to be the most effective 
using this scoring method based on the raw similarity score instead of merely placement we saw a very large improvement in the accuracy of our predictions across all metrics and this became the basis of our most successful algorithm 
we briefly digressed from our algorithm to attempt a different kernel method 
for this kernel method we attempted to split the feature vector into two vectors one comprised of discrete features corresponding to qualitative data country location user ids etc 
and another vector comprised of more continuous and quantifiable data length of stay user distance from location number of children etc 
lets call these two vectors split from an arbitrary vector x x and x 
we also normalize x to have a norm of 1 
using this notation we defined the new kernel function to be where c1 and c2 are some positive constants such that c1 c2 1 
the idea behind this kernel was to account for similarity in continuous features when weighing user similarity 
the rest of the algorithm remained unchanged to the second iteration but overall performed worse regardless of the parameters used suggesting that similarity based on jaccard distance is more representative 
armed with this knowledge we returned to the previous algorithm our most effective algorithm based off of the second kernel algorithm was very similar with one main difference 
we took advantage of the fact that the hotel market and is package features are very highly correlated with hotel cluster as can be seen in the correlation matrix in which weighed the hotel market feature twice as much as other features and is package 1 5 times as much when accounting for the jaccard distance 
using this kernel method and the same algorithm as before we attained the highest total accuracy on our predictions 5 results and discussion overall the best methods were the methods that utilized user similarity and kernels to recommend hotel clusters that other similar users booked 
gradient boosting was also effective but mean average precision seemed to hit a hard cap at 25 regardless of the parameters used or size of the dataset sampled 
as seen in
this project provides an excellent case study for applying machine learning algorithms to large data sets lacking obvious structure 
it also embodies the challenge of recommending items about which we have no features 
conclusion and future work
in addressing these challenges we demonstrated that a creative combination of user similarity matrices and jaccard similarity outperforms gradient boosting a technique currently well known for winning kaggle competitions 
for future work we recommend using ensemble stacking methods to combine predictions from various algorithms 
accurate decision making in the petroleum industry is highly contingent on building a reliable model of the subsurface geological structure 
building a model of the subsurface typically involves solving an inverse problem with acquired data for various model parameters of interest like p wave velocity rock porosity etc 
however issues of poor data quality necessitate regularizing the inverse problem where prior geological information is incorporated 
a big issue is that geology is typically highly heterogeneous non stationary and conventional regularization operators fail to capture this non stationarity 
this project uses autoregressive filters that can learn on training images tis of the geological structure for use as regularization operators in inverse problems 
a nonstationary approach is employed in which multiple filters are trained for a single grid of the subsurface structure 
after training the filters are expected to provide a way of incorporating non stationary prior information about geological structure into inverse problems 
any inverse problem can be viewed as complementary to a learning problem 
here the hypothesis physical model is assumed to be known and the goal is to predict the features model parameters from targets acquired data 
methodology
the commonly used fitting goals optimized simultaneously for model parameters arewhere physical model model parameters d data regularization operator residuals 
equation 2 is the regularization goal which is the topic of interest in this project 
machine learning techniques will be utilized to train regularization operator on prior geological information which is expected to help guide the inversion towards the desired geological solution prediction error filter pef to deal with this issue this project implements a non stationary approach to training the filters 
instead of using a single filter to learn on the entire grid of the ti multiple filters are used over the grid 
however this approach comes with its own complicacies 
for instance in the extreme case of a separate filter for each grid cell the total number of filter coefficients to be optimized increases dramatically and the system of regression equations becomes under determined similar fitting equations are formulated for the remaining cells 
all the coefficients can then be determined by minimizing the residual vector of the whole system in a least squares sense 
a mathematical formulation was developed for the problem 
what is needed now is a way to generate multiple tis to serve as data 
data and features
one method is to use geostatistical simulations
the filters will now be trained and tested for accuracy 
in the initial analysis we used 2 2 filters for every grid cell 
regression equations are subsequently formed for all grid cells as illustrated in equation after training the filters these filters are tested for their efficacy in prediction of non stationary geology 
it is important to note that in an ideal case these trained filters will be used in solving equation
however one cause of concern in this preliminary approach is the danger of overfitting 
filters were trained for every grid cell 
fortes and failings
but how justified is this approach 
do the statistics vary too rapidly to warrant the use of filters for every grid cell 
to reduce the number of filters that are used we can make the filters constant in patches over the grid 
this can minimize the risk of over fitting 
we first tried to make the filters constant along 1d patches
most current ct systems use energy integrating detectors eids which detect total energy over a period of time 
so the energy information of each photon is lost in this process 
in contrast photon counting detectors pcds detect individual photons and discriminate them into multiple energy bins 
from this energy information pcds gain an improvement in ct image quality provide better tissue characterization eliminate electronic noise and improve the detective quantum efficiency dqe 
a major problem with pcds is the slow count rate resulting in count rate loss photons arriving too close in time are recorded as only one event and pulse pileup detected energy of that event is incorrectly higher or lower 
an x ray beam has a range of energy 
spectrum is the number of x ray photons as a function of energy 
spectrum
the example spectra are shown in
the input data is defined as follows where 
is the number of detected counts in energy bin j and 
neural network
is the number of count detected in energy bin j for air scan thus the number of nodes in the input layer is 6 corresponding to one bias and the 5 element of 
the output data is the thickness of two basis materials water and calcium corresponding to the two node of the output layer of the network 
a neural network with either one or two hidden layers is chosen 
for the two hidden layer network if we let 
be the activation in layer l where is a sigmoid function applied element wise in this case 
is a matrix of weight controlling function mapping from layer l to l 1 the cost function employed iswhere 
s are acquired from minimizing the above cost function with trust region algorithm 
for i 1 2 m are training examples the optimized 
forward projections of two phantoms are acquired assuming fan beam ct geometry over 180 views 
both phantoms are water with 8 inserts of calcium varying in density from 0 to 1 550 g cm the measured spectra are distorted using our implementation of an analytical model of pulse pileup effect
simulations
different number of elements 4 to 15 in hidden layer was tested for network with one hidden layer 
a hold out cross validation was performed using the 3060 examples randomly chosen from the projection measurements of the training phantom 
number of elements in hidden layer
the number of elements that results in the minimum empirical error on the hold out cross validation set would be chosen 
the network of one hidden layer with no regularization term in the cost function was compared with the network of two hidden layer with regularized cost function 
for the two hidden layer network the regularization parameter and the number of elements in hidden layer are arbitrarily chosen to generate initial results 
number of hidden layers
the water and calcium equivalent density images are reconstructed from the predicted thicknesses in each projection using filtered backprojection algorithm 
the water and calcium densities bias are calculated from the following equation and are compared where 
is the average value in region of interest roi in the reconstructed density images 
the roi for water density is placed at the center of the phantom while the one for calcium inserts are placed at the corresponding inserts location 
testing data projection phantom 13 elements in hidden layer result in the lowest error on the cross validation set and lowest average absolute calcium thickness error as displayed in
simulations show that neural network is a good alternative to material decomposition for photon counting detector projection data with pulse pileup effect 
the full capability of this approach can be further investigated with more parameter value fine tuning and larger data set 
technological advances such as high speed backbones local area networks and wireless technology have created a dynamic network of systems which have become mission critical for governments companies and institutions
an intrusion detection system ids is often deployed as a primary component for enabling security and resiliency within industrial control systems 
its objective is to detect ongoing intrusive activities in computer systems and networks 
a intrusion detection systems
an ids searches for evidence of malicious behavior by analyzing one or more event streams 
events may be represented by network packets operating system calls audit records produced by the operating system auditing facilities or log messages produced by applications
the problem of maintaining a secure and reliable enterprise can be addressed with a markov decision process mdp model of a host and ids system the evaluation of such mdp systems requires examining the expected utility frontier for policies obtained from varying the model parameters 
however varying all model parameters over all possible values does not accurately reflect the behavior of the mdp system in handling real world malicious network packets in order to augment the analysis of the mdp controller we can leverage information about the adversary through generative probabilistic graphical models 
b cyber enterprises as a markov decision process
this achieves the dual purposes of generating data to better categorize attack types as well as incorporating the generative models into the mdp system to boost resiliency 
while controller based autonomous systems has been implemented widely for applications in defense the contribution of this paper to the existing literature is to provide further exploration of the insight generative probabilistic graphical models yield into the behavior of adversaries in the context of cybersecurity 
our work diverges action a i encodes the action space of the ids i e 
c related work
whether to pass or drop a packet 
action a h encodes the action space of the host i e 
whether to wait for a packet or reset from those cited in its reliance on supervised learning as a means of modeling types of attacks 
we collect a total of 40 different features for each packet ranging from duration to protocol type to the number of files accessed 
we model each continuous variable as a gaussian distribution and model each discrete variable as a multinomial distribution 
a packet model
furthermore we model the bayesian network of packet features with a naive bayes model given the observed state of the adversary the distributions for each of the packet features are independent of each other 
our packet generation procedure creates a new packet by sampling each of the 40 packet features from their respective distributions conditional on if the adversary choose to act normally or maliciously 
to fit the parameters for each distribution we calculate the maximum likelihood estimates for each parameter 
as a baseline approach we model the adversary as a naive bayes model given the probability of an adversary acting maliciously the observed states that of the adversary are independent 
a packet is then generated from the corresponding distribution as described above 
b adversary naive bayes model
to estimate the probability of an adversary acting maliciously we calculate the maximum likelihood estimate 
as an improvement on our baseline approach we break the naive bayes assumption to capture more complex stochastic dynamics of adversary actions over time 
in this model each observed state of the adversary is dependent on past t observed states where t is the number of previous time steps that influence the current state 
c adversary markov model
for example t 1 gives the standard markov assumption that the current state is only dependent on the previous state 
the transition probability for the adversary is given by a ttensor where each dimension has size 2 and each entry gives the probability of transitioning to the next state given the current and previous t states 
to estimate the entries of the probability tensor we calculate the maximum likelihood estimates 
the maximum likelihood estimation of our model parameters relies on the nsl kdd dataset it does not include redundant records in the train set so the classifiers will not be biased towards more frequent records the performance of the learners are not biased by the methods which have better detection rates on the frequent records the number of records in the train and test sets are reasonable which allows for experiments on the complete set 
consequently evaluation results by different research works will be comparable we group our attacks into four types
a dataset
a dos attack is a type of attack in which the adversary makes a computing resource too busy to serve legitimate networking requests 
this denies users access to a machine 
1 denial of service attacks dos 
examples of dos attacks within the dataset include local area network denial land neptune ping of death pod smurf and teardrop attacks 2 probe attacks a probe attack is a type of attack in which the adversary scans a machine or a networking device in order to determine vulnerabilities that may later be exploited 
this technique is commonly used in data mining 
examples of probe attacks within the dataset include portsweep nmap ipsweep and satan 3 remote to user attacks r2l a remote to user attack is a type of attack in which the adversary sends packets to a machine over the internet which s he does not have access to in order to expose the machines vulnerabilities and exploit privileges which a local user would have on the computer 
examples of r2l attacks in the dataset include imap spy phf multihop and guessing the password 
a user to root attack is a type of attack in which the adversary starts off on the system with a normal user account and attempts to abuse vulnerabilities in the system in order to gain super user privileges 
examples of u2r attacks in the dataset include perl buffer overflow and rootkit 
4 user to root attacks u2r 
prior to training each packet was converted to feature space and processed so that all discrete valued features instead take on integers 
we next implement each of our attack types dos probe r2l u2r as a markov model and learn the model parameters via maximum likelihood estimation as described above 
in order to analyze the dynamics of the past t adversarial states we fit each of the above models we then calculate the log likelihood of the data given our learned model and use this as an evaluation metric for the appropriateness of fit 
below we have plotted the log likelihood scores for each type of attack normalized against the maximum log likelihood achieved by that attack model this allows us to examine the effect of t on each model relative to each other 
to examine the appropriateness of fit of the attack models from a different perspective we fit a discriminative classifier using the log likelihood responses of our attack models and then measure the classifier s ability to discriminate between malicious and non malicious packets the intuition behind this procedure is that an attack model should score a malicious data packet of the corresponding attack type as more likely than a normal data packet or a malicious data packet of a different attack type e g 
a packet with the signature of a dos attack should be scored highly by the dos model and low by the probe u2r and r2l models we first trained markov models for each of the four attack types for varying degrees of markov steps from t 1 to t 10 
c testing
we then converted two sets of 10000 and 50000 data points into vectors x i r 4 where each feature in a given x i is the log likelihood score of the data point from one of the four different attack models 
we then trained an svm classifier with an radial basis function kernel
we note that during training most models achieve a maximum log likelihood score with t 4 or t 5 with average likelihood score decreasing for larger t this implies that most packet signatures are primarily influenced by the previous 4 or 5 packets where modeling probabilities for longer timesteps leads to a model with too few independence assumptions that overfits the data 
this result is intuitive as we would expect certain time dependent attack types such as denial of service to be better modeled and predicted by a model that takes into account extended time dynamics 
iv analysis
we also note that the true log likelihood score of each attack model is proportional to its representation in the data set and so our model for a given attack type is only as robust as this proportion during testing we noted that our svm classifier trained on log likelihood scores performed surprisingly well correctly classifying anywhere from 93 to 99 of 1000 5000 validation examples 
we note a particularly surprising result in which our classification accuracy decreases as the number of markov steps our attack models were trained with increases 
this result at first seems to contrast with our training results in which our models attain a maximum log likelihood of the data around t 5 however as
in this work we have constructed a generative probabilistic markov model of four different attack types fitting the parameters of the corresponding distributions from the data 
we found that changing the time dependence of the markov model had a strong influence of the appropriateness of fit of the model in general reaching a maximum when considering the previous five timesteps 
v conclusion
we also showed that the average log likelihood response of our models between positive and negative examples generally converges suggesting the discriminative power of the models drops as we consider more timesteps 
finally we also showed that fitting a discriminative classifier with the collective log likelihood scores of our models generally achieves very good accuracy and supports the claim that our models do indeed capture much of the intrinsic structure differentiating an adversarial model from a malicious one along with the behavior of such an adversary in choosing states 
future includes investigating how we can further leverage these models by incorporating them into an mdp resiliency system or intrusion detection system as discussed in the introduction 
the authors thank professor mykel kochenderfer vineet mehta and paul rowe for their consistent support and inspiration 
the authors also thank dr arash habibi lashkari from the iscx research center unb canada for sharing the nsl kdd dataset 
acknowledgments
research performed at the monterey bay aquarium research institute mbari on gelatinous marine animals commonly known as jellyfish abbreviated jellies here frequently requires that observations be taken over a long period of time 
this is typically done using a remotely operated vehicle rov driven by a human pilot but the concentration required to continually deliver precise thruster control can be tiring for the pilot the pilot aid system previously tested at mbari relies on stereo vision and blob tracking techniques 
to differentiate between jellies and the surrounding ocean a grayscale image is first thresholded to detect all light colored blobs past experiments demonstrated that many of the tracking failures occurred either due to the target moving out of the camera frame or else from a loss of target recognition when other animals pass near the jelly this project presents several new machine learning techniques to improve the robustness of the jelly tracking system 
first machine learning can be used to improve the segmentation processes used to isolate the target in an image frame 
an algorithm to autonomously estimate the 3d orientation of a jelly is also outlined 
to demonstrate the technique a single genus paraphyllina was selected due to its clear axial features
mbari provided a set of several thousand images of jellies taken from previous research dives 
after sorting the images and removing any with a target too small or out of focus 140 usable images of paraphyllina in various orientations remained 
a image processing
this training set was augmented by rotating each image by 90 180 and 270 to provide additional orientations on which to train the algorithm for a total database of m 560 images 
orientation data was manually added to each image by drawing an estimate of the jelly s axis on the frame and an estimated depth angle was entered as either 90 jelly pointed directly away from camera 45 0 45 or 90 jelly pointed directly towards the camera two separate image segmentation methods were tested for comparison 
first a threshold based blob detection algorithm similar to this technique works well for a large number of images such as
to train a classifier to recognize the orientation of a jelly each image i must be represented as a feature vector i that describes the elements of the image 
this feature vector can be generated in several ways 
b orientation classification
in the first method the images are first sorted by their orientation category 
several thousand features are then generated for each image using the red and blue content of all pixels in a sample image 
blue points were categorized by the em algorithm as background and red points were categorized as jelly 
contour lines for the two probability distributions are also shown speeded up robust features surf algorithm 
however not all features generated from surf will be useful for orientation prediction 
therefore the k means algorithm is used to reduce the number of features across all categories and find the features most highly associated with the jellies orientation 
the k means approach reduces the feature set to a 500 element vector for each image once the feature vectors i have been generated by the k means algorithm an svm is trained on the data 
cross validation is used to ensure that the model is not overfitting the training data 
the set of 560 images is divided with 80 in a training set and 20 in the test set 
using a one vsall approach a hypothesis function h k x i t k x i is trained on this training set 
the svm produces a vector k for each of the 14 orientation categories 
to make a prediction for an image x i the margin for each category k is calculated 
the algorithm ultimately selects the category with the highest probability as the most likely orientation an alternative approach to image classification relies on neural networks 
for comparison with the procedure outlined above a neural network was trained using matlab s neural network toolbox on the thresholded images with no orientation data 
this neural network generated a 50 element feature vector i describing each image 
a softmax function then uses this set of features along with the orientation data y to make a classification prediction 
to improve the features generated for each image the neural network can then finetuned with a second pass using the prediction errors from the first softmax function 
similar to the approach above the most likely orientation category is returned either method outlined above can also be extended to other aspects of the jelly tracking system 
for example when a second object enters the image frame the algorithm can estimate which of the two is a jelly by attempting to predict an orientation for both 
the algorithm will have higher confidence for the object with features most similar to the training images 
therefore this single classifier can serve as a rudimentary approach to differentiating between multiple objects in a frame simply by comparing the orientation probabilities for each 
the main task remaining is to fully implement the algorithm to classify jelly orientations from video frames in real time 
this is necessary before the approach can be tested on an rov for use in a control architecture 
iv future work
the image encoder can be stored so it does not need to re run the full k means analysis for each new image 
the trained svm can also be stored which should speed up the process enough to take place in near real time 
while further improvements to the test image classifier accuracy are desirable the current architecture offers enough accuracy to deal with the relatively rare failure scenarios in the existing control system there are several avenues that could also be explored to improve the current classification scheme 
for example the classifier could be extended to take previous orientations into account 
since jellies usually do not move very quickly cases with low prediction confidence could be improved by considering a previous estimate with higher margin and using that data to guide the prediction 
another approach would be to attempt to directly model the orientation of a jelly from the feature set without reducing it into discrete buckets 
this has the advantage of allowing for a loss function that heavily penalizes orientation estimates farther from the true orientation 
following this approach could potentially improve the number of images correctly classified in the within onecategory region finally a second classifier could be implemented to help distinguish between a target jelly and other objects that may enter the frame 
this will require modifying the blob tracking algorithm to follow multiple objects but otherwise the approach is identical to the orientation classification technique demonstrated here 
once a specific type of jelly has been targeted a classifier trained to recognize that genus could be used to reject all other animals that enter the frame 
using this approach a single routine will ultimately be able to take in a new image classify it as target other estimate the 3d orientation of the jelly and then pass all this information to the controller to enable a highly robust tracking system 
using a training set of images two methods for segmenting the image to isolate the jelly have been implemented 
a grayscale threshold based approach was functional for a majority of the images but an approach that uses the em algorithm to estimate the mean and covariance of the color distributions was demonstrated to be more effective 
this method had a slower run time but had the advantage of being able to distinguish between the target and the background for darker images where thresholding failed machine learning techniques were also effective for estimating the orientation of a jelly from an image 
by training an svm on features generated using surf and k means algorithms the approach was able to correctly classify the orientation for 96 of training images and 68 of test images 
the 3d angle out of the plane was also included in the data and the classifier successfully distinguished between jellies pointed towards away and normal to the camera 
finally the classifier was able to roughly distinguish between jellies and other objects using the confidence in its orientation predictions 
a neural network approach was also implemented but it was significantly less accurate than this method and therefore will not be pursued further once implemented on an rov these improvements will make a significant difference to the robustness of the jelly tracking system 
the new image segmentation technique could allow an existing control architecture to get a more accurate estimate of the position and shape of the jelly allowing for better tracking as outlined in
this work would not have been possible without the assistance and training images from the monterey bay aquarium research institute 
prof stephen rock stanford university department of aeronautics and astronautics and the members of the aerospace robotics lab also contributed invaluable advice and feedback 
vi acknowledgments
the fragrance and perfume industry is experiencing a growth of approximately 7 to 8 annually during 2014 and 2020 1 and the size of the global fragrance and perfume market is as large as 40 1 billion in 2016 2 
the continuous growth and huge size of this market is due to the worldwide popularity of perfume use 
for example 52 54 u s households use or buy perfume cologne and toilet water in 2016 the ultimate goal of this work is to provide advice for both perfume buyers and manufactures 
for perfume buyers this work is supposed to help them choose perfumes that will help them smell ontrend high rate high popularity or unique high rate low popularity 
for perfume manufactures we would like to provide advice regarding how to produce perfumes that will become next best sellers 
get raw data and select features 
datasets are obtained through web scraping of www fragrantica com using python library beautifulsoup 4 
www fragrantica com is a perfume encyclopedia website containing information of over 38 000 perfumes 
we first apply six different classification models to predicting both rate and popularity of a perfume 
because classifications models do not work very well for the rate prediction we then apply three different regression models to the prediction of rate 
method
we have used the implementation of these models in scikit learn classification models 
the first classifier we have tried is support vector machines svm 
we choose the kernel to be gaussian radial basis function with parameter a small means two vectors far away from each other will still influence each other leading to under fitting 
a large means that the support vector does not have widespread influence leading to high bias and low variance namely over fitting 
the second classifier we have applied is boosting 
gradient tree boosting has been chosen with decision stumps as weak learner 
using boosting we have also compute the relative importance of different input features 
the relative importance of input features is computed based on the number of times a feature is selected for splitting weighted by squared improvement to the model 
the third classifier we have tried is k nearest neighbors k nn 
k nn works by finding k training samples closest in distance to the new data point and then predicting the output from its k nearest neighbors 
a very large value of k would lead to slow simulation and we stop at using k equivalent 12 
we have also applied decision tree dts classifier 
a large value of maximum depth of the tree leads to high bias and low variance and vice versa 
in addition to these four classifiers we have also tried logistic regression and na ve bayes nb classification regression models 
decision tree dts regression linear regression and boosting regression have been applied to predicting rate 
similar to dts classifier the performance of dts regression also depends on the maximum depth of the tree 
for boosting regression we use least squares loss function and 200 boosting stages to perform 
in through the comparison of different classification and regression models we have shown that classification models such as boosting work the best for the prediction of perfume popularity and can give a test error as small as 9 and regression models work better for the prediction of perfume rate and can lead to a test error of 12 
in summary we have applied different ml techniques to predicting the rate and popularity of a perfume using its various features 
we have shown that some classification models such as boosting can predict perfume popularity with a test error as small as 9 and regression models can be applied to predicting perfume rate with a test error of 12 
conclusions and future work
furthermore we show that some features such as season are more important than other features such as accords to continue this work we would like to include the perfume ingredients as another input feature 
this will help our work provide guidance for perfume manufactures regarding how to produce a popular perfume 
finally we want to interpreter our ml results and try to provide customized advice 
for example we would like to predict the most popular perfumes for different seasons 
also we would like to show what combination of perfume notes would have the highest rate 
the effect of svms and cnns on emotion detection in static images are analyzed in this paper 
the investigation into the svms included testing various parameters and kernels as well as making multiple hypotheses concerning proper feature vectors that emotions depend simply on the locations of facial landmarks and that emotions are dependent on the relationship between landmarks 
the investigation into cnns included parameter fine tuning and extracting the final layer as a feature vector for the svm 
the cnn far outperformed the svm with a 91 3 accuracy 
emotions inform perception and action 
humans are socialized to learn how to act and react based on their understanding of the emotions of those around them 
ii problem overview
because emotion recognition is so important to everyday life we want to train an algorithm for this task 
highly accurate emotion recognition systems could lead to advances in psychology and sociology which would lead to an increased understanding of decision making and consumer preferences among other things 
during the course of our project we compared the performance of svms and cnns on emotion classification 
we considered both generative and deterministic models for our work 
while some real world scenarios may be accurately modeled with certain emotional probability distributions i e 
iii model selection
people are more likely to be happy than angry at social events datasets tend to contain a more equal distribution of emotion 
because we determined that there was no clearly discernable or evidently meaningful a priori probability distribution for general emotion detection in a curated set of static images we decided to focus on more robust discriminative models for this project we chose to use an svm because it has the ability to capture complex relationships in both regression and classification problems 
the svm accounts for nonlinearity in features and can be tuned to balance bias and variance effectively 
in previous research conducted on this topic svms were also shown to provide a baseline for understanding what kinds of features are relevant to this problem 
given the wide number of feature possibilities on a given face the svm was best suited to our task additionally we decided to investigate convolutional neural networks 
cnns have been shown to do well on image recognition tasks and provide versatility in feature learning and they are often considered state of the art for image learning tasks 
in image based static facial expression recognition with multiple deep network learning yu and zhang use deep cnns to determine facial emotions 
their baseline was around 35 accuracy and they were able to optimize this to hit about 55 accuracy 
iii related work
we are using the extended cohn kanade dataset a dataset specifically released for the purpose of encouraging emotion detection research 
the faces show a range of 8 emotions 
iv methodology
additionally images were classified using a pre trained cnn 
the first and second fully connected layers of the caffe imagenet cnn were extracted and used as input for the svm 
to implement the svm we used the python sklearn toolkit for training fitting and testing 
since we chose to classify a range of 7 emotions we needed a multiclass svm which is provided by sklearn in both the one vs one and one vs all variations during the training fitting and testing process we found that the one vs one implementation often performed better than one vs rest which is expected given a unique classifier is constructed for each pair of classes 
v svm
normally the concern for one vs one svms is expensive computation but since our datasets of images were relatively small one vs one training was able to run quickly enough 
to locate the svm with the best results we replicated the experiments with various kernels such as the linear rbf and sigmoid kernels 
for each of these kernels we also performed parameter tuning by trying the experiments with combinations of c and gamma values across the ranges 0 1 1 10 100 1000 and 2 7 2 6 2 1 respectively for feature selection our naive implementation for svm used the direct normalized landmark coordinates received from the google api which meant for each image the feature vector consisted of 34 tuples of x y z coordinates each represented as a float 
however we hypothesized that when humans interpret facial features they look at how the features interact with each other to draw a conclusion 
we then added more sophisticated features by calculating angles between each of the landmarks which was done by choosing three coordinates setting one as the vertex and using the other two to form the left and right legs of the angle 
since this resulted in a computationally expensive process given all the permutations possible we hand picked certain facial landmarks that had the potential to change the most to include in our feature vectors 
for example the angle from bottom lip to left and right mouth corners were experimentally determined significant while the tip of nose landmark was removed completely 
several convolutional neural networks have been trained on the generic imagenet dataset 
these models are optimized for coarse labels i e 
vi cnn
window or cat but the final connected layers of these nets are often fine tuned for more specific tasks on new datasets with different labels 
two different cnns were used in this project the cnn used for direct comparison in this project was the transfer learning model submitted to 2015 emotion recognition in the wild contest by h win et al 
the cnn used for svm feature extraction was the caffe architecture pretrained on imagenet
classification accuracies several of the models were similar and lower than expected 
simple models somewhat outperformed more sophisticated ones on this dataset 
figure 2 results summary
the combination of fc7 features and our manually selected features had the highest performance of the feature combinations 
however the fine tuned cnn far outperformed other methods of emotion classification correctly classifying 91 3 of the test set 
the superior performance of the cnn aligns with the results of previous work 
our svm did not perform as well as we hoped 
much of our difficulty with obtaining performance gains stemmed from the small size of our dataset 
vii interpretation of results
the rbf kernel tended to disproportionately choose the most common category class 7 the emotion surprise even with parameter tuning limiting its usefulness in practical applications 
this bias is likely because there was not enough data to allow for meaningful separability given the complexity of our feature set 
because svm performance is highly dependent on feature sets we believe that the svm can be optimized with more directed features 
given that the svm run solely on features of angles between facial landmarks performed better than the pure landmarks we can hypothesize that features that help an algorithm better understand the interactions between facial landmarks are better however there are still improvements that can be made presently our features fall on a continuous range of values yet this leaves much of the discretization of the data to the algorithm 
we believe that using a feature set of indicator functions e g 
an indicator for a smile or for furrowed eyebrows would boost svm performance as the plane of separability would theoretically become more apparent what is interesting to note is the the features extracted from the cnn seemed to contribute to svm performance gains 
despite the pre trained cnn s relatively poor performance the more generalized fully connected layers served as useful representations 
the combination slightly outperformed both the pre trained cnn and the svm indicating that transfer learning would be worth further investigation the restrictions of our small dataset also prevented us from attempting to fully train a cnn because of the large sample sizes these models require 
the low performance of the emotion recognition cnn from the emotiw challenge is likely due to nuanced differences between our data and that data used for training 
future studies would separate color pictures from black and white pictures as well as ensure image size and proportion compatibility to the emotiw training data before retesting this model 
the extremely high accuracy of the tuned caffe cnn pretrained on imagenet is likely due to some level of overfitting 
because the dataset is so small the neural net is likely learning features specific to this dataset rather than features relevant to emotion recognition as a whole 
we plan to investigate how fine tuning other emotion specific cnns on our dataset affects performance 
a larger dataset is essential for this task and thus future work can train and test on this dataset in combination with datasets such as jaffe 
viii future work
furthermore fine tuning a cnn with higher accuracy on the emotiw challenge sets could produce more useful intermediate layers for input into the svm 
additionally fine tuning on larger datasets could also decrease the effect of overfitting 
future studies could investigate combinations of datasets to add to variation in training and testing data lastly to further optimize the svm we would implement indicator function features as previously mentioned 
related work also suggests that histogram of oriented gradients hog features instead of geometric ones could lead to significant performance gains 
the singular question are we alone has boggled scientists for centuries 
the seti institute operates the allen telescope array ata to observe star systems for radio signals which may provide evidence of extraterrestrial intelligence 
signal events that maintain an intensity above an empirically set threshold across all frequencies are converted into spectrogram images through a slidingwindow fast fourier transform fft and set aside for human analysis a key problem is being able to pinpoint significant patterns in the signal stream that are not associated with known interferences such as aircraft rfi and narrowband zero drift signals 
seti is currently constructing a pipeline to stratify known interferences within signal streams in real time 
in the past few years an unknown subset of signals inelegantly referred to as squiggles has become increasingly prevalent 
squiggles are broadly defined as modulating signals hand curated by scientists at nasa their origin is unknown 
in fact the quest to understand squiggles is an open problem posed by seti using spectrogram waterfall plots collected from the ata seti dataset we hope to make an open source contribution in two ways 1 perform supervised learning to classify squiggles against nonsquiggles and 2 conduct unsupervised learning to identify potential squiggle subgroups and their characteristics 
in the specific domain space of squiggle analysis little work has been done 
for the past decade seti has worked to optimize its real time processing algorithm to identify notable signal events that warrant human intervention 
sonata is a open source software system that currently includes a post processing package to identify common pulsar and linear carrier wave signals there has been a significant amount of academic literature dedicated to the domain of classifying unknown signals streams and feature extraction from spectrogram images 
iversen et al 
we used a dataset of 8490 spectrogram images provided to us by the seti institute 
this includes a library of 883 hand identified squiggles and 7607 nonsquiggles attributed to known human interferences and random noise 
iii dataset
each spectrogram is comprised of a signal event that passed the sonata pre processing phase between the dates of 8 10 2014 and 11 2 2014 
the png format is specified at 768x129 pixels representing roughly 100mhz in bandwidth and 93 seconds respectively xaxis frequency domain y axis time domain 
in exploring the spectrogram dataset we arrived at the conclusion that each squiggle can be treated a discrete time series by selecting one frequency from each time slice in a spectrogram 
our initial approach relied solely on intensity selecting the frequency with the maximum intensity from each timeslice 
a conversion of spectrograms into discrete timepoints
however this approach failed to trace the squiggle accurately in the presence of strong background noise 
furthermore many squiggles had gaps sets of time slices over which the signal disappears almost entirely 
to solve this issue of interpolation and to exclude outlying points we sought the optimal path to minimize the following loss where i x y gives the intensity at some discrete point x y and i n t x t o 1 0 1 2 0 0 i t o 1 x t o 2 represents the intensities of surrounding points 
and are the parameters of our loss function 
we found that 0 5 and 0 produced the best results 
hence our final loss function was the code was optimized in c with the ultimate goal of integration into a real time analysis pipeline 4 
we extracted two features from the data prior to conducting any time series analysis 
1 loss we found that the final value of the loss function resulting from our discretization algorithm over the resultant squiggle proved to be a reliable measure of overall intensity and coherence of the signals in our data set 
b spectrogram analysis
2 width letting i the max intensity of a given time slice and i be the corresponding index we estimated signal width of all 129 time slices based on the number of indices in i 10 i 10 with intensities 1 2 i 
we then took the mean of signal widths falling between the 10th and 90th percentiles 
each spectrogram image has the same frequency units but captures a different window of bandwidth 
unfortunately the frequency ranges of each plot were unavailable to us so we conducted our analysis in a manner agnostic to absolute frequencies 
c time series analysis
before any time series analysis we modified each time series to have mean 0 by subtracting the original mean 
after this normalization we extracted the following features 1 variance we took the overall mean squared error mse of the time series around the mean 
2 modulation we first fit a linear model above to the data using simple linear regression 
we then estimated 2 by taking the mse of the model 
3 arima 1 1 1 parameters using the arima function in r we first tried out various models in the arima class of models before settling on arima 1 1 1 which consistently provided the lowest akaike information criterion aic when fitted to squiggle time series 
we fitted arima 1 1 1 models to each squiggle time series and extracted estimates for the parameters and 2 and incorporated these estimates into our analysis 
4 hurst exponent e r n s n cn h we estimated the hurst exponent a measure of the long term memory of the system using the numpy polyfit function on the lag vector we obtained 
5 fast fourier transform x k t t 0 x t e 2 ikt 128 k 1 
we sampled the fft output at 63 uniformly spaced frequencies from 0 to 
63 we applied a fast fourier transform fft to the squiggle time series to extract the component frequencies in the signal 
we ignored the fft at k 0 which corresponds roughly to the mean of the signal 
our resultant features were the absolute values of the fft at these points 
to ensure an unbiased classification the full dataset was split into 90 training and 10 test 
using the training set we applied 10 fold cross validation to tune model parameters 
v classification
performance metrics were then scored by applying the fitted classifier to the 10 heldout validation set 
we used two performance metrics 1 acc accuracy defined as 1 misclassification error and 2 auc area under the receiver operating characteristic roc curve 
the roc curve measure the true positive rate against the false positive rate at various threshold settings 
a steep slope in the beginning indicates good predictive performance increasing the threshold increases the true positive rate while introducing few false positives 
when the curve flattens we observe the introduction of false positives as the threshold increases 
note that an auc value of 0 5 refers to a random classifier that stratifies positive and negative samples arbitrarily 
in our models we denote a positive and negative label as nonsquiggle and squiggle respectively 
we first applied unregularized l1 regularized lasso and l2 regularized ridge logistic regression using the normalized 129 time series points 
we achieved a baseline accuracy of 87 5 on the test set with all three models classifying the full dataset as nonsquiggle 
a baseline model
and in fact the true negative rate is 100 and the false positive rate is 12 5 for all three models 
we can note the auc of 0 5 denoting a random classifier 
since our dataset is unbalanced with 10 4 squiggle and 89 6 nonsquiggle our accuracy begins relatively high despite using a model that deems all input as nonsquiggle 
improvements in acc and auc metrics are in relation to this baseline 
train
logistic model
we improved our baseline model by transforming our features space from the 129 time series points to the 63 fft frequency samples 
we then applied unregularized l1 regularized lasso and l2 regularized ridge logistic regression once more achieving a significant improvement in both acc and auc 
b intermediate model
unregularized logistic regression produced the greatest auc while ridge regression produced the greatest acc 
in our final model we incorporated all 72 features comprised of the 63 fft frequency samples the 4 parameters from the arima 1 1 1 model the variance modulation and hurst exponent of the 129 slice time series and signal width extracted from the spectrogram and loss from the dynamic programming algorithm 
we applied 4 families of classifiers 1 logistic regression using l1 l2 and no regularization 2 support vector machines svm using linear radial polynomial and sigmoid kernels 3 tree based methods including boosting bagging and random forests and 4 k nearest neighbors knn 
c final model
using 10 folds cross validation on the training set we performed hyperparameter optimization on each classifier 
we identified the optimal parameters shrinkage parameter in lasso and ridge logistic regression kernel parameters and soft margin parameter c for svms number of trees in tree based methods number of neighbors considered k in knn 
we chose the parameter s based on cross validation misclassification rate using the one standard error rule favoring simpler models to reduce the potential for overfitting 
for multi parameter classifiers we performed a grid search in aggregate we achieved our highest acc of 99 2 using the boosting tree based method 
however nearly all tree based methods resulted in relatively low auc metrics 
thus accounting for our unbalanced dataset we believe unregularized logistic regression is the optimal classifier boasting a test auc of 99 7 
in fact one of the key properties of auc is that is it invariant to class skew meaning that an unbalanced dataset of 90 positive labels will result in the same roc curve as a dataset of 50 positive 50 negative labels 
we identified the features that had notable predictive power in the logistic family of classifiers 
for unregularized logistic regression we noted 7 variables with a p value less than 0 001 loss hurst exponent signal width modulation and the 2 parameters from the arima 1 1 1 model 
d feature significance
for l1 regularized logistic regression we noted that 4 variables boasted nonzero coefficients loss signal width modulation and arima 1 1 1 2 
the p values are representative of each features significance as a predictor for the response it should be noted however that correlated features tend to reduce each others significance 
as verified by an analysis of variance anova test we claim that arima 1 1 1 2 and modulation are the two most significant features reducing the residual variance of our logistic model the most 
we can further verify these results using forward or backward stepwise regression choosing features iteratively based on adjusted r squared or akaike information criterion aic 
before clustering we normalized all features to have mean 0 and variance 1 then performed dimensionality reduction using principal component analysis pca projecting our 72 features into 5 principal component vectors capturing 98 of the variance 
we hypothesized that the squiggles likely followed a continuous spectrum rather than occupying distinct subgroups 
thus we sampled a variety of distance metrics including euclidean manhattan and canberra distance as well as various clustering algorithms namely k means and several hierarchical algorithms including single average complete mcquitty centroid median and ward linkage clustering 
we also applied divisive clustering which yielded unfavorable results and tended to place outlier points in their own clusters for each method we plotted the average silhouette score over all clusters while varying k from 1 to 15 
we observed that 4 clusters produced favorable silhouette scores for the methods which utilized the euclidean distance metric 
the question naturally arises whether the four clusters uncovered by each of the algorithms are in fact the same four clusters 
to determine this we mapped the four clusters to each other so as to maximize the proportion of points that are in the same cluster across all three methods 
fig 5 silhouette scores from different methods
we have graphed the corresponding clusterings below 
with the mapping given above the proportion of points that appear in the same cluster in all three instances is calculated to be 536 833 or 0 6070215 
we performed the same set of tests 100 000 times on randomized gaussian n 0 i noise 
the average value of the above proportion was 0 320 and only 98 100 000 test runs produced a proportion higher than 0 607 signifying a p value of around 0 001 with the null hypothesis that the data was generated from gaussian random noise 
the high level of concordance between the clusters generated signifies that the results we found are robust to the exact method of clustering 
below are examples of squiggles sampled randomly from each of the four clusters of the points that were assigned unanimously to a cluster 
with potential subgroups identified we assessed the characteristics of each cluster using known temporal and polarization characteristics 
the analysis of k 2 ward d2 hierarchical clustering under euclidean distance proved most promising 
b chi squared test insights
we ran a series of independent chi squared tests to assess the dependency of temporal month 4 hour timeframe and polarization variables against membership in the clusters above 
the table lists characteristics that were shown to exhibit the most significant dependency on cluster membership 
in particular we can note that nearly all members of the red cluster corresponding to squiggles with large variance in frequency were produced detected between 12pm and 4pm 
this dependency on the 24 hour earth cycle could imply that the source is terrestrial rather than external 
once denser clusters of squiggles are identified we hope to replicate similar chi squared analyses 
as discussed our model is agnostic to absolute frequencies 
we consider this to be a severe drawback especially in clustering as unique signals from the same source are likely to have similar frequencies although we were able to capture a wide range of modulation speeds our ability to do so was hampered by our discretization algorithm which assigned a single value per time point 
vii conclusions future work
at higher modulation speeds the signal simply appears to be a jagged wideband signal 
simulation and image processing could be used to turn modulation speed into a more tangible parameter instead of relying on the fast fourier transform 
prior to pushing our code to the setiquest repository we will also perform model selection to trim our predictors to only the most indicative to decrease both computation time and overfitting for real time classification if squiggles do in fact come from a wide array of sources the logical next step would be to continue uncovering dense clusters of squiggles that come from the same source on a predictable basis 
seti recently opened access to 400 000 unknown spectrogram signals we can apply our existing classifier to curate more squiggles thus providing a larger dataset to conduct unsupervised learning 
our work on this project was also counted toward cs 341 project in mining massive datasets 
our advisor was professor jeffrey ullman and our additional team member for cs 341 was frank fan 
viii acknowledgments
figure first of all we filter the training set by removing the wells whose history is shorter than the total months n of a test curve 
after the normalization on the data we eigen decompose the empirical covariance matrix and extract the first 5 eigenvectors as the principal components then we fit the known part of the test curve by using a linear combination of 5 eigenvectors the coefficients of the linear combination are calculated from linear regression 
results 
where yknown r l is the normalized known history of the test well ul r l 5 is the eigenvectors with the first l dimensions 
our estimation is therefore yestimate 
standarlized error well 1923 lwlr
if we assume high productivity wells are similar to each other and low productivity wells are similar to each other we can group all the decline curves into two categories 
we modify the k means method to be applied into this real situation that different decline curves have different dimensions 
l pca after k means
we calculate the distance between a centroid and a curve by using the dimension of the shorter one 
as comparing two figures in then we run pca again after clustering the original decline curves by k means 
from we apply leave one out cross validation to all the three methods compare the predictions with real production data and calculate the average relative errors as in
in a 2009 piece in this paper we outline our approach to predicting medicare costs on both hospital referral regions hrr and hospital levels 
more specifically we aim to build models to predict an individuals health care costs on the basis of nontraditional metrics by leveraging data from multiple opensource repositories 
in doing so we hope to both improve the accuracy of cost predictions and gain insights into factors responsible for fluctuations in health care costs 
for our project we focused on using two different sources for our medicare data sets the dartmouth atlas of health care dahc and medicare gov 
the dahc provides information on chronically ill patient care claims based medicare spending medicare population demographics post discharge events and more organized according to state county hospital referral region hrr and individual hospital levels 
a choosing our data sets
medicare gov has a dedicated website data medicare gov that provides a diverse range of data sets from which we chose the publicly available hospital comparison data sets which track individual hospital attributes and events such as structural measures complications readmission rates payments value of care outpatient imaging efficiency and more 
within both the dahc and the medicare gov data sets each of the metrics were associated with unique ids e g 
a provider id for a specific hospital or a hrr id 
the data from both dahc and medicare gov contain detailed information on approximately 3 192 hospitals out of the 5 686 hospitals in the united states we elected to use the dap hospital data year xls files from dahc for two reasons 
first this particular set of files was one of the few that the dahc had on the hospital level vs on the hsa hrr county or state level 
in contrast the
in order to utilize any of the aforementioned data sets we needed to perform an extensive amount of preprocessing 
first we organized the different data sets by their respective levels for example we grouped all the hospital level data sets for dahc together because each file utilized the same unique provider id 
b preprocessing data sets
using a single raw data set which contains information for around 3 000 hospitals in a single year would provide poor learning opportunities 
thus when creating our training sets we combined data from multiple years 
we then selected features from our dahc data sets columns in the files correspond to our features and from our medicare gov data sets measure ids correspond to our features and created training sets corresponding to the dahc hrr data one with demographic data to serve as a baseline estimator and another with hrr data from 2011 dahc hospital data over multiple years and medicare gov hospital data over multiple years 
each training example within each training set is specified by a unique provider or hrr id for our four training sets we used the following labels y i once we created our training sets the code to do so is found on our github page we found that many of the training examples especially those from medicare gov were missing information 
in order to address this issue we used the following different strategies 1 missing feature thresholding in our first preliminary approach we omitted any training examples that failed to have a certain percentage of features say 60 or 50 filled 
after this initial thresholding for the remaining training examples that had missing features we replaced them with the mean over that feature s column 
this filling method was used for both our dahc and medicare gov training sets 2 item item collaborative filtering our subsequent data filling approach was to use an item item collaborative filtering recommender system which we wrote with the guidance of cs 246 s recommender system lecture notes in addition to the above strategies for addressing missing data we utilized variance thresholding in order to remove features that yielded little to no variance to our data set 
after performing these preprocessing steps our training sets were ready to be used with various learning models 
all of the learning algorithms we used were implementations found in python s scikit learn package 1 supervised learning algorithms for our datasets we used multiple different supervised algorithms for both classification and regression 
a classification for classification we used both logistic regression and linear discriminant analysis lda 
c learning algorithms
logistic regression was used on our medicare gov training set to predict whether or not the expected medicare cost for an individual hospital was above or below the national average 
we also used multi class lda to predict the expected medicare cost quantile for an individual hospital 
b regression we decided to use the following three algorithms to predict medicare costs 1 linear regression 2 kernelized support vector machine and 3 gradient boosting 
each of these methods was used to predict expected medicare cost on both the hrr and hospital levels 2 unsupervised learning algorithms in addition to using supervised learning techniques like those mentioned above we decided to also use the following unsupervised algorithms 1 k means clustering 2 principal component analysis pca and 3 various manifold learning techniques a k means clustering we ran k means on our initial dahc and medicare gov training sets in an attempt to try and determine whether there were any overall patterns or trends in our data b principal component analysis pca and manifold learning our primary reason behind leveraging both pca and manifold learning was to help us visualize our highdimensional data 
both pca and manifold learning allowed us to project our data onto two dimensional plots however pca makes the assumption that there is inherent linearity in the data while manifold learning attempts to reveal non linear structures in the data 
the following manifold learning algorithms were used in our visualization 
for each of our supervised learning techniques we used k fold cross validation where k 7 
in addition we generated learning curves which show the cross validation and training scores for an estimator to help us visualize our model s performance with varying sized training sets 
d validation methods
learning curves allow us to determine how much we benefit from adding more training data and whether the estimator suffers more from a variance error or a bias error 
for our kmeans clustering we evaluated the quality our clustering by utilized the silhouette coefficient which is a method for evaluating clusters when the true cluster designations are unknown 
the training set constructed using medicare gov s hospital compare data sets generally performed poorly with all of the supervised learning algorithms that were attempted 
because the labels for this particular data set represented how much a hospital s spending deviated from the national average the average being represented with a 1 with under spending being less than 1 and over spending being greater than 1 we leveraged logistic regression to determine whether we could accurately classify an example as over or under spending 
a medicare gov hospital compare datasets
with k folds cross validation k 7 our mean classification accuracy was 0 59847865 unfortunately because of the poor performance of our regression and classification models on the medicare gov training set we elected to omit further results 
using demographic features we created three supervised baseline estimators of medicare cost on the hrr level 
to determine how well the baseline models performed on the test set we calculated the residual sum of squares r 2 score and explained variance score for each model 
we also plotted the training and test set deviance as a function of boosting iteration as well as the most important features used for regression 
the results of this analysis are shown in pca with k means k 5 iv 
analysis a hrr level data our first baseline hrr estimator using traditional metrics such as demographics and ethnicity performed poorly and proved very ineffective at predicting medicare costs when compared to the estimator based on non traditional features 
from our analysis some of the most important features for our non traditional estimator were number of ambulatory cases readmission rate physicians per 100 000 residents and critical care physicians per svm learning curve 100 000 residents 
these results suggests that quality of initial care and health care system complexity e g 
number of physicians per resident may play a larger role in determining a region s medicare costs than individual demographics 
as a result enforcing more thorough initial health screens to reduce readmission rates and possibly ambulatory cases may lead to a decrease in medicare costs 
b hospital level data we found that models trained on our data set consisting of non traditional metrics at the hospital level predicted expected medicare cost very well 
on the individual hospital level some of the most predictive features include percent of deaths occurring in hospital medical and surgical unit days per patient medical specialist visits per patient and number of beds 
a higher percent of deaths occurring in hospitals may be indicative of the hospital s reception of more extreme cases of chronic illness e g 
treatment of eczema vs renal dialysis which may in turn require more medical and surgical unit days per patient and medical specialist visits per patient 
additionally hospitals receive more extreme medical cases usually through a referral process because they are larger either in staff which usually translates to a higher patient capacity and more equipped to handle medical complications that may arise from severe medical conditions 
these patients eventually end up with high medical and surgical costs before the end of their lives 
as with the hrr level finds we see that a system s complexity is correlated with medicare costs 
c learning curves the learning curve graph for our linear regression model shows that the training and validation scores are plateauing to a value of approximately 0 8 
this particular graph pattern shows that our model is currently suffering from higher bias than desirable meaning we are underfitting the data 
in order to address this issue we could introduce greater complexity to our estimator however this visual phenomenon may be indicating that the data may not have a strictly linear relationship 
it is also worth noting that increasing our training set size for linear regression would provide little improvement on the training and validation scores 
with this in mind understanding the visuals provided by the manifold learning plots may be informative in understanding the underlying non linear structure to our data that may explain the observed 
plateauing behavior the learning curves for our gradient boosting and svm models show that we are achieving validation scores of around 0 9 
because the training scores are above our validation scores for both of these models we can see that their generalization and performance can be further improved with additional training examples 
our results indicate that medicare costs can be estimated with reasonable accuracy using non traditional metrics associated with individual hospitals or hrrs 
additionally models trained on non traditional features were significantly better at predicting medicare costs than models trained on demographic information alone 
v conclusions future directions
as a result our analysis suggests that medicare costs are more strongly influenced by location of care hrr region or hospital than they are by individual demographics 
therefore non traditional metrics such as those used in our project should be included in order to assure accurate and realistic medicare cost predictions for patients looking forward we hope to next construct a predictive model for individual patients 
the ability to predict the expected cost of admission to a specific hospital based on an individuals symptoms could be beneficial and more reflective of cost than predictions at the hospital level 
we also hope to improve our current learning models by utilizing methods like grid search to find optimal parameters for our models and incorporating more data into our training sets 
code all the code and datasets used are located at https github com jlouie95618 cs229 project 
we would like to thank the entire cs 229 teaching staff for this opportunity and especially professor john duchi for his investment in providing an incredibly challenging and rewarding offering this quarter 
to increase the capabilities of an automated audi tts we applied learning algorithms to experimental data from both autonomously and professionally driven test runs to determine transient engine behavior 
data was collected at thunderhill raceway by sampling the vehicle s dynamic states over the course of testing 
sparse pca was then performed to prune the feature set and remove redundant data 
learning transient behavior may increase the control system s performance in tracking a desired velocity profile compared to the current engine mapping derived under steady state assumptions 
to learn this transient behavior nonlinear autoregressive neural networks multi layer perceptron networks and random forests are used contrasted and validated 
due to the highly nonlinear nature of neural networks small perturbations in input states result in physically impossible and undesired predictions thus random forests prove to be a more robust predictor 
similar unpredictable performance was shown when implementing the multilayer perceptron regressor which is not included for lack of space 
ultimately the random forest method is chosen to learn the model by estimating the throttle commands required for a given response 
the random forest model is verified by predicting race driver data given the current and past vehicle states during testing 
predictions were made with a mse of 1 8 2 and within a 90 confidence bound of 2 1 
additional validation through model inversion provided little insight into performance of the forest because of correlation with measured velocity and acceleration states 
immediate next steps consist of optimizing the performance of the forest for real time online predictions and implementation on the vehicle for experimental validation 
the dynamic design lab at stanford university performs vehicle automation research on an audi tts shelley 
shelley work focuses on following optimal paths and speed profiles at the limits of friction to achieve lap times competitive with professional racecar drivers 
to minimize lap times the vehicle s controllers must follow speed profiles as shown in shelley s longitudinal controller operates in a feedback feedforward framework to follow desired speed 
part of the controller receives a desired longitudinal acceleration and predicts the throttle needed to achieve it 
its current form is a rudimentary lookup table augmented by the engine s gear and rpm state shown in
the dynamic design lab has recorded several years of shelley s data sets consisting of both human and autonomously driven experiments 
we selected a small subset of these data two autonomous and one professionally driven sets to train our models and one professionally driven set for validation 
problem representation
in total the sets include 58 278 chronological feature target pairs of which 28 form the validation set these data consist of vehicle and engine states captured from shelley s onboard sensors at a rate of 200hz 
the throttle or pedal percentage actuated at each time step formed our target set y twelve variables that might intuitively relate to throttle compose our feature set x 
then to capture the time dependent dynamics of the system the feature set was transformed to incorporate delay each example s feature set was augmented with the features of the five preceding examples to form x 
finally an estimation function was learned to predict the throttle paired to each time step s features shown in additionally we performed sparse principal component analysis spca on our datasets to investigate to the independence and potential pruning of features 
unnecessary features may slow model computation wasting precious seconds in the real time environment we hope to embed our model 
spca is similar to traditional pca but modified to limit the number of features incorporated into each pc traditional pca is not as helpful to identify feature correlation because each pc will be typically built from components of every feature 
after modification however spca concisely builds each pc from only those features most correlated to one another
we first attempted to train a neural network to predict throttle percentage 
matlab s recurrent nonlinear autoregressive neural network with exogenous input narx was chosen due to its design for time series data 
methods results
the network is autoregressive because past target values y are fed into the model as features in parallel to exogenous inputs x 
during training the algorithm is run in open loop meaning the observed target values are used as past throttle states shown in narx network design parameters include the number of hidden neurons and number of delay states 
in the narx network s training and test performance is illustrated in the forest is constructed using the same training data as the narx network consisting of both autonomous and professionally driven tests 
to let the forest predict any output value in the training set we did not limit the number of terminal leaves that each tree could have 
in future optimization for computational speed this feature of the model may be necessary 
the feature set comprising the data consists of the vehicle inputs plus the time delayed feature transformation of these inputs as shown in model validation was performed using a separately recorded data set of a professional driver as shown in in tuning the forest for best testing performance and lowest generalization error number of trees were experimentally varied 
as shown in in validating our predictions against professional driver data using a forest of 80 trees we estimated the actual throttle percentage with a mse of 1 8 2 with a 90 confidence interval of 2 1 
confidence intervals were calculated by considering the upper and lower 5 of predictions made by individual trees in the forest for each given prediction as shown in
this work shows strong potential to provide a throttle function which may outperform shelley s current empirical map 
the random forest algorithm supplied with vehicle and engine states which are available in real time demonstrates capability to accurately predict the throttle necessary to achieve desired accelerations within a mse of 1 8 2 and within a 90 confidence bound of 2 1 
even though narx networks were able to attain a mse of 1 5 2 the computational time required for training and predictions provided limitations on utility 
furthermore the narx network showed limited utility because of its propensity to generate undesirable and physically unattainable throttle values 
similar performance was shown when implementing the multilayer perceptron regressor which is not included for lack of space in the near future we will integrate a pruned random forest model onto shelley for experimental validation 
to do so the model must be optimized for real time performance and the existing control architecture 
future work in limiting the number of leaves pruning features and optimizing for number of trees in the forest will allow us to limit computation time and thus create a model that we are able to experimentally validate on the car 
further work in investigating the impact of delay states will also allow us to consider adding future states to the feature set 
future desired acceleration states are available in more advanced control frameworks such as model predictive control mpc 
this may allow our model to learn the causality of future accelerations 
expanding this work in other domains of shelley s controller such as steering and brake systems may also be able to improve system performance 
we have shown the ability to accurately learn and predict the throttle profile for a given test from a professional with the goal increasing the speed tracking capabilities of the vehicle and minimizing lap times 
transcription factors are key gene regulators responsible for modulating the conversion of genetic information from dna to rna 
though these factors can be discovered experimentally computational biologists have become increasingly interested in learning transcription factor binding sites from sequence data computationally 
though traditional machine learning architectures including support vector machines and regression trees have shown moderately successful results in both simulated and experimental data sets these models suffer from relatively low classification accuracy typically measured by area under the receiver operating characteristic curve auroc 
here we show that learning transcription factor binding sites from sequence data is feasible and can be done with high accuracy 
we provide a sample cnn architecture that yields greater than 96 auroc 
additionally we discuss key questions that need to be answered to improve hyperparamter search in this domain 
since the human genome project which concluded in the early 2000s computational biologists have become interested in learning features of the genome from sequencing data 
revolutionized by the rapid advancement of second and third generation sequencing technologies learning biologically relevant features from sequencing data has become possible 
in the past 5 years scientists have used traditional machine learning techniques and probabilistic graphical models to impute haplotypes learn epigenetic markers and much more 
though we cannot provide a full review here this is done well in
this paper specifically considers the transcription factor binding site discovery problem 
though we cannot provide a review of transcription factors here we refer the reader to slattery et al where the transcription factor binding site literature is well reviewed recently many groups have studied similar genome prediction problems especially in the context of epigenetic features 
problem and related work
leung et al review the progress here in
here we provide a sample convolutional neural network cnn convnet architecture which provides greater than 96 percent accuracy on a simulated data set 
additionally we use a recently published interpretation package to show that the features learned by the cnn are roughly the transcription factor binding sites themselves suggesting that cnns are good models for this problem despite the computational expense 
our contributions
finally we provide a discussion of areas of key difficulty that need exploration to improve the accuracy of cnns for larger scale problems and for experimental datasets 
our work uses a simulated training set of sequencing data and labels 
we note however that this is a standard practice within this domain and many papers that evaluate model accuracy have taken similar approaches
we sampled sequence of length 1000 base pairs 1000 symbols from the genetic alphabet a c t g with a fixed gc fraction of 0 4 which is a biologically motivated result 
this means that the probability assigned to each letter was 0 3 0 2 0 3 0 2 respectively 
simulated data
with these probabilities we sampled a fixed number of reference sequences depending on the problem 
we then uniformly at random sampled an index from which to embed a transcription factor motif 
these motifs which come from the encylopedia for dna elements encode project are non deterministic 
again we are unable to review the results of the encode project here but this is done well in the motifs are accompanied by a position weight matrix pwm which is a n 4 representation of the odds of each symbol of the genetic alphabet at each position over the background frequency of a given letter 
we binomially sampled motifs from the position weight matrix and replaced our sequence data with the sequence of the motifs 
for our studies some experiments tested the number of training examples in which case we typically kept balanced classes of negative and training examples 
negative samples had randomly embedded sequences which provides a more biologically relevant negative class of data 
then from these reference sequences we sampled sequence reads which are 100 base pair substrings 
since sequencing data contains irreducible noise we added noise by flipping some symbols with probability 0 001 at each index based on the approximate error rate of a standard high throughput sequencing technology 
our labels were constructed by creating vectors with dimension equal to the number of transcription factors that were embedded 
we placed a 1 in the ith index of our vector if transcription factor i was embedded in the sequence otherwise we placed 0 
array data is achieved through the one hot encoding of our reads 
our sampling methodology though not identical to any previous study was informed by previous machine learning studies in genomics
convolutional neural networks have three features 1 convolutional layers 2 pooling or subsampling layers and 3 fully connected layers 
the convolutional layer conducts an affine transformation over its input 
convolutional neural network
multiple activation maps are created by multiple convolutional lters 
these pass to an activation function which is a non linearity such as a rectified linear unit relu 
the output is then passed to the pooling layer which subsamples the convolutional output and takes the maximum entries within the domain of a subsampling unit called max pooling 
this process convolution to max pool is repeated 
unlike previous layers the final layer is fully connected and a matrix multiply is done to get output predictions 
our architecture is cl mp cl mp cl mp fc where cl is a convolution layer mp is a max pool layer and fc is a fully connected layer 
between a convolution and max pool layer data always passes through the relu 
additionally convolutional neural networks like other feedforward archictures use backpropogation to update weights during each epoch during training 
we cannot fully review cnns here so again we refer the reader to we implemented our convolutional neural network using keras a deep learning library that wraps around the deep learning software in theano 
we discuss hyperparamaters used in results and discussion 
the input data was the training set as described previously 
the testing and validation data were similarly simulated sets of data 
we conducted two classes of experiments single motif classification and multi motif classification 
the first task is formulated as a single task binary classification problem with scalar labels 
results and discussion
the second task is formulated as a multi task binary classification problem with vector labels 
in the single motif embedding task we first embedded motif tal1 which encodes the transcription factor t cell acute lymphocytic leukemia protein 1 
this transcription factor has the sequence see right based on the second known motif from the encode project this figure is related to the position weight matrix pwm of each motif which is of dimension 4 l l is the length of the motif 
figure 1 t al1 encode transcription factor motif depicted as sequence logo
the ith column of the matrix represents the probability of each of the four states a c t g at base pair i in the motif 
the sequence logo above is generated by calculating for each base the information content which is log 2 4 h i e n where h i is the shannon entropy at each base pair and e n is the small sample collection factor which is reviewed in
from
data needs
the hyperparameters for the single motif task needed to be tuned to get optimal results 
basedo on
hyperparameter tuning
to prevent overfitting we investigated whether dropout was a feasible strategy to improve training accuracy 
to assess this we used a motif with greater heterogeneity than tal1 
dropout
the motif we chose was ctcf discovered motif 5 which displayed more entropy in its position weight matrix 
based on the figure 4 as dropout increases the test auroc substantially increases suggesting that adding dropout to the model may improve its robustness 
for the multi motif embedding task we embedded three motifs known to co bind
multi motif embedding task
recently a group at stanford published a method called deeplift deep linear importance feature tracker which is able to identify feature importance in the input to a cnn cite 
we ran the software for our multi task classification to see what features the cnn found most important to predicting the labels 
feature importance and model interpretation
these figures are encouraging because if compared against
despite good accuracy on the learning tasks described above a number of questions need to be answered before this model can be extended to experimental data 
this work shows that it is indeed possible to learn transcription factor binding sites based on sequencing data after sufficient data and hyperparameter tuning 
further questions and outlook
nonetheless it is still unclear how the optimal width of the convolution filter scales with the entropy and structure of the position weight matrix 
additionally we were not able to investigate how density localization impacts hyperparameters and how penalty coefficients for regularization and dropout probabilities may need alteration as more motifs are embedded 
though our methodology is similar to that adopted by previous computational biology papers we acknowledge that testing this model on experimental data is important 
here we have demonstrated the feasibility of learning transcription factor binding sites from sequencing data 
we note that there are an number of key questions see further questions and outlook that should be explore prior to using these models on experimental data 
nonetheless deep learning through cnns provides high accuracy for learning genomic transcription factor binding loci 
we thank stanford research computing for allowing access to the sherlock computing cluster for access to gpu compute nodes on which to run theano and keras code 
additionally we thank anshul kundaje johnny israeli and avanti shrikumar stanford computer science for providing access to their recently published deeplift code which provided interpretation figures for our cnns 
in this project we explored varieties of supervised machine learning methods for the purpose of sentiment analysis on tripadvisor hotel reviews 
we experimented and explored with the factors that affect accuracy of the predictions to develop a satisfying review analyzer 
we focus on not only the overall opinions but also aspect based opinions including service rooms location value cleanliness sleep quality and business service 
as a result we implemented an analyzer that is able to predict rating and polarity of reviews on individual aspects 
the accuracy of our predictors reached 70 to 75 for star rating and about 85 to 90 for polarity 
nowadays people frequently use reviews on online communities to learn about others opinions and to express their own opinions on business 
while a rating can be a good indicator for the opinion text reviews are usually more elaborative 
at the same time reading text reviews is also timeconsuming 
for this project we propose using machine learning algorithms to help extract opinions from text reviews 
more specifically we would like to build an algorithm that is able to differentiate individual aspects of the review 
being able to analysis the individual aspects is especially valuable since a business can vary in different aspects and that users might have different preferences and priorities 
the results of aspect based semantic analysis can be used in a wide variety of applications including building better customized recommendation systems and generating more informative summaries 
we can also use the results as features of the reviews for the purpose of further analysis 
for this project we will use the tripadvisor data set collected by wang et al 
dataset collection
we selected the bag of words model to represent our review texts 
we experimented with several feature extraction configurations we first experimented with the classic text analysis pipeline where after eliminating the stop words the occurrence of each word in the review text is counted 
feature extraction
then the tf idf term frequency times inverse documentfrequency is calculated to put more emphasis on the less common thus more interesting words during our analysis of some preliminary results we realized that due to the limited topic of reviews the words that express sentiments are actually very common across all documents 
as a result the tf idf transform might not be suitable for our application 
so we experimented with pipelines without the tf idf transformation we also explore the effect of using n grams 
we experimented with both character based n gram and word based n gram 
character based n gram is supposed to help us reduce the effect of mis spelling while word based n gram would provide us more information through phrases 
the extra information that is only carried by multi word phrases includes the extend of an opinion e g 
very good negations e g 
not good and the aspect e g 
good service 
since using n gram drastically increased our feature size we also applied explicit max and mean document frequency constraints to limit the number of features 
we experimented with two models for the problem 
we modeled it as a multi class classification problem and a regression problem 
model
to model the problem as a multi class classification problem we consider the one to five star ratings as the five classes 
to model the problem as a regression problem we consider the rating as a continuous variable that can take the value from one to five 
with both methods we treat each aspect as a separate problem 
one challenge is that aspect based classification inputs and guidelines are quite noisy in some sense 
since it is possible for a user to leave a star rating for a certain aspect without mentioning the aspect in the text 
and even if they did it could be only a small portion of the text 
one measurement is to compare the accuracy of a predictor trained on the aspect star ratings with the accuracy of a predictor trained on the overall star ratings 
in this way we will be able to see if training on specific aspect guidelines is actually helpful for the predictor to understand better 
we applied the multinomial naive bayes algorithm the linear svm algorithm and the linear regression algorithm respectively preliminary results and observations discovered one issue with our training data 
since people give a higher star rating a lot more often than a lower star rating our dataset is very skewed with about 5 one star ratings and about 40 five star ratings 
algorithms
as a result it is very important to balance the input training data 
we experimented with three popular methods to deal with unbalanced inputs class weight sub sample and over sample the class weight method applies a weight to each class based on the sample distribution 
the class weight is calculated by the inverse of the number of class samples in the dataset 
the penalty of each sample is then weighted by the class weight 
as a result a class with fewer samples in our dataset will be emphasized more to compensate the fewer number of samples the over sample method repeatedly sample the classes with lower distribution in the dataset until the same sample number is reached as the classes with higher distribution the sub sample method sub sample the classes with higher distribution so that the number of samples used training match the number of samples in the classes with lower distribution 
to explore the effect of different data set sizes we sampled the available data set into four sub sets of sizes 10 673 30 050 45 970 and 94 926 
for validation purpose we randomly sample one third of each subset as the test set and the rest two thirds of each subset as training set 
evaluation methods
we train our predictor using the training set and make predictions using the predictor on the test set we used two accuracy measurements star rating accuracy and polarity accuracy 
for each test sample a star rating of a certain aspect is generated 
the prediction is then compared with the ground truth to determine the accuracy of the predictor the accuracy is calculated by the number of mismatched predictions divided by the size of the set for the star rating accuracy under the multi class classification model a prediction is considered mismatched when the predicted rating is different than the user given rating under the regression model a prediction is considered mismatched when the predicted rating and the user given rating has more than a 0 5 star difference for the polarity accuracy under both models the results are defined into three groups positive neutral and negative 
all ratings above 3 are considered positive all ratings below 3 are considered negative while all ratings at 3 are considered neutral in the following sections we analyze the various factors we experimented with by examining the accuracy of the trained predictors 
the results in
class balance method comparison
as discussed in section 3 we decided to experiment with two different models multi class classification and regression and three different algorithms naive bayse svm and linear regression the results are shown in the figures below 
as we can see as the dataset size becomes larger the svm algorithm tends to generate better results 
model and algorithm comparison
thus we chose svm as the learning algorithm for our application 
at this point the star rating accuracy is about 50 to 60 while the polarity accuracy is about 70 to 80 
as discussed in section 3 we ran experiments with various n gram configuration and pipelines with and without the td idf transformation 
due to the limited computation resources we only ran the experiments on the sub dataset of size 45970 
feature selection comparison
as we found in section 4 3 the accuracy results generated on the sub dataset of size 45970 and the accuracy results generated on the sub dataset of 94926 are very close the exact n gram configuration setups we used are given in among all the aspects the business service accuracy is lower than the others 
it is most likely caused by the insufficient training set size 
even though all the aspects are trained using the same reviews a large portion of the reviews do not have a rating on the business service aspect 
thus the training set for business service aspect is much smaller than the other aspects 
we believe the prediction accuracy will improve with a larger training set 
lastly we would like to examine the effectiveness of our aspect specific predictors 
for this purpose we compared the accuracy of predictions generated by a predictor trained with only overall ratings and the accuracy of predictions generated by a predictor trained with aspect ratings 
overall vs aspect specific predictor
if our machine learning algorithm is indeed learning unique features for each aspect we expect the aspect based predictor to generate better results than an overall predictor 
the results are shown in as we can see the aspect training results are indeed better than the overall training results in all aspects which means training the predictor using aspect based objectives was able to learn the aspect specific features 
in this project we examined various class balance techniques and data models 
we achieved promising prediction accuracy on text review sentiments 
our prediction accuracy reached 70 to 75 for star rating and 85 to 90 for polarity 
we abstracted the problem into a multi class classification problem selected the bag of words feature model using 2 3 word long phrases and executed the machine learning process with the svm algorithm among the design decisions feature selection played a very important role in our design process we were able to improve the prediction accuracy by about 15 selecting the appropriate features 
therefore more optimization in feature selection will likely further improve the prediction accuracy 
the bag of words model used in this project included full review text even when training for specific aspects 
therefore the input data are effectively very noisy 
wang et al 
in order to help consumers decide which video games they would enjoy and want to purchase we will analyze data about a gamer s trends and favorites and explore various recommendation techniques backed by video games ratings and sales data to determine other games that the user might enjoy 
we can then predict how much a user would enjoy a previously unseen game 
objective
through use of giantbomb s api we scraped data about 100 user reviews of games for our preliminary dataset 
this dataset included details about the user who wrote the review the date of the review the title and description of the game and the rating given to the game from 1 to 5 stars with only whole numbers allowed among other metrics 
data source
we scraped game metadata and ratings from ign obtaining the rating category and console for over 14000 games 
in the future this dataset can be expanded to include reviews from more popular gaming social networks such as steam playfire and internet game databse igdb com 
as a baseline we used metadata username and description of game review obtained from giantbomb to classify a review to a score 1 to 5 using a random forest classifier which is effective in dealing with high dimensional data such as this dataset 
classification of scores
we will then explore the merits of item item and user user collaborative filtering various learning algorithms and different methods of feature engineering in order to determine an optimal way to recommend video games 
collaborative filtering
in our baseline we evaluated our results by calculating the accuracy score of the prediction using the random forest classifier 
in future work we hope to evaluate our results by computing the root mean squared error of the predicted enjoyment for unseen video games 
evaluation
the predicted enjoyment can be computed as some combination of the time spent playing a game the number of trophies accomplished per game and the rating given to a game depending on the consistency of our final dataset 
for our baseline we focused on using just word features of review descriptions to predict the rating whole number in the range of 1 to 5 
this is a basic metric for predicting a user s reception of the game and provides a foundation for the rest of the project as it utilizes much of the framework for more advanced machine learning algorithms and sets up our pipeline 
baseline summary
for our algorithm we preliminarily implemented a random forest classifier to train our baseline we created a training set by using numpy to vectorize username and descriptions of game reviews into counts by word and tf idf matrix respectively using those as features 
we then arbitrarily split this set into a train and test set and trained our random forest classifier using the training half to test our baseline we used the features of the test set to predict the review the user would have given the game 
we evaluated based on the accuracy of the classifier in predicting scores or the number of correct scores divided by the total number of reviews tested 
as previously mentioned our baseline task was to predict the rating of a game based on the review description 
we chose this since it required developing most of the infrastructure for predicting how much a user would enjoy a previously unseen game upon training a random forest classifier using scikit learn we achieved an accuracy of 0 48 in predicting the ratings of games 
preliminary results
compared to 0 2 the expected value of the accuracy from random guessing one of five ratings this score is significantly better indicating that our baseline is on the right track 
what is it about a person that makes you want to approach them 
with the rise of online dating websites for many the approach has changed from walking across a crowded bar to sliding one s finger across a cell phone screen to indicate one s interest in dating 
i introduction a why study online dating 
the initial question about how we make these split second judgments about the desirability of a partner has changed as well as our judgments are often solely based on photographs with minimal additional information 
while studies have shown that our judgments on paper from merely looking at the online dating profile of an individual more often than not fail to predict how much we like them in person
the choice to contact a partner in online dating involves two sets of factors those relating to the individual making the decisions or individual variations in preferences and those characterizing the target of the decision or target features 
one interest is in developing better algorithms for online dating websites to recommend potential matches to their users 
b goal predicting choice in online dating from target features and individual differences
if we understand how to accurately predict a given user s ideal partner choices we could help them find someone they like much faster 
in this study i aim to use both target features and individual differences to predict users choices to contact a given partner in online dating or not 
how does one measure a user s partner preferences 
one way is to simply ask a user what qualities they would want their ideal partner to have ideal partner preferences or to state how important different traits of a potential partner are for their decision i will refer to this as stated preferences 
c why is asking users what the are looking for not enough 
neuroscience may offer another individual differences measure namely how the brain responds to the photographs of different potential partners 
for many years scientists have been interested in neural processes that help us make decisions 
d neural activation as a measure of individual preferences
over the past few decades a large body of studies using methods such as functional magnetic resonance imaging fmri in humans as well as electrophysiological recording electrical and optogenetic stimulation and pharmacological inhibition in animal models has implicated the dopaminergic system of the brain as essential for estimating the expected value of a choice in diverse decision tasks 
specifically research suggests that ventral striatal activity and nucleus accumbens or nacc activity in particular correlates with the value of anticipated rewards see consistent with the notion that the dopaminergic system plays a critical role in initiating motivated behavior multiple studies have shown activity in dopamine projection areas such as the striatum and ventral tegmental area related to love
we recruited 36 caucasian 1 females between the ages of 21 and 25 via social media and mailing lists to submit each two identical photos of themselves selfies such that their face was fully visible 
on one of the photos they were instructed to smile and one the other one they were instructed to keep a neutral facial expression 
a stimuli
the 72 photos 2 per female for the 36 females were cropped to ensure that the size of the face size of the photo and the resolution of the photo were the same across photos 
the photos were cropped to a square so that only the face and part of the hair was in the photo 
we also ensured that the photos were comparable in brightness 
on the qualtrics survey site we recruited a panel of 50 caucasian males between the ages of 18 and 28 who were romantically interested in females and not in a committed relationship at the time 
participants were asked to rate how physically attractive and how happy each target appeared 
b target ratings
the ratings were on a 4 point scale to a very small extent to a small extent to a moderate extent to a large extent 
each participant only saw one photo per female therefore each photo was rated by 25 participants 
the order of the photos was randomized for each participant 
lower lip width distance between the top of the lower lip and the bottom of the lower lip in addition to these features i also manually scored each photo on three additional sets of features hair color hair texture and hairstyle 
the hair color feature has three variations blond brown and red 
each target received a 1 for the heir color she had in the photo and 0 for the other two colors 
similarly the hair texture feature had two variations straight and curly and the hairstyle feature had two features hair up in a ponytail or a bun or hair down 
the online dating task was designed to re create the experience of choosing to contact a partner on an online dating mobile application such as tinder while subjects underwent scanning 
during each trial of the task subjects first saw a photograph of a female target 2 s the cue period 
d task
subjects then indicated whether they wanted to contact that target or not 4 s see
we recruited participants for a study on stock markets that was run prior to the online dating task 
twenty three males romantically interested in females and between the ages of 21 and 31 average age 26 also agreed to participate in the online dating task 
e participants
magnetic resonance imaging mri data was acquired with a 3 0 t general electric mri scanner 
i acquired the standardized using z scores mr signal from 6 regions of interest nucleus accumbens which has been associated with predicting choice the prediction for each example was given by the equation where the x i s are given by the inputs of the training examples and the x was given by the input of the example we are attempting to predict 
f neuroimaging data and regions of interest
in the case of the neural model the input was the signal change from the beginning of the trial to 6 s after the onset of the trial 
in the case of the target model the input was the average attractiveness and happiness rating that the photo received from the raters group 
in the combined model the input was the combination of the inputs of the neural and the target models 
the term x i x 2 2 measures how similar the input of the example we are trying to predict to the i th training example 
the term was determined using stochastic gradient descent on the training examples 
the algorithm outputs 1 for trials when it predicts that the subject chose not to contact the female and 1 on trials when it predicts the subject chose to contact the target 
i ran the three svm algorithms neural target and combined with different bandwidth values 
b finding the optimal bandwidth
in order to see if there is a single region in the brain out of the 6 ones discussed above that provides especially important information for predicting choice i tested the neural model at bandwidth 12 while removing each of the regions of interest 
removing any one of the regions of interest did not seem to have a significant effect on the accuracy of the model see
c removing features from the neural model
in order to see whether adding more features to the model would increase the accuracy of prediction i sequentially added one feature at a time in two different orders 
the first order started with the target ratings followed by the facial features see section c in methods and finally adding the neural features 
d effect of number of features on test error
the second order started with the neural features was then followed by the target ratings and the facial features last 
see
while being able to predict a binary outcome with about 68 accuracy might seem far from perfect it is important to note that we are predicting human decisions about other humans 
quite a few studies of economic decision making where it is possible and in fact quite easy to come up with an estimation for the value of each choice report similar success rates 
e predicting choice conclusions
therefore for a social psychology experiment where the value of each choice is noisy and affected by a large set of unknown individual differences and external factors 68 accuracy is quite high and perhaps as far as we can go given the data set 
for the second part of my project i attempted to predict the signal change in response to seeing a given target photo using a locally weighted linear regression 
i defined my weights in two ways 
iv predicting neural signal in response to target
in the first model the weights were only based on the photo features average ratings on happiness and attractiveness as well as facial features whereas in the second model the weights were also based on the average pattern of neural response for the 6 regions of interest for the participant the training example came from 
to determine the similarity between the features i used the formula from the lecture notes where the weight of the i th training example was given by as in the case of svm the model consisting only of photo features did best under small bandwidth values and had similar performance for larger bandwidth values see
using neural data in predicting choice in online dating allowed for slightly more accurate predictions than merely using photo features as previous prediction algorithms had done 
by doing so i was able to account for participant s individual preferences 
in addition introducing similarity in the average pattern of neural response when predicting signal in the 6 regions of interest increased our accuracy of prediction slightly 
political scientists have done extensive research on the mindsets of democrats republicans and independents and have developed several theories as to why individuals identify as each 
for instance they have posited that pure independents are distinct from partisans and weak independents in their political attitudes in this project we focus on self identified independents who comprised 11 percent of americans in 2012 and have a diversity of attitudes 
our goal is to identify features that predict who is independent 
we are interested in these features in their own right in order to understand the mindset of independents as well as in order to build an effective model 
we use data from the american national election studies anes which surveyed americans political attitudes from 1948 to the present 
this survey is the gold standard in political science and contains 55 674 observations and 951 attributes 
we use only responses from 2012 in order to have comparable and consistent measures of each feature resulting in a training set with 3 365 people and 88 attributes 
1 please see our github repository for all of our code 
the goal of principal components analysis pca is to represent the data with fewer dimensions reducing noise in the data and capturing directions in which the data vary 
for instance one component identified might correspond to having a positive disposition or distrusting the government we first ran pca on the data with all 55 numeric features 
principal components analysis
the primary two components capture a considerable amount of variance related to political identity 
that is the first two components can be used to separate democrats from republicans but independents are similar to both democrats and republicans partisans in terms of these components 
please see the figures below which shows individuals distribution along the primary two components for the full data set left and for democrats and republicans alone right we next ran pca on subsets of the data including only features relating to individuals warmth to certain people and groups and towards certain issues respectively 
the first subset includes measures such as how warmly individuals feel towards black people and congress while the latter includes measures such as support for abortion legalization and welfare spending 
these components were even less effective at capturing variation in party identity as is to be expected 
in order to verify that warmth features and attitudes to political issues are two core types of features we ran factor analysis on a subset of the data with only these features 
we confirmed that warmth features load similarly to one another on the two factors estimated in relation to issues features 
factor analysis
the plot above right shows how warmth and issues features loaded onto the two factors with warmth features shown in blue and issues features shown in red 
warmth features are more positively related to factor 2 than are issues features indicating that they are a different type of feature and that analyzing these feature sets separately is appropriate 
we theorized that it might be possible to split political independents into a number of different groups based on their political ideologies 
the anes dataset has two types of political ideology features issues and political thermometers 
k means
issues features involve questions such as when should abortion be allowed always sometimes or never 
political thermometers ask respondents to rate their warmth towards groups such as blacks big business and labor unions on a 0 97 point scale 
we ran k means for each feature group trying cluster sizes between 1 and 20 and plotting their inertia 
we created 5 clusters for each features set resulting in clusters displayed below using t sne 
we next identified the clusters with the greatest number of independents 
for the political thermometer k means the cluster with the largest percentage of independents was a a cluster of people who had negative opinions of every single group asked about 23 percent independent and the second largest was a cluster of people who mostly felt neutral towards all groups asked about 15 percent independent 
for the issues k means the clusters with the largest percentages of independents were social conservatives fiscal liberals left christians and social liberals fiscal conservatives libertarians 
note that though the aforementioned clusters contained the most independents they did not contain overwhelming majorities 
based on these results we theorized that independents would be relatively more likely to fall into one of four groups pessimists moderates left christians and libertarians 
we conducted a t test for each of these criteria and compared the percentage of independents and partisans in each category 
hypotheses
we found that independents were more likely to be moderates left christians and libertarians but not left christians 
2 in fact we found that about 47 of independents fit in to at least one of the first three groups compared to 26 of democrats and 35 of republicans 
this difference is statistically significant result but does not enable high confidence classification of individuals party identity 
principal components analysis and k means clustering suggest that individuals political attitudes and warmth towards particular groups only partially explain their political identities 
for this reason we use boosting with decision stumps to sift through our data and determine which features best predict independence 
boosting
we ran 2 000 iterations of boosting with a logistic loss function on a training set with 2 524 observations and 88 attributes 
we specified an interaction depth of 3 meaning that two and three way interactions are included 
we find that error on the validation set is smallest at 332 iterations after which the model begins to overfit 
we next ran boosting with an exponential loss function which had the lowest validation error at 356 iterations boosting with logistic and exponential loss functions resulted in very similar rankings of features 
both identified state as the feature with the most relative influence whose inclusion in the model most reduced the loss across all iterations 
other influential features are how interested the individual is in the presidential election whether he or she voted and how warmly he or she feels towards congress because state seems to be highly related to political identity we mapped states percentage of independents in 2012 
please see our interactive map online in which you can choose which year and political party you would like to view 
overall it seems that a few states have particularly high proportions of independents including colorado and new hampshire 
however please keep in mind that our sample size is not very large 3 so this relationship may be largely due to noisy data 
having identified key features that are most useful for predicting whether an individual identifies as an independent we build a logistic regression classifier using several feature sets 
first we estimate models with the principal components we found with the factors we found and with only warmth or issues features 
logistic regression
next we estimate a model with all features as well as models with only the ten most influential features identified by our boosting models 
coefficients from the logistic regression model including the top ten features from boosting with a logistic loss function are shown below 
it seems that people who are less interested in national elections are less likely to be independent as are people who voted in the past national election 
we compared the error rates for logistic regression models with each feature set as shown below 
all models performed similarly poorly severely underestimated the percentage of independents when evaluated on the training set 
this is likely because our features only explain a very small portion of the variance in political identity and incorrectly estimating that someone is independent is more expensive than estimating that they are partisan given how rarely individuals are independent 
that is the safe bet is to classify every individual as independent and only predict independence given an overwhelming amount of evidence 
however our models estimated approximately the correct proportion of independents on the test set as random noise in the new data caused our model to estimate false positives just as often as false negatives 
this is to be expected and is reassuring in a sense as our models will not drastically underestimate the proportion of independents in unseen data 
we find that classifying independents based on their political philosophies is very difficult and prone to high error rates 
nonetheless we find that independents are more likely to be moderate pessimistic and or libertarian than partisans and are more likely to be from certain states although we feel that this analysis is somewhat informative we also feel that we must be very cautious when interpreting our results 
conclusions and limitations
in addition we feel it is important to keep in mind that analyses involving 88 attributes include only individuals who answered 88 questions completely 
in our case that means including only 3 365 out of 4 748 survey respondents and hoping that these 3 365 are representative of the entire sample and ideally the entire united states population 
also we only looked at a single year in our study 
according to professor morris fiorina independents often change whether they classify themselves as leaners or true independents between election years 
our analysis of independents might be tied to the specific circumstances of 2012 
improving the efficiency of optical device is important in a variety of applications 
however this process is typically done empirically 
in this project we formulate a systematically way to improve the efficiency of optical device design with machine learning the optical device we focus on is a silicon si nano structure 
this device is constructed by a series of si nano cells which is represented as the 0 1 binary series in to address the non linear relation between the si structure and the output electrical field we use deep learning as our training algorithm 
in the meanwhile this approach also allows us to reduce the dimensionality of the underlying physics 
one the other hand although the si structure that is represented by an r 1 n vector can take 2 n combinations a functional optical device almost always follows specific patterns and the number of output labels in the neural networks is confined 
therefore by limiting our scope to si structures with physical functionalities we can get a converged solution using finite number of data sets 
the structure of this report is given as follows 
a detailed interpretation of the neural networks we use in this project is provided in section 2 
section 3 describes the procedure of the simulation and the preprocessing of the training data 
the training results are discussed in section 4 
in deep learning the input data is processed through a neural networks that is constructed by a multiple layers of computational units 
an example of a four layer neural networks is shown in the propagation of data in the neural networks is characterized by a forward feeding process 
neural networks
specifically each layer takes a linear transformation of the data from previous layer and process it using an activation function 
at layer l we can express the forwardfeeding process as where z l is the transformed data w l and b l are the weights and bias respectively and f l is the activation function at layer l in the first hidden layer the input data z 0 x which is the input to the neural networks 
for layers with size of h l we have w l r h l h l 1 and b l r 1 h l 
in addition in this project we use a rectified linear unit relu function as the activation function at all layers which is defined as where p is the total number of layers in the neural networks 
the network between the last hidden layer and the output layer gives the prediction to the output data 
because the output data is a vector of size n we represent the prediction function by a linear transformation which is because the output vector in this project is binary we obtain our predictions as the cost function of this neural networks is defined by a mean square error mse function which is because j w b x y is convex the parameters can be trained by solving the following problem 6 minwe solve this problem using stochastic gradient descent in a set of back propagation steps the training and testing errors are defined by the following equation this error definition is used to characterize the convergence of the learning process at each iteration 
in addition to quantify the accuracy in si structure prediction we define the prediction error as we use this error definition as a metric to the performance of the learning algorithm 
we obtain the data for learning from simulations of the optic electric interaction using the rigorous coupled wave analysis rcwa software 
in the simulations the refraction index of si is n si 3 5 and the refraction index of vacuum is n vacuum 1 
problem setup and data acquisition
light with wavelength 1 m is illuminated perpendicular to the si structure from the top 
for each si structure randomly generated by the rcwa software the corresponding electric field is recorded 
for the learning purpose we collected 250000 data sets from the simulations 
we use 80 of the data sets as the training set and the rest of the 20 as the test set 
the inputs of learning are the electric fields retrieved at two specific vertical locations which are represented by x i c 1 51 
in the first case the inputs are taken at the near field that is 0 1 below the si structure in the second case the inputs are taken at the far field that is 10 below the si struture 
note that x is complex because the electric field signal contains both magnitude and phase information 
to avoid the computations with complex numbers we treat the magnitude and phase of the inputs separately 
therefore the inputs used in the learning process are represented by x i r 1 102 
the outputs of learning are the binary vector of si structure which is y i 1 1 n where n 20 is used 
the training curve for the near field and far field learning are shown in figs 
4 a and 4 b 
training outcome and discussion
in both cases the results converges in about 100 iterations 
the test error is slightly greater than the training error 
however the errors for the near field learning which are 34 9 for training and 35 4 for testing are more than two folds lower than the errors for the far field training which are 86 8 for training and 86 9 for testing 
the corresponding prediction error for the near field learning are 3 5 for training and 3 6 for testing and the prediction error for far field learning are 29 5 for training and 29 6 for testing 
the degradation in performance as the data collection location moves away from the si structure is a consequence of the decay in optical intensity of the transmitted light 
information carried by the transmitted light dissipates as the light travels 
this indicates that the electric field in the near field provides a better metric than in the far field to characterize the si structure 
to quantify the far field efficiency of the prediction we pre define an output electric field and apply the parameters obtained from the far field learning to predict its corresponding si structure 
the comparison of the far field efficiency of the 45 transmitted light between the predicted si structure and the optimal structure are shown in
in this project we implemented a two layer neural networks algorithm to study the relation between optical si structures and the output electric fields 
we trained two neural networks using the near field and far field electric field data obtained from the rcwa software 
the size of the data sets used for learning is 250000 
in both setups the results converges in 100 iterations 
the accuracy of prediction using the near field data reached 96 5 and the accuracy using far field data is 70 4 
with local found method an efficiency of 51 2 is achieved by the predicted si structure against the 69 0 maximum possible efficiency for the 45 transmitted light 
compared to the traditional opticaldevice design process the deep learning approach significantly improves the effectiveness of this process without considering the convoluted underlying physics to further improve the method in our future research we will test different cost functions for training such that the binary nature of the si structure is better represented 
the depth of the neural networks and the dimension of parameters will also be adjusted to optimize the results 
we based our project off the hearthstone simulator hearthbreaker an open source platform developed by danielyule and other contributors 3 
the well functioning python 3 game engine is feature complete up to last year s hearthstone rule set 
methodology 2 1 platform
it also comes with an agent framework that is easily extensible to implement our ai 
hearthbreaker also comes with several baseline ais of its own 
there is a random agent which performs a random valid action at every step and a trade agent which looks for the move that trades best with a comprehensive set of hand coded heuristics 
we use the trade agent as the heuristic agent against which we evaluate our ai 
finding an effective representation of the huge game state space is critical to formulating a tractable learning problem 
we make several observations specific to hearthstone based on prior knowledge observation 1 
the current game state provides enough information to determine how well both players are doing this means that the game is well parametrized by the current game state the game history yields little benefit save for allowing card counting techniques that keep track of the set of possible remaining cards in the deck 
hearthstone is a online trading card game released in 2014 that is gaining in popularity 
the basic driving principles of hearthstone are relatively easy to describe and understand 
hearthstone
it is a turn based one versus one game 
both players start by choosing a hero and a deck to draw from 
in the beginning of each turn a player gains certain amounts of resources including mana and cards 
in his turn a player can make as many moves as desired as long as there are enough resources or the rules otherwise allow it 
valid moves include playing cards and using the player s hero s special abilities 
the ultimate goal is to win the game by removing all hit points of an opponent it is natural to model this game as a markov decision process with rewards only at terminal states 
every game situation is a state and the move that a player takes is an action 
like any other turn based trading game it has a sprawling collection of mechanics that apply only in certain circumstances 
thus writing a good ai for hearthstone remains a difficult and interesting problem because of two main challenges 1 
with an mdp the biggest impediment to a learning is the size of the state space 
for hearthstone because there are various special effects and a lot of game mechanics the number of possible game states is huge 
a naive solution based on traversing the mdp would fail miserably because the chance of seeing the same state twice is essentially zero 
the ai has to be able to evaluate previously unseen states in a reasonable fashion 2 
the other limitation is that actions in hearthstone are highly coupled with states 
the rules only allow certain actions to be applied depending on the game state 
this means that a naive approach that enumerates the fixed set of all possible unique actions in the mdp is intractable the set of all possible actions is far greater than the number of actions applicable to a particular state 
even so there are approximately 20 unique actions available at each game state 
the combinations of them result in a big action space that is hard to enumerate these challenges mean that a good ai has to be tailored strategically to the specific restrictions and characteristics of hearthstone a custom built ai is needed to play this game with any measure of success 
we aim to use learning techniques to match or outperform current ais for hearthstone most of which are manually coded heuristic agents 
we based our project off the hearthstone simulator hearthbreaker an open source platform developed by danielyule and other contributors
platform
armed with a game state representation we are now faced with the problem of evaluating the quality of an action 
unlike in deterministic games we cannot simply enumerate the states that result from taking an action because actions can have non deterministic effects 
monte carlo methods for action evaluation
to further complicate matters each player is allowed to perform any number of legal actions in any order within a turn observation 3 
the set of actions that have non deterministic effects is small 
the majority of the non determinism results from drawing cards and from the lack of knowledge of the opponent s deck and hand observation 4 
the number of actions within a turn is always finite there are no cycles in the action graph 
4 implies that the action graph is a directed acyclic graph dag with complete turns mapping to paths in the action dag 
3 suggests that monte carlo techniques will be effective 
hence we arrive at an algorithm for evaluating the quality of an action 
first simulate taking the action on a copy of the current game state 
next perform a monte carlo traversal of the action dag simulating each action from each game state and return the maximum value of the visited states 
this yields a monte carlo estimate of the maximum value at the end of a complete turn after taking the action which we interpret as the value of the action 
in practice the action dag can be too large to be efficiently traversed completely because in the late game each player has more resources and thus more actions to take 
with the large branching factor this results in an exponentially growing number of states to visit 
action dag pruning
we took several measures to prune our traversal and optimize performance 1 
some actions are order independent so there might be multiple paths to the same state 
since we evaluate the quality of an action by the value of the final state it is not necessary to visit each state more than once 
hence we hash visited game states and avoid them in our traversal produces a spanning tree of the visited states 2 
we perform a depth limited search of the dag 
at depths exceeding a certain maximum depth we cease to explore every action available at the node and instead switch to a greedy traversal strategy continuing only into best possible action 3 
we use an a search strategy traversing active game states in an order that prioritizes shallower nodes and nodes with better value 
this ordered traversal enables us to set a hard budget on the maximum number of states to be traversed this gives us a performance tunable search strategy not unlike general mcts
assuming that we have a model that evaluates the value of a state given the feature vector representation combining these components produces a complete ai agent 
at each state we enumerate all legal actions and select the one with the best value according to the evaluation method above 
ai
note that since actions have non deterministic results we perform a fresh monte carlo traversal at each step until the best action is to end the turn 
we now proceed to the most important part of a good ai the way it evaluates the current game state 
we developed two approaches to this task supervised learning and reinforcement learning 
models
both models share the same 60 dimensional feature extractor 
we pit the existing heuristic ai trade agent against itself recording the game state at the end of each turn 
each game history is then processed and a score assigned to each game state 
dataset preparation
the base score f is f win 10 if the player won f lose 8 if the lost and f tie 3 for a draw when both players perish simultaneously 
the score for a game state d turns away from the end of the game and whose current player s base score is f is assigned asthis mimics the q learning discount scheme with a discount of 0 8 
feature vectors are then extracted from each game states and the resulting x y pairs recorded into the dataset 
we used two models for supervised regression a linear regression model and a deep neural network model implemented within the scikit learn
regression model
we used the q learning algorithm to learn the best linear approximator to the q function making several non standard modelling choices along the way 
it is difficult to run q learning directly on the hearthstone mdp as is since each player takes a variable number of actions in each turn 
hence the game progression alternates between players irregularly slowing convergence because the discounted future reward would vary depending on not just the number of turns but the number of actions taken by each player hence we reformulated the mdp to interpret an entire path in the action dag corresponding to a sequence of legal actions as an action with the restriction that the last action must be to end the current turn 
action path mdp
note that unlike with the action evaluation scheme there is no issue with non determinism here we can simply continue the game from the last game state on the action path once we have selected an action path 
we explored two approaches to approximating the q function with a linear function 
given the current state s and the action sequence a which we also treat as a state the resulting game state according to our simulation we can choose to either predict the q value from only a or from both s and a 
function approximation
more specifically the two function approximators tested were the first model final state only corresponds to the assertion of observation 1 
the second model state pair is a generalization of the first but might be harder to train due to the increased number of weights 
with these two function approximators we can simulate games using the game engine by running q learning with an greedy 2 strategy for both players using the same model 
to efficiently select random action sequences we used random walks on the action dag 
q learning
the best action sequence and its value were found using the same search routines used for action evaluation 
the q learning update is as follows having observed that taking action a from state s leads to state s with reward r s setwhere the discount factor 0 8 and the rewards r s are the same as in the supervised learning model f win f lose f tie for winning losing and tied states respectively 
observation 5 
the only non zero rewards occur at the last turn of the mdp unfortunately when using plain q learning this means that no information is learned from all the non terminal moves in the first game since the rewards are all zero 
experience replay
this causes convergence to be haphazard as earlier games have an outsize influence on the greedy exploration strategy to speed convergence and reduce variance with respect to the observed state transitions we implemented an experience replay training scheme inspired by mnih et 
al 
s paper
for the supervised models we simulated 2000 games generating a dataset with about 40000 data points each corresponding to an intermediate game state each game takes about 10 turns per player and thus gives around 20 game states 
we then randomly sampled 10 20 50 200 500 1000 2000 4000 12000 20000 and 40000 data points independently from the dataset into sub datasets and trained both supervised models independently on each sub dataset the reinforcement learning models were both trained with experience replay q learning for 1 5 10 20 and 50 epochs with each training run performed independently all four models each with various training set sizes are tested against the existing heuristic ai using the monte carlo action evaluation strategy with a maximum depth of 2 and a traversal budget of 25 
model training and evaluation
we record our ai s winning rate after simulating 100 games against the benchmark heuristic ai 
currently our ai agents readily beat the existing baseline heuristic agent provided in the engine 
we report the learning curve for all four models with increasing training set size for supervised learners figure 1 and training epochs for reinforcement learners figure 2 
note that the training times are measured in different units for our supervised and reinforcement models 
the supervised learning models perform well against the heuristic agent both achieving a maximum win rate of 81 
the deep neural net appeared to converge significantly faster with both models tapering out at approximately 75 the reinforcement learning models still matched the heuristic agent but with a less impressive margin 
the state pair model hovered at around a 50 win rate and the final state model initially outperformed it to achieve a 65 win rate 
however both models deteriorated after too many training epochs 
it is somewhat surprising that our supervised models perform so well against the heuristic agent especially given that they are trained with data extracted from its behavior 
examining the learned weights for the best linear model reveals reasonable values intuitively bad things such as the opponent s hero health and total attack damage of opponent minions have negative weights and good things such as our player s hero health and healing multiplier have positive weights 
supervised model
thus our ai agent appears to have learned a general strategy for playing hearthstone 
this hints at a possible explanation for why it fails to exceed a 90 winning rate apart from the inherent randomness in the hearthstone game it is also not taking advantage of compounding effects from multiple spells minions and special hero powers 
since the deep neural model appearing to be very adept at learning to make use of the information we provide this suggests that it might be possible to extend our game state representation with more specific identifying information about the individual minion and hero types 
though the reinforcement learning models do not perform as well as the supervised models we must note that they received less training 50 training epochs is approximately equivalent to a training set size of 1000 
in addition the lack of supervision in q learning means that the models were responsible for their own exploration of the strategy space and were not able to leverage the knowledge of the heuristic agent when training there was significant instability in the model weights learned especially during the initial few games 
this is problematic because the q learning strategy is dependent on these initial weights so any initial biases might result in a feedback effect 
bad initial experiences lead to worse exploration strategies and poor learning 
the experience replay mechanism also has the unintended effect of giving outlier games more influence on the model 
further analysis is required to determine if this was the cause of the fall in the winning rate at 50 training epochs 
nonetheless a possible remedy is to implement bootstrapping training the model on a small number of games between the heuristic agent and itself before switching back to the q learning training procedure 
the ability of our machine learning ai agent to outperform the hand coded heuristic agent in hearthbreaker is certainly encouraging 
its success proves that it was able to learn the essence of the game and excel at it even without leveraging specific interactions between game components 
the challenging next step will be to improve it to the extent that it is able strategize with respect to all the special interactions between game components and match or surpass expert human opponents 
general game playing is a relatively new field in which people try to make game players capable of playing a game well without knowing the rules until runtime 
with game specific players such as alphago and other ai s it s pretty easy to see the benefit machine learning can have these programs can have data from millions of matches of their specific game if not more and be able to choose optimally based on this and heuristics specific to their specific game 
with general game playing things are not that simple 
because you do not know the rules of the game until runtime you do not have a sufficiently large dataset for some of the more advanced machine learning techniques 
in addition you lack the insight on the game that you would have if you were playing a specific game and thus also lack the specific heuristics and specific features to learn 
but that s not to say that it s impossible just not as clear cut as it is in other situations 
the goal of this project is to explore different methods of machine learning in general game playing the best ways to implement them and how effective they are 
the best method for general game playing is by exploring the tree using monte carlo tree search a method in which you send out probes to find terminal states of the game and decide which move is best based on the average score returned to you by these probes 
even in light of our research this is still the best method 
choosing a move a heuristic approach
this method is so good because it generates its own heuristic based on terminal states which are specific to each game rather than the general heuristics used in fixed depth searches 
but there could be cases in which even mcts is not fast enough to explore the tree for long enough that it is a reliable way in choosing a good move 
or there could be cases in which you do not know what moves will be available in the next state or 10 or 20 states down the line and that you cannot just play a game and must rely on a heuristic 
typical heuristics include maximizing or minimizing your mobility maximizing or minimizing opponent s mobility and simply using the goal value of the state as a heuristic 
all of these have their flaws 
if making the right decision early leads you on a path where the only choices possible terminate in a winning state the maximizing your mobility will miss it every time 
you can think of similar scenarios for the other states 
the benefit machine learning will bring is that it will come up with a useful heuristic specific to its game 
unlike goal proximity which could return a score of zero even if it is one proposition away from winning our machine learning heuristic will be able to properly evaluate how close a state is winning based on the propositions present and the data it s learned 
in order to perform any machine learning we first needed to come up with meaningful features and a good way to represent them 
our first thought for features was to have a vector the size of all of the base propositions that made up the game and to have the values of the indices of all of the propositions that were true when the game terminated to be a one if they were true and a zero or negative one otherwise 
input features
classifying each vector proved to be slightly less straightforward than we thought 
while in a two player game it s pretty easy to determine whether or not you won 
if your score was higher or equal to the other player then that counts as a win 
this philosophy can be expanded to three or more players as well 
but in a single player game it s not always true when you ve won 
an easy but not necessarily complete fix is to say that you ve won if you ve scored the maximum number of points possible 
but this does not account for a game like hunter where you are awarded points for capturing pieces but can never capture all of the pieces and thus can never achieve the maximum number of points 
so we decided to settle on counting a win in single player as having 70 or more of the maximum number of points 
this means that it is possible for the player to trend towards maybe getting the second most amount of points possible but we thought this was better than the alternative as there is never anyway to know if the maximum amount of points possible is the same as the maximum amount of points allowed 
to obtain our data we run a bunch of simulated games during the start clock 
in each terminal state we check to see which propositions are true and match up the propositions to the correct index by using a sorted list of all of the propositions present for the game 
implementation
if a proposition is true we set the value at that index to 1 
if it is false we set it to zero we then have a separate array of classification for each data point for computing a heuristic for a given state using naive bayes we found all of the propositions that would be true if we chose a certain move 
we then used the naive bayes formula to find the probability that each move was a winning state and returned that as our heuristic for svm most of the work was done in the start clock where we kernelized the data and ran the training algorithm using gaussian kernels and the same techniques used in problem set 2 
when it came to choose a move we ran the propositions that would result from a move through our svm 
whereas normally it doesn t matter how positive or negative the output of the svm is here we treated more positive values and less negative values in dire situations as better moves 
the first algorithm we implemented was naive bayes 
with this method no extra work needs to be done in the start clock which is a huge plus 
naive bayes
however the more data you have the longer it takes you to process it 
the same is true for propositions 
games with a low number of propositions like buttons and lights can typically be run very fast causing the dataset to be rather large but still kind of quick to go through because there are so few propositions 
but there is not always an inverse relationship between the number of propositions and number of games we are able to simulate 
the game rainbow for example will end in the same number of moves as buttons and lights but has around 20x more propositions in games like this it is super easy for the player to timeout on the start clock typically given to games of this size 
this is the biggest flaw with naive bayes the more data you get the harder it is to process in a general game playing setting 
svm s had the opposite problem of naive bayes 
svm took almost no time in the play clock which is probably the more important clock when it comes to general game playing 
support vector machines
still without knowing anything about the game including the start clock it is hard to know when to stop collecting data and start kernelizing it 
given more time you could probably work out some sort of algorithm using the number of propositions that typically worked but you also have to deal with the other processes going on during the start clock such as building the state machine which is essential in getting all of the propositions so the problem cannot be solved with threading 
in order to truly compare the two we chose two single player games hunter and hamilton that we thought best tested the strengths of our players 
these games contained manageable numbers of propositions and could be simulated enough time to get some good data 
in depth comparison
they also had scores that were functions of how well you did rather than the 100 or 0 scoring present in a lot of the single player puzzle games hunter is a single player game played on a 5x3 board 
the game begins with a single knight in the upper left corner of the board and pawns on all other squares 
on each step the knight moves like a knight in chess capturing pawns on any squares to which it moves 
the goal of the game is to capture as many pawns as possible in 14 moves 
note that it is impossible to capture all pawns in 14 moves and thus it is impossible to reach 100 points hamilton is a single player game played on a graph with two interconnected nodes 
the player starts out on one of the twenty nodes and on each step moves to an adjacent node 
the objective of the game is to visit as many nodes as possible in twenty steps 
the reward at the end of the game is an increasing function of the number of nodes visited for each trial we capped the number of matches it could simulate in the metagame phase 
we then averaged all 10 trials to get that player s score for that number of data points 
it is important to note that the start clocks and play clocks were consistent across all players but were guaranteed to be sufficient for the players to perform at their best ability i e 
no start clock timeout for svm no play clock timeout for nb 
hamilton both games show similar trends 
naive bayes is pretty good right out of the bat showing no improvement in hunter and minimal improvement in hamilton 
hunter
svm starts off slow but ramps up to surpass nb 
with svm there is a pretty direct correlation between number of games and how well it does so it has a lot more potential than nb assuming the start clock is sufficient 
the main problem we ran into as well as what causes the obvious oddity on the graphs is that java would run out of memory when trying to kernelize the larger data sets 
an unfortunate flaw but a fatal one 
overall i think that given certain settings our machine learning algorithms can be successful 
however at some point it stops feeling like general game playing and more of trying to fit a round peg into a small hole 
the results produced were good but a lot of adjustments were made in order to preserve the spirit of testing over the spirit of ggp 
for example we adjusted the start and play clocks but kept them consistent so that we could get some interesting data out of it without our players just timing out 
interestingly enough procuring the data took almost no time it was the act of kernelizing and performing naive bayes which took the most time 
given the typical clocks of ggp 20 15 we were using 120 45 the machine learning parts seem unfeasible but more optimized code could make it more feasible especially in competitions built to accommodate machine learning algorithms 
but right now i think machine learning is best saved for specific problems and games however there is an upside of this that i think could be really helpful 
our algorithms still proved to be useful on unknown games 
given a new game and sufficient time our players especially an svm uninhibited by java could prove to be quite fearsome 
the makers of ggp are thinking about having a special tournament where you get a 10 minute start clock and then play a bunch of that game in a row and i think machine learning would thrive in this environment there can still be massive improvements made 
in our proposal we discussed two suggestions provided by michael genesereth and bertrand decoster 
prof genesereth suggested that instead of strictly using propositions we might try some of the heuristics we looked at earlier in class mobility opponent mobility etc 
and use machine learning to come up with the best weights for each heuristic 
we then also talked and considered implementing a boosting algorithm using these heuristics as weak learners although we need to flesh out more details here 
bertrand suggested using combinations of propositions as features so that instead of just having singular propositions we can logically combine and negate propositions to see if these complex features are more useful than our simple ones 
unfortunately coming up with a way to get meaningful data took way longer than any of us had expected as we had to try out a variety of combinations of games start clocks and play clocks and single games could take upwards of 10 minutes to play out 
but i think implementing their suggestions while probably not making the players any faster would make them better at accurately playing unknown games given a sufficient amount of games 
lending club is the world s largest peer topeer marketplace connecting borrowers and investors 
they claim to transform the banking system by operating at a lower cost than a traditional bank and thereby making credit more affordable and investing more rewarding 
over the last 8 years the number of loans in the marketplace has increased exponentially yet little is known about the algorithms that determine if a loan is approved and if it is the interest rate a loan is offered at 
in this paper we attempt to demystify the inner workings of this marketplace by applying machine learning techniques to lending club s publicly available dataset using a basket of supervised learning techniques we find that we can build highly accurate models with an f measure of up to 98 that predict if an application will be approved 
we also find that if a loan is approved we can determine the interest rate at which the loan will be offered at 
we provide an analysis of the performance of different machine learning models applied to our dataset with the models generated we discover that lending club has gradually relaxed its application loan approval criteria 
we hypothesize that this was due to the company preparing for its initial public offering which eventually happened in 2014 
in addition we find that certain features such as if the loan is a credit card refinancing loan are constantly predictive of whether a loan is approved or denied 
using this newly discovered insight we suggest some ways to game lending club s system to increase an applicant s chances of approval finally using effective clustering and visualization techniques we uncover and exhibit structure in this rich dataset which can be exploited to artificially generate more examples specially for the years which only a limited number of training examples are available 
lending club as an online banking platform is becoming increasingly popular ever since it started in 2007 
by applying machine learning techniques we intend to investigate following questions using supervised methods can one predict if a loan application would be approved given that an application is approved can we correctly predict the offered interest rate has the standard of lending club approvals changed over the years of 2007 2015 especially after their initial public offering can we extract a trend of how the significance of various features has changed over the years 
can this information be used to game their online system to increase applications chance of approval can we find some structure among this rich dataset which can be used to generate artificial data for our models especially for the earlier years of lending club 
the dataset available at lending club website is a comprehensive dataset of all applications for peer topeer loans on the lending club platform between 2007 and 2015 
the data files are csv files which are split by whether the loan is approved or denied 
data pre processing
the following is a plot of the lending club application statistics each year note that the number of training examples grows exponentially over the years as lending club has expanded rapidly 
the amount of loan applications grew from 5 000 in 2007 to over 3 million in 2015 denied applications contain far fewer features than approved applications 
for the approval classification problem we maximize the available data by combining the features available in both the approved and denied applications 
for the interest rate regression problem we do not have to analyze the denied applications and hence we can use all the features available in the approved applications 
hence we decide to separate pre processing for classification and regression 
to determine if the criteria for approval has changed over the years we first split up the datasets according to the year the loan was issued 
we use r and python to pre process the data 
classification task loan approval
all pre processing scripts used can be found on our github repository before we start our analysis we extract the common subset of features from the approved and denied files and combine the two datasets together for each year we also notice that in the approved dataset there are only 14 unique values for the purpose of loan column while in some years of the denied dataset there are over 10 000 unique values 
however we observe that the top 100 unique values for each year in the denied dataset represents over 99 of that year s denied loan applications with the 2007 data as an exception 
therefore in hope of cleaning the data for each year we create a function that maps the top 100 unique values into the 14 unique values in the approved dataset 
we delete the last 1 of denied loan applications 
using our general intuition we carefully select 22 out of 111 features from the approved data where about half of the features are empty 
loan amount interest rate and loan quality are among some of the selected features 
regression task interest rate
we continue processing the data as highlighted in the previous sub section for each year we notice that many of the features such as income have a right skew 
therefore we log normalize the data with mean 0 and variance 1 to ensure our algorithm treats each feature equally 
we run linear regression on this data 
in order to find the model that works best we apply several machine learning algorithms onto our dataset and compare their performances 
for each year of data we independently run support vector machines
classification task
after successfully training a classifier that can predict if a loan will be approved next step is to predict the interest rate for the approved loans 
for accomplishing this task we apply regression techniques after normalizing data 
regression task
in order to measure the accuracy of our model we decide to measure performance by using root mean squared rms error because we want to heavily penalize the model for incorrectly predicting the interest rate 
hoping to find latent structure within the data we use unsupervised techniques to cluster the data 
to investigate this we remove the state and purpose of loan features and cluster the normalized data using k means
clustering task
for each year of our data we calculate the f measure associated with the dataset using the confusion matrices 
we use this as our primary measure of performance for each of the models because we notice that evaluating model performance based on classification error is potentially misleading 
classification results
as only a small fraction of the applications 5 15 are successful in any year classifiers such as ada boost for the 2009 data can get a low classification error just by classifying every loan as denied 
therefore we use the f measure which is a combination of precision and recall to evaluate our models performance 
as we run the models on datasets with more examples we expect the accuracy to trend upwards 
we believe the models accuracy for 2007 in
we see convincing evidence that lending club has gradually relaxed its loan approval standards 
a possible explanation for a relaxation over the years is that the management team wanted to generate greater revenue growth and higher profits to prepare for an eventual initial public offering which happened in 2014 
has loan quality changed 
since a relaxation would result in the approval of more loans and since lending club charges a percentage fee on every loan that is funded on its platform increasing the maximum loan amount and dti would result in higher profits and higher valuation of the company 
we notice that there are certain features that are constantly predictive of whether a loan is approved or denied 
in we notice that educational loans are likely to be denied whilst credit card consolidation loans are likely to be approved 
this can be explained through economic intuition 
education loans are likely to be denied because seekers of these loans students are unlikely to have a stable source of income and hence are likely to have a higher chance of defaulting 
on the other hand credit card refinancing applications are likely to be approved because people who want to refinance credit card debt must already have a credit card which itself requires a stringent credit approval process renewable energy loans tell a particularly interesting story 
in 2008 and 2009 renewable energy loans were highly predictive of loan approval 
however this predictiveness disappeared soon after and by 2013 renewable energy loans were actually predictive of loan denial 
an explanation for this effect is that in 2008 the energy improvement and extension act was passed which provided tax credits to renewable energy initiatives 
therefore the borrowers had in effect higher disposable income and hence a higher probability of paying back the loan compared to before 
by 2014 many of these tax credits had been phased out and therefore lending club has reversed their algorithm to account for this change an immediate takeaway from this analysis is that applicants should state that the purpose of the loan is for credit card consolidation to maximize their chances of approval 
we use regression to predict the interest rate for approved applications 
after running linear regression we find that the rms errors are very low less than 0 003 in 2015 because loan grade is included as a feature 
regression results
as loan grade is determined by an algorithm within lending club we decide to not use it and implement different data processing techniques on the remaining data we perform pca and run regression on the transformed data 
we hope that by reducing the number of dimensions we would be able to counter noise present in the data and account for less data especially for the earlier years and consequently decrease the generalization error of our interest rate predictions 
to determine the number of principle components to include we run pca on the normalized data for each year 
the results are shown in figure 9 
we notice that there is a noticeable kink in the data after 3 principal components which indicates that most of the variance is captured by the first three eigenvectors 
hence we decide to use k 3 for pca 
analyzing the results we note the following insights loan grade feature is a near perfect predictor of the interest rate this is not surprising as lending club states that they determine the interest rate based on the loan grade calculated for that loan 
we notice that we generally do not have perfect predictions on interest rates for different loan grade 
this is likely because loan rates for different loan grade will change over time but the low rms error shows that interest rates for a specified loan grade do not vary drastically over a year increased complexity of lending club s system over the years even though there are more data samples each passing year our rms error has steadily increased 
in 2007 our simple linear regression algorithm did a good job of predicting the interest rate of a loan with a rms error of 0 02 
using the same type of model our 2015 rms error is over 0 03 
it also seems that their model now uses more features some of which may not publicly available low dimensionality of the approved data taking the first 3 of over 30 principal components generates decent predictions 
we realize a rms error that is about 25 greater than that of taking all principal components 
this shows most of the important determinants of interest rate can be represented by the first three principal components 
while trying to build models to predict the interest rate and if a loan will be approved we notice some structure in the data and hypothesize that we may be able to find clusters that are highly indicative of interesting trends 
we decide to apply techniques from our unsupervised toolbox to find structures but quickly discover that there isn t any intuitive way of visualizing the results of our experiments 
after doing some research we find t sne as one of the ways to visualize our results 
we discover the following interesting trends as seen in
we would like to thank dr duchi and all 229 tas for guiding us and consistently assisting us throughout the project 
for our project we classified hand movements based on surface electromyography emg signals from upper limb amputees 
approximately 38 to 50 percent of patients with upper limb amputations discontinue use of their prosthetic because the cost of carrying it outweighs its limited usage
the data set was provided by
a total of 18 features were examined in our project 
these features are popular in modern research for emg pattern recognition where n is the total number of samples and x i is the sample at index i 
b features
the moments are then scaled and normalized by a factor of to make them more robust to noisewhere a value of 0 1 was used 
the six features f 1 f 2 
each feature describes a particular aspect of the emg signal 
f 6 are extracted as follows f 4 is a measure of sparseness and f 5 is a measure of the ratio of the zero crossings to the number of peaks as given in thus we have defined all of our features taken into consideration for each channel 
for example waveform length gives a measure of the emg signal s complexity and both wilson amplitude and log detector give a metric for the level of muscle contraction observed 
mean absolute value mav the number of times the sign of the slope changes with a threshold zero crossing zc the number of times the signal crosses a threshold 0 05 frequency domain featurestime domain power spectrum descriptorsx i i th sample of the emg signal n length of the emg signal f j frequency at the j th sample of the spectrum p j power spectrum at f j a j amplitude spectrum at f j m length of the spectrum
features definition time domain features
before extracting features it is important to process the raw emg signals to ensure the features of interest are not obfuscated by various sources of noise 
to do this we apply a cascade of five filters a high pass fifth order butterworth filter with a cutoff frequency of 20 hz a low pass third order butterworth filter with a cutoff frequency of 450 hz and three notch filters with stop bands centered at 50 hz 150 hz and 250 hz
a preprocessing
the preprocessed signals are segmented into windows of length 300 ms with 50 overlap 
this is done to leverage the fact that the signals are pseudostationary in small regions and leads to better classification results than if one used non overlapping windows
b feature extraction
real time classification is needed for modern myoelectric prosthetics 
for this reason it would be advantageous to reduce the size of the feature set from 180 to a smaller number to reduce computation time 
c feature selection
for each classifier we performed forward feature selection to determine what the optimal features were and how few features could be used while maintaining high test accuracy there are two potential ways to perform feature selection one in which we consider all 180 features across the ten channels and one in which we find a subset of the 18 features which is applied to each of the ten channels 
due to changes in electrode placement from subject to subject we chose to find which of the 18 features per channel would give the best results 
using this method the optimal feature set can be applied irrespective of the channel placement or number of channels feature selection was run on each individual for each classifier 
the test errors were calculated using 5 fold cross validation 
the optimal feature set for each classifier was decided by using a voting scheme to average the results across each of the four subjects 
a weighting was applied to each feature depending on its rank in the output of feature selection 
we chose the optimal feature set for each classifier based on the features that cumulatively ranked the highest across all individuals 
we trained three different classifiers to predict hand gestures linear discriminate analysis lda na ve bayes and multiclass svm 
for the na ve bayes classifier the feature vectors are discretized into 128 different values 
d training procedure
the classifier is then trained using a multivariate model 
for the multiclass svm we took the one vs all approach where we trained a single classifier per class with the samples of a specific class as positives and all other samples as negatives 
for testing the classifier yielding the highest confidence level determined the predicted gesture 
the classifiers were trained on each subject individually due to the fact that emg signals can vary from person to person due to biological and environmental factors 
two different training and testing procedures were performed one in which only the gestures were classified 6 classes and one in which both the gesture and force level were classified 18 classes 
the feature selection algorithm was run for the classification of hand gestures without force labels 
the selected feature sets for each classifier were then used to classify hand gestures with and without force labels 
iv results and discussions
the results of the feature selection algorithm on lda and na ve bayes are shown in finally lda proved to be best classifier for this application with high training and test accuracy 
also it was observed that increasing the number of features did not lead to overfitting for the lda classifier unlike na ve bayes and svm 
svm gave high training accuracies but testing accuracies lesser than lda hinting that regularization may have helped to improve the testing error 
in this project we provided an effective feature set for real time classification of emg signals using lda na ve bayes and multiclass svm and showed that lda provided the best overall performance 
we were also able to predict with fairly high accuracy common gestures that would be useful for modern prosthetic limbs 
v conclusions and future work
we also demonstrated that the same set of features resulted in fairly high performance for the classification of gesture and force simultaneously 
for future work it is worth exploring ways to make the multiclass svm classifier more efficient and effective by regularization and by using different kernels 
feature selection can be implemented for all 180 features across all channels which can help to understand the dependence of features on different channels different locations on the arm 
also it would be useful to implement a method to identify important channels out of the 10 channels 
this information could be used to make prosthetic arms less cumbersome by including only the necessary channels for classficiation 
lastly it would be worthwhile to explore if the feature sets chosen could generalize to all emg signals by testing on data acquired from other sources of the body 
for instance one could acquire data on a lower limb amputee to characterize foot movements and use our methodologies for prediction 
this project was a part of the cs229 machine learning course at stanford conducted by prof john duchi 
we would like to thank prof duchi and the tas for this insightful course and for guiding us through this project 
the goal of the project is to develop a hybrid model for better hotel recommendation 
at this moment the majority of the recommendation systems are content based models which only consider the searching paramaters input by customers but not the users preference 
for instance expedia focuses on the searching criterion and recommends the top popular local hotels 
personalizing the user search by their preference is a burning need for better hotel recommendation 
collaborative filtering is considered as the starting point of this project 
it has been widely used in recommendation systems but rarely in hotel recommendation nevertheless there are still related works 
ryosuke saga 
et al in this project hybrid model is applied to combine user preference and item properties 
based on the final comparison of accuracy the model achieves good results 
more details are as follows 
in this part three models will be introduced 
content based model and collaborative filtering are traditional methods in recommendation system 
ii methodology
hybrid model compensates the shortcomings in two models by combining these two models successfully 
at the same time it introduces new methods 
content based filtering is a common approach in recommendation system 
the features of the items previously rated by users and the best matching ones are recommended 
a content based model
in our case the local popularity of the hotel clusters based on ratings by users is used to be the main feature in the contentbased model 
more details will be explained later there are three main shortcomings of this approach 1 
it is limited by the number and the types of features associated with the objects for recommendation 2 
it may involve the issue of over specialization as no inherent method is included for finding something unexpected 3 
it may not predict precisely for new users 
usually a content based recommendation system need enough ratings to provide accurate recommendations in our project we used content based filtering as a reference for result comparison 
the philosophy of collaborative filtering is to identify similar users and give recommendation based on the preference of similar user 
but collaborative filtering have the following issues 
b collaborative filtering
first of all user behavior are chaotic such as the gray sheep problem 
the gray sheep refers to the users whose preference do not consistently agree or disagree with any group and itself 
in our dataset of the next experiment we find some users choose different hotel cluster every time 
this makes collaborative filtering extremely ineffective on those gray sheep users 
secondly collaborative filtering assumes that users make decisions purely due to their preference 
however we find their choices are highly correlated with hotel destination 
a destination sometimes only have certain types of hotel and a certain type of hotel is very famous or popular in that destination 
thus the users choices will be significantly limited by destination or influenced by destination 
besides the chaotic user behavior and hotel destination influence collaborative filtering have utility matrix sparsity and data scalability issue that we will address them in detail in next hybrid model section 
the goal of hybrid model is to resolve two big problems 
first of all we need to work out three big issues of cf mentioned in part b 
c hybrid model
on the other hand we would like to combine users preference and popularity of hotels to recommend 
the utility matrix gives each user item pair a value represents the degree of preference of that user for that item 
in the later experiments we will use user id representing user and hotel cluster representing item 
utility matrix
when a hotel cluster is viewed by a user rating of 1 is given when a hotel is booked rating of 5 is given 
in terms of scalability problem hierarchical clustering is applied to cluster the large number of users into different clusters 
most of the users have little booking history less than 5 booking history in the data 
hierarchical clustering
this leads to a very sparse utility matrix 
also the number of user in the data is massive which makes it impossible to implement matrix factorization on the original utility matrix 
therefore users are classified into user cluster and utility matrix is compressed based on that 
the hierarchical clustering method builds a hierarchy of clusters by moving up this hierarchy similar pairs of clusters are merged as a cluster 
to be more specific cosine distance is applied to measure similarity between users 
normally cosine distance requires data normalisation 
the original rating is substracted by the average rating of that user cluster where i represents user cluster k represents hotel cluster and k is the total number of hotel cluster 
m i k means the rating of user cluster i on hotel cluster k and m i k is the normalised rating m represents it in the rest of paper 
cosine distance is written as follow where a k and b k is the ratings on hotel cluster k by user cluster a b respectively and k is the total number of hotel cluster 
after clustering the users based on their preference in utility matrix the utility matrix might still be super sparse because it is also rare to a cluster of users to rate most of the hotel 
we would like to find a method to fill the unrated entries in utility matrix by smallest error 
svd singular value decomposition method
here svd is applied to do that svd seeks a low rank matrix x u v t where u r n c and v r k c n is the total number of distinct customers k is the number of distinct items and c is the dimension factor that minimizes the sum squared distance to the fully obseved target matrix m here is clustered utility matrix with the dimension r n k 
matrices u and v are initialized with small random values sampled from a zero mean normal distribution with standard deviation 0 01 
we minimized the following objective function where is the regularization parameter in order to avoid overfitting 
to solve this objective we can use stochastic gradient descent sgd 
after taking derivatives of the objective with respect to u and v and the following is the update rules where is the learning rate parameter after estimating u and v by iterating over the known i k pairs in the data user i s recommendation for product k can be estimate by computing
in order to resolve the cold start problem ontology model is introduced 
the ontology decision model decision tree is to predict the cluster label of a new user by inputting the user s profile data 
decision tree classifier
decision tree is a high level overview of all the sample data which not only can accurately identify all categories of the sample but also can effectively identify the class of the new customer 
in order to avoid overfitting cross validation method is adopted to obtain the best decision tree 
a procedure to do that in hotel recommendation is in
the second problem we would like to resolve is combining the user preference with the item properties 
take the hotel recommendation as an example 
combination
set a evaluation metric 
if the hotel was booked it is rated as 5 points 
if it is just clicked it gets 1 point 
otherwise it is unrated 
based on booking history we have a ranking matrix in terms of hotel properties i e destination can be created by average all ratings based on counts of booking or clicking shown as follows where j is the total number of hotel destinations k is the total number of hotel type or hotel cluster from the previous procedures a clustered svdutitlity matrix m is attained 
we created a new matrix r with the dimension n k by m and d that is where i is customer id j is destination id k is hotel cluster id n is the total number of users that have booking histories m reflects the user preference and d represents the popularity of hotel in local destination 
matrix r connects two attributes 
the other benefit is for a single user s perspective 
by clustering customers booking histories from several users are combined together 
but for a single user he or she might not go to the hotel with the high rating of that cluster which produces a great error if recommending the top hotel by purely clustered utitliy matrix 
it is about 13 accuracy in the later experiment 
furthermore the top cluster recommended by utility matrix might not in that destination input by the customer which also leads to big bias 
however in the end matrix r can get better tradeoff all the process is shown in the flow chart in
in order to test the validation of hybrid model datasets from random 20000 customers on expedia are accepted to do experiments 
the dataset is shown in tab 1 
iii experimental results
when applying hierarchical in order to control whether two clusters should be merged or not a distance threshold should be specified 
here we set the threshold based on from the hybrid model results in prediction with 53 6 accuracy on testing data 4 improvement on content base model 
this result is consistent with our hypothesis both user preference and hotel popularity are vital in recommendation system 
in kaggle the benchmark content based model data leak method has 49 8 accuracy 
our hybrid model can be further improved in these two aspects 1 
larger dataset will be applied in this model so density based clutesring method should be used instead of hierarchical clustering 2 
more features such as hotel country and hotel market might be included to test their impacts in prediction 
v reference
the goals of this project are 1 build a model for six major u s airlines that performs sentiment analysis on customer reviews so that the airlines can have fast and concise feedback 2 make recommendations on the most important aspect of services they could improve given customers complains 
in this project we performed multi class classification using naive bayes svm and neural network on the twitter us airline data set from kaggle 
significant accuracy has achieved which shows that our models are reliable for future prediction 
in recent years twitter has become the de facto online customer service platform 
thus a companys image on twitter is of central importance and this is especially true for airlines given that many tweets are travel related in nature 
in fact research has shown that responding to tweets has revenue generating potential drives higher satisfaction than other customer service channels and perhaps most importantly satisfied twitter users spread the word 
in this project we use tweets gathered from twitter to learn about people s flight experiences and give airline companies suggestions on how to make their trip more enjoyable the data set contains about 15 000 tweets collected from february 2015 on various airline reviews 
every review is labeled as either positive negative or neutral 
first we want to build a model to perform sentiment analysis on the data set 
second more interestingly we want to assign a reason to each negative response such as late flight lost luggage etc 
in our data set about 80 of the negative reviews has a negative reason label yet the rests are labeled as can t tell 
our goal is to assign a label to this unspecified group 
by knowing every review s negative reason we can give specific suggestions to different airline companies on how to improve their service 
nowadays developing and testing different models for a natural language processing problem is an interesting and challenging task 
however due to the nature of the problem the accuracy of sentiment analysis on single sentence like movie reviews never reaches above 80 for the past 7 years since tweets texts are usually short and verbal the same problem presents in our data set as well 
background and related work
however even though the tweets are short there are strong indicative words 
specific words can be used as indicators for spam ham emails and achieve good test accuracy 
therefore we believe that tweets review without many negating negatives can be predicted well using the frequency vector representation 
to prove this we will use recurrent neural network model and the glove word vector
the sentiment analysis labels are positive 20 negative 60 and neutral 20 
the negative reason labels are bad flight 7 45 canceled flight 9 62 customer services issues 39 77 damaged luggage 0 84 flight attendant complaints 6 05 flight booking problems 6 19 late flights 1 99 long lines 19 97 and lost luggage 8 23 
in the preprocessing step non english word symbols and website links are eliminated 
then the whole data set is randomly separated into training set 10000 samples 70 and test set 4636 samples 30 
preprocess of dataset
the dictionary is made based on the training data and all sentences are broken down into list of words 1 delete common words such as a an to of on etc 
with high frequency but little semantic usage 
dictionary
2 stem words such as thanks and thank as one word 
3 delete low frequency words that appear once to reduce the size of dictionary for calculation efficiency 
a feature matrix is built to convert the textual information into numerical information 
in the feature matrix the number of rows indicates the number of samples the number of columns is the length of the dictionary and each element indicates whether the specific word has appeared in the current review 1 for existence and 0 for absence to get a sense of correlation presented in our feature matrix i e 
frequencies matrix
bad and suck may have a higher chance to present together we perform pca to capture the variance 
the result shows that for the first component variance explained is 2 3 and for the next nine components the variance explained is all around 1 0 
this shows that there isn t significant correlation between words and to achieve better accuracy we include all the words in the dictionary 
we propose that the lack of correlation comes from the nature of the text data 
most of them are very short sentences and extremely verbal 
naive bayes with multinomial event model from sklearn is used 
1 
input is the frequency vector and laplace smoothing is used 2 
support vector machines with linear kernel and rbf kernel are used in this project 
svm uses the same input and implementation package as naive bayes 3 
neural network tensorflow is used in implementation 
input is the frequency vector that represents a review 
the output is a vector with probabilities for different classes and the highest is selected as prediction 
label is a one hot vector that represents the class 
loss function is cross entropy plus a regularization term 
the vanilla neural network that we use 4 
recurrent neural network a bi directional gated recurrent unit network gru can capture the structure features of a sentence 
also it solves the vanishing gradient problem which many recurrent neural network models have 
bi directional gru is commonly used in text analysis which we want to compare with our models 
package scikit is used for implementation 
in gru word vectors instead of frequency vector will be used and we choose glove twitter 27b zip 
similarly for 
before tuning the regularization svm with linear kernel rbf kernel results in 0 23 0 21 test error respectively 
therefore svm with rbf kernel is excluded from future tuning due to the higher initial test error 
support vector machine with linear kernel
l2 regularization is used to avoid overfitting 
according to the graph shown below the lowest test error 0 200 is achieved when l2 regularization is 0 02 
in each stochastic gradient descent step only a batch of 100 samples is used in sgd to increase the training speed 
tuning parameters are learning step and regularization term which are 0 01 and 0 respectively 
one layer neural network
the best test error for this one layer neural network is 26 3374 
word vectors from glove with a dimension of 50 will be used in gru 
a 2 layer gru has a test error of 26 5 
bi directional gated recurrent unit network
a 3 layer gru has a test error of 25 6 
learning step is 0 01 l2 is 0 
in sentiment analysis task svm with linear kernel achieves the best test accuracy 
therefore svm is recommended in this section 
sentiment analysis result
according to the result from linear svm virgin american performs the best according to its lowest negative review composition in its total reviews 
in this section the goal is to determine the most negative reason on flight services 
all the negative reviews have been collected for this task and separated into labeled set and unlabeled set 
negative reason prediction
we will make predictions on the unlabeled set 
using naive bayes classification the test error is average to 29 26 after ten fold cross validation with a laplace smooth factor of 0 5 
naive bayes classification
l2 regularization is tuned 
the best test error for svm is 32 82 when l2 regularization 0 03 
support vector machine
given by the lowest test error naive bayes is used for the prediction of unclassified data 
result is shown below that most complaints are on customer service 
negative reasons classification
one postulate might be due to the high volume of contact 
since various reasons can lead to calling customer service 
thus correlations between classes may play a factor in determining this result 
it is pleased that our vectors work 
it is surprising that svm and naive bayes perform better than deep learning methods 
and the accuracy is very high 80 
we think the reason behind this is that while movie reviews have a lot of sarcasm however it is too early to say that neural network can not perform better than bag of word models 
the frequency vector used in vanilla neural network is so large that takes enormous time to train roughly 6 hours for 10 000 iterations now 
therefore clever ways of reducing frequency vector size are needed 
meanwhile better tuning parameters can be figured out once training time is significantly decreased another possible reason is that for recurrent neural network gru in our project labeling every node is very important 
while this model can achieve as high as above 80 accuracy using stanford sentiment tree bank dataset 4 our results show that without sufficient labeling this model is not able to achieve an accuracy above 80 which means rnn family needs strong supervision 
however most of the online reviews and other documents only have limited labels 
better labeling algorithm on new data set should be thought about in future work 6 reference
in recent years food reviews have become increasingly popular on social media 
people are posting their reviews or comments on facebook yelp twitter etc 
when selecting local restaurants or food people also tend to make their decisions based on these reviews 
hence it is important for both restaurants and individuals to quickly get the information and score of a food item or restaurant from thousands of reviews 
it is also beneficial for some platform to provide different customers with their personal recommendations mcauley and leskovec marx and yellin flaherty
our analysis focus on an amazon food review database consisting of 568 454 instances 
the data set contains the texts of reviews scores and helpfulness 
ii datasets and features
in more detail product id the product that this review is for 
the total number of items that are rated is 62 279 
and the average number of reviews for an item is 9 2 
user id the user who wrote this review 
the total number of users is 178 554 
so the average number of reviews that each user gave is 3 2 text the main content of the review helpfulness number of people who find this review helpful score the score this user gave to the food item 
the histogram in
word to vectors 1 skip gram instead of using the traditional countbased representation of words we train the word vectors from our own data 
a 
first we download the glove global vectors for word representation which was trained on the crawl data of twitter 2b tweets 1 2m vocab however we do not directly use this representation for our final representation of words 
iii methods
instead we just use this as an initializer of our self trained word to vector model 
we train our word vectors by skip gram model which uses the method of predicting the surrounding words in a window of certain length 
the objective function is to maximize the log probability of any context word given the current center word where represents all variables we optimize and p w t j w t is the probability where o is the outside word vector and c is the inside word vector 
every word in this model has two vectors 2 phrase vectors now we have got the vector representation of each single word 
but our aim is to classify the entire review in sentences 
hence we need to derive a method to vectorize the sentences 
a natural approach is to average or concatenate all word vectors in a given sentence 
it turns out that this works well and can be very convenient to implement like the we are not satisfied so far since neither concatenation nor averaging treats each word equally and neglects the relationship between words and the structure of the sentence 
hence we use a recursive parsing tree to get our sentence vector 
concretely for every representation of two candidate children we calculate the semantic representation if the two nodes are merged as well as a score showing how plausible the new node would be 
the score of a tree is computed by the sum of the parsing decision scores at each node 
we train our tree by maximizing the max margin parsing 
we know that the standard recurrent neural network computes hidden layer at the next time step directly by in gru we first compute an update gate according to the current input word vector and the hidden state then we compute the reset gate with different weights and obtain the new memory content and the final memory 
by feeding a review into each x we allow each batch review to influence each other s prediction an idea that may seem counterintuitive since each review is independent 
however different reviews may have a certain connection with the same food or restaurant 
right direction h
only through this structure can we explore the deeply connected information of reviews 
after we fetch y i from gru we use cross entropy loss for the network and train the weights beyond the 3 layers gru network we also obtain good results from a 2 layers gru 2 layers convolutional neural network and 3 layers convolutional neural network 
we summarize the accuracy later to compare these models with different hyper parameters 
2 long short term memory lstm after the classification we want our model to automatically generate some reviews 
concretely we first specify a score representing the rank of the reviews like 5 means the best and 0 means the worst and then feed this score to the neural network 
fig 4 3 layers gru
we want our neural network to generate the review corresponding to the specified score the model we use is lstm which is also a modified version of recurrent neural network but with some reset and forget state 
this method selects the most probable word at each time depending on which class we are in 
choose the largest probability class of h i 1 to be x i 1 2 
we use latent factor model to recommend food items to a user 
as shown in denote iu the derivative of the error e with respect to r iu thenand the update equations for q i and p u in sgd are after 40 iterations of sgd we obtain the final p and q 
c recommendation system
then we predict the score the user will rate an unrated item and recommend 10 items of the highest scores to the user 
in this part we use the traditional count based word representation and compare the results with the previous approach 
we utilize count vectorizer followed by term frequency inverse document frequency tf idf for finding the words that contribute the most to each of the five scores of the food reviews first we use feature selection to select 1000 best informative words out of the 10000 features from tf idf 
d features visualization and spam reviews
the next step is to build a softmax regression model on these features and to fit between words x and scores y 
in softmax regression we maximize the log likelihood finally we choose the largest 50 coefficients in i of the probability of each class y i and get the corresponding words as the most important words for the score since the relationship between a word and the helpfulness of the corresponding review is not as clear as the relationship between a word and the score of its corresponding food item we define an helpful review to have a helpfulness score greater or equal to 4 where as an useless review has 0 as helpfulness score 
thus this becomes a binary classification problem with truncated data set that does not contain reviews with 1 2 or 3 helpfulness score 
we fit the data to softmax regression model quadratic discriminant analysis model
the results of gru and cnn model and the loss sequence of 3 layers gru are summarized in the table below we list part of our experiments as above 
we start with the vanilla neural network of a single layer and its test accuracy is 0 63 which is not bad when we have 5 different
1 gru 
here is an example text generated from our lstm of score 5 food review 
we show the process of picking the highest probability from the last hidden layer and fetching into the next input layer 
2 lstm 
with the the number of iteration increases the generated sentence makes more sense to us including the punctuation 
for each class we select some reasonable generated sentence to present here 
in the sgd process we have changed the learning rate to get the lowest convergence error after 40 iterations see we can get the recommendation for an user by finding the largest scores in the row corresponding to this user in the utility matrix r there is a sample recommendation for this user shown in
b recommendation system
after fitting the data to the softmax regression we compute 50 words that have the largest coefficient and thus are the most significant words for each score 
we organize these words into tag clouds we obtain decent training and cross validation accuracy when fitting our data to both qda and decision tree model 
c features visualization and spam reviews
we vary the regularization parameter and plotted training and cross validation accuracy versus regularization parameter
we use multiple neural network models to classify our text 
our results show that bi directional gru has the best performance with high efficiency 
v conclusions and future directions
another advantage of our gru is that it is immune to extremely unbalanced data 
during our experiments we find that when the distribution of data is not uniform 
for example when most food items are in level 2 most classifiers will just assign any input to the majority class to reach a high accuracy 
however gru can still learn it well and not stick to the majority 
a natural generalization of our model is to apply it on different review tasks say yelp twitter walmart ikea etc 
this requires our model to automatically choose the hyper parameter and structure in order to accept the input under different context 
this will be our new direction to design a more robust classifier which can be used upon any kinds of customer review our recommendation system is based on the basic latent factor model 
we have reached a relatively low error by changing the learning rate in sgd process but the recommendation result is not very satisfying 
this model can be further improved by adding some terms such as user item bias implicit feedback temporal dynamics user associated attributes and confidence level softmax regression succeeded in selecting words for a specific score but it failed in modeling the helpfulness of a word 
for the latter task both qda and decision tree model fit the data well mainly due to the non linearity of the frequency of a word versus the score of the food item 
modern molecular and cellular biology depends on markers that can very selectively bind to a target molecule 
unfortunately the process of finding such a marker is generally slow through trial and error and often in vivo 
introduction aptamers and their selection
aptamers short nucleotide sequences are a promising type of marker that can be generated and evaluated synthetically 
currently the process of aptamer selection involves the generation of several thousand different nucleotide sequences and the measurement of their target affinity using fluorescence 
those aptamers with high affinity will fluoresce brightly 
through successively stronger washes those aptamers that do not bind the target tightly are removed and the batch is refined down to those aptamers that very tightly bind the target 
new technologies such as microfluidic chips and dna arrays are now enabling millions of aptamers to be generated and evaluated in a single day but it is still costly to run each batch and only a small number of the possible aptamers of a fixed length can be evaluated 
this project investigates how support vector machines svm can be used to classify proposed aptamers after being trained on the fluorescence data from a previous batch 
this will hopefully reduce the number of batches required to converge to a good aptamer by focusing resources on those aptamers with the most promising nucleotide motifs 
project goals
different types of string kernels for the svm e g spectrum and mismatch kernels will be built and evaluated 
further by viewing the function mapping aptamers to fluorescences as a gaussian process the upper confidence bound reinforcement learning algorithm will be used to develop a model that can be used to suggest highperforming aptamers for future batches 
using the data from a single batch of aptamer selection these algorithms could ideally be used to suggest an additional fifty or so aptamers that are likely to outperform any of the aptamers in the actual training set 
the dataset consists of approximately 3 000 nucleotide sequences of length 45 along with their associated fluorescences 
the starting and stopping subsequences common to all of the aptamer nucleotide sequences were not used in any computation 
for many learning algorithms involving strings kernels provide a computationally efficient means of finding the inner product of high dimensional feature vectors without explicitly forming the feature vectors and taking their inner product 
there are many possible kernels that could be used to measure the similarity of two aptamers 
string kernels
some are based only on the underlying nucleotide sequence e g spectrum or mismatch kernels but others are based on the structural properties of the folded aptamer in solution 
for this project all kernels were based upon the underlying nucleotide sequence of the aptamer 
the spectrum kernel as described in 
using tree structures this can be calculated directly in linear time in the length of the strings 
spectrum kernel
the mismatch kernels differ from the spectrum kernels by incorporating spatial information 
the simplest example is the 1 mismatch kernel which simply compares each nucleotide in a string with the nucleotide at the same position in the other string 
mismatch kernels
it returns the total number of such matches 
for k 1 there are several variations of this basic idea which instead compare substrings of length k the sliding k mismatch kernels use a sliding window of length k starting at each position within the string 
the chunk k mismatch kernels break the strings into non overlapping substrings of length k and return the total number of these that match 
because of the structural and chemical similarities between a and t and those between g and c i hypothesized that mismatches of a and t should be penalized less than those of a and g or a and c and similarly for the other nucleotides 
so for this project i also created modified mismatch kernels return 0 1 for a mismatch between either of the above pairs rather than 0 
finally the weighted mismatch kernel consisted of a sum of chunk k mismatch kernels with weights of 1 k for k 1 5 
note that the definitions of mismatch kernels given here differ from those in the literature 
an svm learns a hyperplane from training data which can be used in binary classification 
using the kernel trick allows the svm to learn and use this hyperplane in the feature vector space without explicitly computing feature vectors 
employing this kernel trick with the kernels described above several svm were trained 
the dataset was divided into training 60 and test 40 subsets 
the training data were then labeled using an arbitrary cutoff so that aptamers with fluorescences in the top 40 of the dataset were labeled as 1 good and those that were not were labeled as 1 poor 
the optimization was carried out using stochastic gradient descent 
the logarithm of the fluorescence was used in these computations 
for the gaussian process upper confidence bound gp ucb algorithm it is assumed that the underlying function f on the space of strings that determines the associated fluorescence is a sample from a gaussian process with mean function x ef x and covariance function k x 1 x 2 
for each step t in the algorithm a new point x t x that maximizes t 1 x t t 1 x is selected and then a noisy measurement y t f x t t where t is distributed n 0 2 is collected 
gaussian process optimization using upper confidence bound
then including all the noisy samples up to iteration t we calculated the updatedandwhere k is the kernel matrix over all x 1 
the parameter t decreases with t and guarantees bounds on the regret 
y t t 
support vector machines five support vector machines were successfully trained and tested 
the weighted mismatch kernel performed the best with training and test accuracies of 89 4 and 85 6 respectively 
however the 3 mismatch sliding and 3 mismatch chunk performed nearly as well and were faster to compute 
the modified 3 mismatch sliding did not perform as well as the unmodified mismatch kernels 
this indicates that binding of the aptamers is highly dependent on the actual nucleotide at each position and not necessarily the cross nucleotide binding characteristics of the nucleotide at each position 
all mismatch kernels were significantly better than the spectrum kernel 
there is an important explanation for this 
the spectrum kernel completely disregards spatial information included in the nucleotide sequence 
this information determines how the aptamer loops on itself in its secondary structure which strongly determines the tertiary folding of the aptamer in solution 
because it is this tertiary structure that actually fits into the target molecule it makes sense that the spatial information is important in predicting how tightly an aptamer binds the target 
these results suggest that a more accurate kernel would weight different locations within the string depending on how strongly changing the nucleotide at that location changes the performance of the aptamer 
it would be expected that certain locations within each string are very sensitive to changes e g 
locations which are involved in closing the loops of the secondary structure 
this weighting on the string could come from either prior physical knowledge of aptamer secondary folding or from hyper parameter fitting 
the gaussian process optimization using the upper confidence bound algorithm was successfully implemented on the entire dataset 
as can be seen in
gp optimization
question answering qa is a highlyversatile and challenging task towards real artificial intelligence 
it requires machines to understand context knowledge and the question query and provides an answer 
the recent achievements of neural networks or deep learning in encoding and decoding very complicated information encouraged us to apply them to qa 
in this paper we design and implement a memory network model and compare its performance with lstm based models 
our experiments show the memory network model outperforms lstm based models by a comfortable margin in almost every task 
question answering qa has enjoyed much academic interest due to its flexibility 
but with this flexibility also comes great challenge 
in this project we focused on questions based on finite amount of information 
the system will rely on nlu and some small amount of reasoning to answer the questions 
the tasks are discussed in section 3 in this project we implemented a family of lstm and a memory network model for qa task 
lstm has been applied successfully on the sequence modeling tasks 
memory network
attention mechanism mitigates the long termdependency problem in traditional lstm by enabling the system attend to different parts of internal representation
we used the babi dataset by 1 
the dataset consists of context question paragraphs in 20 different tasks 
each answer is designed to be a single word 
the dataset can provide insights into different aspects of a qa system 
the vocabulary however only measures at 150 words and prevents the dataset from being more realistic 
in a general setting we define a qa problem as a collection of context question answer tuples 
in our project we are only concerned with questions that base solely on the context 
for each contextquestion tuple we want the machine to supply an answer to work with neural networks we encode each word in the context and question by a word index in the vocabulary 
therefore we encode contexts c as integer matrix where c ij is the word index of the j th word in the i th sentence in the context 
similarly we encode question q as a vector of word indices and answer is a single word index all of our models first summarize c q to a vector g c q and then compute the probability distribution of a given c q aswhere w is a learned weight matrix 
now we introduce our models 
a memn2n contains a memory store of contexts 
each word in a conetxt sentence is embedded with an embedding matrix a 
end to end memory networks memn2n 
we denote the result of embedding as c a 
then we combine word vectors into a sentence representation with combination function f c 
similarly we embed question q as with an embedding matrix b as f c q b finally we embed the contexts with yet another embedding matrix c to extract a candidate word for answer 
with all the embeddings we evaluate an attention score over each context sentence as f s c a u 
by default the scoring function is simply a cosine distance function 
finally the output word is chosen among the candidate words by a softmax on the attention score 
in a k hops version the output of previous hop is fed into the next layer 
this can be stacked for multiple layers with little additional computation complexity 
in addition we also experimented with the idea of using a whole lstm as the combinator function 
we call this variant a lstm end to end memory network or lstmmemn2n 
another improvement we attempted was replacing cosine distance function f s with a multilayer perceptron architectures 
we name this variant mlp end toend memory network or mlpmemn2n 
rnns especially lstms are very useful tools to model sequence data 
we can use lstms to encode a g c q representation 
lstm family
attention is a powerful mechanism for long sequence such as how contexts in our problem can be 
in these lstm models we still embed context sentences the major difference between variants of lstm models is the design of encoding function for contexts f r there are two problems with a traditional lstm model 
on one hand lstm tends to forget the data it receives a long time ago 
one the other hand it doesn t take the question representation into account when it encodes context sentences to mitigate the first problem one solution is to implement a pyramid structure where we feed every the result vector after reading each sentence into another lstm and get a more stable and longterm representation attention mechanism can address both problems 
attention mechanism comes in many different flavors 
the basic attention mechanism called token level global attention 
computes the attention scores with a multilayer perceptron 
the attention mechanism can also combine with the pyramid structure 
we implemented our model with theano 2 and trained on babi en and babi en 10k task sets 
the results show memory networks perform well against the baseline lstm in almost all the tasks 
experiment
one exception task 18 size reasoning is hard for memory networks using bag ofword combination likely because of many noncommutative relations in the text 
but position encoding and lstmmemn2n overcame this issue 
the results are shown in we also trained memn2n on 20 tasks jointly reaching a mean test accuracy of 75 
by comparing the results in variants of lstm models don t perform as well as memn2n 
there are larger gaps between training accuracies and test accuracies and they are more computationally complex which makes them harder to train and generalize 
also all the attention based lstm models only conduct single hop reasoning 
it may help by increase the number of hops which will unfortunately worsens the computational complexity some of the tasks are clearly hard no matter which model we use 
we think these factors contribute to the difficulty of a qa task the number of supporting facts involved the complexity of the relation itself and the length of the context 
the number of supporting facts needed to formulate an answer is an important factor 
for example task 1 2 3 need one two and three supporting facts respectively 
there is a clear decrease in performance from task 1 to task 3 for each model 
there may be a number of reasons behind this 
the straightforward one is the limited expression power of a fixed length vector 
since in our training process we apply the same hyperparameters to all 20 tasks for a model some tasks with more supporting facts becomes hard to fully encode with the hyperparameter chosen the second factor is the complexity of the relation within and between the supporting facts 
for example our experiments show no model produces an accuracy higher than 20 for task 19 path finding where the system needs to encode sentences describing the relative positions between two points 
in the end the question asks about a path more precisely a general direction going from some point a to another pont b 
the machine needs to be able to connect a and b through a few intermediate steps which requires more sophisticated representation than simple matching 
another example is task 18 size reasoning which predominantly features relations that are non commutative a is smaller than b 
in these sentences the order of word a and b is very important 
as we have discussed previously this requires the use of position encoding and other sophisticated encoding 
in our experiments lstmmemn2n achieves a test accuracy of 98 8 beating all the other models the third factor contributing to difficulty is the length of the context 
context length in task 3 three supporting facts is unusually long compared to other tasks 
whereas other tasks commonly have 10 to 20 context sentences task 3 can have up to a total of 249 such sentences 
the performance on this task turns out very poor for all the models 
for lstms large context length translates to very long term dependency which can results in undesired phenomena like gradient vanishing or gradient exploding which causes the lstm to forget relevant facts
two main areas of future remains to be explored 
first we can improve the model performance on the babi task set 
second we can generalize the current model to more complicated tasks currently memn2n achieves nearly perfect training accuracies for most tasks 
but among these tasks some show a low test accuracy showing the sign of overfitting 
we should design proper regularization to better generalize on the test data and shrink the gap between training and validation accuracies other task sets we can test our system on include the children book test cbt task set 3 for memn2n the complexity of the model comes in the forms of number of hops 
but because the necessary complexity of the model can vary tremendously by tasks it would be useful to design a memn2n model which can learn to control and regulate its own number of hops 
moreover a better sentence encoder may also help the performance the main problem facing the lstm models is long term dependency 
explicit memory representation like those used in memn2n may help mitigate this issue 
another possible approach to tackle the long term dependency issue is an idea called neural turing machine
we experimented with memn2n and lstms as the two major qa solutions 
in our experiments memn2n outperform lstm models 
but the combination of the two models lstmmemn2n achieves even higher test accuracies 
since language is a sequential data by nature however we think lstm models also have a large potential to be explored 
professor chris potts and professor bill maccartney helped us defining our project jiwei li helped us debug the model when it wouldn t converge the lisa lab deep learning tutorial 5 gave us a great head start with the lstm model 
acknowledgement
online social networks such as face 
as of august 2015 facebook an immensely popular online social networking service had over 1 59 billion monthly active users 
the site allows users to create a user profile and add other users as friends 
users can then categorize their friends into lists such as close friends or people from work 
however with the average facebook user having about 338 friends manually picking out these friend circles becomes a laborious process 
the purpose of our experiment is to explore algorithms that can automatically detect these circles so that users can more easily personalize to whom they share their information 
for example a user would most likely not want to share the same information with acquaintances than with family members 
in order to detect these circles we will consider three sources of information the user s profile features the user s friends features and the network structure 
in general we want the friends in each circle to share certain common features such as same school same work etc 
and also have many common friends within the circle 
connectivity within a circle can also provide information on which groups are more casual and which are more tight knit 
finally we should also consider the possibility of friends belonging in multiple circles 
for example someone who went to the user s university could also be a coworker 
therefore our problem at hand is not a traditional clustering problem where each example falls in one cluster 
we will address the multi cluster problem by using an algorithm that determines a multinomial distribution of the circles for each of the users friends 
we will explain this algorithm in more detail in the latter sections 
this section gives background information on the methods that have been explored to solve the task of social circle discovery 
we will then discuss latent dirichlet allocation which is a method from natural language processing that we will apply to our task 
background
mcauley and
for social circle discovery we turn to latent dirichlet allocation lda originally devised by lda is a generative algorithm that views documents as mixtures of topics with each topic being a multinomial distribution of words 
lda models the production of each document in a corpus in the following fashion 1 
latent dirichlet allocation lda 
produce an n poisson the length of the document 2 
produce a dirichlet 
represents the distribution of topics within a document 3 
for each word w i in the document from this model we can formulate the probability of a document w since this formulation produces a loglikelihood maximization problem that is intractable furthermore in line with work done by hoffman et al 
in the previous section we discussed the lda method which we will now apply to the problem of social circle discovery 
this involves modeling the user s friends as documents features as words and social circles as topics in the documents 
the features we used not only included individual friends features birthday workplace etc 
but also the id s of each user s friends to capture some sense of the connectivity of the graph 
we also added features which each friend shares with the user ego node 
concretely for each friend document we have the feature labels for each exhibited feature on his her profile the feature labels for each feature he she shares with the user node and the user ids of all the friends that he she is connected to by an edge to evaluate the performance of our lda algorithm we also ran two variants of k means clustering using just the feature vectors of the friends profiles 
however since in many cases the feature dimensions exceeded the number of friends we compressed the feature vectors using tsvd see section 3 2 
to summarize we ran the following algorithms lda using the network structure user s profile and friends profiles 
we set the number of circles k the number of ground truth circles 
we will refer to this algorithm as lda lda as above except we set k using the aic c selection algorithm described in section 3 1 
this will be lda c k means clustering using only the compressed feature vectors of the user s friends 
we set k the number of ground truth circles 
we will refer to this algorithm as kmeans k means as above except we set k using the aic c selection algorithm described below 
this will be kmeans c 
with the lda algorithm it is vital to pick the number of topics which we will denote by k to model 
we accomplish this with a stepwise procedure through a grid search of varying values of k to choose the model that minimizes the aicc where ll is the log likelihood of the model with respect to the dataset n is the total number of words across all documents total number of features summed across all users and p k m 1 d k 1 is the effective number of parameters with k being the number of topics circles m the number of distinct words features and d the number of documents users to obtain the log likelihood of a model we utilized the perplexity measure produced by the online lda and used the following formula relating perplexity and log likelihood with c representing all the documents in a corpus i e all users in a network 
parameter tuning
the traditional aic criterion helps to choose the model with the greatest likelihood while penalizing models with large numbers of parameters which mitigates over fitting 
the aic c criterion however also corrects for finite sample sizes that are small with respect to the dimension of the parameter space
to set a baseline comparison we decided to use k means clustering which is capable of assigning each user to only one circle so is expected to be less robust than our lda algorithm 
to pre process the data before k means clustering we used the truncated svd method with our data matrix with rows representing users and columns representing the feature values for each user we formed the svd u v by a rule of thumb let s say we truncated the svd to the m largest eigenvalues 
baseline comparison
our tsvd is then denoted by u m m v m 
to form the dimensionreduced dataset we simply take u m m 
the data that we will use for training testing is provided by the stanford network analysis project and all of our data comes from facebook 
the data is divided into ego networks which consists of the ego node all of the nodes it is connected to called alters and all of the edges there may be among these nodes 
within each ego network we have the following circles these are the circles that the user manually chose the ground truth circle 
the circles are not necessarily disjoint so one user can be in multiple circles 
we will compare our results with these sets of users edges this contains every edge in the ego network other than the implicit edges that connect each alter to the ego node 
an edge n 1 n 2 signifies that alters n 1 and n 2 are friends on facebook features for each alter we are given a binary array where a 1 in index i signifies that feature i is satisfied and 0 otherwise 
the features are constructed in a tree structure where example features include education university stanford education university harvard education year 2018 etc feature names this contains the names of the features that correspond with the feature arrays 
in general we will just use the numerical labeling of the features 
after running the lda algorithm we get the multinomial distributions of the circles for each of the friends in the ego network 
at this point we can choose a cut off probability to choose which circles each user actually should be assigned to 
results analysis
for example if a user is assigned a probability 05 of being in circle a then this user is likely not actually in circle a 
in choosing this cut off probability we have to consider how many circles we are actually predicting 
let n be the number of circles we predicted then we will place user u in circle c if p r u c 1 n in our k means algorithm each user was automatically labeled into one circle once we have established these circles we want to be able to directly compare the automatically produced circles with the ground truth circles which are the circles that the ego user manually chose 
to do this we must determine an optimal mapping from our circles to the circles which the ego user hand picked 
first we need to determine some error cost function which we would like to minimize 
for the purpose of our experiment we used the balanced error rate ber as did the ber cost function equally weights the fraction false positives and false negatives 
if we compute the ber for every pair c i c j we can construct the cost matrix where the ij th entry is ber c i c j 
note that since the number of circles which we predicted does not always match the number of truth circles our cost matrix is not always a square matrix 
the number of matchings that we will get in this case will be min c c we want to find a circle matching f c c which gives us the least total error 
if we were to try every possible f and then compute the cost this would take o n 
however with the kuhnmunkres algorithm we can solve the assignment problem in o n 3 time for our final ber score we take the average of the ber rates from each circle assignment then subtract that from one for each algorithm we average the ber f values from each of the 10 ego networks 
we get the following results kmeans reports a ber score of 652 kmeans c obtains a score of 701 lda a score of 622 and lda c a score of 657 
for both our k means and lda algorithms we achieved better results when we predict the number of circles using aic c rather than just setting k the number of ground truth circles 
this is because in the latter case we are overfitting the data in one of the networks for lda using our predicted number of circles k 5 rather than k 46 improved the ber score from 632 to 851 
many of the ground truth circles only contained 1 2 people and by abstracting away these circles we actually got better results another surprising result was that the k means algorithms which used only the feature vectors of the user s friends but did not consider the network structure or user s own profile features did better than the lda algorithms which considered all three components 
however our implementation of the lda algorithm places larger weight on the network structure because most of the user s friends have many more connections within the ego network than 1s in their feature vectors 
since we are treating each connection as a word in the documents the user s friends the documents will largely be composed of network structure 
this implies that profile features even using the compressed vectors may tell us more about circle formations 
we hope to explore other combinations of features to include in our lda model such as interaction terms and indicators of edge strength 
we also hope to devise ways to factor networkconnectivity into our model building without having it overwhelm the other features present 
another area of work would be to explore parameter tuning with bic and aic and compare results with aic c selection to verify our theoretical decision to use aic c 
in this paper we apply reinforcement learning techniques to traffic light policies with the aim of increasing traffic flow through intersections 
we model intersections with states actions and rewards then use an industry standard software platform to simulate and evaluate different policies against them 
we compare various policies including fixed cycles longest queue first lqf and the reinforcement learning technique q learning 
we evaluate these policies on a varying types of intersections as well as networks of traffic lights 
we find that q learning does better than the fixed cycle policies and is able to perform on par with lqf 
we also note reductions in co 2 emissions in both lqf and q learning relative to a fixed cycle baseline 
according to a study by texas a m americans waste about 7 billion hours and 3 billion gallons of fuel in traffic each year for this project we aimed to reduce traffic by finding better traffic light policies at intersections 
an ideal intelligent traffic light systems can reduce traffic through several techniques 
motivation
it can turn lights green for longer in directions with more traffic use sensors to dynamically respond to arriving cars and changing traffic conditions and coordinate between lights to create runs of traffic that flow through many lights reinforcement learning is a promising solution to this problem because it can represent all of these techniques 
by using a general notion of actions we can decide when to turn lights on and for how long 
it excels at dynamic control and is designed to adapt to new conditions 
lastly we can define our states to incorporate as much information about the system as we want and share it across many traffic lights to allow them to coordinate 
we use q learning with function approximation to learn the best traffic signal actions 
before we detail our specific problem formulation we first describe the q learning algorithm for a general markov decision process mdp 
algorithm 2 1 q learning
under a mdp with fixed transition probabilities and rewards the bellman equation equation 1 gives the optimal policy if the q function can be correctly estimated then a greedy policy becomes the optimal policy and we can choose actions according to equation 2 we use the function approximation abstraction from
the heart of q learning is the q function used for estimation 
in the naive case the q function can simply be a lookup table that maps states and actions to q values in which case q learning is essentially the same as value iteration 
q functions
however q learning lets us generalize this framework to function approximations where the table of states and actions cannot be computed every part of equation 3 is differentiable so if our q function is differentiable with respect to its parameters we can run stochastic gradient descent to minimize our loss for our implementation we use stochastic gradient descent on a linear regression function 
we also performed sgd with a simple one layer neural network 
we approach this problem as a mdp with states actions and rewards 
problem formulation
to model an intersection as it would exist in the real world and to evaluate our policies we followed other researchers the sumo package also comes with traci a python api that allows a user to get information from a traffic simulation and modify the simulation as it runs in response to this information 
we built an architecture that made use of the traci interface to get information about queue sizes carbon emissions sensor data and traffic light states in order to successfully deploy our algorithms 
simulator
our road network featured 4 connected traffic lights in a 2x2 square grid spaced 150m apart 
each traffic light was also connected to a 425m long road which we used to mimic a highway on off ramp 
setup
thus we had a total of 8 source nodes and 8 destination nodes 
each road contained 3 lanes with a speed limit of 45 mph and we used sumo s default configuration for left turn and right turn lanes 
each traffic light was set to allow right turns on red 
yellow lights were set to be 5 seconds long 
on the roads in between the traffic lights induction loop sensors were placed in each lane about 50m before each traffic light on the roads leading into the network the induction loops were placed about 100m before each traffic light 
we used sumo s default car configurations 
every second for each possible source destination combination we generated a car at with probability 0 01 
there are many ways to formulate an objective function for traffic optimization 
ideally we would try to model the lost utility of drivers time spent waiting in traffic and the cost and environmental effects of wasted gasoline 
objective and reward function
these effects are all difficult to estimate 
fortunately however all of these effects are positively correlated with increasing traffic so any metric the captures the general trend of the amount of traffic will capture the general trend of these effects we specifically chose the throughput of cars through intersections as our reward function where throughput is defined as the number of cars that pass through the intersection per unit time 
while there are many other reasonable measures of the amount of traffic e g 
wait time co 2 emissions and total distance traveled in a given time interval we chose throughput as our reward function for two reasons 
first this reward is more directly tied to the action taken meaning that the learning algorithm has to do less work separating the signal from the noise in the rewards 
second the reward varies more linearly with traffic flow than other metrics which makes it easier to fit sumo does not provide a direct way to calculate throughput so instead we calculated the sum of the speed of cars through the intersection per time step 
this sum approximates a discrete integral of speed over time giving the total distance traveled by the cars through passing through the intersection 
then the total distance traveled divided by the width of the intersection is equal to the number of cars that pass through the intersection 
thus our reward function is proportional to throughput and off by a constant factor of the length of the time step and the width of the intersection 
we formalize each traffic light as an agent that can perform a set of actions 
after each 15 second interval the traffic light chooses which direction to turn green 
actions
for example a traffic light with no left turns may choose from the actions north south green east west green 
we designed the traffic lights in our network to allow left turns so they had two additional actions 
while some approaches use signal durations as actions
our goal is to have each agent i e 
traffic light learn the optimal policy i e 
features
which direction to turn green based on inputs that would be available in the real world 
to this end we give our model three types of features 1 
sensor data sumo has the capacity to simulate induction loop sensors 
in the real world these loops are generally installed under streets to provide nearby traffic lights with information about cars passing above them 
in our simulation we placed induction loops in each lane before every traffic lights for a total of 12 induction loops per traffic light 
these induction loops inform their respective traffic lights with the number of cars that have passed over them in the last time step along with the average speed of these cars 
we also provide each stoplight with the previous five time steps worth of sensor information 
we provide each stop light with the previous five time steps worth of its phases 
a phase in this context refers to the specific permutation of lights colors for each lane in the intersection 
stoplight history 
each phase is represented by a number in the feature array 
in order for each traffic light agent to learn to coordinate with the other agents we provide each agent with the features given to the adjacent stoplights as in in total we represent each state with 420 features due to our backpropagation algorithm it was important to normalize features to the same range 
the gradient value is multiplied by the feature value during backpropagation so features with high values get high weights during training 
features from adjacent traffic lights 
and since they had high values to begin with their influence is increased quadratically in the final regression value 
to figure out the optimal hyperparameters for the qlearning algorithm we used a combination of random and manual search 
we ran a smaller number of training and testing iterations to achieve acceptable models and then extracted the average number of cars waiting as our metric for comparison the discount factor parameter in q learning had no significant effect on results 
parameters
our learning rate and regularization were fairly standard 
for linear regression we used 10 2 and 10 2 
for our neural network we used 10 4 and 0 1 
if we decreased regularization or increased learning rate by too much our function values exploded due to the recursive nature of the q function in q learning it is important to balance the need to explore the state space with the need to choose good actions 
for this reason we used an greedy algorithm which chose the highest q action most of the time but occasionally chose a random action 
we found that too many random actions could cause disastrous results across the grid including gridlock making it very difficult for our algorithm to learn well 
to address this we used the lqf algorithm described below to choose a heuristically good action some fraction of the time instead of the random action 
this gave our algorithm a warm start and helped prevent gridlock 
we found that choosing the heuristic action instead of a random action 50 of the time worked best we also realized that we might get better results if decreased over time reflecting the fact that the algorithm needs to learn as it converges 
to this end we incorporated a parameter denoting the half life of the epsilon parameter so that it would decay exponentially over time 
we found that setting the half life to 200 time steps worked best 
we implemented three other algorithms for comparison against our q learning approach to traffic optimization 1 
short cycle changes the phase every 15 seconds in a round robin fashion 2 
baseline
long cycle change the phase every 45 seconds in a round robin fashion 3 
longest queue first lqf lqf chooses to let the direction with the highest number of cars be green 
previous research
we trained q learning on an episode of 1000 time steps using the greedy approach as described above and then used the parameters to test on a new episode choosing the action with the highest q value every time figure 2 
average number of cars waiting at stoplights during the simulation 
all differences are highly significant as shown in
we believe that our linear regression algorithm performed so well because the hypothesis class of our features and ac tions is expressive and tends to represent many common control paradigms for traffic lights 
cycles can be represented with the previous action features 
for example if we want to switch from action 1 to action 2 after four time steps then action 2 can have a high weight for performed action 1 four time steps ago 
actuated systems that respond to arriving cars can be represented by high weights on the sensor features 
lastly networks that coordinate to let runs of cars through can be represented by our network as well by incorporating previous action features from neighboring lights 
looking at our feature weights and our simulation results we saw all of these kinds of learning taking place together our linear hypothesis class was incapable of fully representing lqf however 
estimating the number of incoming cars between the sensor and the light is simple 
estimating the number of cars beyond the sensor can only be done in expectation using features for neighboring lights and sensors 
the fact that q learning can learn the number of cars arriving on average explains why q learning and lqf had comparable performance for average waiting time 
and the fact that this estimation is noisy explains why we see more emissions in the q learning scenario meaning that the cars stop and start more frequently as the light makes small errors 
however lqf should be representable as a general function of our features if not a linear one 
for this reason we implemented a neural network model 
our neural network model performed worse despite having a more expressive hypothesis class 
in fact because our neural network used a relu activation function the hypothesis class of linear regression was a subset of that of our neural network 
this means that under perfect training our neural network can never do worse than a linear regression 
clearly we were not training our neural net work perfectly 
since our hypothesis class was larger the neural network had lower bias but higher variance 
since we are using a simulator we have access to infinite training data so in theory more iterations and a lower learning rate would solve these issues 
however these techniques had only modest gains for us suggesting that the changes needed to match the performance of linear regression are drastic overall our results show that reinforcement learning is effective at learning good traffic light policies 
our algorithm was able to perform as well as lqf which had more information about the system than any real world algorithm would have available 
though not the optimal policy it is a high bar to meet and suggests that q learning can be a powerful technique one of the key takeaways of this work was that domain knowledge can be used to confine the hypothesis class to a set of more reasonable policies 
we are not the first to come to this realization 
algorithms that are now standard for traffic control choose over a library of possible light timings q learning shows promising results and model free systems like q learning can use the power of machine learning to discover trends that are overlooked by the heuristics and approximations of explicit optimization algorithms 
a reinforcement learning system has the potential to provide adaptive control and coordination theoretically matching the current state of the art 
on top of this multi agent reinforcement learning is a distributed technique which gives it fault tolerance as well as the potential to scale up to a larger network 
for all of these reasons we believe that reinforcement learning is a promising paradigm for traffic control 
we thank our cs 325 classmate alex tamkin atamkin stanford edu for his contributions to this project 
we also thank the sumo team for their well documented traffic simulation software suite 
we collected over 130 data sets spanning about 8 years quarterly for each company in the standard and poor 500 s p 500 and the russell 2000 
the s p 500 consists of the 500 largest companies by market capitalization while the russell 2000 consists of 2000 small companies 
ii datasets
we were curious how our methods would perform between the two datasets 
however we ended up abandoning the russell 2000 due to the large amount of missing data relative to the s p companies 
the data took the form x i y i where x i is a matrix of features in r 32 133 for company i 
the rows of x i represented time the columns represented the features 
a data processing
the label for company i was given by y i in 1 2 3 4 5 32 
rows corresponded to time in 2006q2 2015q4 we thinned the original dataset including only features and companies with an adequate number of observations 
we prioritized keeping companies in the dataset over features 
from an initial 133 features and 505 companies we narrowed the dataset to 499 companies and 100 predictors which included balance sheet items like cash stock and goodwill income statement items like revenue and several macroeconomic indicators e g gdp growth 10 year treasury rate we added percentage change over the previous period as a feature for each existing feature 
we anticipated that in some cases the percentage change from the previous quarter would be more significant than the quanitity in a single quarter we made an important modeling decision to treat each feature as a random variable and assumed that this was iid across companies 
using this assumption we restructured the data such that instead of a sequence of matrices 
knowing that linearly dependent features would be removed by our feature selection procedure later on we chose the maximum number of lags that a feature demonstrated for any company and included those lags as features missing values were a significant problem in our dataset 
in order to attempt to preserve the time information before restructuring in this way we applied a simple autoregressive model to each data series on each company to find the number of significant lags 
we chose to approach the problem with 0 1 dummy variables 
we replaced missing data with an arbitrary value 0 and included a dummy feature where each element corresponding to a missing value took a value of 1 and was 0 otherwise 
we found that this method conformed with our lack of an opinion with respect to missing values 
we randomly divided our data samples into training 80 and test sets 20 
we trained several supervised learning models on the training data using 10 fold cross validation to evaluate model performance 
b model training and evaluation
we also performed parameter tuning using 10 fold cv to select the optimum performing metrics for each model algorithm 
we then fit our model on the entire training set and predicted responses on the test set 
we divide our learning tasks into two parts as seen in specif icity t n f p t n 3 where t p true positive rate t n true negative rate f p false positive rate f n false negative rate for regression we use root mean squared error rmse to evaluate modelswe briefly describe some of the methods used 1 support vector machines we used support vector machines for classification and support vector regression an extension of svm where the response is continuous instead of binary for regression 
svm s are quite effective in highdimensional spaces and fairly versatile in terms of choosing different kernels e g linear polynomial radial basis functions 
we select the optimum models by tuning parameters such as cost gamma and kernels for classification since we have k 2 classes we used the one versus all classification approach 
here we fit k svms and each time we compare one of the k classes to the remaining k 1 classes 
for example if 0k 1k pk are the parameters that result from fitting an svm comparing the kth class coded as 1 to the others coded as 1 
if x is any test observation then svm assigns this observation to the class for which 0k 1k x 1 2k x 2 pk x p is the highest as this corresponds to a higher likelihood that the test observation belongs to the kth class rather than any of the other classes 
in case of support vector regression the method seeks coefficients that minimize an epsilon loss function where only residuals y i 0 1 x i1 p x ip larger in absolute value than some positive constant contribute to the loss function 2 k nearest neighbors k nearest neighbor is a nonparametric method where given a positive integer k and a test observation x 0 knn first identifies k points in the training data that are closest to x 0 represented as n 0 
knn then estimates the conditional probability of class j as the fraction of the number of points in n 0 whose response values equal j 
then knn classifies the test observation x 0 to the class with the highest probability by applying bayes rule 
knn is useful as the cost of the learning process is not large and no assumptions need to be made about the characteristics of the concepts 
however in our case knn is computationally expensive since number of features is large 
we used 10 fold cv to select number of neighbors 
in knn regression instead of combining discrete predictions of k neighbors we combine continuous predictions 
these predictions are combined by averaging 3 lasso ridge based methods in ridge method a linear model is fit by penalizing the coefficients using l2 norm by virtue of a tuning parameter 
as approaches a large value the 2nd term in the eqn 6 called the shrinkage penalty grows and the coefficient estimates approach 0 
this forces some of the coefficients towards zero 
the ridge method will therefore include all the p predictors in the model which is a disadvantage if we have a large number of predictors 
in lasso method the coefficients are penalized using l1 norm by eqn 7 
as becomes large enough some of the coefficients will actually shrink to zero 
hence lasso performs variable selection and the models are easier to interpret as it will give rise to a sparse model 
we select the value of using 10 fold cv 4 tree methods tree based methods involve separating the features into a number of regions 
we then estimate the mean of training samples in the region for which a data point which we want to predict belongs to 
basic trees are simplistic and do not generalize very well 
hence we evaluate approaches such as boosting bagging and random forests 
bagging takes a subset of samples of the training observations and fits several trees 
the predicted values on are then averaged which helps to reduce the high variance in a basic tree 
random forests gives a slight improvement over bagging by decorrelating the trees by randomly sampling m predictors from the full set of p predictors as split candidates every time a split is considered 
boosting involves sequentially growing trees where each tree is grown by using information from previous trees 
we optimize parameters of these tree based methods by estimating the out of bag error estimates 
as we are dealing with k 2 classes multinomial we used grouped lasso penalty on all k coefficients of particular variables 
this shrinks them to zero or non zero together 
for regression we started out by evaluating ordinary least squares model for predicting stock prices which had a test rmse of 41 1 
to improve performance we applied shrinkage overall with respect to classification most of our models mildly improved over the naive benchmark achieving accuracy of around 86 compared with 82 the naive benchmark 
b regression
it would be interesting to see if the information captured by this improvement would be valuable for security selection 
a simulation and related security selection procedure could give answers to this with respect to regression we achieved great improvements over simple linear regression by applying different approaches 
we managed to get rmse from 41 1 to 27 0 in the best model via boosting some ways in which we might improve our error or extend analysis we might include polynomial terms nonlinear transformations or interaction terms in the models 
it might be more interesting to predict analyst labels further in the future because they are less related to the previous label it might be interesting to model the analyst recommendations as markov processes and calculate metrics such as average holding time 
how to search articles effectively is significantly important to researchers in academia 
currently researchers use search engines like google scholar and search by keywords eg 
machine learning topic modeling k means etc 
the typical search results are large amount of articles which match the keywords exactly but are on many different topics 
it is very time and effort consuming to read through the papers manually and select out desirable ones 
we propose a solution to search papers by topic 
we apply the vector space model and tf idf to vectorize and model documents 
then we use k means algorithm and latent dirichlet allocation lda method to train on a large document set and analyze every paper in it to generate its distribution over topics 
the algorithm recommend papers to reader by analyzing whatever the reader has read and compare with the distribution of all papers in the training set 
the papers with most similar distributions are those recommended to the user 
in this way we realize searching in the database by topics rather than keywords 
we use 1298 papers from past cs 229 course project reports as data source 
without any title or keywords matching experimental results have demonstrated that our method can successfully extract the topics beneath the words of an article and recommend closely related ones 
in academia a proper method to effectively extract topics from papers and search for papers with similar topics have long been desired 
currently people are familiar with searching by keywords using tools like google scholar 
however these searching results are merely based on the matching of input keywords 
but we want to search by the semantic contents which is beyond keyword matching 
even by skimming condensed abstract section substantial time will be consumed to acquire the topics on literature review 
especially considering the vast volume of literature on internet it is almost impossible to retrieve accurate topics and find best matching papers manually 
the problem we are to address is that given a user input a paragraph of interest or several journal papers which we call reading list how to create a recommendation list of papers to help the user decide a clear order of pursuing and provide him her with abundant information as well as a rough depiction of the framework of our proposed method we will first need a collection with enough literatures ideally across many different areas 
it would be the best if we have a literature database like sciencedirect 
objectives and outline of our approach
then we process on the papers in the collection including format converting word trimming high and low frequency word filtering and non english word removal to obtain a training set formed by word frequency statistics 
then we will calculate topic distribution for each document given the number of topics arbitrarily selected 
when making recommendations the algorithm analyze the reading list and compute its topic distribution 
then the algorithm judge the similarity between this distribution and every document s distribution in the training set and recommend the most similar ones 
our method can be used as an effective way of paper recommendation 
first by topic modeling we will improve both the efficiency and accuracy of paper searching towards a particular topic 
project benefits
second by conducting topic search it is easier to find important literature bridging two different academic fields 
in our project we use an advanced topic modeling method to help researchers explore and browse large collection of archives 
historically topic modeling can be used to improve the accuracy of classification by their contextual information 
background and literature review
for example guo et al 
in recent years some research concerning topic modeling focused on how to train the dataset in the environment of twitter previous research and other related works will be served as reference and comparison of our research work 
we incorporated some ideas applications and structures from them as a reference 
data source currently used in the research is previous cs229 course project reports 
we are using all the project reports from year 2011 to 2015 and the number of articles is 1298 
data and preprocessing
ideally online open source research papers are our best source of data but let s restrict the data source to course reports for testing purposes the first step is to convert formatted pdfs to plain txt files 
the articles are published on course website in pdf format as are most of the other research literatures 
they are converted to program readable plain txt files with no information lost or distorted using an online tool http pdftotext com 
we then remove all numbers signs symbols and non english letters and convert all english letters to lowercase 
next porter stemming algorithm is applied to remove the morphological endings of the words in the documents 
for example make makes making are stemmed to mak and surprise surprises surprising surprised surprisingly to surpris 
the last step of text processing is to remove trivial stop words like the and was we etc 
from the data 
after all these preprocessing steps we finally have the non trivial and sequential as in the original article english words written in plain txt files denote the document set bywe apply the vector space model on all the documents 
every unique stemmed word in a document is treated as a dimension 
the magnitude of vector component in that dimension is the total number of word appearance in that document 
repeat this process for all the documents and we generate the text feature matrix x 
the elements of matrix x isnow every column in matrix x represents a feature for learning algorithms to learn later 
we then apply threshold on every element of x to filter out words like names abbreviations etc by testing we find 3 is a good choice for threshold 
a thing worth mentioning here is after the thresholding many columns in matrix x will become all zeros which means the corresponding feature is considered indiscriminative 
we remove all these zeros columns for the sake of both code efficiency and features meaningness 
before finally send x to learning algorithm we further process matrix x using the concept of inverse document frequency to account for the intuition that a word appearing in too many incidents should be less discriminative that a word that appears only in few documents regardless of their actually frequency in the individual documents the weight ln d 1 d w j d emphasized more the words appearing in less documents 
1 is added to avoid zero denominator 
as stated before the final training set is presented to the training algorithm as the matrix x with rows representing training samples and columns representing the text features in the training part we applied both k means and latent dirichlet allocation lda to perform unsupervised learning unsupervised learning is the natural choice here because the lack of label for each sample i e 
topic classification of each course report at the end of training process both k means and lda will analyze all reports and calculate weight distributions over topics for each of them 
training algorithms
the weight distributions indicate the likelihood of a document belonging to each topic 
the similarity between these distributions will be our criterion for recommendation in the following discussion topic and cluster essentially mean the same thing 
we set the topic cluster number k to be 20 
in our case the data sent to k means algorithm is x with 1298 rows and 39588 columns 
we first trained the data using k means clustering method 
we used cosine distance in our algorithm 
denote the row vectors of x by r 1 r 2 r n where d i d j are i th and j th document 
cosine distance or cosine similarity measures the cosine of angle between two vector 
it can be easily applied to any high dimension space 
unlike euclidean distance it measures the difference in orientation rather than magnitude 
cosine distance is suitable for text vectors because scaling a text vectors should not change the topic of this document after convergence k means assigns each report to a cluster 
the next step is to analyze the weight distribution over all topics for each report 
in our vector model for documents each vector in high dimension represents a document 
the intuitive idea is that if a vector is surrounded by many vectors belonging to cluster i then this vector should have a high weight on cluster i 
in practice we artificially set a threshold and define hypercone r r i to be the vicinity of r i 
we admit there is some arbitrariness in choosing 
we count the number of vectors assigned to each topic in the vicinity of r i 
the number count can be presented as n 1 n 2 n k 
after normalization i e 
divided by the total number n of vectors in the vicinity we have the weight distribution of document r i over all topics 
the distribution is n 1 n n 2 n n k n 
denote the distribution matrix for all vectors by t kmeans 
lda calculate the weight distribution using a different method 
we apply the matlab lda programs offered by http psiexp ss uci edu research programs data toolbox htm 
the lda algorithm has the exact same input as k means i e 
the matrix x m by n mentioned above 
each row of x is a sample and each column is a word the algorithm outputs another matrix z n by k with its rows represent words and columns represent topic 
z i j is a number between 0 1 and it indicates how likely word i w i belongs to topic j j 0 k 
with matrix x and z we can calculate the weight distribution over topics for all documents 
first we need to normalize x by row and getx 
then t lda xz denote the rows in matrix t kmeans and t lda by wd i 
as mentioned in the motivation part we want to extract reading interest and topic preference from the reports the reader has already read and recommend according to similarity between weight distribution over topics 
in practice given the reading list containing papers the reader has read we can calculate the word frequency vector r for k means algorithm we put r in the high dimension space and count the number of vectors in each topic in its vicinity 
recommendation algorithm
the procedure is the same as above 
for lda algorithm we normalize r by its total number of words and get r then rz is the weight distribution over topics denote the weight distribution of this new reading list by wd 
by comparing the new distribution wd to row vectors wd i in matrix t kmeans and t lda we are able to rank the reports in the training set by their similarity to wd and recommend those with highest similarity to judge the similarity between wd and wd i we use both kl divergence and cosine distance 
the actual algorithm used all 1298 reports from 2011 2015 and the cluster number was set to 20 
the scale of this set is too large to be clearly presented in the final report so we run a clustering on 2015 reports 
k means clusters visualization
by doing so we can take advantage of the classification available on the course website http cs229 stanford edu projects2015 html and compare them with clustering results 
note the course website only provide classification for 2015 reports 
after all the preprocessing matrix x for 2015 reports has 273 rows and 17202 columns 
we tried various cluster numbers ranging from 1 to 15 
experiments show that cluster number between 8 to 10 give the best results 
we judge the clustering results by extracting the most frequency words in each cluster that are not present in other clusters and manually determining if they are closely related 
it turned out that all the 2015 project reports are already classified into 9 groups according to the course website 
thus we clustered the reports into 9 categories and compared the number of samples in each cluster with results on the website 
they are shown in
in general the training is satisfying and clustering is rather obvious 
all 273 samples are automatically labelled by k means algorithm 
clusters
labeling can be manually done in future for higher precision 
we also plotted the high dimension vectors in 3d space after dimension reduction using pca algorithm 
the clusters in 3d space are shown in
visualization
we provide recommended papers based on the reading history of users 
two pieces of articles are randomly selected from the dataset as user s reading list testing data 
recommendation
by using methodology stated in methodology section we generated corresponding reading list papers compound distribution for both k means model and lda model 
the recommendation algorithm returned three papers for both of the models as shown in we can expect just from the title that the two input papers are related to image detection or classification using super vised machine learning approaches 
and the returned result from both models share similar topics indicated by keywords in titles like traffic sign classification object classification pedestrian detection svm identification images of faces etc 
noticing that one of the recommended papers is named equation to latex this result demonstrated the distinct advantage of our model over regular search engines it discovers the topic of documents without replying too much on keywords like image machine learning or detection 
this paper actually describes how to detect equations in pdf documents and convert them to l a t e x codes by machine learning approaches which is a good reference for the user who intends to inquire topics like image processing with machine learning methods 
to conclude k means and lda model respectively on distribution over topics of documents and words 
the recommendation algorithm following each of them recommends different results but they all satisfy our expectations 
our algorithms successfully achieve our goal of topic extraction and article recommendation 
the three of us would like to thank professor duchi for teaching this great class and offering the opportunity of working on a project as a team 
we also want to thank all the tas who have always been helpful in both the project and homeworks 
last but not least we thank all our classmates who we have sought help from 
the hla human leukocyte antigen is a family of receptor proteins found on the outside of nearly every cell in the human body 
they are one of the most important components of the innate immune system 
they bind randomly fragmented peptides from discarded proteins floating around the interior of the cell then pull them out to the cell surface to present them to the immune system 
generally hla will bind peptides representing the repertoire of human proteins 
but if a virus has infected a cell hla will present peptides from viral proteins to the surface of the cell for other cells to detect and trigger an immune response 
in this way the hla serve as a window for the inner contents of a cell to be seen without other cells having to invasively sample it from within 
a cartoon depicting the function of is shown in developments in mass spectrometry have allowed biologists to sequence these peptide ligands that can bind to a single hla allele in a healthy human cell line 
older assays could only offer a dissociation constant or a metric of how well a ligand is thought to an hla without offering direct proof of whether a ligand was actually bound by hla 
a newer method has been developed that definitively shows binding with an hla allele 
however it does not come with dissociation constants and we cannot sequence the ligands that did not bind earlier machine learning methods for peptides based on these older experiments relied on a threshold for the dissociation constant and made a two class problem by labeling peptide ligands as good binders and poor binders non binders 
however this new dataset has only has good binders 
unfortunately these datasets cannot complement each other as they relied on different experimental protocols my two goals in this project were to model the distribution of peptides that bind to each hla and compare them and use this data to predict whether untested peptides will bind or not 
my unpublished experimental dataset has been given to me by curtis mcmurtrey and william hildebrand 
there are peptide sequences from 5 different hla alleles 
in this experiment i will be using just the peptides that are 9 amino acids long as it has been shown that these nonamers are the likely canonical peptides that hla are best designed to fit 
it is important to note that all of these sequences were shown to bind there is no experimental data for peptides that do not bind 
a brief summary of counts of sequences is shown in rather than use sparse encodings of these peptide sequences i e 
create 20 binary variables for every 9 positions of the amino acid for the presence of one of the 20 amino acids at every position i opted to convert the sequences into physiochemical properties creating a numerical pseudo continuous range of values 
the four properties i used were molecular weight surface area isoelectric point and hydrophobicity index thus converting every nonamer string into 36 features 
because the data is a one class set i sought a generative approach by building a model that would best explain the sequences 
i settled on using multivariate kernel density estimation for this purpose 
kernel density estimation
it addresses several quirks with the data including its irregular multivariate distribution sparseness in its dimensions and its high covariation among not only just the 4 physiochemical properties assigned to each position but also across positions 
i could then later fit new peptides into this model the likelihood of binding with an hla however the significant drawback of this approach is its complexity 
because creating a density in 36 dimensions is prohibitively costly i decided to first reduce the dimensionality of the data 
i tried two approaches principal components analysis and autoencoders pca kde pca is a simple linear transformation of the original variables that also maximizes the total variance in its first few components 
so the first few principal components can be used as a substitute for the original dataset 
this method s downsides are that pca cannot capture nonlinear trends and it misses out on the lesser components which while less important represent unique aspects of the data after pca i chose the first five 5 principal components of the data use as my features for the multivariate kde which accounted for approximately 40 60 of the total variance depending on the hla allele to verify the results of this method i used 10 fold cross validation for the dataset in addition to using 1000 randomly generated peptide sequences 
they were then plugged back into the kde after conversion with the original loading matrix to obtain likelihood values 
any conclusions drawn from their model fit should be taken cautiously as some of these random sequences may very possible bind to the hla in reality at each iteration of the cross validation i used 90 of the sequences pcs to construct the kde model then the reamining 10 and the fake peptides to test the model 
the bandwidth matrix of the kde was optimized using the smoothed asymptotic mean squared error samse 
instead of using a pca and choosing the top 5 principal components to use for the kde an autoencoder with one hidden layer with 5 nodes was first constructed using the training set of 90 of the true peptides 
the autoen coders were manually tried under a variety of parameters to assure optimal information retention and feature diversity among hidden nodes 
similar to the pca kde approach 90 of the sequences pcs to construct the kde model then the reamining 10 and the fake peptides to test the model 
to visualize the overall predictive performance i created roc curves to visualize and compare the accuracy of these methods 
the roc curves in addition to their auc values for a single k folds iteration is shown in for each of the k folds iterations for both methods an auc score was calculated 
kde performance
these auc scores were then averaged over the ten iterations 
these values are plotted in sampled kullback leibler divergences the two sets of kde likelihood distributions were also used to study the difference among hla probability distributions 
rather than use the empirical distribution of 20 9 possible sequences i used randomly generated sequences to obtain sampled kullback leibler divergences 
the matrix of kl divergences is shown in prediction of viral proteome binding to test the accuracy of all four predictions methods on an actual dataset i first chose the optimal likelihood threshold using all available data and chose optimal values of for the svm models with all the data to attain the best overall accuracy 
then all possible 9mers from the swine flu virus strain were tested 
the sensitivity and accuracy of these predictions are shown in in my kde approaches pca approach tends to fare better than the autoencoder approach across the board 
this may largely be because of the small sample size of this dataset 
pca tends to be more robust in this situation the svm models to predictably poorly as with the experimental data and random sequences in the sine flu virus peptide prediction 
this may because of the high dimensionality of the dataset and small sample size as well pca kde in general appears to perform the best in terms of prediction capability 
in fact it is very close to the accuracy of predictions run through one of the leading peptide prediction algorithms netmhc 
future directions i will try to use the set of sequences from the larger databses of peptides to see if binding predictions can be improved 
these data however as mentioned in the introduction use a different method that cannot explicitly tell whether a peptide had bound or not 
at the very least recreating models with the larger database can shed light on whether the newer deep ligand sequencing is on par with the older information i am also looking into the accuracy of other simpler baseline methods by using a combination of dimension reduction and feature selection with svm or random forests 
hildebrand for the sequencing data and information regarding the technicalities of deep ligand sequencing i also thank the parham and bustamante labs in particular hugo hilton for regular discussions on this project and elena sorokin for suggestions on machine learning approaches lastly i d like to thank john duchi and the tas of stat 229 in particular claire for teaching this incredibly challenging and rewarding course 
acknowledgements i would like to thank curtis mcmurtrey and william
signal detection in finance remains a difficult topic in machine learning especially for practical applications like price prediction 
successive points in a time series are not necessarily independent and identically distributed so predictions of a dependent variable s future value need to take into account past values as well as independent variables 
furthermore financial asset returns often non normal and display non ergodic patterns which can lead to overfitting when standard assumptions are applied 
the signals which are easier to detect often are useless as markets drive most to equilibrium price such that trading on them becomes unprofitable 
often signal is drawn from data about the underlying assets 
for mortgages we look at characteristics of borrowers for companies debt to equity ratio 
hundreds of analysts are paid to develop theories about individual companies and trade on them with this in mind i attempt to use options data to predict stock returns 
an option is a contract sold by one party to another offering the buyer the right to buy or sell an underlying asset at an agreed upon price during a certain period of time 
the right to sell is known as a call and the right to buy a put 
the agreed upon price is the strike k to be distinguished from the price of the options contract itself v 
options can be thought of as bets on the underlying stock price at a given point in the future 
the intuition behind this study is that certain aspects of options market behavior could reflect movements of informed investors 
relevant options features relate to the puts and calls traded and implied volatility 
implied volatility is the perceived future volatility of the underlying a key input into options pricing models most famously black scholes determining what contracts are worth 
the data analyzed for this report consists of the stock prices and options data of 57 healthcare companies over the period from 1 3 2007 to 12 4 2014 
from the stocks data time series of returns was calculated for each company using the formula the returns time series can be smoothed to capture the broader trends in stock behavior 
ii data
this can be done by taking the simple moving average sma over a given interval of days the exponential moving average ema where the later days in interval are given more weight and a gaussian moving average gma where the days in the middle of the interval have highest weight the raw options data comprises 39 features relating to the volume of call and put contracts traded each day and various parameters derived from black scholes 
the data was further split into total at the money in themoney or on the money contracts and the relative price of puts to calls characterized by the put call parity deviation pcpdev 
it also includes variables associated with the implied volatility its spread and skew adding up to a total of 39 features 
given the large number of potential predictors many of them highly correlated with each other the question we seek to answer is which features are most important for predicting the future price of the underlying stock or equivalently future returns and what is the best machine learning model for doing this 
as a prelude to building the learning model the data was studied through visual plots to get a sense for any obvious correlations between the returns data the outcome variable y and individual options features one indicator that traders have used to gauge market direction is the put to call ratio or pcr 
this is obtained by dividing the volume of puts traded by calls traded on a given day 
iii linear regression
typically traders buy stock options to hedge their underlying equity positions lending credence to the notion that pcr might indicate market sentiment which in turn might predict market performance 
3d scatter plots of returns two other options features on a given day were made and the points color coded according to whether the returns on the next day were positive or negative 
an approach based on thresholding both the pcr and the fractional change in pcr was next attempted 
this is based on the idea that correlations in movement of the market and pcr happen mainly when the pcr breaks above or below certain levels that indicate whether the market is bullish or bearish 
this can be seen in
the use of thresholds suggests the non linear classification by decision stumps essentially a one level decision tree which predicts an outcome based on individual stumps based on single features are however unlikely to give much better results than chance 
the algorithm can be called a weak learner and we look for some way of combining multiple weak hypotheses to build a much strong classifier the ensemble learning method we implement is adaptive boosting adaboost for which its inventors schapire and freund won the godel prize in 2003 
v decision stumps boosting
adaboost takes as inputs a weak learner algorithm and a distribution of probabilities p i over the training data 
it iterates over the hypothesis space of the learner choosing the hypothesis j x giving the lowest prediction error on the weighted training data 
with each iteration p i are updated to emphasize examples that were wrongly classified 
the weights j on the t learners chosen up till current iteration t are updated via coordinate descent to minimize after t iterations the model is based on the weighted sum of predictions of the t learners chosen in this case the weak learner is decision stumps and the hypothesis space includes all possible features and thresholds for each feature 
the following features were included in the feature space taking into account the observations from the previous sections a ema returns days t 1 t 2 b ema put to call ratio days t 1 t 2 t 3 c put call parity deviance days t 1 t 2 d implied volatility days t 1 t 2 e implied volatility spread skew days t 1 t 2the outcome variable predicted is the t 26 day ema return which can be obtained recursively after initializing the very first interval ema0 using note how we are able to recover a predicted raw value of the return for each day rt once we predict emat 
10 fold cross validation was performed where data from 5 out of the 52 companies was set aside as the test set each time 
the number of boosting iterations t was chosen to be 100 
this was repeated for several different combinations of features with the aim of finding out how predictive different features are 
the results and error plots obtained from the experiments are summarized in the results section vi 
results 1 
ema returns 2 mpiv spread civ spread 3 cpcpdev mppcpdev 2 and 3 both relate to differences between calls and puts the difference in implied volatility in the case of 2 and contract price in the case of 3 
as such it s not surprising that they contain information about the directionality of the underlying 
the prefix c and mp refer to different methods of calculating the each feature from the table we see that the including past ema returns improves the prediction error dramatically to 11 5 
this is not surprising given that we would expect a given day s return to depend a lot on the most recent trend especially after random fluctuations have been smoothed to some degree 
as comparison the same boosting algorithm was also applied to predicting raw returns from both raw and smooth returns and the error rate averaged 48 5 basically not much better than random see bottom left cell in interestingly adding feature sets b c d e did not improve prediction performance once returns were included as a variable 
in fact from the learning curves in here i briefly suggest another method that builds on the results from adaboost 
the idea is to use the margin based on which the outcome prediction 1 was made as the independent variable in a linear regression to predict the magnitude of the returns 
that is this would only work if a more positive boosting margin which we would interpret as a higher probability of a 1 label also correlates with larger positive magnitude and vice versa to see how feasible this is the ema returns at time t were plotted against the un normalized boosting margin as seen in as a sanity check we attempt to convert the predicted ema return back into a raw return described at the end of section v the mean squared error of this prediction over the whole dataset was then calculated by comparing it to the actual return values 
this came up to 7 18e 06 
as a benchmark an elastic net regression with 0 5 has mse ranging from 6 66e 06 to 1 82e 05 as the l1 2 norm constraint is tightened 
in other words this method of regressing on the boosting margin does not seem to be significantly more inaccurate than the most accurate elastic net regression 
finally these raw predicted returns were converted to binary labels and compared with the actual returns labels 
the error rate was 44 4 lower than the 48 5 obtained by prediction using boosting directly on the raw data 
while the regression model clearly needs more work and rigorous testing this is a promising start 
future work could focus on refining the classification model to improve performance for example by incorporating local weighting into the probability distribution assigned to the data in adaboost 
another possibility is to use multi level decision trees as the base weak learning algorithm instead of just decision stumps going beyond methodology the feature space could also be expanded to combine the information present in options data with other variables that are known to be relevant 
vii conclusion
ema smoothing could be tried over different intervals of time to find the optimal length 
models could be built attempting to forecast returns further into the future than just one or two days ahead 
i am very grateful to steven glinert for proposing the original research question patiently explaining finance concepts that i was new to helping acquire the data set used and providing invaluable advice over the course of the project 
viii acknowledgements
recognizing human actions is a popular area of interest due to its many potential applications but it is still in its infancy 
successful human action recognition would directly benefit data analysis for large scale image indexing scene analysis for human computer interactions and robotics and object recognition and detection 
this is more difficult than object recognition due to variability in real world environments human poses and interactions with objects 
since researches on human action recognition in still images are relatively new we rely on methods for object recognition as basis of our approaches 
in particular we were interested in seeing how convolutional neural networks cnn 1 perform in comparison with past feature selection methods such as bag of words bow 
also we experimented with various supervised and unsupervised classifiers examined our methods properties and effects on our action data set and also pre processed our data set in order to better our results 
in past decades many ideas proposed to solve this problem 
some people put interest on understanding human object reaction 
bourdev et al proposed poselet to recognize human body part and further research on human pose relation 
although those methods have very impressive result hand crafted feature method still can t be very generalized to all purpose 
they all are used for specific goal to conquer that krizhevsky et al 
to fully understand cnn we looked into feature extracted by cnn 
we thought some preprocessing to image will be helpful to human action recognition 
our goal is to recognize human action 
we though background should be irrelevant noise 
to reduce training cost we will use pre trained model 
we will finetune it and change some hyperparameter to improve the prediction 
we utilized caffe python and pycaffe and matlab to create and run our cnn and bow models 
we rented a server with 30gb of harddrive space and 4gb of nvidia gpu memory costing roughly 400 including usage time 
data and setup
due to hardware limitations we had to reduce our data set size so we chose to classify 8 actions out of the stanford40 data set using 1839 images for training and validation for cnn and 456 images for testing 
with such small data set we allocated more images for training which only had 100 images per action for training 
instead we used a train val test ratio of 7 1 5 1 5 
we were at risks of overfitting but we took precautions to prevent overfitting as a default we used the images as given in the data set 
then we applied cropping to our images in two ways one with a tight bound to isolate our subject and nearby objects and one that is 50 larger than our tight bound to capture some background information 
lastly we pre processed images to a color segmentation process using k means 
in general objects in an image can be described by feature descriptors forming a histogram for the image 
a collection of histograms from different images form the bag of words bow model which can be learned by a classifier 
bag of words
during training we used a scale invariant feature transform sift method to extract features then we utilized spatial pyramid matching spm to obtain histograms from increasingly small sub regions 
stacking these histograms together helps us maintain spatial information 
we then used k means method to cluster the final histograms into k code words 
during testing match the histogram of the input image with our bow model 
bow is unaffected by position and orientation of objects in the image and the spm method gives us more spatial information to help us localize objects 
we used caffenet
then we experimented with changing learning rates and hyperparamters for each layer which are kernel size padding amount stride and number of outputs 
hyperparameter tuning involves changing the sizes of the cnn layers creating a very different cnn despite having the same number of layers 
figure 2 caffenet architecture
to study the effect of locality sizes on our results we conducted two tests with the first layer s kernel size being 15 and 7 respectively and different amounts of paddings were used to keep other layers the same 
in a third test we also changed the first layer s kernel size from 11 to 27 then decreased our kernel sizes in the following layers until the 5th layer matches the original 256x13x13 dimension we also created cnn s from scratch using our customdefined layers and hyperparameters 
below is a summary of our three custom models we only show kernel size k since we only adjusted other parameters to suit our new k our custom cnn 1 is a small cnn with 3 layers 
the other two are larger 
the difference between our custom cnn 2 and 3 is that custom cnn 3 has a dropout layer 
this is to prevent our network from overfitting by giving each neuron a 0 5 probability that its activation will become zero in each iteration 
in other words a dropout of data 
this avoids co adaption of units we also ran googlenet for comparison which uses an atypical architecture embedded with inception layers that contain multiple convolutions 
in terms of recognition googlenet is known to yield better results than caffenet but it is more difficult to fine tune so we kept caffenet as our basis 
we used the t sne algorithm to help us visualize the features obtained from the last fc layer of the caffenet in relation to our actual data 
features from this layer is a high dimensional histogram for each image and t sne allows us to cluster these images together in 2d space 
t distributed stochastic neighbor embedding
with t sne we set similarities of high dimensional points distribution q and low dimensional points distribution p as two different joint probabilities where a higher probability indicate similarity 
the cost function is then a kullback leibler divergence of distribution q from p coincidentally since t sne is an unsupervised method to cluster our data we also tested to see how well it classifies our data by applying a k means algorithm on top of t sne 
similar to using the t sne algorithm we extracted activations from the last fully connected layer of our cnn s as features and put them through various classifiers 
we are interested in using features from cnn for image classification problem but skip the softmax layer that caffe uses 
cnn classifier
the second term 1 let us can have margin less than 1 
c control the two goal want to achieve keep 2 small and make margin less than 1 to use this linear svm on our multiclassifier data set we used one vs one comparison 
we first experimented with one vs all method then used one vs one for better results 
we used one versus one for our dataset 
for one versus one method if we have n class there will be n n 1 2 classifier each classifier is for two classes from our dataset 
multi class support vector machine
we are going to solve the following optimization problem each classifier will vote to one class and the most voted class will be final result
additive chi square kernel does normalization to the feature histograms so that spikes in the histograms will not be heavily affect the result 
we used the one vs one comparison 
additive chi square kernel
choose an integer k knn classifier will find the nearest k neighbors of x0from training data 
according to the class of nearest k point it give conditional probability for each class 
k nearest neighbor algorithm
a random forest method is an ensemble method 
it build a series of simple trees which are used to vote for final class 
random forest
for classification rf predict the class that predicted by most trees 
the predictions of the rf will be the average of the predictions by simple trees
we first obtained data from running our data with pre trained weights of caffenet and googlenet 
we obtained these accuracies 
default cnn
top1 accuracy caffenet 0 8223 googlenet 0 8552we then examined some properties of caffenet 
we verified that our model has converged by looking at the 1 st layer weights to verify there s no noise 
testing on training data yielded an accuracy of 0 9833 
this may indicate some overfitting but we believe it is mostly due to the original models doing well 
figure 4 nicely converged 1st layer weights left vs noisy weights right 
this is because using pretrained weights and giving 0 learning rates to some of the weights should provide enough generalization we examined caffenet s first layer s outputs and noticed that while caffeent can capture large features correctly it sometimes recognizes background noise and irrelevant information as key features 
for improvement we believe it would be beneficial to filter out noise and have larger locality of features it becomes difficult gauging the activations in later layers due to the locality of each neuron so it was not used 
based on preliminary results we wanted our cnn to capture larger features and ignore smaller objects or noise 
hence we created our custom cnns as described in the methods section 
custom cnn
none of our custom cnn s matched the default model s accuracy 
this could be we did not have the time to train our models for long enough because we could only run for 20 000 iterations which takes half a day 
but we noticed that the 1 st layer s weights appear to converge nicely so it s also possible that the default caffenet was designed to be the best cnn of its kind 
hyperparameter tuning is we realized an optimization problem of its own 
we noticed that unexpectedly a larger kernel size at the first convolution layer yielded lower accuracies 
we compared the 1 st layer s outputs and noticed that while a larger kernel size does give us larger locality and capture bigger features in the images as intended it is perhaps too broad for our cnn 
cnn
the smaller kernel size on the other hand captures too much detail 
we tried to filter out the background by changing our k size for the k mean cluster but it s not inherently obvious how many codewords to use 
we tried k 200 and k 300 
figure 7 bow features
we saw that for the most part k 200 performed better 
but this may not be optimized since number of code words is heavily related to the properties of images so there is no best way to find k but trial and error like finding cnn hyper parameters 
according to our result k 200 is better so we can deduce that sift doesn t use as many distinct features from our images so that we don t need too many words a more useful takeaway is looking at our results of our cropping 
after we cropped the image based on tight bounding box we saw that accuracy actually dropped 
this is contradictory to our expectation 
we thought that removing background noise would reduce error and improve our result 
however we realized that contextual information is actually important for classification we then expanded our bounding box by 1 5 times to include local background information 
as predicted we saw an improvement in our result 
as shown above if we train svm and other classifiers on top of features extracted by cnn we achieve better results than using cnn alone 
this was surprising since cnn s own accuracy was already high we again thought it may be due to the overfitting issue described in previous section 
so when we use svm for classification we made svm resistant to overfitting by tuning the parameter c although kernel trick perform better than linear svm in bow model we didn t use it on cnn feature because cnn feature is very high dimension 
using kernel on cnn will be time consuming with not a better result 
so we simply use linear svm here for cnn feature we observe that svm knn rf all perform well on our dataset when using cnn features even though in our bow model knn and rf both did badly 
even though cnn is not perfect at extracting features it is much better than bow model which takes in too much noise from the image we saw cnn knn have even higher accuracy after we cropped the image 
we can see that the background is necessary when the action relies on the environment 
some action is highly related to the background like climbing where as some do not like jumping 
if we could recognize the relationship between the background and the action we can achieve better results 
we also tested the classifiers on non trained caffenet 
surprisingly we found svm give a pretty good accuracy 
it is only a little lower than fine tuned feature 
knn and rf are not like svm their accuracy is much lower than fine tuned caffenet feature 
this confirms that caffenet s pretrained model does a very good job at recognizing objects such that when we insert our data set we do not need to train much 
after applying the t sne method we noted that the accuracy was only 0 5203 much like our other classifiers 
what s more interesting though is visualizing our data 
feature examination t sne
we see that images with clearly distinct objects holding an umbrella riding a horse playing guitar etc are more distinguishable 
on the other hand actions that require environmental interactions jumping climbing are not as obvious 
also images taken from afar or from unconventional angles would be harder to cluster 
this could be due to the introduction of background noise or occlusion 
it becomes obvious that pre filtering our data set would be an important step prior to training 
we experimented with and validated many methods and techniques in our project 
the most useful takeaways for future work is that for either supervised or unsupervised learning it is important to include sufficient but not excess background and contextual information prior to training for human action recognition 
the key point is how to select the region from image 
we saw that cropping is a strong tool to use but we cannot crop too much or too little background then we found that knn performs well with fine tuned caffenet model on our dataset 
knn is a very fast calculating model 
for future work we will test and evaluate knn using the whole 40 action dataset in general cnn is a great tool at extracting features from images even though it lacks the ability to distinguish subject object and background similar to bow 
even so it significantly outperforms bow model as we expected from literature 
for small size dataset using svm knn on cnn feature gives even higher accuracy than cnn itself 
we thought that cnn could overfit such small size dataset 
in small size dataset it may be more accurate to combine svm knn with cnn feature 
rhetoric is a crucial factor that citizens use to decide which candidate they vote for 
it affects how citizens perceive candidates on both the conscious and subconscious level therefore it is an important topic to research and characterize 
this is especially the case in the 2016 united states presidential election as the candidates are perceived to have particularly different styles of speaking from trump s brash style to sanders s exasperated tone 
rather than relying on stereotypes and preconceptions we hope to rigorously define these differences by objectively characterizing the candidates speech patterns 
furthermore we can use these quantitative characterizations to actually predict given a new speech transcript which of the candidates is the most likely speaker 
for the reasons just described politicians rhetoric has been an active topic of research for several decades 
one study in sweden was able to classify politicians by gender political affiliation and gender another group was interested in studying the speech patterns of japanese prime ministers a third group at northwestern university examined speeches given in united states senate meanwhile a group from the american enterprise institute utilized a bag of words technique to examine the differences between the most frequently used words by conservative and liberal 2016 presidential candidates during debates
literature review
first we needed to gather text data of candidates speeches 
from a number of online sources including various news publications the candidates respective campaign websites and project vote smart a nonpartisan database of information on candidates for public office 
gathering raw data
essentially we want to reduce a whole speech into a feature vector 
one way to do this is with a bag of words that is represent the speech with a vector the length of the dictionary whose i th element is 1 if the i th word in the dictionary appears in the speech and 0 otherwise 
feature selection
this is a perfectly valid feature vector that would likely make good predictions about which candidate gave a given speech 
however this representation does not reveal anything meaningful about how it made the prediction 
that is we would not be able to examine the parameters and say something significant about the candidates 
the best we could do is to say that for example clinton is more likely to say this particular word than trump which in general is not very interesting instead we want our features to be significant on their own for example how much a candidate talks about the economy or the average word length in a speech 
if we can reduce a speech to a small number of meaningful features and we still have a lot of predictive power this tells us much more than the bag of words approach 
to that end we divide our features into what a candidate talks about and how a candidate talks 
in the former category we chose 9 policy topics that we believe represent important issues as well as issues that differentiate the candidates 
for each topic we generated a list of buzzwords relating to that topic 
for example the list of words corresponding to economy includes economy economics middle class wall street banks mortgage income financial 
then we go through each word in a speech and if that word is a buzzword of a policy that policy is incremented 
in the end for each topic we find the frequency per 100 words with which a candidate mentions a buzzword from that topic s list for the other category we seek to characterize how the candidate speaks regardless of what he or she is speaking about 
this includes features such as mean sentence length and frequency of using conjunctions 
the complete set of features is listed in
we treat a speech and its speaker as a single example 
then the x i r k vector contains the real values for each of the k features in is the frequency of trade related terms in the 10th speech 
data representation
essentially we take a full text transcript and boil it down to a vector in r k 
there are a few options when choosing an algorithm for a multinomial classification problem 
one is logistic regression or softmax regression 
algorithm selection
this method is robust in that it does not make very strong assumptions about the data 
however since it is a discriminative algorithm its parameters do not tell us what a clinton or trump speech looks like on its own but only a relative probability between the two another option is gaussian discriminant analysis gda 
this makes stronger assumptions about the data namely that the data is indeed gaussian 
our feature set contains features with distributions that are distinctly gaussian mean word length number of unique words and some features whose distributions are more poisson frequency of policy buzzwords 
however we feel comfortable approximating every feature as a gaussian and trusting that gda is robust enough to make good predictions this proved to be true in practice another advantage of gda is that it tends to require fewer training examples to learn well 
one of our major limitations is sheer number of speeches a candidate gives as well as the availability of transcripts 
as a result we managed to collect 30 40 speeches per candidate which is not a very large training set 
despite this gda should be able to generate good results 
following the gda strategy we fit a k dimensional gaussian to each of the candidates training data 
to do this we must determine the mean vector for each candidate as well as one covariance matrix that will be used for all candidates 
generating parameters
the parameters are determined by the following intuitive equations where m is the size of the training set 
given a new speech we want to predict which candidate is most likely to have given that speech 
that is however we only know p x y namely the gaussian we can relate the two through bayes rule to get this is how the algorithm makes a prediction 
making a prediction
intuitively we find which candidate s gaussian the new point is most likely to lie on 
there are several results we wanted to obtain from this study the ability to predict which candidate was most likely to have given a speech the ability to compare meaningful statistics about the candidates speech and the ability to classify historical speeches to current candidates 
to make a prediction we pre process the new speech example as we do the training data to create a feature vector 
using this feature vector we determine which candidate s gaussian the new example is most likely to lie on then compare the result to the identity of the known orator 
predictive power
we also measured several interesting results about each candidate s speech patterns 
for example bernie sanders spoke the most about the economy 
comparing candidates
ted cruz spoke the most about religion the constitution and foreign threats 
donald trump spoke the most about immigration and had the shortest mean sentence length 
trump tends to use first person singular tense significantly more than cruz 
the two democrat candidates tend to speak more about manufacturing than the republican candidates 
from this testing we found that some of the speeches our model attributed to ted cruz were barry goldwater s acceptance speech at the 1964 republican national convention as well as president reagan s speech on the evil empire 
among those predicted to be hillary clinton speeches were john f kennedy s moon speech at rice and obama s victory speech on the night of the 2012 general election however interestingly enough bill clinton s speech at the 2012 democratic national convention was predicted to be from bernie sanders 
our project demonstrated the effectiveness of an alternative method to the commonly used bag of words for characterizing speech patterns by representing speeches only as a vector of feature values instead of much larger word vectors 
our method also provides us with much richer characterization as opposed to just prediction capabilities in the future we can add additional features that will allow us to analyze speeches in more depth 
we were limited to features that were fairly simple to extract but with more background in natural language processing we could expand our features to include measures of the sophistication of language used by candidates i e 
candidate 1 speaks at a 7th grade comprehension level whereas candidate 2 speaks at a university comprehension level which tells us how accessible a candidate may be to different demographics 
we could also add features that extract a candidate s position on different issues rather than just how often a candidate speaks about an issue this method has potential to substantially contribute to the data driven aspect of politics and allow voters to compare candidates in a quantitative manner 
in this paper we consider the multi label multi class prediction problem in the context of classifying undergraduate education requirements at stanford university namely given a course s description determining what general education requirements it satisfies 
given the extremely tough challenge of using a small dataset we opt to consider models and make modifications that are able to handle multiple classes and produce multiple labels with very limited training examples 
we present a slightly modified version of naive bayes an application of boostexter and several iterations of linear classification techniques 
then we explore the performance of techniques from deep learning including rnn lstm and gru 
we believe that there is a flaw in the undergraduate graduation requirements if a student makes the interesting choice of fulfilling his her formal reasoning ways requirement with lie algebra as of now he she will not be able to graduate 
clearly the ways requirements are currently not assigned completely and or sufficiently 
for undergraduates stanford requires completion of a series of requirements known as ways of thinking ways of doing ways 
undergraduates must take 11 courses across 8 different ways 
these 11 courses are not fixed and multiple courses can fulfill a given way 
a course can also fulfill multiple ways 
the problem we would like to explore is as follows given a course s description if we know that the course satisfies at least one way can we predict the way s that it satisfies 
this is a multi label multi class problem as the multiple ways represent multiple classes and the multiple ways a course could satisfy represent multiple labels as the ways system has only been recently introduced not all courses have been approved to or marked as satisfying a way 
problem
we believe that this could be of interest as a tool for the registrar and may take steps to make it available to them as part of a maturing ways program in our particular case this problem is extremely challenging in that we have very little data our motivation is to help more data of this form be possible and yet we need to be able to predict multiple classes and labels with limited training knowledge 
to our knowledge no group has worked on this problem with this dataset 
additionally to date multi label multi class problems have not been extremely well researched due to complexity 
there are a few modern standards for said problem class 
schapire and singer developed boostexter 1 an extension of adaboost well studied boosting algorithm 
elisseeff and weston developed a generalized version of svms that minimizes a rank based loss instead of pairwise binary losses
our data will come from stanford s explorecourses which for a given course provides both a full course description and a list of way s that it satisfies 
we were granted access to this dataset 
part of the motivation for studying the multi class multi label classification problem in this setting using only a course description and not necessarily all other tags was that we would be able to explore the problem in a generalizable manner that we could then use for other problems 
in order to create a more general multi class multi label text classification module we restrict our analysis to descriptions using word frequency features only 
generalizability
later we describe some potential improvements from using dataset specific features as mentioned in section 11 
since we were interested in studying the multiclass multi label problem in general adding hand engineered features would decrease the portability of the models and especially the neural models attempt to gather that information as a part of the process 
we have collected the course descriptions and ways of 14336 courses from explorecourses out of these 14336 courses only 1571 unique courses had ways 
when we say unique courses we only took one course from a set of courses with the same description to avoid biasing the data with duplicates these courses collectively satisfy 2085 ways meaning for courses that do satisfy ways they satisfy on average 1 33 ways as the unlabeled courses could qualify for satisfying some way but are currently not marked as such we have not included them in our data and focused only on courses that do have ways then we removed stop words frequent words as they typically do not provide any insight into the text due to their high frequency and presence in every class 
data processing
we also converted all words to lowercase and removed all punctuation 
to assure that all words with the same stem but different suffix were treated similarly as they have the same intention for the most part we also stemmed our data 
the data was not stemmed for the deep learning models in which we do not stem the words since we are using word vectors as inputs 
finally we tokenized the data because our models work on the word level at the moment we then performed a random 90 10 split of the formatted data to constructed a training set and dev test set 
there are two error metrics that we have been using to gauge our performance in a loss function independent manner between classifiers 
we use the first metric only for baseline methods when we have multiple classifiers 
error metrics
the second metric the hamming distance is the metric we will use to ultimately evaluate all classifiers the first the false negative metric is a naive approach that is useful for independent classifier methods but is too simple for the multi label concept 
we train an independent classifier for each way 
for each training example there are 1 or more way labels 
for each given test label we run the relevant classifier 
if the classifier misclassifies the example we increment the error count 
the metric is then given by d n number of misclassified labels number of labelsthe second metric is hamming distance calculated as follows 
take a training example x i 
let a i be the set of true ways of the example and let b i be the set of labeled ways by our classifier s 
then the hamming distance d i h is computed asthe average of these d i h over all training examples is our overall metric by using two metrics we have a complete picture of the performance of our classifiers on a single label and multi label classification test 
in addition the two metric system allows us to dig deeper into the methods that are inherently multi label as opposed to those that use a combination of binary classifiers 
in our baseline we decided to construct models which involve developing a binary classifier for every single way 
this would mean we have 8 classifiers each indicating whether or not a course satisfies a particular way 
baseline models
we would train the classifiers individually 
then at test time we would run each classifier over a course description and determine which ways the course satisfies 
we have constructed three such sets of models 
we constructed a standard linear model using word frequency counts from descriptions as features we were able to achieve 0 0039 false negative metric on a training set and 0 0466 error on a dev test set 
additionally using the aforementioned hamming distance we achieved 0 311 as our error to better understand our performance we performed a finegrained analysis over how each way classifier was doing 
linear classifier
we found that some of the more stem oriented ways such as formal reasoning fr applied quantitative reasoning aqr and scientific method analysis sma had better performance than other ways 
we constructed a naive bayes model from scratch 
to account for the small amount of training data and the skewed proportions of the positive and negative examples we constructed an analog of the negative subsampling method used by mikolov et 
naive bayes 7 3 2 i word2vec style subsampling
al in their loss function for word2vec
we were able to achieve 0 0068 false negative error on a training set and 0 0214 false negative error on a dev test set 
using the hamming distance metric of error we achieved an error of 0 487 
7 3 2 ii results
this in addition to the proceeding figure seems to indicate we have a series of reasonably good single way classifiers but their combined performance is not as well to better understand our performance we performed a finegrained analysis over how each way classifier was doing 
we see that way er and way ed perform exceptionally poorly whereas all the other ways perform reasonably the same 
in addition to the baseline models we leveraged the previous baseline approach using support vector machines svms k x i x j x t i x j and a radial basis function kernel of the formfor the linear kernel we achieved a training error of 0 000 and a test error of 0 823 error here is average hinge loss for the rbf kernel we achieved a training error of 0 000 and a test error of 1 000 
the models we implemented here seem to indicate that svms are a poor model for the multi class multilabel problem 
as further evidence we show the roc curves below left is linear right is rbf 
the macro average roc curve is generated by considering the average of the roc over each class the micro average curve is generated by considering all positives and negatives as a collective single set 
note importantly that for both the macro and micro average roc curves the area under said curves is less than 0 7 and in the micro average case the curve is effectively linear this is a strong indicator that in general the svms are not expected to rank a randomly chosen positive and randomly chosen negative sample any differently i e 
the svm test is effectively random worthless 
work by robles et 
dimensional reduction via principal components analysis
indicates that both for general text classification tasks and for specifically multi label textclassification tasks dimensionality reduction techniques may help reduce model variance and improve overall performance especially in the context of high vocabulary feature size in our case v 9000 
in the onevsrest style models we make the assumption that the classes are all independent 
however this assumption may not be true and inter class dependence may yield important interactions not accounted for by just the descriptions 
class independence assumption
in particular consider the correlation matrix of ways frequencies shown below obtained using graphical lasso since the dataset is fairly small 
we noticed the clear interclass correlations and nonlinearity of features suggesting we try something that can easily capture these nonlinearities 
boostexter is an extension of adaboost that is well suited to multi class multi label learning and text classification we found that boosting methods exhibit the standard behavior of overfitting on our dataset 
boostexter
it seems that we need more data so that we can have better test performance 
at the point where the training and test errors diverge we have a hamming loss of 0 56 
we believe that the other reason boosting doesn t perform well is that there isn t a way to encode prior knowledge of the terms in the text 
as an example kant is a term that would immediately lead one to an ethical reasoning label but our software is unlikely to have learned such an association 
although we restricted ourselves to word frequencies to create a general use tool for this particular dataset there were some features independent of raw text that could have some predictive power 
as an example below we show result of adding the course department as a feature in addition to the first 10 principal components in a linear classifier 
improvement with domain specific features
similar results were seen for other methods as well another feature we could have added was the old db ec or ihum requirement labels but we considered that this was not a useful feature for future data 
it also would make our classifiers highly nonportable to other problems by using only descriptions for multi label multi class classification we focused on experimentation with models that could hopefully apply generally rather than hand engineering features 
as our other models were working on the token level they were not able to necessarily capture the non linear interactions between words 
to handle this we experimented with neural sequential models 
neural sequential models
these models generally take a sequence of inputs and try to somehow capture properties about the sequential nature of the data a general recurrent neural network where the bottom layer of nodes are inputs the middle layer are hidden states and the top layer are output states we experimented with 3 sequential models with keras in all of these models we applied a relu layer followed by a softmax layer on the last output of the sequential model 
the output vector of the softmax layer served as a probability distribution for each of the 8 possible ways and we tuned a parameter that would determine some probability threshold for whether or not a course deserved a particular way label 
we used binary cross entropy loss for our loss function we used word vectors released by stanford that were created using the glove method as inputs to our sequential models for the 3 models experimented with we plot their loss over time it seems that a simple rnn did not change the loss over time one hypothesis for this is a vanishing gradient problem as described by bengio et 
al after seeing the superior performance of the gru we focused on tuning the gru 
one hyperparameter we tuned was the output size of the gru before applying the softmax layer we found that changing the output size at small sizes had little effect on accuracy 
we hypothesize that despite there being many inputs the gru does not utilize too much information and hence does not have a much different accuracy with higher dimensional embeddings 
a high dimensional output however seems to harm performance potentially due to more unnecessary parameters 
we compare the performance of all of our models we see that of all our classfiers the best two were the linear classifier and the gated recurrent network 
the gru performed slightly better than our linear classifier 
while in practice grus tend to typically have superior performance to linear classifiers we believe the dearth of data prevented the gru from learning its plethora of parameters well 
we believe that our most limiting factor was the lack of data available to us 
we believe that with more examples available as they will be over time as the ways program matures our various methods will be able to increase their effectiveness beyond research we also reached out to the registrar for their potential interest in such a tool as the ways program matures 
forward steps
thank you explorecourses for giving access to course data 
we aim to learn hypernymy present in distributed word representations using a deep lstm neural network 
we hypothesize that the semantic information of hypernymy is distributed differently across the components of the hyponym and hypernym vectors for varying examples of hypernymy 
we use an lstm cell with a replacement gate to adjust the state of the network as different examples of hypernymy are presented 
we find that a seven layer lstm model with dropout achieves a test accuracy of 81 4 on the linked hypernyms dataset though further comparison with other models in the literature is necessary to verify the robustness of these results 
in the last several years distributed word vectors have shown the ability to learn semantic and syntactic information without receiving explicit inputs outlining these properties 
particularly both the word2vec how these properties manifest themselves within the word vectors however is not well understood 
this is especially true of paradigmatic linguistic relationships like hypernymy and synonymy 
there is significant linguistic benefit in understanding these complex properties a robust hypernymy model would greatly facilitate automatic taxonomy construction for example and wolter and gylstad have noted that paradigmatic relationships tend to be similar across languages and could be used to improve machine translation models 
we set out to build a deep recurrent neural network to learn a function h h wo that produces the hypernym vector given the input hyponym vector of word w o h wo 
a long short term memory lstm models seem well suited for our task 
we hypothesize that hypernymy and other linguistic relationships which distributed vector models seem to automatically learn are baked directly into the individual components of the vectors 
stacked lstm model
the lstm architecture contains a cell state c t at each given time step t in our case this represents our activation matrix which maps a hyponym vector to its hypernym vector 
the hidden layer then consists of four calculations dubbed the forget input activation and output gates 
at each time step we input the hyponym vector h to into the first cell where i f o c denote the forget input and output and activation gates with their respective bias terms and h t 1 e denotes the predicted hypernym in the previous time step in essence each of these gates calculates which components of the vectors within the cell carry meaningful information and updates the cell state accordingly 
the forget gate scales down the components of the input state c t 1 the input gate scales the components of w t to be added to the cell state the activation gate determines the new component values to be added to the cell state and the output gate yields a vector to scale the predicted hypernym our lstm cell combines the input and forget gates into a replacement gate which only adds the output of the input gate to the cell state in components which were forgotten 
we then update the cell state and calculate the predicted hypernym where denotes element wise multiplication 
we hypothesize that the input forget and output layers should be able to identify the components of the hyponym vectors which best predict hypernymy and update the weights accordingly 
since hypernymy is hierarchal and a single word can have hypernyms on varying levels of generality these components could change from example to example but should be handled appropriately at each step t in the replacement gate we minimize the quadratic loss function over our training set using mini and optimize in the number of layers in the model and the number of hyponymhypernym pairs processed at each iteration of mini batch sgd in order to prevent overfitting and improve our model s robustness outside of the validation set we introduce dropout in between the penultimate and final layers for our best performing model we implement our stacked lstm model using tensorflow 
we compare our performance to the supervised model outlined by fu et 
piecewise projection model
we first train highly dimensional word vectors using the publicly available word2vec module 
distributed word vectors
the linked hypernyms dataset lhd provided by kliegr as part of the dbpedia project contains 3 7m hyponym hypernym pairs 
the lhd set contains tokens for phrases longer than 3 words like history of the united states and also contains many words which are not in the vocabulary of our vector model 
as such we prune these pairs from the set 
this leaves roughly 1 5 million hyponym hypernym pairs 
we then sample 20 of the remaining pairs to and evenly split them for use as our validation and test sets we also evaluate the performance of our model on the bless dataset used in previous hypernymy classifiers 
the most common tokens for both datasets are shown in appendix a 
both of these datasets are publicly available 
we trained a skip gram model the parameters which achieve the highest accuracy are d 300 c 9 s n 9 
word vector model
we trained our piecewise projection model using our pre trained word2vec vectors 
we reserve 20 of the data for our validation and test sets and training on the remaining pairs 
we choose the optimal k based on the f1 score of each model with 10 fold cross validation on the training set and find k bless 30 bless 3 5 and k lhd 15 lhd 4 0 
our stacked lstm model has achieved moderate results on both bless and lhd though they are difficult to compare due to the widely varied results of the piecewise projection model on the two datasets the small size of the bless set the abundance of many general words object artifact and the repetition of several hyponyms castle glove cottage are likely biasing both models 
it is clear that the lstm model without dropout is overfitting on the bless set because of its small size relative to the number of total cells in the network 
discussion
though the model still performs well on the test set this is likely due to the test set containing words which also appear in the training and validation sets on the other hand it seems that the piecewise projection is also simply learning features of the words in the bless set 
for a low threshold increasing the number of clusters greatly increases the model s f1 score on the validation set nonetheless the test accuracy that the seven layer lstm achieves on the lhd set shows promise for learning a hypernymy mapping 
the neural net should improve or stabilize as the dataset grows especially if dropout prevents it from memorizing as it trains on more data 
the significant positive offset in validation accuracy during training as the batch size is increased suggests that the model may benefit from seeing more levels of hypernymy in a single iteration 
though the information that the network state receives at a given time step from previous time steps is not particularly clear the broad diversity of tokens present in the lhd set suggests that hypernymy is indeed complex but not so broad that it manifests itself differently in word vectors in different domain spaces and across pairs of varying generality 
the model was able to pinpoint the location of the input s hypernym with some success in spite of these complexities without making any assumptions about hypernymy s behavior like the piecewise projection model does we note that the model still has a major shortcoming in that it only predicts a single hypernym vector for a given hyponym 
hypernymy however is a many to one mapping and several papers
we note that future studies are necessary in order to verify and improve the validity of our stacked lstm model 
if the model indeed succeeded in learning a hypernymy mapping it should perform well at predicting the hypernyms of a noun hierarchy 
as such a well documented hypernymy tree like that of wordnet would serve as a reliable benchmark for testing this model additionally further tests can use different word vector models to see if other learned representations such as those in glove are better at learning semantic relationships which are more easily parsed from the vector components 
if the lstm cell is indeed capable of identifying these relationships similar models could be built for other semantic relationships like synonymy 
here we present a list of some properties of the bless and weeds datasets 
bless
the data set contains 105 nda documents from a single investment firm which include approximately 4 000 sentences 2 400 unique tokens and 202 000 tokens total 
each nda was initially written by a company running a sale process and then edited by the investment firm text processing was completed to create sentence and word tokens and remove word stems as detailed below 
data set
most importantly the correct original inserted or deleted label for each tokens was identified based on comparing the original and edited versions of each nda 
across the entire data set 87 of tokens were original 7 were inserted and 6 were deleted text processing steps 1 
identify the inserted or deleted portion of the text by running an advanced version of the diff terminal command on the original and edited version of each document 
html tags mark the start and end of these edited portions of the text 2 
tokenize the text into sentences with the punkt tokenizer provided by the natural language tool kit nltk python package 
the punkt tokenizer recognizes periods ending sentences versus those within sentences using unsupervised learning 3 
tokenize the sentences into words using a customized regular expressions tokenizer in the nltk package which accommodates the html tags 4 
normalize the tokens by removing suffixes with the porter stemmer from the nltk package 5 
label each token as original inserted or deleted based on its position relative to the html tags 
original text will have any legal effect 
within ten days after being so requested text with html tags representing edits will have any legal effect 
text processing example
del within ten days del ins promptly ins after being so requested sentence and word tokens after stemming will have ani legal effect within ten days promptli after being so requested adding ner to the text processing was seriously considered and tested because ner should be able to increase the similarity between documents by replacing document specific names with generic ones e g 
replacing google with company 
however ner was ultimately not included because it did not meaningfully increase edited token classification accuracy on the training data 
in part this was likely caused by existing ner models mislabeling capitalized legal phrases e g 
evaluation materials or contemplated transaction as organizations or people and could be remedied by a ner model trained on a legal document corpus in order to maintain at least some context from prior tokens and labels each sentence was treated as one observation 
90 of the sentences were randomly chosen as training examples and the remaining sentences were testing examples 
within a sentence each token can potentially have a different label 
consequently classification results were calculated based on identifying individual token labels correctly it is very difficult to gather ndas because these documents are private contracts and may only be obtained from one of the signing companies 
as a result this paper s data set has a small sample size 
this leads to sparsity and a limited number of inserted and deleted token examples 
furthermore due to the imbalance between original and edited tokens in the data model performance on the original tokens is more heavily weighted than performance on edited tokens in training unless additional adjustments to the data set are made 
hidden markov models structured averaged perceptrons and conditional random fields were evaluated based on their ability to successfully classify inserted and deleted tokens within each edited nda sentence 
these models were chosen because they are often relied on in nlp for similar tasks e g 
2 5 methodology
pos tagging and ner that involve tagging text sequences 
the hidden markov model hmm is a generative model that estimates the probability of an unobserved sequence of inserted deleted or original label states when there is an observed sequence of word outputs 
from the markov property the model assumes the current inserted deleted or original label state and word output only depend on the prior state 
hidden markov model
therefore no additional features beyond the likelihood of the current word output conditional on the prior state and the likelihood of transitioning to the current state from the prior state are taken into account when the hmm makes predictions the supervised hmm from the nltk package determined the maximum likelihood output and transition probabilities based on the frequencies observed for the word tokens and states in the training examples 
the model subsequently tagged tokens in the testing examples based on the maximum likelihood state sequences generated with these probabilities by the viterbi algorithm 2 
the structured averaged perceptron sp uses online learning to continuously improve its original inserted or deleted label predictions 
this discriminative model updates the weights placed on different features after an incorrect prediction and leaves the weights unchanged after a correct prediction 
structured averaged perceptron
as commonly used in pos tagging the features include a combination of suffixes prefixes entire words and labels drawn from the two prior tokens current token and the two following tokens as well as a constant bias term the sp from the nltk package determined the weights with 20 iterations through all of the training examples 
after training was completed average weights were calculated and tokens from the testing examples were tagged with the highest weighted label in which prior token features were based on the sp s prior predictions
the conditional random field crf model can be considered a more expressive version of the multinomial logistic regression model that makes predictions for sequences of original inserted or deleted label states 
like other discriminative models the crf maximizes the probability of each label conditional on a rich feature set derived from the text 
conditional random fields
after testing the abundant features available a restricted feature set was chosen which included the same set of features from the sp model along with extended sequences of words and labels the linear chain crf from stanford nlp group primarily used for their ner classifier finds the maximum likelihood conditional probabilities using stochastic gradient descent 
the model tags tokens based on monte carlo inference approximations of the highest probability state sequence using gibbs sampling on possible state sequences 3 
most of this paper considers models trained on all training example sentence 
in order to counteract the bias towards original labels from the unbalanced data set two additional training processes were considered 
adjustments for unbalanced data
the first process trained on edited sentences discarding sentences with only original tokens from the training data the second process breaks the training sentences into bigrams and randomly undersamples the bigrams where the second token has an original label resulting in an even distribution between original inserted and deleted labels for the bigrams second tokens 
the first token serves as context for the second token and consequently its label is not factored into the undersampling resulting in more original labels appearing across both tokens in the bigrams 
while 87 of the tokens were original in the all sentences training data 74 and 37 were original in the edited sentences training data and the randomly undersample bigrams training data respectively 
when trained on all sentences all three models had over 90 accuracy on the testing examples driven by identifying over 98 of the original tokens correctly 
highlighting the clustering of accurately classified tokens seen in although these high level results are strong a better indicator for these models performance is their classification accuracy on edited tokens only because these labels represent the editing processing and appear infrequently in the data 
the crf is revealed to be a significantly better classifier based on edited token accuracy which reflects its more flexible feature set that classification improved by making adjustments for the unbalanced data set in training particularly for the hmm and sp models 
as expected hmm and sp benefited from a higher frequency of edited tokens which placed more weight on correctly classifying these tokens in training 
this meaningfully increased recall at the cost of reduced precision and low overall accuracy given a large number of original tokens were tagged as edited 
the best example of improvement is the sp where edited token accuracy increased by 15 to 49 for training on edited sentences only and by 30 to 65 for training on randomly undersampled bigrams compared to training on all sentences most notably for the other models when trained with randomly undersampled bigrams hmm s edited token accuracy improved by 14 to 52 and crf s dropped 12 to 53 
the drop in classification accuracy can likely be explained by the crf s reduced ability to decipher context when considering only two tokens at a time because this is a preliminary look at applying machine learning to legal editing these results require several caveats the ndas are sourced from a single firm and consequently the models identify that specific firm s preferred edits 
other firms may have different editing preferences which these models may not be able to recognize or may find more difficult to learn in the training process leading to poorer classification results than shown in this paper because there are only 105 documents the text reflects a limited subset of the edits made the language used or situations addressed in an nda 
additional data would be required to further generalize the three models and to further mitigate unbalanced data sets given that identifying edits is highly context dependent treating sentences as observations may remove important context available earlier in the paragraph or earlier in the document 
models incorporating prior sequences into predictions e g 
recurrent neural networks and longer training observations if additional ndas are available should be evaluated text processing feature selection and model training could be further customized for legal documents hopefully leading to improved classification results 
this has already proven to be a successful strategy with the enhanced performance of models trained on edited sentences or randomly undersampled bigrams the crf can have a tendency to overfit the training data with complicated features that reflect nuances in one or two training examples 
the feature set was deliberately restricted for the model in this paper narrowing a potentially large gap between training and testing accuracy to 3 98 training accuracy vs 95 testing accuracy 
despite caveats this paper s classification results should be treated as a positive sign that applying machine learning to editing legal documents is feasible and worthy of additional research 
approximately 90 overall accuracy and 70 edited token accuracy in the best models for this simplified classification task show that supervised learning models can handle the complexities of sparse unbalanced context rich legal document data as long as the right text and training processes are in place 
many additional steps need to be taken before machine learning lightens lawyers workloads 
the long term ideal model that automates editing original ndas will need to identify deleted tokens similar to the tagging done by the models in this paper 
yet deleted tokens had the poorest classification performance and were at best identified correctly 60 of the time 
preliminary testing of models trained on edited documents to find words that should be deleted in original documents were unsuccessful 
in addition the long term ideal model will have to generate text to insert into the original document 
with more options on how many and what particular words are inserted this will be the largest and most rewarding challenge 
this paper explores applications of machine learning to analyzing media bias 
we seek patterns in event coverage and headlines across different news sources 
for headline wording we first validate the existence of informative trends by examining the performance of multinomial naive bayes and svm classification in mapping titles to news sources 
we then perform keyword analysis to determine which words are most indicative of certain news sources in event coverage we use unsupervised clustering techniques to profiles news sources by the events covered 
we vary the scope of our analysis from global news to israel and palestine from 2014 to 2016 and israel during the summer of 2014 
we were able to observe meaningful trends in both headline key words and event coverage and are excited about this methodology as an objective lens to the analysis of media bias 
in cognitive science bias is defined as deviation from the norm or true value previous work has examined geographical overreporting variation in event coverage promptness differences in writing style readability and variation in intensity of coverage 
other work examines biased adjectives and utilizes natural language processing to understand bias in writing style 
problem and background
mladenic examines networks of cross referencing between news sources and news providers to understand which voices news sources are choosing to represent 
we adopt a naive bayes model for keyword analysis discerning the words most indicative of which source is reporting about a certain topic 
for example leban used keyword analysis to study bias across a variety of subjects including the conflict in crimea media bias affects all stages of news publishing 
because headlines most affect the general consumer we focus on wording and event selection cherry picking or selection bias 
for the data for this project we use the eventregistry org api
for an initial phase of the project we used the event registry api to curate over 160 000 article headlines for articles published by the top twenty news websites as ranked by alexa web traffic metrics between 2014 and 2016 
the results of applying keyword analysis to this dataset were used as a baseline for our main goal of applying a similar analysis to media surrounding the israel palestine conflict for the second phase of the project we used the event registry api to curate over 1 500 articles focused specifically on the israel palestine conflict 
headlines
for this dataset 8 news sources were selected specifically for their collective propensity to provide a wide range of opinions on the conflict for both the baseline and primary datasets naive bayes and svm models were trained to predict the news organization that published an article given the headline of the article 
article headlines were preprocessed using a combination of scikit learn s count vectorizer and tf idf transfomer tools 
to study event selection bias we gathered data for 600 events related to israel between june 1st and september 30th 2014 
the data included 1 143 news sources all news sources were used to normalize vector norms but only the top 100 by total number of articles were clustered 
events
accuracy statistics achieved by the multinomial naive bayes and svm classifiers on the larger international news dataset 160 000 articles are reported below 
as was noted in section 2 results on the larger dataset were intended to act as a baseline for the accuracy of these same classifiers when applied to the smaller more focused dataset of articles covering the israel palestine conflict 
headlines keyword analysis
the results of classification on this dataset are presented below 
we believe that word counts and event coverage profiling can serve as a highly objective lens for the study of media bias 
common approaches in nlp such as quantifying inflammatory adjectives or the readability of text undoubtedly introduce bias and are hard to generalize across new languages 
this type of analysis could hold news sources more accountable than one fraught with subjective metrics the results of headline keyword analysis proved particularly interesting and we observe that the keywords judged to be most indicative of various news outlets display significant correlation with the established political leanings of each outlet 
additionally the results seen here prompt new and more exciting questions and applications surrounding this research 
several immediately apparent next steps include a rigorous evaluation of the idea of indicativeness and an analysis how best to compute this metric an expansion and cleansing of the dataset which was subject to the limitations of the eventregistry api and an exploration of practical applications of these trends interesting trends emerged in events clustering although the model is quite naive 
for example the clustering on page 3 groups newspapers stereotypically geared towards israelies and jewish americans cluster 1 palestinian and irish british sources cluster 4 and liberal german and american news sources cluster 2 
we believe that more meaningful clusters can be generated by adding features such as event categories keywords much of the inspiration for this research came as a result of the authors own experiences with the so called echo chamber effect in which an individual consumes only media that validate his or her views 
in attempting to classify news outlets based on their political leanings and biases one potential application of this research would be to generate a set of news sources representing a comprehensive span of opinions on a given issue 
such an application would hopefully promote a greater awareness of the intricacies of important issues and facililate a more objective productive discussion surrounding them 
since video games are challenging while easy to formalize it has been a popular area of artificial intelligence research for a long time 
for decades game developer have attempted to create the agent with simulate intelligence specifically to build the ai player who can learn the game based on its gaming experience rather than merely following one fixed strategy 
the dynamic programming could solve the problem with relative small number of states and simple underlying random structure but not the complex one the reinforcement learning is one of the most intriguing technology in machine learning which learns the optimal action in any state by trial and error and it could be useful in the problem not amenable to closed form analysis 
therefore we selected and modified the snake game to investigate the performance of reinforcement learning 
the goal of this project is to train an ai agent to perform well in a revised snake game snake is the game popularizing over the world with nokia mobile phones 
it is played by sole player who controls moving direction of a snake and tries to eat items by running into them with the head of the snake while the foods would emerge in random place of bounded board 
each eaten item makes the snake longer so maneuvering is progressively more difficult 
in our game we slightly changed the games rule into scoring as many points as possible in fixed time instead of counting points in unlimited period q learning is an example of a reinforcement learning technique used to train an agent to develop an optimal strategy for solving a task in which an agent tries to learn the optimal policy from its history of interaction with the environment and we call the agent s knowledge base as q factor 
however it is not feasible to store every q factor separately when the game need a large number of the sarsa algorithm is an on policy algorithm for temporal difference learning 
compared to q learning the major difference of sarsa is that the maximum reward for the next state is not necessarily used for updating the q values instead a new action and therefore reward is selected by using the same policy that determined the original action ii 
work review there are abundant works about the artificial intelligence research for game agent 
in td gammon a backgammon playing program developed in 1990s is one of the most successful example in reinforcement learning area 
in tsitsiklis and van roy with the revival of interest in combining deep learning with reinforcement learning sallans and hinton
abstractly each step of the snake game could be considered as finding a self avoiding walk saw of a lattice in r 2 
a realization of snake game can be mathematically represented as the m n game board is represented as g v e where v v i is the set of vertices where each vertex corresponding to a square in the board and e e ij v i is adjacent to v j 
a mathematical analysis of snake
the snake is represented as a pathwhere u 1 u k are the head and tail respectively np hardness viglietta 2013 
due to the np hardness of finding optimal solution for snake we developed a heuristic algorithm that does considerably well for snake problem and used it as benchmark for the reinforcement learning algorithms we will discuss later 
the algorithm is shown in algorithm1 
b approximation algorithm for snake
to conduct and implement successful reinforcement learning algorithms to play the game one of the fundamental obstacles is the enormous size of state space 
for instance denote the size of the game board as n n naively using the exact position of the snake and food each cell could be parameterized by one of the four conditions contains food contains the head of the snake contains the body of the snake blank 
c reduction of state space for reinforcement learning
by simple counting principle the size of the state space s satisfies s n 8 which is immense when n is large and learning in state space of such size is infeasible 
hence to accelerate the learning rate one simple reduction technique is to record only the relative position of the snake and the food with the current condition of the snake 
precisely each state in the reduced space is of the form w s w l w r q f q t in the above expression w s w l and w r are indicator functions whether there is a wall adjacent to the head in straight left and right directions respectively and q f q t are the relative position of the food and tail with respect to the head algorithm 1 the deterministic heuristic algorithm for snake find the longest path between s 1 and s k using such mapping the total size of state space is only 2 3 4 2 128 and the performance is would be improved prominently in the learning process 
in q learning an agent tries to learn the optimal policy from its history of interaction with the environment 
a history is defined as a sequence of state action rewards for the snake game the process is indeed a markov decision process and the agent only needs to remember the last state information 
d reinforcement learning q learning
we define the experience as a tuple of s a r s next 
these experiences will be the data from which the agent can learn what to do 
as in decision theoretic planning the aim is for the agent to maximize the value of the total payoff q s a which in our case is the discounted reward 
in q learning which is off policy we use the bellman equation as an iterative updatein the above equation s s are the current and next state r is the reward is the discount factor and is the environment 
and it could be proven that the iterative update will converge to the optimal q function 
since the distribution and transition probability is unknown to the agent in our approach we use a neural network to approximate the value of the q function 
this is done by using the temporal difference formula to update each iteration s estimate asthe action set includes three possible outcomes turn left turn right go straight an greedy approach is implemented here 
the exploration probability is is changed from 0 5 to 0 1 with a constant density 0 01 during training 
once it reaches 0 1 it holds constant 
this propels the agent to explore a lot of possibilities in the beginning of the game when it doesnt know how to play the game 
this leads to a great amount of random actions thus enable the agent to exploit much enough to narrow down the optimal actions 
e reinforcement learning state action reward stateaction sarsa state action reward state action sarsa is an onpolicy reinforcement learning algorithm which estimates the value of the policy being followed 
we could describe an experience in sarsa in the form of this means that the agent was in state s did action a received reward r and ended up in state s from which is decided to do action a 
this provides a new experience to update q s a and the new value which this experience provides is r q s a 
so sarsa could be described as below so sarsa agent will interact with the environment and update the policy based on the actions taken and that s why it s an on policy learning algorithm 
q value for a stateaction is updated by an error adjusted by the learning rate q values represent the possible reward received in the next time step for taking action a in state s plus the discounted future reward received from the next state action observation 
the algorithm of sarsa goes in algorithm3iv 
the discount factor was set to be 0 95 moderately decreasing learning rate starting from 0 1 and the rewards were set as shown in we want the agent to control the snake to go to the food quickly and the last column in table 1 is a punishment for taking for one movement which encourages the agent to traverse shorter walk to the food 
this negative rewards acts as similar function as the discount factor 
a tuning parameters
we performed trial and error on different combinations of reward for different cases to get current combination of reward values as one optimal combination among all the trials 
the learning curves for q learning we could easily observe that the performance of agent with q learning get improved faster than that of agent with sarsa in the beginning i e 
in a short run agent with qlearning algorithm outperforms the agent with sarsa 
b learning curve
but as the number of training iterations increases the performance of agent with q learning doesn t improve much while the performance of agent with sarsa still gets improved comparatively significantly 
q learning does well compared to sarsa when the training period is short 
but in the long period sarsa wins the reason why the agent of q learning doesn t perform well in some cases in the long period training is that qlearning algorithm would reinforce its self righteous q value even with a very small learning rate 
this would lead to considerably volatile performance 
although the agent with sarsa seems to outperform the agent with q learning algorithm in a long period training it also has a comparatively sluggish learning curve in our cases 
we took the performance of approximated optimal solution algorithm as the benchmark 
as we mentioned before due to the fact that snake is a np hard problem the best benchmark we could get here is the approximated optimal solution 
c performance comparison
and it could be shown later that our agents with reinforcement learning algorithms could not beat this approximated solution even with a considerable long training period then we compared the performance of the agents based on two reinforcement learning algorithms with the benchmarkthe performance of our approximated optimal solution 
at the same time we consider the effects of training period on the performances of snake agents 
we performed 10 4 10 5 10 6 training iterations on agents based on different reinforcement learning algorithms and evaluate their performances respectively we set the time limit to be 1 minute for the game run 1000 tests for agents with different algorithms and compute the average score different agents could achieve 
the reason why we choose 1 minute to be the time limit is that within 1 minute the agents with those reinforcement learning algorithms could control the snake survive so we could make sure that the difference lies only on whether different agents could find shorter path to the food 
1 minute is a comparatively long time period to show the significant performance difference among different algorithms while it is a relatively short time period that 1000 tests could be handled with our pc in a reasonable running time the results are shown in
explore the stability of q learning algorithmwe found that in some training cases even with a decreasing exploration probability the performance of qlearning algorithm is not very stable 
so it s worth exploring principles and methods to improve the stability of the qlearning algorithm 
v future work
for example one intuitive way to address this problem is to add various tuning parameters to improve the probability of convergence for q learning algorithm 
other approximation of the state space could be explored for better performance 
currently we use a quadrant view state mapping technique as discussed in the previous section 
b study the other state space approximation methods
using such means the snake does not have a good sense of precaution for hitting himself 
hence a more rigorous state mapping technique should be developed 
such reduction mapping must not only approximate the relative position of the head and the food but also obtain a concise sense of the position of the body without tremendously enlarging the size of the state space 
in order to further improve the learning rate of the snake agent expected sarsa could be used 
van seijen et al 
c expected sarsa
2009 
in this project we have shown an implementation of both q learning and sarsa by approximation of state space in neural network 
we anticipated that the difference between performances of q learning and sarsa might become apparent after long training period and the outcome verified our expectation 
vi conclusions
also we compared this two methods with an approximated optimal solution and found that neither of the two agents could achieve the performance of the approximated optimal solution while they exhibited prominent learning 
also we observed the instability of q learning algorithm in some cases and it s worth exploring feasible solutions for future work 
with improvements in sensing technology and computational power robots capable of moving through an environment in real time are becoming more and more feasible 
one such example is an autonomous four wheeled vehicle which obeys differential constraints a typical constraint is to simulate actions into the future and confirm that at least one possible action does not result in collision 
if no such actions exist the current state is called an inevitable collision state ics in any case in order to perform the simulation assumptobyb stanford edu no slip is assumed so differential constraints must be obeyed which limit the vehicle s movement abilities in the lateral direction the path of a car as it moves through time in an unknown environment 
it s knowledge of the environment improves as it progresses 
green is the executed path magenta is the planned path given its current knowledge pink is the boundary into the unknown and blue are the inferred walls tions must be made about the unexplored state space 
in the most conservative approach the robot treats any part of the environment not already explored as an obstacle 
this approach maintains zero percent probability of collision but has multiple drawbacks as well 
for instance a robot rounding a hallway corner will slow down so that it can sense the unexplored path as it s turning resulting in more time taken 
in the same scenario a human knows from experience that it is very unlikely a hallway dead ends and subsequently will turn the corner sharply 
if a path contains many sharp turns the lost time can add up resulting in much slower completion time there are multiple methods to speed up the robot the first is to relax the safety constraints and approximate the probability of collision in some manner 
this combined with a penalty for collisions allows the robot to naturally gauge whether an action s risk vs reward is worthwhile 
this method often requires hard coded behavior which can be detrimental in scenarios differing from the algorithm s original intent another method is to maintain safety constraints but encourage behavior that reduces the likelihood a robot finds itself in an unfavorable scenario 
with the same example as previously when taking a sharp corner at high speed a robot can swing out wide in order to increase its vision around the corner and give itself room to take evasive action if necessary 
if the hallway is clear it will continue its turn without having to accelerate back to full speed 
this paper uses the latter method as a baseline planner and compares it to the probability based planner which utilizes bayesian inference 
there are many methods to tackling the problem of high speed planning in partially observable environments each with their own strengths and weaknesses first off it can be advantageous to recast this problem as a pomdp as many people have and use this framework to work in the belief space instead of the state space one such method was recently proposed by charles richter et 
ii literature
their planning algorithm utilizes machine learning to estimate the probability of collision for a given state algorithm 1 probability modeling low data density region and the other with the potential to observe a region of high data density the latter action should be chosen 
charles richter uses four features to predict the probability of crashing 
they are as follows 1 minimum distance to the nearest obstacle along the path 2 average distance to an obstacle or horizon in a 60 angle in front of the robot along the action 3 average free straight path directly in front of the robot along the action and 4 total speed at the end of the action in order to test additional features and see how the results change i chose four additional features to test 5 ratio of sensed new cells to total cells 6 ratio of walls to free space 7 total turn angle 8 number of obstacle cluster calculated using k means clustering training data d is generated by randomly sampling a feasible configuration and action within a training map 
iii proposed solution
an observation is made from the configuration and the features are calculated with respect to the chosen action 
next emergency braking maneuvers are executed from the end of the action for a variety of steering angles to see if the vehicle is in an inevitable collision state in order to choose the next action the cost of each feasible action is calculated 1 and the minimum is chosen 
posterior probability of collision is calculated according to a non parametric bayesian inference model algorithm 2 bayesian learningis the radial basis function gaussian kernel 
the prior pseudo counts and act as a form a laplace smoothing where the counts are a function of the features present for the given action 
if action a t results in an inevitable collision state when assuming that all unknown space is an obstacle then is set to a positive value 
otherwise it is zero 
this ensures that when the vehicle enters a region with little to no training data the prior distribution dominates and a similar safety constraint compared to the baseline is used to guide the vehicle through the region 
algorithm 2 outlines this process the result of the action cost function is that when the algorithm is very confident a collision will not occur due to high training data density and a small number of recorded collisions then the robot will maintain high speed while steering towards unknown regions of the map 
the planner assumes the space past the boundary will be open and that no collision will occur 
conversely if the data shows that actions with similar features crashed a majority of the time then the cost will drastically increase making it unlikely the planner will choose such an action and will choose a safer alternative 
in our simulation the full dynamics of a car are used complete with applied force at the front axle to act as the electric motor or braking system 
the available controls are steering angle and applied force 
iv simulation experiments
the original planner was trained on a maze like environment while the extended planner was trained on all three types of maps now that the specifics of how the simulation was run are taken care of we can examine the results 
in contrast observe how the probabilistic planner cuts the majority of the corners very closely 
this is because the machine learning data tells it that each corner has high likelihood of continuing past the unknown boundary so it associates low risk with maintaining high speed while moving through the region 
presented in table i are the time taken to complete three different maps 
simulations were ran with randomized starting locations within a bounding start box 
the original machine learning planner experiences speedups in maps where it has some familiarity maze hybrid and is slower in the unknown environment forest 
i have shown two variations on a greedy path planning algorithm which reduce time to the goal by up to 19 2 compared to a baseline planner 
this is because the baseline is very conservative maintaing hard coded safety constraints and naive it simply tries to maximize velocity throughout the course this speedup comes with considerable risk involved 
v conclusions
of the 50 trials ran with a collision cost of 0 8 only 21 completed 
if collision cost was increased or more training data generated the number of failed runs should decrease finally i have also shown that selecting too many features without properly generating enough training points can lead to worst behavior than a naive baseline 
yelp is one of the largest and most popular platforms for crowd sourced reviews about local businesses with over 145 million monthly unique visitors and 102 million reviews to date the ability to identify business features that are most indicative of success on yelp can help restaurants devise sensible strategies to improve their own ratings 
specifically we will explore how features gain and lose importance as we vary geographical location by country 
for example are americans more inclined toward a late night snack than their german counterparts 
do canadians value a take out option more than those who live in the united kingdom 
through our work we aim to bring to attention the various and sometimes non obvious cultural nuances that impact the dining experience 
in the future these methods can also be utilized to determine how varying other attributes restaurant size price range etc instead of location affect the critical features chosen and overall classification performance 
science stanford university yuex stanford edu seminal paper yun wu and wang have explored using part of speech pos analysis to predict a business rating based off of user generated text alone 
while they claim that this representation cancels out subjectivity iii 
ii related work computer
data we obtained our dataset from the yelp dataset challenge webpage which contains a total of 77 000 businesses 2 2 million reviews by 552 000 users and 566 000 business attributes 
because our project focuses only on predicting restaurant success we filtered out all non restaurant businesses 
this left us with 25 071 restaurants from four different countries the united states the united kingdom canada and germany 
the datasets for the businesses and reviews are stored in two separate json files with one object per line 
we ran a python script to convert the json data into a csv file 
to prune the yelp dataset which contains 98 feature categories we performed the following procedure first we removed features that are not relevant to predicting restaurant success such as business id and business name and features that pertain only to nonrestaurant businesses such as type of hair specialization 
additionally we removed all features for which approximately fewer than 30 of the restaurants did not have a value 
for example while the majority of restaurants had information about restaurant ambience very few had information about whether or not it accommodates halal dietary restrictions 
we narrowed down the feature categories to 28 some of which include attire good for kids noise level outdoor seating price range review count has take out and has wi fi 
we converted all categorical features such as restaurant attire to numerical values 
for instance we represented casual dressy and formal as 0 1 and 2 respectively 
for feature values that are true and false we converted the boolean values into their respective integer values 
in addition to the aforementioned 30 threshold we placed on feature selection we considered several different approaches including case deletion regression imputation and mean imputation to account for missing data therefore we chose mean imputation a widely accepted method in the statistical community to fill in our missing data values
we used univariate feature selection to identify which twenty features were most important in predicting success for all the countries combined and for each country separately 
we considered two different scoring functions that return univariate p values in order to select the most important features chi square and anova 
b feature selection
because both our features and classes star ratings are represented as discrete values we decided to use a chi square test 
anova tests are mainly used when the feature variables are discrete and the classification variable is continuous 
we considered two different modes of restaurant classification based on star ratings binary and multiclass 
in the binary case restaurants with a star rating below 4 0 are classified as 0 and restaurants with a star rating of 4 0 and above are classified as 1 
v models
the machine learning models we used to train and predict the data are naive bayes logistic regression support vector machine svm decision tree random forest and gaussian discriminant analysis gda 
in the multi class case restaurants are classified from 0 to 5 based on the integer value their star rating rounded 
the models used are multinomial logistic regression decision tree and random forest for every prediction model we trained and tested on the data from all countries combined and then on each countrys data separately 
in total there are 25 071 restaurants of which we used 14 000 to train on and 11 070 to test on 
we only considered six cities in the u s for which yelp had the most data instead of all of the cities 
for each country we trained on approximately 70 of the restaurants and tested on the remaining 30 8000 training and 3775 testing for u s 850 training and 364 testing for the u k 150 training and 65 testing for canada and 320 training and 134 testing for germany 
furthermore we conducted multiple iterations of training and testing for each model with randomized testing and training data each time 
therefore our results are averaged using monte carlo cross validation
we implemented the multinomial naive bayes classifier with a laplace smoothing value of 1 0 
using all of the countries data the test accuracy was 0 55174 
a naive bayes
the results for each separate country are included in the results section of our paper 
the test accuracy ranges from 0 47253 for the u k to 0 58462 for canada 
the equations used are included below 
for logistic regression our test accuracies varied depending on the strength of regularization 
we implemented logistic regression using scikit where the inverse of regularization strength is represented by parameter c increasing or decreasing c by factors of 10 does not yield significant differences in test accuracy but we noticed that a higher c value less regularization
b logistic regression
to compare the results of different linear svm classifiers graphically we display the following 2d projection of the yelp data set 
since we can only graph two features of the data set we select two features that are among the most informative for a restaurant s success across all four countries noise level and alcohol since noise level and alcohol are both discretized variables with alcohol having three possible values none beer and wine and full bar and noise level being quiet average loud or very loud there are only 12 possible combinations for a training example to possess as seen below to consider how our svm model would perform on variables that are not categorical we chose to consider the review count feature which has a mean value of 31 and a standard deviation of 100 we see that both linear models have linear decision boundaries while the nonlinear kernel models have irregular decision boundaries that are representative of the corresponding kernels and parameter values 
c support vector machine
however there are slight differences in the decision boundaries produced by svc with linear kernel and linearsvc which may be due to the fact that svc minimizes the regular hinge loss while linearsvc minimizes the squared hinge loss 
we see that the rbf kernel produces the best results 
then we experimented with tweaking the c and gamma parameter values to see if we can achieve even better accuracy 
we conclude that the best svm classifier uses a rbf kernel with parameters as gamma 100 0 and c 1 0 
we implemented decision trees for both binary and multiclass classification 
the decision tree is more complex and is a generally more accurate model than decision stumps which are the simplest version of a decision tree
d decision tree and random forest
for binary classification we implemented gda using scikit 
the test accuracy for the full dataset with all of the countries is 0 55257 
e gaussian discriminant analysis
the test accuracies for each of the countries is enumerated in the results section 
the test accuracies range from 0 55497 for the u s to 0 6 for canada 
thus gda is generally the best performing model explained in more detail in the sections below 
the formulas used for this generative model are the following 
apart from our core analysis of business features we also investigated the top review text features that were indicators of restaurant success 
the purpose of this section is to identify important features that were not part of yelps official set of business features 
vi analysis of text based reviews
for example slow is not in yelps feature set but may be mentioned many times in negative reviews we can then infer that customers place high value on speed of service to preprocess the review data we gathered all of the reviews available for each of the four countries 
a review was labeled positive if the reviewer gave the restaurant 4 or more stars negative otherwise 
the data was tokenized using the the natural language toolkits nltk word tokenizer which split off white space and punctuation other than periods 
we chose not to convert all letters to lower case because upper case often indicates strong emotion naive bayes is a generative supervised learning model that is commonly used for text processing and sentiment analysis tasks and assumes independence between pairs of features 
we used the nltk naive bayes classifier to train and test our data and to identify the most informative features 
this classifier uses the same model as the multinomial naive bayes classification discussed above we performed 10 fold cross validation to acquire the test accuracies below this approach seemed to work best with medium to large datasets 
if there is too little data the classification was made based on random words that happened to appear more in one type of review 
thus we decided to drop germany from our analysis of informative features because its accuracy was worse than random guessing 50 
the informativeness of a word is max labels p word label min labels p word label the top 20 most informative features of us uk and canadian reviews are outlined in the table below sentiment heavy words such as disgusting unhappy worst dominated all three lists 
uk customers did not like food that tasted like it was microwaved or lukewarm though reviewers often added a redeeming quality to a negative review softening the overall harshness of the review the strongest words for us customers were overwhelmingly more negative with use of capitalization to emphasize intense negative emotion 
us customers seemed to crave personal attention and focused strongly on a servers attitude we also tested three other models a unigram model with stemming stop word removal and a bigram model 
however stemming lowered the 10 fold classification accuracy by around 2 3 for each country 
removing stopwords mysql stop word list caused the accuracy to remain about the same for each country for example canada s accuracy was at 0 67888 instead of 0 68009 
this is likely because the stop word probabilities for each category are very similar and did not have high impact on the class decision 
bigrams also had similar accuracies though the majority of the most informative bigrams were extensions of the words on our unigram list and less indicative of specific business features some examples include tasteless and horrible service 
after running univariate feature selection with the chisquare scoring function we selected the 20 most important features for the us uk canada and germany 
ultimately our goal was to identify features if any that are considered globally important indicators to a restaurants success as well as possible features specific to a particular countrys idiosyncratic cultural values we found that there are 6 features that are highly weighted in all four countries availability of street parking ability to make reservations review count casual ambience noise level and attire 
vii discussion and results
other features corresponding to high star rating include outdoor seating classy ambience touristy ambience waiter service hipster ambience garage parking trendy ambience wi fi intimate ambience good for kids good for groups allows smoking and has tv next we examined features that are indicative of success for restaurants situated in a specific country 
we found that the divey ambience was solely important for restaurants in the united states thereby demonstrating an american appreciation for lower end dining establishments 
in addition in the north america region customer satisfaction is positively influenced by the existence of parking lots and parking valet services 
we speculated that parking is more important in the u s and canada due to a higher percentage of drivers whereas in europe public transportation is more popular 
in europe restaurant success is also positively correlated with availability of alcohol 
this is perhaps due to the lower drinking age in europe than in the us 
furthermore by analyzing the review text we were able to infer from the most informative features the tendency of americans to be more vocal negative and service oriented in their reviews using the 20 most important features we applied various models to the data 
the resulting test accuracies are summarized in the table below 
for most countries the gda model outperforms the others and the multi class decision tree performs the worst 
this is because gda assumes p x y is distributed according to a multivariate normal distribution 
if this assumption is correct gda is asymptotically efficient which means that with large training sets we dont expect many models to be strictly better than gda
in the future we plan to implement multiple imputation 
in general this method better identifies data variability than single imputation but due to the time constraint we reserve these models such as mixture of gaussians for the future 
another way to make up for the missing values is to gather more data perhaps from a future yelp dataset challenge furthermore we would like to improve our textual analysis accuracy through leveraging human annotated multilingual sentiment datasets online and exploring languageindependent textual analysis especially for datasets where the primary language is not english 
acute myeloid leukemia aml kills over 10 000 people in the united states each year no two cases of aml are alike and specifics of a patient s particular instance of cancer can indicate how long the patient will survive 
in particular cancer cell gene expression frequencies indicate specific characteristics within a cell 
a weighing the risks of cancer treatment
gene expression frequency data coupled with patient survival data can estimate a particular cancer patient s odds of survival 
this project had two primary objectives 
our first objective was to determine a reliable way to reduce our high dimensional data set while preserving meaningful structure 
b project goals
genes often interact in complex ways many genes solely exist to regulate the frequencies of others 
to determine the diemnsion reduction algorithm that created the best low dimension visualization of our data we applied a variety of both linear and nonlinear dimension reducing algorithms to find one that best grouped patients based on chance of survival best maintaining the variances within the data relevant to survival diagnosis our second objective was to predict a patient s chance of survival based on the patient s gene frequency data 
while it is difficult to appropriately divide high dimensional data into groups based on survival probabilities we used our reduced dimension data that best captured relevant cell variances to achieve high prediction accuracy 
these predictions would provide a practical use to our dimension reduction analysis demonstrating a technique that could aid patients with a wide range of diseases beyond aml 
dna microarray technology is increasingly used to produce gene expression frequency profiles of patients 
these microarrays contain hundreds of short dna segments attached to small regions upon a slide 
a procurement
to determine gene frequencies from a sample large quantities of dna or rna from the sample cell are tagged with fluorescent molecules then injected onto the slide 
dna or rna sequences that match those attached to the slide bind to these molecules and those that do not are washed off 
when exposed to the correct wavelength of light the molecules bound to the slide glow 
the intensity of the hue of a particular region of the slide indicates the relative frequency of the gene in the original sample 
our gene expression frequency data came from a collection established by researchers from the stanford center for cancer systems biology 
the data is available at precog stanford edu our visualization data selected from the precog collection was originally procured in 1999 by the aml
b gene frequency data structure
the types of genes in our frequency data set varied widely 
genes ranged from cell membrane protein sequences to chromosomal reading frames 
because these genes were from a random sampling only a few genes from this random selection were likely to actually be responsible for decreased life expectancy of a patient to determine which genes were most relevant to predicting patient survival rates we used a cox regression model to relate change in gene frequency to the length of time before the individual either passed away or was verified to be alive 
we then sorted the genes with the greatest absolute z score which specifies how strongly the change in a particular gene correlates to a change in patient life expectancy we also reduced the data to have 0 mean and variance of 1 for our dimension reduction algorithms that required normalized data 
we reduced our data with 50 selected features using a variety of linear and nonlinear methods 
iii dimension reducing methods
pca is frequent first choice for dimension reduction 
however it requires that data be linearly correlated 
a pca
although a linear algorithm like pca may not be able to capture non linear relationships between data dimensions we apply pca as a first step 
locally linear embedding is a clustering algorithm 
it takes into account the relationships of data in multiple dimensions preserving relative distances between these points in the lower dimensional space 
b lle
since multiple gene frequencies of those that we selected may correspond with increased probability of death a clustering algorithm may be appropriate to ensure that reduced data maintains the relative distances of influential dimensions 
similar to lle t sne relies on the proximity of data points in the high dimensional space 
however it models distances with a probability distribution that tends to emphasize local clusters 
c t sne
again because dimensions of our data could be related be related t sne is a reasonable choice 
diffusion maps is an algorithm group points based on their connectivity the probability of moving from one point to another during a random walk 
diffusion maps have been used for a wide variety of applications from speaker identification to image compression
d diffusion maps
to determine effectiveness of our dimensionreducing algorithms we again used cox regression to relate each of the reduced dimensions to the survival time of each patient 
since our data gives the alive dead status of each patient after an arbitrary time period and this period differs for each patient so we cannot simply gauge algorithm effectiveness by seeing if our algorithm correctly classifies patients as alive or dead thus we use the resulting absolute value of the cox z value on each dimension as a confidence metric for how well our algorithm preserved desired structure related to patient life expectancy in our data 
e determining visualization quality
the black circles represent patients verified dead between 1 2 years and the blue triangles correspond to patients found to be alive after 2 years 
the vertical and horizontal lines mark the median points of the data for either dimension 
only the horizontal dimension cox z indexes are displayed 
gene expression data mapped to a low dimensional space can help us predict how long a patient will live 
from our whole data set we can build a kaplan meier plot to determine how long a patient with aml is expected to live 
v gene prediction methods
then by grouping data based on lowdimension location we can provide estimates for how long a patient will live 
after training a parametric algorithm like pca on one data set we can use our learned pca parameters to map a patient into a portion of a low dimensional space correlated with a specific specific survival curve 
the kaplan meier survival estimator creates a survival curve based on survival data in which the status of any given individual is only known at an arbitrary time after a certain period 
the function takes the following form at any time t the probability that any individual is still alive is given by the the difference of the number of alive and the number dead n i d i divided by the number alive n i multiplied at all intervals from 0 t for our full set of training data our kaplan meier plot is given by
a determining accurate survival probability
we used the same set from the visualization portion of this paper for our training set
b data sets
to predict the survival rate of a patient we first reduced the dimensions of our training data with pca 
we split data along the dimension corresponding to the largest cox z index using the median of these points projected on the x axis 
c dividing data and predicting
we can then create a separate kaplan meier plot for each of these two groups of our training data to see how well we separated the data as in
as is clear in to determine the accuracy of our prediction we compared the high low patient divide determined by our training pca parameters median divider to the high low divide from when we reduced our test set with a pca and median determined from the test data itself 
the groupings of patients were fairly consistent with only 5 of patients grouped differently in the training data set than in the test data set to determine whether the two kaplan meier curves determined by our training accurately mirrored those of our test data set we plotted the test group s own kaplan meier curves produced by our training data set 
vi gene prediction results
unfortunately the curves were quite different as can be seen by comparing
we would like to thank andrew gentles stanford university for his support and for providing the data for this project 
in this paper we aim to create the optimal hotel recommendations for expedia com customers that are searching for a hotel to book 
we will model this problem as a multiclass classification problem and build variations of classic support vector machines svms and decision tree classifiers to predict the 5 most likely hotel groupings from which a user will book a hotel 
we use feature selection techniques to select optimal feature subsets then build a unique combined svm and decision tree model that achieves a higher precision and recall than either individual model alone 
the combined model is derived using a scoring technique that is based on the supplied evaluation criteria mean average precision 5 
our project is based on the expedia hotel recommendations challenge on kaggle com https www kaggle com c expedia hotel recommendations 
the goal of this project is to predict which of the 100 hotel clusters that a random expedia visitor will book a hotel from 
the high level application of this project is to allow expedia to provide the optimal personalized hotel recommendations for the user based on a user search event which will increase the number of hotels booked through expedia and simultaneously increase user satisfaction in the product 
however since the problem involves presenting optimal recommendations which the user is presented to choose from this problem is not simply another multiclass classification problem 
we must instead predict five hotel clusters that the user is most likely to book 
the evaluation metric mean average precision 5 evaluates each list of five predictions for each testing example 
the map 5 scoring formula is as follows where u is the test set n is the number of hotel clusters predicted by the algorithm and p k is the precision in the list of clusters at cutoff k with this scoring system we are awarded a higher score for the correct cluster being as close to the front of the list of predictions as possible 
the given dataset includes a training dataset with about 30 million examples and an evaluation dataset of about 2 6 million examples with hidden output hotel cluster 
the 17 features shared by both the training set and evaluation set are listed in the table on the following page 
between each of the 17 individual features we discovered no strong correlations 
we then used principal component analysis pca on the whole dataset before appending the 3 principal components and discovered that 99 of the variance was accounted for by 5 principal components as pictured below 
the destinations file contains 149 numerically encoded features labeled 1 149 which describe each possible destination 
this makes intuitive explanation challenging because it is not given how these numbers were computed or what the 149 latent features mean 
latent features
thus we normalized the features then used principal component analysis pca and discovered that 99 of the variance was accounted for by 5 principal components 
the first three of these principal components were appended to the whole dataset via a join operation on destination id to bring the total number of features to 20 
we filter out all training examples where the user did not actually book the hotel cluster that they clicked on 
we do this because the actual evaluation dataset on kaggle contains only booking events 
downsampling
we begin by selecting 15 000 different user ids out of about 500 000 unique user ids randomly from the training dataset 
we take all training instances from those user ids and discard the others 
this is because in the evaluation dataset user events are preserved and it would be irresponsible to arbitrarily discard certain events from the same user while keeping others 
to select the optimal subset of features for our models we chose to use a wrapper method with forward search using the average k fold cross validation score 
this allows us to maximize our models prediction accuracy 
we arrived at the same optimal subset of features for svm and decision trees 
these consisted of the last 5 elements in the table above srch destination id srch destination type id hotel continent hotel country and hotel market the fact that these 5 features were selected as the optimal subset is quite surprising since these are all related to the destination 
though the removed user specific features like number of rooms and length of stay appear at first glance to be the defining characteristics of any user recommendation platform they are actually far less indicative of booking outcome than destination specific features 
it is likely that introducing these extra features produced lower k fold cross validation scores because of the resulting increase in model complexity 
furthermore the principal components extracted from the latent destination features provided no performance gain and also added to model complexity 
the expected value of the map 5 evaluation score for an output list of five hotel clusters produced by random guessing is one benchmark for evaluating our work 
we calculate this benchmark as follows 
constructing a benchmark
suppose we were randomly selecting 5 clusters for each testing example at each position in our final 5 guesses 
we get more points the closer to the front of our guesses the correct cluster is and 0 if it is not present 
this means that our map 5 score for random guessing would be in the above n represents the number testing examples and score j represents the score awarded for successfully guessing the correct cluster in the jth position 
note that the 100 comes from the fact that there are 100 possible hotel cluster assignments 
we get 5 points if the correct value is in the first position 1 if it is in the last position and 0 otherwise 
so we can define score j as follows score j 5 j 1 6 j substituting this into the above equation we have we solve the above for and we get map 5 random 0 15625 
in order to generate improvement over map 5 random we looked primarily to several different supervised learning techniques 
these include multinomial naive bayes multiclass logistic regression multiclass svm and multiclass decision trees 
supervised learning
we were able to discard the first two models based strictly on their poor training set accuracies compared to the last two 
finally a novel convolution of the last two models allowed us to achieve a fairly high score 
we first attempted to train a multinomial naive bayes classifier with laplace smoothing 
with the feature subset we selected we achieve around 5 k fold cross validation accuracy 
baseline model approaches
with multinomial logistic regression we achieve a 4 5 accuracy 
multiclass adaboosted decision trees multiclass gradient boosting random forests all achieved a k fold cross validation accuracy of around 2 3 
although better than 1 the random guessing benchmark of selecting predictions randomly of 100 possible hotel clusters these models fall short of svm and decision trees 
we omitted extending these models to the case of 5 predictions per training example based on the fact that svm and decision trees increased our k fold cross validation accuracy by threefold and fivefold respectively 
the multiclass svm is very time consuming yet we hypothesized that given enough data it can be a very accurate model 
to test our hypothesis we trained an svm on a subset of the training data consisting of only one feature the destination ids 
multiclass svm
we use the downsampling method described in the downsampling section above 
this initial baseline was able to achieve an average of 23 7 training accuracy and an average k fold cross validation accuracy of about 12 3 which is very good considering that many training instances contain the same destination id with different hotel cluster values 
the average k fold cross validation accuracy increased to 13 5 after using a subset consisting of the 5 optimal features as described in the feature selection section 
the decision tree classifier has the advantage of being extremely efficient 
although one downside is that decision trees are subject to overfitting our precautions of selecting an optimal subset of features counterbalanced the negative aspects of this model by reducing model complexity 
decision trees
furthermore most other models we tried required downsampling before performing k fold cross validation however this model can use the partitions of the entire dataset for cross validation and still runs significantly faster than the others 
using just the destination id feature we achieved a training accuracy of about 17 1 and an average cross validation score of 16 3 
the average k fold cross validation accuracy increased to 18 4 after using a subset consisting of the 5 optimal features as described in the feature selection section 
once we decided which supervised learning methods to use we needed to develop ways to 1 instead of classifying each training example with a single output label produce a list of 5 predictions ordered by certainty to match the required submission format 2 combine their predictions to maximize the map5 evaluation metric by using a novel convolution of both classifiers 
model modification and combination
first we needed to modify both supervised learning tools to make 5 predictions rather than 1 
for the svm classifier this required a bit of intuition regarding the way the classifier works 
classifier modification
the multiclass svm we used needed to classify among 100 different possible cluster assignments 
rather than using 100 traditional one vs rest classifiers we elected to use 1050 one vs one classifiers 
our intuition is as follows in order to make decide among n distinct classes we need a total of n n 1 2 classifiers such that there exists a single classifier comparing each distinct i j for every i j c where c is the set of possible classes 
once each classifier is trained every classifier is used to make a prediction for a given test example 
we initialize a dictionary containing a key for each possible class 
every time one of the n n 1 2 binary classifiers decides in favor of a single class the value for the key corresponding to that class in the dictionary is incremented by 1 
after all n n 1 2 classifiers have been tested the dictionary is sorted in non increasing order by value and we retain the first 5 results in this list to use as predictions 
the decision tree classifier is inherently multiclass so only slight modifications were required to return 5 predictions 
once this modification was done for each of the two supervised learning classifiers we combined their outputs as follows 
first assign normalized weights to each classifier 
the idea here is to give higher weight to the classifier with the smallest generalization error assuming that its results are more likely to be correct than the results from the other classifier 
we can calculate this constant for classifier i as follows 1 err j once we have the constant for each classifier we need to find the highest scoring 5 clusters out of the possible 10 we are given 
we need to account for the fact that a cluster could be used twice so we create a dictionary with 10 keys 1 for each cluster whose values are initialized to 0 
we iterate through each item in the list returned by each classifier and update the cluster s dictionary value with a score calculated as follows in the above equation c i is the weighting constant for the list from the respective classifier and n i is the index of the element in the list from 0 to 4 inclusive once we have done this for each element we sort the dictionary by value in non increasing order and then take the 5 elements with the highest values 
thus our list is complete 
before submitting to the competition by running on the entire testing set we generated statistics to evaluate our model s success 
since the testing set does not provide the cluster values we needed to do this using a validation subset we generated earlier from the training data 
accuracy on validation set
we ran trials for all three of our methods including svm decision trees and the convolution of the two described above 
for each trial we plot two normalized confusion matrices below 
the first defines a classification as correct classification only when the true cluster value is the first value in the list 
the second is more lenient and considers a classification correct if it appears in the list at all 
here are the respective confusion matrices we can see that the optimal model convolution has higher average precision and recall than its two components the svm and the decision tree classifier do alone 
verifying this the next step was to run our algorithm on the test dataset and submit to kaggle 
after running our algorithm on the entire test dataset and then submitting we achieved a highest score of 0 38315 for the map 5 evaluation with the highest score on the leaderboard being slightly over 0 5 
our results are quite good in comparison to other submissions considering that those submissions took advantage of a data leak whereby some of the examples in the evaluation set were leaked into the training data without removing key identifying information about the example orig destination distance and user location city could be used to exactly identify duplicates in the evaluation and training data 
submission results
in the future with more time and computing power we would like to apply other machine learning techniques such as neural networks and compare them against our current combined model 
we plan to remove the filtering and test this method on the entire set of 30 million training examples 
next steps
with such a large dataset techniques like neural networks can be very powerful 
this paper uses the supervised learning techniques of linear regression and support vector machines in an attempt to predict the merging behavior of drivers on u s highway 101 based on current traffic patterns 
using highway data from the federal highway administration the paper approaches the problem from numerous angles eventually concluding that using a twostep approach achieves the best result 
the first step is to cluster the drivers based on driver history and train separate models for each of the clusters 
this results in the best results in every case 
understanding human driver behavior is a critical component in the development of autonomous vehicles 
in previous research laneshifting and regular highway driving has been well studied but the analysis of merging behavior is sparse 
we are developing a model for predicting merging behavior on highways given positional and trajectory information of the traffic 
the model predicts vertical position of a merging vehicle given the current traffic and time elapsed 
our data is from the ngsim database and includes positional data for vehicles over a stretch of highway 101 and i80 
we transformed the data from the ngsim database to include directional velocity and ultimately arranged it in grid format to allow for a constant feature size to be input into our models 
for each grid entry we keep an average of the features of the vehicles in that grid area 
the grid has low resolution to maintain a small feature set size and we further cut down the feature set size by limiting the window of interest to a subset of the highway from when the merge begins to when it ends 
for each frame the vehicle is in the input is the current grid of traffic with the merging vehicle removed and the time elapsed since the vehicle first entered the area 
the output will be either the x or the y position of the merging vehicle in its raw form we only have position and x velocity and x acceleration 
we decided to augment our data to include velocity and acceleration on the y axis 
since we are studying the merging and positional behavior of vehicles this data is extremely valuable to streamline the runtime of our prediction algorithms we precompute each of the data representations that we plan on using 
we have created several models as described below and plan additionally we built several visualization tools to help us gain intuition about our dataset as well as sanity check our work 
a frame from our visualization tool is shown you can clearly see congestion building up as well as cars merging on and off of the roadway 
we generated our baseline results using two naive feature implementations 
the first consisted simply of the mean velocity of vehicles on the road at the time 
preliminary results and baseline
our second baseline consisted of a simple grid with no other information besides the number of vehicles within each grid square 
we ran both svm and linear regression on this dataset and the results are shown below 
although the overall scores losses for these baselines were relatively similar the distribution of what contributed to the losses were different generally it appears that the performance of the predictor based off of just the mean roadway velocity is better at earlier timesteps whereas the predictor based off of vehicle location is able to better fit the data at later timesteps 
as we enhanced our data our results would more closely match the grid baseline 
as we developed our model it quickly became clear that svm regression was the superior of the two techniques for our application 
linear regression vs svm
one of the main characteristics of an svm is the ability to tune the hyper parameters the penalty weight and the epsilon value 
these alter the bias and variance of the model so choosing the best parameters is crucial to producing accurate results 
tuning svm
using a subset of the data various values were tested including penalties centered at the default value of 1 maxing out at 10 000 and bottoming out at 0 01 
the epsilons were tested between values of 1e 6 and 100 
all pairs of possible combinations performed worse than the default penalties 1 epsilon 0 1 except for the pair with a penalty of 0 1 and epsilon of 10 which performed better in our scoring metrics 
however these results were too invariant to the different possible car trajectories 
thus the default parameters were chosen for the rest of the project as they provided the best bal ance of average accuracy and producing trajectories that matched the shape of the test vehicles 
seeing as how absolute data is not always as useful as relative data one attempted strategy was to mean center the grids before creating the model 
this did not have any significant change to accuracy of the models linear regression and svm and it took a fair amount of time longer so the rest of the project uses non mean centered data 
mean centering data
the next logical step to examine variation in the model such that the best possible model can be produced is variation of the features 
initially the features were the grid of current traffic the time elapsed since the merging vehicle entered the area 
tuning features
the grid itself is what was first examined for improvement 
the grid first contained the number of vehicles in the grid and the average instantaneous velocity and acceleration of those vehicles in the x direction but there were many other features that could have been chosen 
the first of these features to be tested was headway 
the motivation behind headway was that not all of the vehicles are the same size so it might be more useful for the model to know how much space is between cars than other features 
time and space headway
the results were disappointing with the svm model doing worse than without headway and linear regression doing marginally better 
similar results arose from including the average instantaneous velocity and acceleration in the y direction 
the hypothesized cause for these failures is that adding more features made it harder for the svm to find the correct weights 
y velocity y acceleration
another examined variation on the feature set was giving the models 2 frame grids per training example the frame from 10 time steps ago as well as the current frame 
the goal of this approach was to give the model more information about the history of the traffic as supposed to only the current snapshot 
frame history
doubling the feature size unsurprisingly resulted in a significantly longer runtime with limited to no improvements to show for it 
one feature that increased the accuracy of both models was including the initial position of the merging vehicle along with the current traffic and time elapsed 
this helped because not only did it only add 1 feature to the training examples but it gave perspective to the model as it is possible for different merging vehicles to begin in very different locations of the merge ramp 
vehicle initial position
up until this point the models have been predicting the trajectory given only information about the current situation 
additionally the models tend to perform well on merging vehicles that only merge one lane but poorly on vehicles that move multiple lanes over immediately after merging 
k means clustering to classifying driver behavior
so the problem becomes how can the model better between cars that are likely to merge numerous lanes and cars that are not without cheating it and telling it the future 
the answer is with behavioral data for the driver based on the driver s historical information 
for example a historically aggressive driver is more likely to merge into a small gap or to speed up to overtake a car in the lane that it is trying to get to along with being more inclined to traverse multiple lanes immediately after merging 
however ngsim does not historical data for the drivers and how can the behavior be calculated in such a way as to be helpful for the learning models the approach taken was to create clusters based on the headway acceleration and velocity of the cars 
the motivation behind this was that an aggressive driver would keep less headway between them and the car in front of them and would have shaper acceleration and a higher speed 
to ensure that no unfair advantages were used the clustering data was taken from the portion of the highway after the merge which still provided about a quarter mile s worth of data and positional data was not used 
additionally before the clustering began the data was adjusted to be relative to the current traffic 
what this means is that there will be no bias of the data where cars that travel in heavy congestion will are more inclined to be aggressive because there is less headway in front of them 
all vehicles were classified to give a broader data set than just using the merging vehicles 
after this meancentering was performed we ran 3 clustering on the data to output different classifications of drivers 
we separated our models based on these clusters and tested performance the results from this were fairly interesting 
the clusters were not evenly distributed with the vast majority of vehicles belonging to one cluster 
however the performance on that cluster was much better than any other models and the performance on the other clusters was only slightly worse than normal 
however linear regression performed terribly 
no examples 0 01
another method in which we attempted to improve our results was through varying the size of our feature grid 
we recognized that it was likely that some grid positions i e 
reducing grid size
the leftmost lanes furthest from the merging vehicle would have little effect on predicting the merging vehicle s behavior 
additionally these increased features could have led to overfitting of our model 
reducing the size of our grid from 60x30 indices to 60x10 and ignoring the top lanes decreased our loss across all examples 
our most successful models came from using clustered models trained on the smaller grid size 60 x 10 with each grid position having features for number of vehicles x velocity and xacceleration 
it is important to note that our predictions were trained and scored over the entire time that a vehicle was present in the grid frame 
final results and analysis
however the grid frame for some merging vehicles extended over 600 time frames so we are tracking in total one minute of vehicle position data 
as one would expect the losses at earlier points additionally it seemed that throughout our experiments with feature modification the overall shape of our predictions were relatively similar between runs 
for the vehicles that remained within the first lane this allowed us to achieve incredibly high accuracy with some examples having average losses of below 1 5 feet across the entire dataset 
however this resulted in worse performance over vehicles that merged across several lanes right away we hypothesize that the result described above can be attributed to several factors 
the first simple explanation is that we could be overfitting our data 
however we believe there may be a deeper issue 
in our training data we have approximately 130 vehicles per 15 minute time period that merge onto the road 
although we break these down into hundreds of training examples each we were unable to get around the fact that out of these approximately 130 vehicles very few of them changed into multiple lanes 
with the dataset used we found it difficult to identify features that directly correllated to whether or not a driver would merge across multiple lanes consistently even with kclustering of driver behavior 
we have identified several viable methods which we hope can produce better results in the future 
running more tests on feature selection and identifying the features that most strongly correllate to multiple lane merges would likely greatly improve results 
much of the observed losses were a result of our model being unable to accurately predict the trajectory of a car merging multiple lanes so a good first step would be to classify based on historical driving data of the specific driver if the driver will merge multiple lanes and if so use a different model than for merging only one lane 
in this section we describe the procedure of extracting features of energy consumption of appliances 
we use publicly available redd dataset which contains both aggregate and individual appliance power data of 6 households 
ii appliance modeling
we used the data from first home measured at every 2 to 5 sec for our appliance level supervised learning 
we identified the major energy consuming appliances total of 7 and extracted their power curves 
to this end for each appliance we chopped the power usage for period of times in which the appliance was on 
this will provide a multi observation sequence which can potentially result in more accurate training 
we then used a histogram analysis to identify number of states of each appliance and provide reasonable initial guesses to the training algorithm 
where x is the observation and i and 2 i are respectively mean and variance of i th state 
t i is the probability of being in state i at time t and is defined aswhere t i and t i are backward and forward variables 
the other parameters such as prior probabilities and transmission matrix are similar to the case of standard hmm 
the result of gaussian emission hmm in the case of dishwasher is shown in
once all the appliances are modeled the task of disaggregation is addressed 
we use a difference hidden markov model for this 
iii disaggregation
for the disaggregation of total load we have two input vectors x time series of the total load curve and y time series of difference in x for consecutive time states i e such that y t x t x t 1 hence this model is referred to as a difference hmm 
this is done because y is a more important feature than the total load itself since any change of state is an appliance results in non zero y t values and zero y t values for steady operation 
the transition probabilities from state i at t 1 to state j at t are represented by the transition matrix a such that we assume that each appliance has a gaussian distributed power demand where w zt is the actual power draw for the appliance 
then y t can be modeled as a gaussian distribution which is a difference of 2 gaussian distributions k and k and 2 k are the mean and variance of the gaussian distribution describing this appliance s power draw in state k so far change in state of an appliance does not ensure that the appliance was capable of being in the initial state or even if the appliance was on 
we can do this by imposing a constraint on the total power demand x t as a higher limit for the appliance power draw z t we further need another constraint which ensures a change of state is not predicted based on noise in the observed total load x 
in order to achieve this we filter out the change of states when the joint probability is below a certain threshold c specific to each appliance t s ifcombining equations 1 through 6 mentioned above we end with a joint probability using equation 7 we can figure out the most probable state for any appliance at a given time t if we know its state at t 1 
we still do not have a unique state change a non zero y t could predict change of state for more than one appliance 
in order to overcome this we employ the viterbi algorithm which is a dynamic programming algorithm that returns the most probable change of state across all appliances 
for running the custom viterbi algorithm we make the assumption that there can be only one state change across all appliances 
this is a fair assumption owing to high sample rates for the data used 2 5 seconds 
only the most likely change of state as predicted by the viterbi algorithm is accepted and all others are discarded at each time step 
the results of this algorithm are discussed in the next section 
in
one application of the disaggregated data is to provide energy saving recommendations to individual households 
this could be from identifying inefficient appliances better usage recommendations such as using the washerdryer during afternoon when the electricity price is low etc 
applications and scope
one major feature required in a recommendation system is that it should be relevant to the consumer 
in order to address this we cluster the users into 5 different clusters based on their energy habits using kmeans algorithm 
we observe that k 5 gives the best results 
individual household data is obtained from pecanstreet data which provides 600 homes load curves 
the results are presented in
we notice that the difference hmm modeling approach to energy disaggregation based on hmm appliance models has a good prediction rate in the case of fixed state appliances 
in our example we have an l1 error norm of 2 58 of the total load 
it should be noted the power surge that typically occurs when appliances start from the off state is not explained using the current model 
further infinite state devices such as dimmer lamps cannot be modeled accurately using a finite state hmm model and hence their contribution is not disaggregated accurately 
further we state a feasible application for the work carried out in the form of a novel energy saving recommendation provider by comparing households with similar energy patterns 
codeforces on codeforces users are assigned a numerical rating based on their performance in past contests 
users are then assigned one of ten ranks based on their rating ranging from newbie to legendary grandmaster 
these ranks are shown in the data set section in the goal of this project is to predict a user s rank within one rank and country based solely on a single passing source code submission 
as well some interpretation of the learned models is done to find differences in coding styles between skill levels and countries 
since only passing submissions are considered predictions are based only on coding style and not whether the code works or not all code works analysis from this project will not only highlight the coding techniques of competitive programmers but may also be relevant for code written in industry or academia 
while code written in programming contests differs from real world code some coding best practices may apply to both and the analysis techniques used here may also be applicable to other code bases 
code from programming contests however is easier to analyze than most code for the reasons described above e g 
written by exactly one person ii 
related work a study by more recently recurrent neural networks rnns such as long short term memory lstm based networks have been used to classify source code 
techniques similar to doc2vec le et al 2014 recurrent neural networks may not be better than n gram based methods however 
the data set currently consists of 10 contests on codeforces from aug to nov 2018 
all of the contests are combined division contests open to users of all ranks 
iii data set
each contest has 6k submissions for a total of 60k with the data format shown in the number of submissions for each rank in the data set is shown in for the country analysis we used a subset of the data consisting only of the users in the 10 most common countries 
these countries cover about 70 of the full data set 
a summary of this data set is shown in both data sets have a significant class imbalance 
various techniques were needed to handle this as described later we implemented a custom scraper for codeforces in python using lxml source code is first converted to a sequence of strings 
it is run through the clang c compiler to produce a list of tokens and an abstract syntax tree ast 
comments are removed as our focus is coding style 
to help the learning algorithms generalize better all tokens equal to a variable or function name in the ast or representing a string or character literal are replaced with special tokens 
var 
fun 
str and 
chr respectively 
the ast is converted to a list of strings as a pre order traversal where additionally a special token endblock is added when all of a node s children have been visited 
to simplify later processing the processed tokens are concatenated with the ast traversal to produce a single sequence of strings 
the sequence of strings is then processed further 
bigrams and unigrams with at least 1 frequency in the training set are counted to produce features 
to help prevent the learning algorithms from favoring shorter or longer solutions each count vector is normalized by the l2 norm 
tf idf in the data set the average number of tokens in a program is 428 the average length of the ast traversal is 627 and the average length of the concatenated sequence is 1055 
several learning algorithms are used to predict a user s rating and country based on their code 
here m denotes the number of training examples x i denotes the feature vector for example i and y i denotes the label for example i 
v methods
linear regression is used to predict the user s rating and the rank is inferred from the rating using
a linear regression
int n int main scanf d n 
raw tokens
int 
var int main scanf 
processed tokens
str 
translationunit vardecl endblock functiondecl compoundstmt callexpr 
ast traversal
concatenated sequence int 
var translationunit vardecl endblock 
libclang c parser
normalization and scaling approx 
2k features the weight w i is the inverse of the number of users in the training set with the same rank all of the other methods are classification algorithms rather than regression as they seemed to work better see results 
bigram extraction min 1 freq 
the maximum likelihood estimators of each class mean k and the covariance matrix are computed where again the weight w i is the inverse of the class size prediction assumes a uniform prior due to class imbalance 
b gaussian discriminant analysis gda 
for country prediction we use softmax regression with a weighted cross entropy loss 
again the weight w i is the inverse of the class size i k denotes the predicted probability that example i is in class k for rank prediction since the goal is to predict within one rank of the actual rank we trained a separate logistic regression model for each rank 
c logistic regression
each training example of rank r is considered to be a positive example in the models for ranks within 1 rank of r all examples are considered positive examples in three ranks except for examples of the lowest and highest ranks which are only considered positive in two ranks 
therefore their weight is multiplied by 3 2 here 
empirically this results in a model where the classification accuracy for each rank is more even 
the neural network model is similar to logistic regression and uses the same loss functions 
a single fully connected rectified linear layer with 100 units is inserted between the input and output layers as shown in
d neural network
all experiments were conducted using 10 fold cross validation 
for each type of model we trained 10 models where each model is trained on 9 contests 54k examples and tested on 1 contest 6k examples 
vi experiments
the values reported here are averages over the 10 models 
with this methodology the models are tested on problems never seen in training 
this ensures that the models are not learning specific features about the problems in the training set due to the class imbalance described before accuracy is defined as the weighted accuracy where the weight w i of each example is the inverse of the class size in the test set 
for rank we allow the predicted rank to be within one rank of the actual rank 
if y i is the actual label and i is the predicted label for example i the weighted accuracy shows how well the model can predict all classes and not just the majority 
a model that strongly favors larger classes would achieve a high unweighted accuracy but low weighted accuracy for the linear regression model we also report the weighted root mean squared error rmse for the predicted rating 
the accuracies obtained for each model are shown in classification was found to work better than regression when predicting the rank 
this may be because classification optimizes what we actually care about which is predicting the correct rank rather than the rating 
vii results and discussion
the linear regression model had a weighted rmse as previously defined of 545 when predicting a user s rating in the test set 
given that ranks have a rating range of 200 this is a fairly large error 
gda worked surprisingly well achieving accuracies that are almost as high as logistic regression 
while gda assumes that p x y is multivariate gaussian logistic regression does not make that assumption and is capable of modeling a large variety of other distributions 
since the accuracies are similar this indicates that p x y is gaussian to some degree out of all the algorithms the neural network had the highest accuracies 
the neural network was probably able to learn more complex relationships between the features compared to the other algorithms 
perhaps some combination of several bigrams is highly indicative of rank or country 
interpretation of the neural network is out of scope of this project however the high training accuracies compared to test accuracies may indicate overfitting 
in the neural network dropout helped reduce overfitting as described before but no other regularization techniques were used 
we briefly tried using principal component analysis pca to reduce the number of features and l2 regularization on the parameters but these techniques decreased the test accuracy 
more data helped reduce overfitting as the accuracy values are about 5 higher than initial tests performed with 5 contests instead of 10 for each actual rank and country the neural network test accuracies are shown in high skilled competitors appear to use ifdef significantly perhaps to change the code s behavior at compile time by defining macros in the compiler flags 
also they appear to use assertions and c function templates low skilled programmers appear to use cin and cout for input 
this makes sense since scanf and printf are faster input methods and often preferred by experienced competitors it is interesting to see translationunit as a strong indicator of low skill level 
translationunit is the root of the ast and appears exactly once per per program but since the count is normalized by the l2 norm of the count vector its value will be higher in shorter programs 
thus it appears that gda has learned to associate smaller programs with lower skill levels despite having the l2 normalization to try to prevent this 
it makes sense that a long program would likely indicate a hard problem and a high skilled competitor tables vii and viii show the features with the highest class means for chinese and american competitors respectively 
it seems that chinese competitors often use getchar to read single characters from standard input and import c input libraries like cstdio 
american competitors seem to often spell out std in their code like std cout std endl instead of importing the entire namespace with using namespace std and use ld which is a commonly used alias for long double 
in this paper we studied the application of machine learning techniques in predicting the rank and country of a codeforces competitor based on a single source code submission 
the neural network model achieved the highest accuracy of 77 2 accuracy in predicting rank within one rank and 72 5 in predicting country 
ix conclusion and future work
despite not achieving the highest accuracy the gda model was easier to interpret and we were able to find unigrams and bigrams that were the strongest indicators of certain skill levels and countries future work may include testing rnn or lstm based models as discussed in related work 
acquiring more data may help reduce overfitting 
token processing could be improved for example by replacing class and macro names with special tokens in addition to variable and function names 
n grams with n 2 could be tested as only unigrams and bigrams were considered here 
more hidden units or layers could be added to the neural network 
interpretation of the logistic regression or neural network model could be attempted 
deep neural networks dnns have shown tremendous progress in accurately performing regression classification and control tasks that traditional programming paradigms have not been able to achieve 
however their lack of interpretability and performance guarantees presents challenges when considering deployment to sensitive tasks with limited margin of error such as control of autonomous vehicles military equipment or robotics 
one pressing problem with modern neural networks is the existence of adversarial examples where small perturbations in inputs can result in dramatically different output classifications 
to combat this problem developing adversariallyrobust networks is an active area of research 
currently model robustness is evaluated against first order adversaries i e 
adversaries generated via gradient methods it is an open research question whether these first order methods are good metrics for testing the overall robustness of neural networks to all adversaries 
to tackle this problem we use a neural network constraint solver developed by stanford s reluplex team 
by using a novel algorithm that optimizes the number of linear constraints that must be checked certain properties about general robustness can be guaranteed for all attacks rather than only first order ones 
we examine the robustness of two simple models one trained only on mnist data and one adversarially trained using a modern first order adversarial defense strategy called logit pairing 
in both networks we found that the first order gradient well approximates the guaranteed closest adversary and that adversarially training against first order attacks generalizes to all attacks 
future directions involve scaling reluplex to handler deeper networks to verify this trend 
following the success of applying convolutional layers to image recognition although they outperform humans at many things there are downsides to the use of modern neural networks 
firstly deep neural networks lack interpretibility although they get the correct answer we are often not sure why 
unlike traditional coding pipelines where modularity enables manual proofs of correctness and isolated failures deep end to end models are notoriously hard to interpret and when they fail it s difficult to understand why 
although some advances have been made in determining what each filter might represent in a convolutional model adversarial examples an interesting question is what guarantees can even be made about networks being robust against adversarial examples especially ones trained to defend against these adversarial attacks 
although this problem is np hard recent work by the reluplex prior to complete solvers like reluplex first order attacks that use the gradient of a network to compute adversarial images have been the standard to both train and test for adversarial robustness 
however it is an open research question whether these gradient based defenses actually improve robustness against all attacks or just first order attacks 
in our project we evaluate the robustness properties of two simple mnist classifiers 1 
are first order attacks good universal indicators of robustness 2 
do first order defenses generalize to non firstorder attacks 
much work has been devoted to protecting models from adversarial examples 
generally adversarial examples can be generated in two ways white box attacks where the attacker has access to the network and its weights and black box attacks which don t have access to network specifics adversarial training against first order adversaries is the main method of making models adversarially robust where models are trained on both the train data distribution and an adversarial distribution generated from that data distribution one recently discovered problem with determining the robustness of models against adversarial examples is the issue of gradient masking where the defense techniques make the gradients of a network less useful in generating adversarial examples but don t make them less susceptible to examples generated through another mechanism these obfuscated gradients which hinder iteration based first order attacks have been shown to give networks a false sense of security where first order robustness is a result of bad gradients rather than a sound defense 
one of the main advantages of using a complete solver like reluplex is that robustness is guaranteed because it solves for examples within nearby l balls 
reluplex is a decision procedure to solve linear equations that have non linear constraints such as neural networks with relu activation functions 
as mentioned earlier this task is typically np hard making it intractable to compute in the worst case 
marabou reluplex
however the novelty of reluplex when translating what robustness of a network means we use the definition by the reluplex team in english the above definition is basically saying a point is locally robust if the l ball around x 0 contains no misclassification errors 
in reluplex we encode the negation of this robustness property to solve for a norm perturbation which results in a misclassified image 
running reluplex was the clear bottleneck of our project as it could sometimes take up to 10 minutes to verify whether a value of was robust or not 
as expected adversarial guarentees require a fair amount of compute and reluplex has yet to be optimized for parallel computing 
we trained a simple multilayer perceptron mlp network i e 
a model we learned in class that is not a deep model to classify 28x28 pixel images from mnist a database with 60 000 training and 10 000 testing images of handwritten digits 
baseline vanilla mnist network
we divide the pixel values by 255 to normalize the pixels between 0 and 1 
the mlp consisted of a single hidden layer with 50 nodes with relu activations and an output layer with 10 nodes and a softmax activation corresponding to class probabilities 
we train with respect to a categorical cross entropy loss using an adam optimizer with learning rate 1 0 10 4 
we trained for a total of 60 000 batches of batch size 64 or 64 epochs by which time our network had converged 
where x 0 is the original image t is the number of time steps is the step size and l x y is loss of the network output with respect to some adversarial label y the class we are trying to misclassify as 
we initialize the adversarial search process with a random noise perturbation within our l ball and we clip to make sure pixels stay within this sized ball 
adversarially trained network
while training we ran 40 iterations when generating a batch of adversarial examples which were targeted towards random class labels 
the step size for iterated fgsm was simply a fraction of the ball and the step size 
when adversarially training our network we ran 250k batches of size 64 about 265 epochs where each adversarial batch was generated within a 0 3 normalized l ball or a 76 5 pixel ball 
since our accuracy on clean examples began to drop by a notable amount we trained an additional 250k batches on examples only within a 0 1 normalized l ball or a 25 5 pixel ball so that the aggressive adversaries wouldn t have as large of an impact on the loss 
we stopped after these iterations because our network performance had converged 
since we are typically interested in having a loss that rewards classifying adversaries and clean images correctly the loss function is typicallywhere l orig is our typical loss function with respect to an input batch in this case the cross entropy loss 
in addition to this traditional adversarial loss the modern defense adversarial logit pairing defines the new loss where f x i is a function mapping inputs to the logit layer of a model and l is any loss that promotes closeness of its two inputs in their paper and ours we simply use l 2 loss 
adversarial loss with logit training
similar to other regularization methods discussed in class this added loss is equivalent to the probabilitic assumption that the error in the logit layer between original and adversarial images is best represented by gaussian noise 
while training we use a value of 1 
finding the closest adversaries was relatively straightforward 
first we found the closest adversarial example for iterated fgsm by repeatedly binary searching over our balls for fgsm and seeing if the network misclassified it starting with min 0 and max 1 and attempting for every class as opposed to a random class 
finding closest adversaries
we only found these adversaries for examples that a network initially classified as correct 
we performed a similar binary search with reluplex to find the guaranteed closest adversary 
after training we evaluated both the vanilla and adversarially trained models on the 10 000 clean test examples in mnist and 10 000 adversaries generated from those examples with 0 2 normalized balls generated by fgsm 
we found that the vanilla model got 97 accuracy on the clean examples but only 14 accuracy on adversaries 
test accuracy on vanilla and adversarial trained network
on the other hand the adversarially trained network got 96 5 accuracy on the clean examples and 89 accuracy on the adversaries 
a concern we had was whether the small capacity architecture we used could learn an adequate mnist classifier and an adversarially robust model with logit pairing 
the significant increase in accuracy on adversaries and comparable accuracy on clean examples in the adversarially trained model when compared to the vanilla one indicates that adversarial training indeed did increase robustness towards first order attacks even under the low capacity constraint 
the next thing we tested was the cosine similarity between the closest adversaries generated via fgsm and reluplex on both the vanilla network notably the cosine similarity of the perturbations in the adversarially trained network were notably lower than in the vanilla one 
we hypothesize a few possible explanations for this behavior first we can think of our mlp as a piece wise linear model with 2 50 different configurations depending on the activation state of the relus 
similarity between fgsm and marabou adversaries
since we are searching a larger ball in the adversarially trained network there are a greater number of possible linear modes that the network could be in as there is a wider input space over which the activation state of a relu could flip 
therefore it is possible that the greater variation in linear modes creates potential adversaries in different linear modes that gradients will not find which posits an explanation for the lower cosine similarity 
alternatively in the vanilla network the model behaves more linearly in the sense that there are a restricted number of linear modes 
this provides an explanation for why the perturbations were more aligned as the only way to minimize a loss in a linear model is through the first order gradient a second explanation is athalye et al 
s hypothesis of obfuscated gradients examples of perturbation alignment and nonalignment in the case of the adversarially trained for each network and each adversary method can be seen in
we compare the robustness defined as the distance of the closest adversary in both the vanilla network and the adversarially trained network in the vanilla network
similarities in values for fgsm and marabou
in conclusion we were able to provide some preliminary answers to open research questions surrounding non firstorder adversarial attacks 
we were able to for the first time provide answers to some of these questions through access to reluplex a complete and sound linear constraint solver that could be be used to find the guaranteed closest adversary around a point our main results were two fold 
first first order attacks provide good approximations of the closest adversary in both adversarially untrained and adversarially trained networks suggesting that benchmarking against first order attacks is a good measure of overall robustness 
furthermore adversarial defense strategies targeted towards first order attacks do a very good job of generalizing to non first order attacks a clear direction of future work is improving the reluplex solver to support larger architectures so we can expand the current study to larger and deep networks 
it d be interesting to see if the more complicated loss surfaces and greater variation in linear modes present in deeper models could lead to more pathological non first order attacks 
this direction would also provide us with the means to better test athalye et al 
s hypothesis of obfuscated gradients on similar networks to those studied in the paper 
in this project we primarily explore how supervised learning classifiers of increasing complexity generalize on gans synthetic mnist images and secondarily how semi supervised learning classifier performs on real images 
the methods utilized encompass linear classifiers neural networks and modified gans 
the supervised learning classifier seemed to generalize reasonably well on the synthetic images while semisupervised learning classifier performed worse than expected on real images 
generative adversarial networks gans have been introduced in 2014 combining learning generative models with game theory 
in the past four years some implementations of this new unsupervised learning model reached quite significant results 
they generated qualitatively convincing replicas of the training dataset from random noise 
but how convincing are those images for a classifier trained with supervised learning techniques 
also could gans opportunely modified improve standard supervised learning classifiers results 
to answer these questions we evaluated how well supervised learning classifiers see methods first subsection trained on the standard mnist dataset generalized on images generated by pre trained gans
deep neural networks and in particular convolutional neural networks have been extensively used for solving the mnist digit classification problem and are considered state of theart
the real mnist dataset has been downloaded using keras apis 60000 training examples 10000 test examples while the synthetic mnist dataset 6336 unlabeled examples has been generated using this deep convolutional generative adversarial networks each mnist grayscale image has dimensions 1 28 28 
the input features are the pixels that compose the mnist grayscale image 
for what concerns the features learned by the classifiers the neural networks derive new features in the hidden layers and the convolutional ones extract additional features in the convolutions 
for consistency we used the cross entropy loss and adam optimization for all classifiers 
specifically we used keras sparse categorical cross entropy loss function which is the cross entropy function for discrete distributions with the constraint that the output labels must be integers ranging in values of the classes softmax has been used as activation for all output layers k 1 e x k in addition the semi supervised classifier also uses the sigmoid activation for its second output layer
we built and trained a total of 4 supervised learning classifiers a linear softmax classifier a 2 layers fully connected neural network nn the fully connected nn takes in the input image with dimensions 1 x 28 x 28 then flattens it and sends it through relu activation at 256 neurons and then the softmax layer at 10 neurons for classification 
the cnn 4 layer starts with the input image and convolves it with 32 filters with kernel size 5 
supervised learning classifiers
then it goes through max pooling with pool size 2 and the output is flattened and sent through relu activation 128 neurons before the softmax output layer 10 neurons 
the cnn 5 layer is identical to the cnn 4 layer architecture but it has an additional convolution layer with 64 filters and kernel size 5 after the first one for building training and testing our models we used tensorflow keras high level apis
the semi supervised modified gans discriminator has the same architecture as the cnn 5 layer but with two output layers 
semi supervised learning classifier
our primary quantitative metric of evaluation was accuracy the accuracy was calculated as the number of correct classifications over the number of total number of classifications to better understand which digits were most affected by misclassification we also calculated the confusion matrix for the best performing classifier against both the real and the synthetic test dataset
experiments and results discussion
the test accuracy obtained by the various supervised learning classifiers over synthetic images was overall slightly worse but comparable with the real images where the most complex convolutional nn cnn 5l performed best 
overall the various supervised classifier seemed to generalize reasonably well on gans synthetic mnist images 
we expected the cnn to perform better than any other classifier since it is considered state of the art for the mnist digit classification problem for each attention map there are three different ways we modified the backpropagation gradients 
vanilla is the case when there is no modification performed on the backpropagation gradients 
the relu modifier clips negative gradient values and the guided modifier modifies the backpropagation to only propagate positive gradients for positive activations 
the guided modifier seems to produce the clearest attention maps the confusion matrix describes the performance of the classifier model and displays how many and which examples were classified correctly or incorrectly 
analyzing the results from the confusion matrix of real images in comparison to the real images we observe the results for the confusion matrix on the synthetic images overall based on the results of the confusion matrices the cnn 5 layer performs very well on the real images but comparatively not as great on the synthetic ones though total accuracy is still quite high 
interestingly the attention maps showed that the cnn s focus features for identifying a real image may be different for the same number synthetic image 
for example the features to identify real image 5 and synthetic 5 are quite different 
we trained four supervised learning classifiers on real mnist images and tested them on real and synthetic images for classification 
we also trained a semi supervised learning modified gans discriminator to classify real images as well as discern whether an image is real or synthetic 
the cnn 5 layer performed the best on classifying the real and synthetic images 
the cnn 5 layer performed better probably because it had an extra convolution layer with 64 filters so it could extract features that the other models could not for future work the goal would be to implement some approaches to solve issues noticed during error analysis for the supervised learning classifiers since the cnn was overfitting pretty severely the next step would be to add some regularization to help reduce the overfitting such as a dropout layer 
another approach to try out is implementing a resnet to see if it could perform even better than a cnn for the semi supervised learning gan the next step would be to build a complete semi supervised learning gans classifier such as the one suggested
both team members worked on coding up the various classifier models with visualizations running said models to gather results labelling synthetic images and writing up the proposal milestone poster and final report 
contributions
our cs229 machine learning project is collaborated with a company zipline http www flyzipline com some introduction about zipline zipline delivers blood and supplies to remote hard to reach hospitals and health centers via drone 
zip drone is a small fixed wing aircraft launching from a distribution center flying low over a hospital and dropping a cake box with a small paper parachute 
inside the box is blood or medical supplies the hospital has ordered 
zipline currently does about 30 flights a day in rwanda delivering 20 of the national blood supply our project aims to help zipline with predictive maintenance of the parts used in the flight before the flight takes off 
with this effort they the attempt is to prevent failures that can be detected to achieve more delivery accuracy 
since the data includes previous flight records and analysis along with the labels this would be treated as a supervised learning problem 
problem of predictive maintenance has been one of the major concerns for most industries using moving parts 
as discussed in 2 has a good description on the performance of svm classification for machine condition diagnosis 
they have also provided promising result for the ability of svm for future fault diagnosis 
once a flight has completed its delivery the bat tery is plugged in to a board which generates and c ollec t s information related to the flight 
such information can include things like power usage weather conditions total trip details and other telemetry signals which are the input features for our machine learning algorithm 
this data is analyzed by a set of scripts maintained by zipline developers which post the analysis as a yaml file describing its features 
below is a small snippet of a yaml file that is stored as amazon s3 buckets 
figa snippet of raw data the image shows a very small subset of how the flight yaml file looks for a file 
this file is generated after running analysis script on the telemetry signals generated by the battery after a flight 
each flight has an associated with flight yaml file with 1400 such features stored as nested objects in the yaml file as of today we have data for 3199 flights out of which 430 flights are labelled as failed flights 
so our value of m 3199 
n number of features is variable based on the flight label but is generally of the range 1200 1500 
out of 430 flights 29 flights were flight crashes 
so we had 3 labels namely success mission failed where flight returns back to origin and flight fail where flight deploys parachute as previously mentioned this data is stored into amazon s3 buckets along with all the raw telemetry data and other log files 
on amazon s3 the data is stored as flight logs dd mm yyy nest 1 flight id our work included using the amazon s3 boto c lient t o parse through these files and download the relevant data for our purpose 
this required us to write a s3 ut il script to only query the data relevant to run machine learning algorithms 
this data is stored in ap south region servers in amazon and it took substantial amount of time to query only the relevant data that we need one more challenge in getting the data was to fet c h good enough number of failure flights 
this was achieved by querying through a document storage database 
we used the fields inside the yaml file to query for failure and passed cases and then invoked our s3 util to fetch the relevant yaml files we started with 3000 flight yaml files so m 3000 with average of 1200 features for each flight n 1200 
we used python pandas to load the yaml file and t hen concatenate based on the columns 
we generat ed a csv file mxn fields 
this gave us a first real pars e able input structure 
our first work on this input included normalizing the data 
we used the standard scaler from sklearn which essentially subtracts the mean and divides by standard deviation 
feature reduction correlation 
after doing this we spent some time in removing features that had none or nan values for most of the flights lot of the earlier flights recorded did not capture all the features 
we used zero values for those missing features in order to preserve the flight information features also consisted of fields like flight id commit id description and some other string fields which we needed to clean 
we wrote several utility functions to do this part 
small snippet of how data looks after the above steps after cleansing the data we were left with 700 features 
looking at the number of features and the values in most of them we decided to go ahead with feature reduction before starting with any classification algorithms 
for feature reduction we generated a correlation matrix between all the features and the input features we used this matrix to remove the features bas ed on the following rules 
if the correlation between the features was too high i e 
if two features were too closely correlated then we removed one of them 
we also removed the features which were very highly uncorrelated with the output label 
below is the plot of the correlation matrix which we got after feature reduction figc correlation plot for 18 features after feature reduction using the above methods we were left with 18 features so our initial m n of 3000 1400 was reduced to m n of 3000 18 evaluation metric that we used was to compare the predicted label success mission fail flight fail with the actual value of highest failure level captured by the flight analysis 
the error reported is in perc ent age of prediction accuracy 
we chose the complete 3000 examples as our t rain set 
our test data were real time flight logs collected in recent time 
models and results
we started with a test on 250 flight s and kept adding daily flights normal equations first set of machine learning we applied was logistic regression using normal equations 
as per normal equations we got the parameters using the output was calculated using sigmoid function after several rounds of trial and error we selec t ed a threshold of 0 65 to classify the flight as failure i e 
if p x 1 0 65 
this gave us the following results on train and validation sets the next algorithm we implemented was the used was the gradient descent algorithm 
as per the gradient descent algorithm we chose to update parameters by minimizing the cost function we chose the following parameters for our gradient descent algorithm 
we tried this algorithm with without l1 l2 regularization
gradient descent algorithm 
a s per t he locally weighted linear regression algorithm we fit our parameters to minimize and then used sigmoid function as mentioned above 
locally weighted linear regression gave us poor results as compared to the other two methods mentioned above 
locally weighted linear regression 
even with varying the values of t au we used the maximum accuracy we reached with t his method was 71 69 
decision trees employs a top down greedy search through the space of possible branches with no backtracking 
we have used classification tree as the target is to classify the flight as one of the three classes mission success mission fail or flight fail maximum depth of the decision tree controls the tradeoff between error due to bias and variance in our model we have optimized it to give the best bias variance combination with a max depth 5 ensemble method which combines several independent base classifiers to construct one classifier class is random forest 
trees and forests 
each base classifier is trained on a set sampled sample size with replacement from the original training set guaranteeing independence bagging or bootstrap aggregation 
additional randomness is introduced by detecting the best split feature from a random subset of available features feature size sqrt n in order to visualize the data in 2 dimensions we decided to run pca on it principal component analysis pca is used to reduce the dimensions of data to two major components before applying more sophisticated dat a analysis methods such as non linear classification algorithms and decision trees analysis on test and train set with two principal components was performed on logistic regression lwr decision trees random forests and s v m wit h linear kernel and radial basis kernel rbf 
logistic regression classifiers accuracy was not very good 
this probably means that the decision boundary is nonlinear hence svm with rbf kernel was the best choice out of the above we trained and tested an svm with rbf and linear kernels and optimize the hyperparameters gamma 
apply pca and see how the number of principal components influence the accuracy 
below contour shows the accuracy of svm with rbf kernel with t wo principal components as 82 
the hyperparameter gamma obtained by running svm on 2 principal components are used as starting point on the complete feature set svm 
if t he value of gamma is too large then the model can overfit and be prone to low bias high variance as gamma of linear rbf kernel controls the tradeoff between error due to bias and variance in our model we have optimized it to give the best bias variance combination 
support vector machines svm linear rbf kernel for classification 
as you can see in both the learning curves the distance between the training and cross validation scores narrows down with the number of examples we achieved good results with svm on rbf kernel 
below is the confusion matrix for the same our model got 100 accuracy in predicting flight failures 97 accuracy in predicting success and 63 accuracy in predicting mission failure cases 
missing features in the earlier flight makes some of the mission failure cases to be categorized as success 
w hat t his model tells us is that if the flight is categorized as mission failure or flight failure with the previous run being successful then it might need maintenance 
our model will be serialized and used in the analysis scripts of zipline 
the predicted probabilities of each label will be added along with the flight data and us ed for predictive maintenance 
this sums up the work we have carried out t ill now 
to summarize first we spent quite a lot of effort to extract the data from amazon clean it perform feature reduction 
conclusion future work
after doing that we ran the above three algorithms that we have listed and got some results 
from the results the method of applying svm with rbf kernel is getting us good results as a future work we can implement rulremaining useful life to predict the number of flights left in all the parts 
this will help plan the maintenanc e cycles in a better way and increase the confidence with which a flight takes off 
another approach can be to have an unsupervised model to detect anomalies in the telemetry numbers reported 
new renewable energy sources require improvements to the current electric grid 
the recent surge in the number of intermittent energy generation facilities such as solar and wind farms has resulted in a need for improved monitoring and control methods for the electric grid due to increased supply side uncertainty 
mitigating the uncertainty in the amount of electricity privately produced would greatly increase power generation efficiency resulting in less waste of fossil fuel generated electricity one major component of supply side uncertainty comes from residential solar panel installations 
today installing solar panels on residential homes is easy and affordable and will only become easier and more affordable as time progresses 
as a result it is difficult to know how many solar panels exist and supply power to the grid 
if energy companies had more insight into this piece of the supply side puzzle they could better model an area s energy production and balance power plant production accordingly resulting in lower energy costs and less environmental impact for this project we implemented and optimized an artificial neural network nn and a support vector regression svr algorithm to predict the number of solar installations in a given tract from census data 
the input to the model consists of geographical and demographical characteristics such as land area average household income climate data number of residents of age 30 39 etc 
the model takes these census data and then outputs the number of solar systems in a given tract 
the model is trained using supervised learning on a labeled dataset 
we also used the models and principal component analysis pca to determine which features have the most influence on modeling the number of solar systems i e 
which features are most strongly correlated with solar deployment density 
there are surprisingly few previously published studies that use census data to make demographic predictions considering the availability of census data the few that were found are described here 
the university of california irvine s uci 1996 census income dataset has been used to predict whether income levels are below or above 50 000 per year with logistic regression and random forests achieving classification accuracy of around 92 neural networks have also been used in conjunction with census data 
previous work
predicted population counts using a standard feed forward neural network with 4 6 hidden layers to our knowledge the soon to be published paper
the dataset used for this project contains 155 different demographic statistics of each respective census tract along with the number of solar systems in each tract the label number of solar systems refers to the count of solar installations not individual panels 
this means that a rooftop solar panel array such as on seq buildings and a large solar farm such as the ivanpah solar power facility in western california are each counted as a single solar system 
this labeling system is used to avoid skewing the data with large solar farms 
in addition this labeling system is more useful for mitigating the uncertainty in the number of solar panels the majority of which is due to rooftop solar arrays rather than large solar farms 
based on the limited amount of previous work on census data related predictions we decided to implement two different machine learning algorithms support vector regression svr and a fully connected neural network nn to predict solar system deployment density 
the nn was chosen because of the aforementioned preceding work for census prediction tasks while the svr was chosen as an candidate for improvement on the logistic regression used in previous works 
these two algorithms were also chosen because both can learn highly nonlinear trend lines and after carrying out pca it was immediately clear that the data is nonlinear 
the following sections detail the pca svr and nn algorithms 
in order to visualize the data as well as gain insight into which features are the most influential in modeling two component principal component analysis pca was implemented 
the first k principal components of a dataset correspond to the first k eigenvectors of the matrix 1
principal component analysis
a nn consists of layers of neurons which first linearly combines all inputs x i into the neuron asj is the j th neuron in layer l w i is the weight for the i th feature and b j which allows the model to learn non linear trends 
x 155 and then predicts the number of solar panel systems for the given input features 
this result is then passed to the next layer of neurons and similarly to the next until the final layer outputs a prediction for the number of solar systems in the backpropagation step the nn optimizes the weights w and biases b of each layer by minimizing them against the loss function l y where y is the ground truth label using gradient descentwhere is the learning rate 
the derivatives are calculated by using the chain rule of calculus starting from the loss function working backwards through the layers 
further details can be found in the neural network designed for this project was coded using keras where y is the mean of the labels y the hyper parameters of the final nn were chosen after two rounds of hyper parameter searches 
in the first search we ran 25 models with randomly selected hyper parameters and then compared the r 2 and mae of each model 
a summary of the varied hyper parameters are shown in the table below 
tuning based on the hyper parameter ranges in table 2 the best performing model achieved an r 2 of 0 77 and a mae of 10 6 with a learning rate of 0 003 a batch size of 512 2 hidden layers 493 and 216 neurons in the hidden layers and a dropout probability of 0 22 and 0 24 for the hidden layers to arrive at the chosen hyper parameters for our final model we performed a second optimization by varying individual hyper parameters and comparing the metrics one model at a time 
the performance of the final model is shown in
in classification tasks support vector machines svm work by maximizing the margin between the classes 
support vector regression svr works similarly but instead attempts to find a best fit curve 
support vector regression
svr s can also utilize kernels to fit non linear curves by mapping features to a high dimensional space 
below are the kernel functions used in the svr models where x i x j are generic feature vectors the kernelized svr model was explored using these three kernels 
the penalty parameter was first set at 300 kernel cache size at 200 and the kernel coefficient set at 1 n x 
model selection and parameter tuning was performed with scikitlearn s gridsearchcv module
the svm performed best with a radial basis function kernel while the nn achieved similar performance after two rounds of hyper parameter searching 
these results are summarized in
model performance
for this application we computed numeric gradients using the nn to determine the influence of each feature on solar system deployment identifying which features correlate most directly with the number of solar systems 
to do this numeric gradients of the label with respect to each input feature were calculated by changing a single feature by a small and completing a forward pass to make a prediction 
feature influence
features that have a large positive numerical gradient have a strong positive correlation with the number of solar systems while features with a large negative gradient have a strong negative correlation 
the results both validate the model and offer some interesting insights into the data 
for example a strong positive correlation with yearly solar irradiance is expected 
a strong negative correlation with number of frost days is also expected since cold areas do not get as much sunlight as warmer areas thus these census tracts are less likely to have high concentrations of solar panels 
the strongest negative correlation is with respect to population density which is also justifiable because areas with a lot of large houses or low population density are bound to have more solar panels than dense urban areas where residents do not have the same control over their own roofs the most striking but not surprising results are the strong positive correlation with democratic voters and the strong negative correlation with respect to republican voters 
in both 2012 and 2016 the results rank near the top of each respective category 
this distinction between the two political parties is stark though not surprising given each party s respective stance on climate change 
both an svm and a standard feed forward nn can predict the number of solar systems reasonably well from us census data 
both models have similar performance achieving an mean absolute error of around 11 solar panels systems and an r 2 value of the best fit line between the predictions and the true labels of around 0 79 
principle component analysis determined which features contributed the most variance to the labels and nn numeric gradients of the prediction with respect to each feature quantified each feature s influence on the model 
strongly correlated features include voting tendencies solar irradiance median housing values and population density opportunities for future work include separating residential and commercial installations which could even more accurately help model un tracked energy contributions to the grid 
furthermore it would be worth exploring how much power is generated in a given census tract which would be a function of not only number of solar systems but solar panel area and weather conditions 
similarly it could be useful for companies selling solar systems to be able to more accurately predict the monetary gains from energy generated over time in a given neighborhood as a selling point or revenue predictor 
github repo https github com brettski15 cs229 solar branch master see included readme md for running instructions
code
in sub saharan africa an estimated 184 million people rely on hand pumps for their water supply
our project is a tool for development agencies and governments to understand the state of water resource infrastructures in underdeveloped and vulnerable regions of the world 
in 2015 an estimated 184 million people living in sub saharan africa relied on hand pumps for their water supply
the taarifa dataset that we used in this study and variants of it has been extensively explored in the field of access to water sanitation and hygiene services in developing countries 
most of those studies however do not use machine learning methods to analyze the dataset 
a recent study from 2017
the dataset used in this study was collected by the tanzania ministry of water aggregated by taarifa and downloaded through kaggle com 
to train and test our algorithms we initially split the dataset by randomly assigning 25 of it to the test set and 75 of it to the training set 
however by doing so we became aware that there was a class imbalance problem where some of the classes for all three classification problems had a relatively small number of points less than 10 of the total for functionality which has three classes 
dataset split
we therefore decided to apply the synthetic minority over sampling technique smote as described in
we first performed a feature screening and decided to use only 24 of the 40 features 
our screening process excluded 16 features for the following reasons irrelevance some of the features were deemed irrelevant to our project and we decided to exclude them to reduce the computational cost of our algorithm redundancy some of the categorical features had exact or almost exact duplicates and we decided to only keep one out of the two or three identical features 
in these cases we kept the most granular feature 
in particular this reduced the number of geographical features 
we then transformed most of the remaining categorical features into binary variables through a one hot encoding ohe process 
finally we imputed values where data was missing and replaced those data points with the mean continuous or mode categorical binary of the feature that was missing 
this allowed us to keep over 24 000 data points that were missing at least one feature 
we tested the following algorithms on each of the three classification problems tackled and performed them both for the original train test split with class imbalance and the smote resampled split 
ultimately all models were trained on the set that was resampled from 75 of the original data and tested on the remaining 25 
we tested logistic regression lr gaussian discriminant analysis gda support vector machine svm decision trees dt and neural networks nn algorithms 
based on preliminary results both in terms of computational time and accuracy of the results we decided to only optimize the lr dt and nn algorithms 
models were optimized using grid search cross validation 5 fold to fine tune hyperparameters and final results were obtained using 5 fold cross validation on the test set 
algorithms were evaluated and optimized based on the micro f1 score and on the matthews correlation coefficient mcc because of the class imbalance of our test set 
the voting ensemble method was used to optimize our final results 
lr dt nn and vc algorithms optimization is described below logistic regression lr logistic regression was chosen because it is a robust learning algorithm that makes few and usually reasonable assumptions about the data 
the penalty factor and the type of regularization were optimized 
best performance for this algorithm on all three classification tasks was achieved for an l2 regularization with penalty factor of 1 0 decision trees dt a decision tree model seemed a particularly promising idea given the number of features used in our algorithm especially after the ohe process 
having many features none of which have an obvious effect on the output alone means that the causal relationship between the features and the output might come from different combination of the features that cannot be modelled well by algorithms that rely on assumptions about data distributions 
we tried the random forest rf adaboost and bagging and after tuning the parameters of each algorithm rf performed the best neural networks nn our last algorithm was a nn because of the nn s ability to generalize and to respond to unexpected patterns 
during training different neurons are taught to recognize various independent patterns and then the combination of all the neurons manages to capture all those different patterns and combine them in one final output node 
since our dataset likely contained unexpected and difficult interactions this was a promising choice voting ensemble classifier vc none of the methods discussed above provided exceptional accuracy for all classes so the last optimization step was to combine the best three methods rf nn lr into one ensemble method the voting classifier which provided great results 
we mostly used hard voting i e 
majority class but for water quantity we used soft voting which averages output probabilities from the three input models 
the parameters involved in our final models are as follows 
all three of our classification algorithms provided us with probabilities of the samples being classified into one of the classes 
since we are dealing with multiclass classification outputs our algorithms assign each point to the class that holds the highest probability 
in order to evaluate our models performances we produced confusion matrices to compare predicted and true values 
since the class imbalance was preventing our algorithms from learning the characteristics of less represented classes well enough to predict them we found the smote resampled dataset to produce better results than the regular dataset split 
the results are presented in three forms confusion matrices which give an idea of how the accuracy of the predictions is distributed between classes and an evaluation of the prediction through two scores that balance precision and recall 
precision and recall are defined by precision t ruep ositives t ruep ositives f alsep ositives 1 recall t ruep ositives t ruep ositives f alsen egatives
while the micro f1 score is a good measure of the overall accuracy of our predictions it is not a good evaluation metric for the less represented classes 
to deal with this problem we decided to use the matthews correlation coefficient mcc as defined below for the multiclass case because it takes into account the ratios of the four confusion matrix categories true false positives true false negatives the mcc is a good evaluation metric for imbalanced datasets 
figure 4 micro averaged f1 scores for train and test datasets
upon evaluating the mcc for all our algorithms we found a similar pattern to the micro averaged f1 score in that rf always had the highest scores followed by the voting classifier as shown in in
there is definitely a lot to be done to keep improving the performance of these classification tasks 
one possible future step would be to keep optimizing our models using specific strategies trying different activation functions for each layer in the nn performing feature selection for rf testing more kernels for svm or using a nonlinear combination of the features for lr as these might increase the accuracy of our predictions 
because of time constraints we were only able to search a grid of parameters that were reasonably but arbitrarily chosen for each algorithm so given more time we would likely implement a more consistent and formulaic way of choosing a grid to search for optimal parameters 
furthermore it would be useful to explore the input features some more and see which ones contribute the most to a successful prediction 
a summary of how much variance is explained by each feature would help decide which measurements are crucial for future work 
another path would be to look at differences in predictions between countries or geographic regions to test the robustness of our algorithm 
we also think there is value in going deeper into the results and seeing where other than geographically the algorithm fails i e 
see if there are similar characteristics for the examples for which we are predicting incorrectly finally adapting the model to predict when a pump will fail would make it a more applicable tool for managing agencies 
the project code can be found on https github com jackieff cs229project
project code
the performance of machine learning algorithms is being scrutinized more and more using metrics other than just accuracy as algorithms are applied to numerous and diverse aspects of society fairness in ml has become a central concern in building inclusive and socially responsible technologies 
specifically machine learning models trained just by maximizing predictive accuracy often reflect and incorporate unwanted biases present in their training data 
i introduction and motivation
we consider the case of predicting some label based on features x where x contains a protected feature z with respect to which we want our predictions to be unbiased 
the applications for this setup are numerous predicting recidivism scores assessing criminal risk making hiring decisions approving loans or predicting salary protected from gender race or zip code in order to augment a machine learning algorithm to be more fair we must formalize definitions of fairness and develop methods to incorporate fairness into the algorithm for instance via the loss function or via constraints 
the notion of fairness itself i e 
what it means for the output of a machine learning model to be fair to different groups or demographics is not clear cut 
multiple definitions exist demographic parity requires the prediction and the protected variable z to be independent equality of odds requires the prediction and the protected variable z to be conditionally independent given the true label y intuitively all demographics should have equivalent true positive and false positive rates and equality of opportunity requires the prediction and the protected variable z to be conditionally independent given the true label y 1 intuitively all demographics should have equivalent true positive rates we plan to compare the performance of algorithms tailored to these different measures of fairness in highstakes real world applications 
we use adversarial methods to encode these definitions of fairness into machine learning models a predictor model optimizes for the target task while an adversarial model jointly optimizes to predict the protected variable from the predictor s output 
we combine and compare these adversarial methods with post processing 1 ahenz stanford edu 2 jyc100 stanford edu methods that alter classification thresholds in order to satisfy certain fairness metrics we are specifically interested in trade offs between performance on the target task and the fairness metric 
how are changes in methods and models reflected in the results in terms of both accuracy and fairness 
can we retain model performance while jointly pursuing fairness 
fairness definitions lum et al 
adversarial debiasing past work has attempted to incorporate demographic parity and equality of odds i e 
2 of the 3 definitions of fairness proposed into machine learning models via adversarial learning techniques 
by adding an adversary which penalizes predictions which are biased or skewed based on the protected variable z to the neural network this setup counteracts unwanted biases present in the training data to make fair predictions 
this approach is generalizable to any prediction tasks classification or regression any predictor and adversary models any protected variables and any definition of fairness 
thus the adversarial model was used by beutel et al 
model structure let p be our base predictor model and a be our adversary model 
the predictor p takes in input features x including some protected variable z and outputs a prediction 
a adversarial model
since we want to satisfy a given fairness definition we add an adversary to the original predictor which penalizes the original predictor if its prediction is biased against the protected variable z as shown in demographic parity model a takes as input the prediction and learns to predict protected variable z 
if our prediction is biased against z we will be able to predict z from and a will have high predictive accuracy equality of odds model a takes as input the prediction y and true label y and learns to predict protected variable z if and z are not conditionally independent given y the pair y will be indicative of z and a will have high predictive accuracy equality of opportunity model a takes as input the prediction for all input examples where y 1 and learns to predict protected variable z if and z are not conditionally independent given y 1 will be predictive of z for examples where y 1 and a will have high predictive accuracy 
since the model goal is to predict accurately while satisfying some fairness metric we want predictor p to have a high prediction accuracy and adversary a to have a low prediction accuracy 
in this situation will be a high accuracy estimate of y debiased to z as required model training we use the binary cross entropy logistic loss let l a y be the logistic loss for adversary a and l y y be the logistic loss for the prediction of in predictor p we define the loss function for predictor p to bein order to pit the predictor and adversary against each other where is a hyperparameter that indicates the importance of debiasing according to z 
we avoid using the projection term in this loss function as done by zhang et al 
as presented by hardt et al 
demographic parity demographic parity holds if and z are independent i e 
b post training processing
p z is equal across all groups z 
in post processing we pick a fixed value p such as to maximize overall accuracy and then select thresholds t z such that for each z p z p equality of odds equality of odds holds if and z are conditionally independent given y i e 
the true positive rate tp p 1 y 1 z is equal along all groups z and the false positive rate fp p 1 y 0 z is equal along all groups z 
in post processing we pick a point along the class specific roc curves which give the tp vs fp rates for various thresholds where all class specific roc curves intersect so tp z is equal for each z and fp z is equal for each z 
hardt et al 
equality of opportunity equality of opportunity holds if and z are conditionally independent given y 1 i e 
the true positive rate tp p 1 y 1 z is equal along all groups z 
in post processing we pick a tp t p i e 
point on the y axis of the roc curve and select the thresholds t z for each group z giving the point where the class specific roc curve reaches t p 
in each case we choose the thresholds that maximize predictive accuracy given these constraints 
these methods guarantee that the given fairness constraint holds on the dataset used to pick the thresholds i e 
the validation set 
for each fairness definition we use different metrics to evaluate to what extent the fairness condition is achieved demographic parity the demographic parity gap for a given class z is p z z p z z 
if demographic parity is satisfied the demographic parity gap must be 0 for each group z equality of odds for a given class z the true positive gap is p 1 yif equality of odds is satisfied the true positive gap and the false positive gap must be 0 for each group z equality of opportunity if equality of opportunity is satisfied the true positive gap as defined above must be 0 for each group z 
c metrics and evaluation
salary prediction as this project intends to explore the trade off between accuracy and fairness we will experiment with multiple fairness definitions and methods 
we chose salary prediction as our application as it serves as a standard benchmark to compare performance to previous work 
iv experimental setup a uci adult income dataset
pre processing we pre process the data as done by zhang et al 
protected variables the protected variables z used are either sex race or age 
we experiment with multiple protected variables on the same data set out of interest and as these features have different numbers of buckets sex can take on 2 values race can take on 5 values and age can take on 11 values 
we focus our analysis in this paper mainly on sex as a protected variable since the two class protected variable yields simpler processing and more easily understandable results and visualization 
for the other two protected variables race and age we present variance in measures of parity and true and false positive rates as there are too many gaps to present simply one gap per value of the protected variable 
model architecture to choose the model architecture for our predictor model we compare logistic regression and a shallow neural network 
the logistic regression model achieves an accuracy of 84 5 on the validation data while the neural network achieves an accuracy of 85 3 
b experiments
in this dataset and in the context of fairness joint relationships between demographics seems quite important in making effective and fair predictions of income 
the linear model cannot express these joint relationships 
thus we choose the shallow neural network as our predictor p and our adversary a 
after performing a hyperparameter search using accuracy on the validation set as our target metric we use a single 10 unit relu hidden layer a learning rate of 10 3 for 3 000 iterations dropout regularization with dropout probability 0 5 and an adam optimizer for p and a 
for binary classification we use a sigmoid output layer 
for multi class classification i e 
predicting protected variables race or age in the adversary we use softmax regression experimental variables we choose sex race and age as our protected variables 
for each protected variable we run one set of experiments in which we debias the predictions against this protected variable for each adversarial setup i e 
for each fairness definition setup 
further for each adversarial setup we run the model for multiple values of where is the hyperparameter balancing the predictor and the adversary cross entropy losses 
the values of tested are 0 1 1 10 100 300 500 700 900 
these values were chosen in order to analyze the spectrum where the adversary seems to play little role in the final model to models where the adversary dominates and predictive accuracy suffers significantly 
we present the values of that achieve reasonable accuracy while producing significant improvements on the fairness metric post processing we focus on post processing methods that maximize demographic parity and equality of opportunity 
we do not implement methods maximizing equality of odds as equality of opportunity and odds are very similar metrics and for this dataset we care most about the precision of our positive predictions we care more about misclassifying high income people as low income than misclassifying low income people as high income 
tables iii v and iv show the accuracy and fairness performance metrics for experiments with sex race and age respectively as protected variables 
for each protected variable we show the performance of our basic predictor model no adversary and our adversarial model geared towards equality of opportunity equality of odds and demographic parity respectively 
we present the values achieved setting tradeoff factor 10 as this achieved a reasonable balance of accuracy and fairness 
as discussed above a parity gap approching 0 indicates that demographic parity is satisfied a tp gap approaching 0 indicates equality of opportunity is satisfied a fp and tp gap approaching 0 indicates equality of odds is satisfied debiasing against sex for sex as a protected variable we compare our model s performance to that of the shallow neural network used by beutel et al 
we further apply the post processing step to the basic parity and opportunity models during which we enforce that equality of opportunity or demographic parity must hold on the validation set 
for both methods we see that the basic and special tailored adversarial models perform almost identically after post processing 
thus while we see large gains in adversarial models without post processing if we tailor the thresholds class by class we can get fair and accurate predictions using a basic predictor 
debiasing against race and age for race and age as protected variables there are no results in literature against which we can compare 
as each of these protected variables can take on more than 2 values we chose to report the population variance of to approximate the demographic parity gap i e 
the variance of p z across all demographics z 
similarly to approximate the fp and tp gaps we report the population variance of fp and tp 
we note smaller changes from model to model perhaps due to the decreased sensitivity of these metrics but again see that adding an adversary does not substantially decrease accuracy and decreases var when optimizing of demographic parity decreases var tp when optimizing for equality of opportunity and decreases var fp and var tp when optimizing for equality of odds as compared to the basic model 
as the protected variable can only take on 2 values and this case is studied in literature we limit our discussion to considering sex as a protected variable we have seen the trends for race and age to be similar equality of opportunity demographic parity
vi discussion
to conclude we explore an combine two ways presented in literature to incorporate fairness metrics into machine learning predictions we use and adversarial model and a post processing step analyzing roc curves to ensure that fairness constraints hold 
we perform this task for multiple fairness defintions and multiple protected variables using the task of predicting an individual s income given census data as the prediction task 
vii conclusion and future work
we conclude that adversarial methods provide powerful tools to balance between optimization of the target task and preserving fair predictions 
careful postprocessing can match these results given a representative validation set even on a basic model that does not factor any notion of fairness into its training as future work it would be interesting to extend this analysis to the multi class protected variable case and different datasets and protected variables to see if the trends regarding the fairness accuracy tradeoff as specified by the roc curves still hold 
similarly testing different predictor and adversary models and architectures in terms of the structure of the chosen neural network would again enable us to broaden the analysis conducted here 
to further deepen our discussion we could analyze feature importance in the various models 
finally incorporating the various fairness definitions discussed into a differentiable loss function would make it possible to take into account fairness constraints in a simple model without an adversary 
we contributed equally to the project both designing writing the code poster and report 
the code for our project can be found here https github com jyc8889 fad 
viii contributions and code
humans are naturally skilled at walking since evolution has shaped our physiology to naturally optimize motor control metabolic cost is a measurement of rate of the energy required to perform a certain task at the tissue level 
energy cost is optimized naturally by animals and is a useful metric when determining whether a motor skill is being performed optimally 
collecting metabolic cost data however is very difficult and limiting 
firstly it cannot be easily measured outside of a laboratory setting and requires subjects to wear a restricting mask in during experiments 
subjects are also required to fast for a few hours prior to recording metabolic data which may be especially difficult for long trials 
metabolic cost is slow to change as the human adapts to a new control law making it difficult to use as feedback for control parameters 
it can take over a minute for metabolic data to reach a steady state value for any particular task further complications of human in the loop optimization involve the adaptation of the human to the hardware itself 
na ve users of the exoskeleton likely perform at a different level than expert users of the exoskeleton and therefore different optimal control laws are applicable for individuals of varying expertise 
na ve exoskeleton users are simultaneously training and improving their expertise while the control parameters of the device are being optimized for their current level of expertise 
during training the optimal control parameters shift framing the simultaneous training and optimization as a non stationary process due of the success of stochastic optimization algorithms in providing optimal control parameters and improving human energy consumption recent work has shown that many biological gait parameters can be robustly measured outside of the laboratory environment 
these include but are not limited to joint angles
an ongoing study in the stanford biomechatronics laboratory examining the effects of simultaneous training and optimization using bilateral ankle exoskeletons is the source of all the data presented here 
only naive subjects are used in order to study the effects of training 
ii methods
the control strategy which was used in the human in the loop optimization study by zhang et 
data processing 1 metabolic data baseline metabolic measurements are different for every person and can shift significantly from day to day 
for this reason metabolic data is recorded for one 6 minute standing trial and two 6 minute normal walking trials 
the mean metabolic rate from the standing trial is considered to be baseline for that day of experiments and is subtracted off from all other metabolic data from that day 
the metabolic measurements from the first half of the normal walking trial are discarded as participants metabolic rate is still increasing from the prior inactivity and the average metabolic rate from the remainder of the normal walking trials is used as a normalizing factor for the metabolic data taken from that day 
measurements are only taken from the last 30 seconds of each 2 minute sub trial to allow for the subject time to adjust to the new control parameters 2 step data using signals from heel switches mounted in the shoes and ground reaction forces from the treadmill the gait phase can be easily determined 
for each 30 second interval of data recorded the following features were extracted from the raw data on a per step basis peak vertical reaction force before toe off maximum minimum ankle angle stride width stride time as a very short or very long stride may significantly shift the mean for these parameters the median of each step parameter of the 30 second period is chosen instead 
ground reaction forces are normalized by subject s weight on the day of the experiment to account for small changes in weight between days 3 emg as emg signals are noisy ac signals the signal mean is not a well correlated to the muscle activations 
a standard approach to processing emg signals is to run the data through a high pass butterworth filter followed by rectifying the signal and filtering with a secondary low pass butterworth filter 
the rms of the resulting signal can then be correlated with the magnitude of muscle activations although emg sensors are placed in approximately the same locations on every day in the experiment small deviations in sensor location can lead to large changes in the magnitude of the electrical signal 
to account for this emg signals are recorded for two 6 minute periods of normal walking on each day 
after initial processing as described above the peak value for each signal is calculated 
these peak values are then used as normalizing factors for each signal during the optimization trials b 
algorithms 1 curve fitting to set a baseline prediction value to compare against a neural network implementation we investigate a simple linear regression model for predicting the metabolic cost 
as the total number of features to attempt improve on the prediction accuracy of the linear regression we use a standard neural network with one hidden layer a tanh activation function between the hidden layer and the output layer and a linear activation on the output 
as the data set is small and inherently noisy from metabolic measurements we found that we were not able to create statistically meaningful train validation and testing splits from the data and instead ran a k fold cross validation to tune the network and choose an appropriate value for the number of neurons in the hidden layer 
we performed our network training using a bayesian regularization algorithm 
although significantly slower than other training algorithms bayesian regularization has been shown to be more robust at avoiding overfitting for noisy and small data sets the features can be broken up into three categories 1 step data 2 emg data and 3 exoskeleton control parameters 
to investigate which category of features are most important for making accurate predictions the cross validation described above for linear regression and network tuning was performed four times on using 1 all features 2 step emg features 3 step only features and 4 emg only features 2 dimensionality reduction the number of input features characterizing our dataset is quite large compared to the amount of samples 
in total we have 29 features including controls and 180 data points 
therefore it would make sense to reduce the dimensionality of our features to prevent overfitting 
we used forward step wise selection to select features in a reduced model and perform cross validation with the new models to select the best subset of features using the network tuned in the k fold cross validation step 
due to the variability of forward step wise selection we ran several trials to determine the features that are consistently significant across trials 
for more stable results we also used principal components analysis pca after normalizing our dataset to compare model selection methods 
the mean mse values from the k fold cross validation can be seen in it is note worthy that the predictions on subject 1 consistently had lower mse values than the predictions on subject 2 
one explanation for this is that the data for subject 1 exhibits much higher variance than the data for subject 2 as can be seen in the constant prediction mse column of over several runs we selected features which were present in 60 of the trials 
iii results
results from running the forward stepwise selection algorithm using our tuned network on the no controls data for subject 1 resulted in the selection of 23 features for a new model out of the 29 total features 
this new model predicted with an mse of 0 0761 
we also ran pca using our tuned network on our data for subject 1 which gave us more stable results 
pca with 23 features captured 98 8 of the variance of the data 
the new features found using pca predicted with an mse of 0 0700 
our initial approach to the neural network fitting consisted of a network with same basic architecture as the one described above but with more neurons in the hidden layer 
we initially experimented with 2 to 10 neurons in the hidden
iv discussion
step emgstep layer using a standard levenberg marquardt algorithm to update the weights 
in our initial configuration we split the data randomly in separate train validation and testing sets 
constant prediction mse all features
the validation sets were initially used to prevent overfitting and halt the training at the epoch that led to increased error on the validation set 
although this approach at times led to tantalizingly low mse values on the validation and test set we found that retraining with slightly different splits lead to vastly different outcomes 
this led us to the approach described in the methods section of using both regularization and cross validation to ensure consistency of the outcomes and report values that are actually indicative of the predictive power of the algorithm 
data from all days of the study was pooled and split randomly for cross validation due to the limited number of points 
ideally a better test of the predictive power of the algorithms would be to make predictions on a a full day of data that the algorithm has not seen in order for these results to be generalizable and useful for other exoskeleton optimization studies it would be ideal if we could achieve high accuracy predictions without any knowledge of the control parameters since the structure of the control parameters and an individual s response to them may differ between devices 
the dimensionality of our features were reduced using pca and forward stepwise selection methods 
running the forward stepwise selection algorithm many times allowed us to select the features which were deemed significant more consistently 
however testing these new models resulted in relatively high mse values 
the model built using forward stepwise selection varied over many trials due to the greedy nature of the algorithm the one we selected to produce after testing different datasets we found that all of the models considered perform with relatively the same accuracy using data with controls compared to data without controls as features 
it can also be determined that the emg data can be significantly better predictors than the step features alone 
therefore we could potentially do well in predicting metabolic cost using emg alone the results of the predictions and model selection process show that it is difficult to use the data to predict the metabolic cost of different individuals 
in the feature reduction stage some features were more significant for an individual compared to others and therefore resulted in different features selected 
for instance for subject 1 step width was an important feature that showed up much more often in the trials and for subject 2 left and right peak force were consistently significant 
another observation from the feature selection step was that for certain individuals and for certain features which consider both the left and right sides of the body e g 
right and left ground force features from mostly one side appeared as significant features 
this may imply that capturing the same type of data from both legs may be redundant and unneccesary 
a potential improvement to the dataset could therefore be individual specific features which in addition to collecting data on more individuals could improve generalization of the predictions the variations in predictions between the two subjects indicates that trying to create a network to generalize these predictions for multiple individuals would likely yield poor predictions 
however with significantly more subjects and the inclusion of subject specific parameters such as height age weight and general fitness it may be possible to create a generalized predictor model 
additional kinematic data could also be collected during experiments using wearable inertial measurement units imus which would augment the feature space and potentially improve accuracy 
in the best case our algorithms were able to achieve an mse values on the normalized metabolic rate of 0 0089 corresponding to approximately 10 error 
this is a significant improvement over a constant prediction which would yield an error of approximately 25 
v conclusions future work
it is also a significant step towards the goal of 5 accuracy which can be considered as the baseline noise level for metabolic data future work would include significantly more subjects in the study and perhaps the inclusion of more kinematic data either from imus or from a motion capture system 
as further increasing the feature space will increase our chances of overfitting a significantly larger volume of data would need to be recorded 
currently the large number of sensors used in this experiment may be prohibitive for future work 
given the quality of emg based predictions it would be value to identify which emg signals contribute the most to the predictions 
we would like to acknowledge zhang et al eley ng performed the model selection work 
erez krimsky processed the data and worked on curve fitting 
github repository
both contributed equally to the network tuning ideation and writing of this report the code used in this project can be found at https github com ngeley cs229project 
education is often an expensive gatekeeper to earning potential and more generally quality of life as a consequence 
as such we were interested to better understand what factors determine a successful education 
while we initially performed linear regression and k means experiments on data from the world bank we later shifted our focus our challenge was the sparsity of our data and without more information we could not answer more difficult questions 
therefore following our milestone work we concentrated our efforts within the united states and within tertiary level education 
we analyzed the college scorecard dataset which consists of data for each academic year from 1996 97 to 2016 17 for over
within the us the earnings gap between high school and college graduates has more than doubled over the last three decades 
the two strongest predictors of children s educational attainment are parental education and parental earnings in a separate study different classification techniques such as decision trees logistic regression ensemble classifiers and support vector machines were used to predict whether a student will pass or not 4 
we thought using a variety of techniques with the same covariates and seeing similar results was a great way to validate assumptions 
the study achieved good accuracy with all methods due to having good student level data 
since our problem is similar in nature but we lack student data we can expect our classifiers to not be as accurate 
we chose to only work with the 2013 14 college scorecard dataset since our inquiries were not in regards to the evolving institution profile but rather the effect of a given experience on the future of the students our first task was the selection of an evaluation metric 
financial success is captured in the dataset by several statistics mean median standard deviation for six and ten years post enrollment in the institution 
the ten year post mean had the greatest variability and was arguably most suggestive of longer term differences than six year post enrollment earnings therefore we selected it as our evaluation metric our next task was parsing through the many features provided in the dataset and determining which might prove to be predictive 
to this end we generated various plots of individual variables against our success metric and computed their correlation coefficients helping guide our feature selection both quantitatively and qualitatively 
included in this set of visualized features were admission criteria statistics including act sat general and sat subject test scores seen in we also considered finances both of the student population and from the perspective of the institution through metrics such as tuition average debt upon leaving college average faculty salary and institutional spending per student 
we used big query to generate early models optimizing on mean error adding features one by one and seeing improved accuracy in a linear regression model to get a sense of whether our feature selection was helpful or imposing bias that was not supported by the data 
using just a handful of features we obtained results better than models with more features i e 
method 1 linear regression
those in which we reduced our feature selectivity 
this put confidence in our feature selection as we moved to a logistic model for higher accuracy on a less precise question 
our early logistic models attempted to determine the average salary at the 50th percentile 
we were able to identify that determining whether a college will be above or below the mean salary 10 years after college was not very difficult with the afore mentioned covariates 
method 2 logistic regression
this task however is much more difficult when trying to identify whether or not a school will be above or below a higher threshold 
therefore we decided to try to use logistic regression to determine average salary at the 80th percentile for the initial model we decided to include all the previous covariates in addition to covariates regarding the percentage breakdown of majors 
because there were a large number of majors represented in the dataset similar majors were joined together to better capture the effect certain areas of study might have on future earnings 
in addition we replaced two covariates tuition at public universities and tuition at private universities 
in place of these two we created a covariate that is just the sum of these two and to account for the public versus private tuition difference we added a second binary covariate that indicates whether the institution is private or public at a high level logistic regression seemed like a good model for this task since we are interested in creating a probabilistic model that can classify at a particular threshold 
the hypothesis function for logistic regression is where g represents the sigmoid function which returns 0 or 1 
logistic regression optimizes the hypothesis function by using the derivative of the log likelihood function and performing gradient ascent with the following update rule 
our second approach was to use k means clustering on different indicators to find related features and then compare the mean earning of students working 10 years after entry for each cluster 
generally kmeans algorithm groups unlabeled data points into few cohesive clusters 
method 3 k means clustering
the first step of k means algorithm is to initialize cluster centroids randomly 
then repeatedly assign each data point to the closest cluster centroid and move each cluster centroid to the mean of the points assigned to it 
the formula for calculating the cluster centroids and cluster number are as follows after testing on dozens of features we found out that 5 of them are the most correlated listed below in the table in the discussion section 
due to insufficient data we decided to replace all null entries with their mean values to compensate 
after deciding to use 4 clusters we ran k means 30 times to get the trail that has the minimum distortion loss 
in order to visualize the cluster we applied pca which is an algorithm that projects high dimension data points 5 dimensions in this case into low dimensional subspace 2 dimensions here 
our best linear regression model included only 4 input features tuition sat average scores admission rate and average expenditure per student 
the inclusion of sat scores improved our model the greatest amount from amongst those 4 
linear regression
in an attempt to get better accuracy we included demographic data which worsened our model 
we inferred that the magnitude of errors for the instances of underprediction were greater than that of over predictions in our first logistic model from error analysis 
only about 18 of errors were under predictions balanced out the cost incurred by the far more numerous over predictions 
this observation combined with our greater inaccuracy in linear regression being in the higher income brackets motivated an analysis focused on learning a higher income threshold 
we were able to get average results using logistic regression 
after analyzing where the model was making mistakes we noticed that most of the mistakes were happening during the classification of institutions at high average post graduation earnings specifically of the mistakes that were made the average salary was at the 94th percentile of all earnings 
this was our initial problem as well 
adding additional covariates capturing major breakdown information helped address this problem but after seeing the results we believe we need additional data preferably at the student level to be able make a logistic regression model better 
intuitively this makes sense since institutions at the top of the earnings scale are often influenced by individual students who are outliers and go on to have very high earnings 
to prevent overfitting and to view the change of parameters between each cluster we chose 4 as the number of clusters 
also due to the fact that k means algorithm may converge on local optima we ran the algorithm 30 times and got the minimum average distortion loss of 17764272 
k means clustering
however this number does not provide useful information as we were comparing features with different limits on their value ex admission rate is from 0 to 1 faculty salary usually over 5000 
thus we applied pca to normalize the data and project it to a 2 dimensional subspace to confirm that the clusters make sense 
as seen in the pca figure those successful schools have significant extreme values in their features than the other schools 
after investigation as expected those are the most elite schools in the world including stanford cal tech yale and so on 
after getting the clusters information we realized that the more selective schools low admission rate and high average sat score and more resources rich school high faculty salary and high instructional expenditures are more likely to lead to high earnings of students 
surprisingly we noticed that the larger the fraction of part time students are at a school the more likely the school is to be less successful 
much of the variables determining clustering and predictions through regression were not unexpected 
while we made strides towards answering more nuanced questions of a higher income threshold our challenge was always and remains data 
this is the case in two ways 
first by the nature of our question there will always be fewer earners at the highest earning threshold thereby making that region of the income distribution difficult to capture in addition to having greater variability 
this greater variability grows from the fact that as incomes rise so do too income gaps spreading the data over a larger range 
second compounding this difficulty is our lack of access to individual level information which may shed light on the nuanced detail required to capture what factors contribute to outstanding earning potential our experience with this dataset provides an interesting commentary on feature selection 
while we were not on the whole shocked at the features that did make an impact it was interesting to observe that abandoning our hypotheses and throwing all features in arbitrarily tended to worsen our models 
this observation in combination with the former concluding paragraph highlights that the successful use of machine learning relies on both critical reasoning of the problem outside of the data as well as the quality of the data targeted to capture the variables needed to answer specific question at hand 
contributions ipchun chan manisha basak zoe pacalin
spam is a large and growing problem that is becoming increasingly difficult to tackle at scale 
with the proliferation of devices that are connected to the internet the amount of search queries increases each day with google currently processing over 3 5 billion queries per day 
there are significant monetary incentives to achieving a high rank in search results especially for queries with commercial intent like hotel or plane tickets 
as a result spammers have tried a variety of techniques to rank highly in search results 
search engines have limited resources and cannot crawl every page on the web 
assuming a fixed crawl throughput for each spam page that is crawled a page that could have been useful to users will not be crawled 
thus an algorithm that is able to identify a spam web page before crawling the page would be very useful 
not only would this save precious crawl resources by allowing the crawler to bypass spam urls but it would also remove the possibility of that spam url from later being served to user queries thereby improving the quality of search results the input to our algorithm is a url string along with metadata such as registered domain holder first seen date and traffic 
we then use a svm or random forest to output a prediction of spam not spam 
previous work on this topic has involved content analysis of the page itself other work attempt to classify web spam into buckets such as link spam redirection cloaking and keyword stuffing another approach is to first determine what are important features in terms of ranking in a search engine and then find which features are likely to be used by spammers while relying on the page content and links increase the amount of data available for spam classification there are strong motivations for being able to classify spam prior to crawling a page 
this paper explores using the url string as the primary feature in spam classification 
while there are a variety of features that one can use to classify if a web page is spam this project aims to use only the url and limited metadata information to classify if web pages are spam not spam 
this choice was made for performance reasons as scraping html from web pages is resource intensive and not useful since the page must have already been crawled 
a overview
in the context of a search engine it is often very useful to be able to detect if a given url is spam prior to a page being crawled 
this way urls that are likely to be spam can be deprioritized during crawling and those resources can be used to crawl more useful pages that are less likely to be spam 
the dataset consists of 40 000 manually rated urls with 50 labeled as spam and 50 as not spam collected over the course of two months 
additional information that would have been available at crawl time was also included such as first seen dates whois contact data and site traffic 
b data
data is representative of the types of spam that appeared in google search in september and october 2018 
all numerical features were scaled and standardized to have mean 0 and variance 1 
categorical variables like whois name and tld suffix were one hot encoded with a minimum cutoff frequency threshold 
each url was broken down into domain top level tld and deep url sections 
for example in the url http www pcwebopedia com index html the domain name is pcwebopedia com the top level tld is com and the deep url is index html 
c features
any characters preceding the domain name were discarded such as http www except for subdomains like sub1 pcwebopedia com each deep url was split into words by using various characters as delimiters such as and 
the full regex is shown below words were kept if they were more than 2 characters long otherwise they were ignored 
a multinomial event model was fitted on these words with laplace smoothing and a dictionary limited to words that appeared with a frequency of at least 0 1 times the size of the dataset 
the output probabilities from this classifier was used as a feature in a multinomial event model we assume that the way a url is generated is via a random process in which spam not spam is first determined according to p y 
then the spammer composes the url by generating words xi from some multinomial distribution p xi y 
all words are assumed to be chosen independently and from the same distribution 
note that the distribution according to which a word is generated does not depend on its position within the url string 
the likelihood function and maximum likelihood estimates of the parameters with laplace smoothing are given below where v is the size of the vocabulary dictionary this model had an accuracy of 83 2 and the top 5 indicative words for spam urls were shgbcz bdouwo1 yboyiw5k yulsef qoricksjw character counts of both domain name and deep url were used as features 
the idea here is that longer urls in general are more likely to be spam although this is a weak relationship that doesn t necessarily hold true for this specific dataset as illustrated by the graph below additional features include the first seen date of the url and the first seen date of the site 
this helps to capture any trend in time of various types of spam 
for example during black friday and other peak shopping holidays we might expect higher amounts of spam related to keywords such as iphone laptop or tvs 
the difference between the first seen date of the url and the first seen date of the site was also used as a predictor with the idea being that urls with a large difference may be more likely to be hacked the ratio of web traffic to the url vs the site was used as a predictor 
if a url has a lot more traffic than the corresponding site home page then it indicates that something suspicious is occurring 
the url may have been hacked or contain a forum that has been overrun by comment spam 
i added 1 to the denominator site traffic for each observation to avoid nan issues from dividing 0 by 0 when available whois data was also obtained for each url consisting of the registrant name previous dns and current dns 
a change in dns would indicate a change in site ownership which could indicate that a site has fallen into the hands of a spammer top level tld was encoded as a categorical variable with the following distributions for spam and not spam urls 
while com is by far the most common tld in the data we can see that the relative frequencies of other tlds varies across the spam vs not spam datasets for a sample of 2000 spam and not spam urls 
svm with both linear and gaussian kernels rbf were fitted on the training data 
the support vector machine attempts to find a separating hyperplane that separates the data with the maximum geometric margin 
iv methods
however since not every dataset is linearly separable we introduce slack variables to allow the model to make some mistakes with a cost c this results in the following optimization problem compared to the linear svm rbf svm allows for a richer set of features as the gaussian kernel represents an infinite dimensional mapping 
while this may seem computationally expensive by using the kernel trick we are able to avoid blowing up the required time complexity 
specifically we replace the dot product between x s with the gaussian kernel we also use random forest to fit the data 
random forests are an ensemble learning method constructed from an agglomeration of decision trees 
a decision tree makes binary splits of the data using the features and cutoff points that leads to greatest possible reduction in the loss in a greedy manner 
random forests aggregate a large number of decision trees using a method called bagging bootstrap aggregation and outputs the class that is the mode of the individual trees 
the bootstrap method is first used to create random sub samples of the dataset with replacement 
then decision trees are fit on each sub sample and their output averaged over all decision trees for classification problems you take the mode instead of average 
typically random forests limit both the max depth of the trees and the number of features available for prediction for each tree to improve performance 
this leads to greater variance reduction in the overall estimator because the pairwise correlation between each tree is lower we used the gini loss which is shown below v 
results 10 of the dataset 2000 examples with roughly 50 spam not spam was set aside as the test dataset 
the primary evaluation metric is accuracy 
however in real world implementations the more important metric is probably precision since failing to crawl a legitimate page results in significant loss to the business or site owner 
on the other hand crawling a spam page while not ideal is not as severe as a mistake since the spam can still be suppressed or removed at serving time 
results are shown below for a variety of models and parameters 
varying the cutoff frequency threshold did not seem to have a large effect on the accuracy of the models 
any categorical variable that was one hot encoded had the value dropped if it did not occur at least the cutoff frequency threshold number of times in the training dataset 
increasing the threshold from 5 to 50 resulted in the number of feature variables dropping from 406 to 81 
while it is surprising that reducing the number of features by 5x does not degrade model performance this may be due to the size of the test data which was one tenth the size of the training data 
a combination of not enough training data on 5 examples and low prevalence frequency in the test dataset likely reduced the usefulness of the majority of categorical features the two models highlighted in red represent the models with the best tuned hyperparameters as found by grid search for svm and random search for random forest 
the optimal svm had hyperparameters of cost 1 and gamma 1 
the optimal random forest model had hyperparameters of min samples leaf 1 max features sqrt number of features min samples split 2 number of trees 1400 bootstrap false and max depth 3 
because of how random forests are constructed they tend to be resistant to overfitting 
however the plot of test error vs training error over training time seems to suggest that the model has high variance and could benefit from more training data 
this is true for the rbf svm as well 
my hypothesis is that given the vast variety of spam 18 000 training examples 12 000 in the plots below which use 3 fold cross validation is simply not a large enough sample to adequately cover all possible variations of spam the confusion matrices for the best rbf svm and random forest models show that both ended up with a fairly balanced false positive rate and false negative rate making roughly equal number of mistakes on both sides 
the roc curve clearly favors the random forest model over the rbf svm 
however for a model that would be used in production we would likely increase the cutoff threshold for a positive label from a probability of 0 5 to something higher to reduce the false positive rate 
a false positive i e 
predicting a url is spam when it is in fact not spam is a more serious error than a false negative i e 
predicting a url is not spam when it is in fact spam 
false positives create more damage to a search engine s reputation especially if the false positive is on a prominent site and harm real businesses businesses that a search engine relies upon in a symbiotic relationship 
in addition false negatives at this stage can also be caught later at indexing or serving time 
hence we would probably want to trade off a lower recall and higher false negative rate in exchange for a lower false positive rate 
actual spam 27 1030 an ablation study was also performed to see which feature had the most impact on the model 
note that only single features were removed and the effects are not cumulative 
vi conclusion
we can see that the na ve bayes output probabilities see features section for description of how these were calculated were the most important feature 
this is corroborated by the feature importance ranking from the random forest model shown below 
surprisingly the suffix net feature was the second most important feature indicating that spammers favor net domains significantly differently than non spammers 
we have presented a variety of models in an attempt to identify spam urls prior to crawling the page 
random forest with gini loss performed the best out of the models tested with 97 25 accuracy on the test data 
since spammers frequently alter their techniques we expect model performance to degrade over time if the model is not retrained with new data 
gathering more data across a variety of spam types and time periods would help the model generalize to more types of spam 
training specific models for each spam type or geography language would also likely result in performance gains the most important feature was the na ve bayes output probabilities which was created by splitting the deep url portion into words and fitting a multinomial event model with laplace smoothing 
since there is likely significant headroom to improve this feature future work would involve more sophisticated feature extraction methods on the deep url portion 
for example using a bag of words model or n grams would likely result in significant improvements 
in addition deep learning methods such as neural nets could also be explored given more time and computational power 
in this paper we examine the possibility of building a model to classify whether a conference paper can be accepted or rejected to a certain conference 
we used the peerread dataset to build models to classify paper acceptance to iclr using 18 features including but not limited to number of authors and figures abstract bag of words and whether the abstract contains words like deep or neural 
using accepted and rejected papers from iclr 2017 we trained the following models on the 172 accepted and 255 rejected papers from iclr 2017 logistic regression with l2 l1 regression svm with the rbf kernel random forest adaboost and a fully connected neural network 
we found that the svm with the rbf kernel performed the best with an accuracy of 71 an improvement over prior research s best of 65 3 
in recent years there has been an explosion in scientific research applying machine learning onto ever growing datasets thanks to recent advances in computational power 
in 2017 alone 3 120 papers were submitted to the neural information processing systems nips conference but only a mere 679 papers were accepted 
while the peer review process is the most important way to judge the quality of research work the scientific community has identified potential issues with the process ranging from consistency to bias issues 
one clear way to avoid these issues is to use a computer to evaluate submissions directly 
the goal of this paper is to predict the acceptance of a given academic paper we receive raw pdfs and their reviews and labels accept reject as our input transform them into json files using science parse a library created from kang et 
al
kang et 
we took the 427 papers submitted to iclr 2017 including 172 accepted and 255 rejected papers 
for each paper we extracted 18 features 
to simplify the model all of our features are numerical or boolean 
some coarse features include length of the title the publication year whether the fancy terms like deep or neural appear in the abstract 
there are also more sophisticated lexical features extracted from the abstract of each paper 
we used word2vec techniques to capture the information of the abstract 
we reconstructed the linguistic contexts of words 
in this case we get a 300 dimensional vector space 
all of the words in the abstract get mapped to a vector in this space 
thus it is much easier for us to measure the similarity between word vectors 
we can also get a visualization of the words by dimension reduction technique e g 
principal component analysis pca linear discriminant analysis lda or t distributed stochastic neighbor embedding t sne 
when we actually put this feature into the model we only take the word counts to make this feature numerical and be consistent with other features we have 
the full feature list can be found in the appendix 
here are all the models that we explored in logistic regression we classify a training example as positive if h t x 0 5 negative otherwise 
regularization helps prevent model overfitting random forest we varied the number of trees as well as overall depth to prevent overfitting random forests are an ensemble method where each tree is fit to a set of bootstrapped training samples 
each tree has high variance but random forests reduce the overall variance by averaging across all trees in the random forest svm with l2 regularization using a rbf kernel as defined below adaboost we used 50 weak classifiers as shown in although a cnn was recommended we determined it inappropriate based on our features 
typically convolutional layers are useful for temporal or spatial relationships such as those that can be found in time series or images 
however our feature set does not include any of these 
we used sklearn in our case we performed the highest performing model was the svm model with rbf kernel but we expected the neural network to perform better 
one reason for this is that our dataset is relatively small with only 427 samples 
typically neural networks require at least an order of magnitude larger dataset for good accuracy we also note that the adaboost and random forest models are significantly overfit 
indeed our experiments showed that adaboost with 50 weak classifiers is no better than random guessing 
for the random forest model even when we limiting tree depth and number of trees we still observed some overfitting with even poorer accuracy we also used pca to visualize to the iclr dataset given our feature set in an attempt to understand the relatively poor classification accuracies that we observed 
our work focused on the iclr dataset which has limited examples 
similar studies can be done on other conferences with more submissions like nips or for the same conference but with submissions across years 
one interesting experiment we could try is to use our trained model from one conference to predict the acceptance of a paper for another conference 
this will tell us the preference of different conferences 
if two conferences are looking for similar values the model should provide an equally well prediction result 
however it could also be the case that the model performs poorly from which we can conclude that two conferences prefer different styles of papers to improve classification accuracy on the current iclr 2017 dataset we need to employ nlp techniques to extract features to represent the core paper content 
we can also extract additional features such as figure and table content but this will require additional modifications to the scienceparser tool a significant issue that we ran into was the lack of labels for papers 
kang et al artificially expanded their training set through a set of heuristics using review comments as well as looking for references to an unlabelled paper from a known published paper 
we believe that a semi supervised algorithm e g 
semi supervised em can potentially use these unlabelled papers to improve predictions william wjen worked through the public peerread dataset and code and set up the necessary infrastructure to process the raw dataset 
he also verified kang et 
s results on their prediction methods and helped implement the models shichang shichang examined the data in depth performed pca visualizations to better understand the data and gave recommendations based on the data 
he also provided insight into feature extraction as well as background on the adaboost algorithm which was not covered in depth during class muyun muyunc worked on feature extraction and worked with glove to gain a better understanding of the framework 
she also implemented the various models when we met in person on shichang s laptop finally all members collaborated in writing the final report 
the repository can be found here https github com collielimabean peerread
the corporate world deals with task management in a variety of ways with each having some form of triaging process to correctly assign tickets to developers 
automation of this task has proven elusive with less than 60 accuracy of latest ml solutions 
project epsilon explores this area by deploying a deep neural network to predict assignees 
web and saas companies handle high volumes of tickets in the form of exceptions support requests user reported bugs and crash reports 
mostly with dedicated teams that work on aggregating triaging and assigning these tickets to the right individual or team 
however effective automation is essential to improve productivity and obviate the tedious work of manually triaging tickets project epsilon aims to eliminate this overhead by experimenting with supervised learning classifiers to intelligently and automatically assign tickets to a developer 
with high failure rates in state of the art solutions we aim to deliver higher accuracy for predicting assignee for a new ticket based on past tickets using deep learning models the input to our algorithm is a collection of historic jira tickets in json format 
these tickets are preprocessed and featurized and following which we predict the assignee for new or unassigned tickets using 3 different methods svm naive bayesian and deep neural network classifiers 
we then compare the performance for these 3 methods as pertaining to 2 primary input datasets a public expium generated dataset as well as a dataset with real jira tickets from linkedin 
classification on open bug reports with supervised learning strategies has been done in the past 
popular strategies are naive bayes and svm classification on a multinomial event model input featurized as a bag of words 
the primary research revolves around interpreting text to predict assignee based on observed history 
deep neural network implementation to ticket categorization seems to be uncommon and implementing dropout and skip layers is an interesting area of research to improve our implementation of the ones detailed in section 10 2 makes stronger assumptions using implied behavioral patterns and developer dependencies to improve performance 
3 on similar lines extracts and utilizes intention from the ticket text fields while 4 uses hierarchical attention based contextualization for more robust classification 
1 is most relevant research close to our deep neural network experiments and promises even larger datasets and varied projects 
the linkedin dataset s security requirements of keeping data on linkedin assets and preventing the external distribution of jira tickets has unfortunately split our development and implementation environments 
as such the development of our models metrics and experiments was completed on the generated www jumble expium com dataset for developing the algorithm and then applied the architecture to train and test on the private linkedin foundation team support dataset a jira ticket from our datasets has the following json structure for a deeper dive into the featurization process the input json data is parsed into a bag of words and a multinomial event model is used to vectorize the jira ticket 
datasets and features
words in the description body and comment sections are concatenated and converted to a feature vector via a dictionary mapping of words to indices 
we count words that occur more than five times 
in our preprocessing step we remove words that occur in our list of stopwords and remove duplicate jira ticket entries 3 666 or 8 43 all of which is implemented using a modification of our homework code with a future plan to use word2vec the words are assumed to be independent and are expected to have been chosen with separate distributions at create time 
we ve analyzed and explored cross entropy error across different parameters of a neural network and compared runs with a support vector machine classifier and a naive bayes classifier 
while developing our solution we make the following assumptions the labels are developers we have seen before represented as integers which came to a total of 1403 unique developers 
we then create a matrix of a multinomial input vector and an integer class label representing the assignee and go ahead with training and testing 
the shape of the expium datasets after vectorizing was m x n 5435 1970 
the linkedin dataset however repeatedly demonstrated a wide feature vector m n throughout different slices of the data m x n 559 2936 28200 46595 43483 65350 demonstrating that even after we remove stopwords there are a lot of unique words per ticket instead of breaking up our dataset to explicit train cross validation and test sets we opted for k fold cross validation
the project aims to evaluate the applicability of deep neural networks to classify ticket assignment or classification using all previously assigned developers as the labels classes for classification and previously assigned tickets as training data 
the problem is treated as a text classification problem with a novel experiment in using a deep neural network grid search to find optimal architecture and activation functions 
the wide format of the feature vectors proves to be a challenge with the highly non linear nature of deep neural networks which resulted in overfitting on the data 
we suspect that much larger datasets will be required to overcome the sheer variance of available words to create tickets 
to give an idea of the performance of the deep neural network we implemented these traditional methods and report accuracy values when using the currently assigned developer as a label and body of text as the feature vector to evaluate cross entropy loss on the data set 
the implementations still use the same feature vectors the naive bayes classifier works by calculating the probability of a particular word mapping to a developer and those parameters are used during predict time to select the most probable developer 
naive bayes and svm classifiers
we use a support vector machine with a linear classifier and the algorithm works by forming an optimal separation between the data points close to the boundary 
using a kernel the algorithm is able to form highly non linear classification by operating at higher dimensions 
our neural network classifier operates by constantly updating weights by back propagating after observing a cross entropy loss 
during each backpropagation round the various layers update their weights to be able to fit the problem better 
deep neural network classifier
the activation functions are useful by making the neural network non linear i e by preventing the entire architecture from simplifying to a linear regression with a combination of the weights 
the components are the learning rate hidden layers height activator function loss function and the final output layer we chose a deep neural network because of its capability to fit highly non linear data sets and our use case benefits heavily from this property due to its freeform nature 
we chose a softmax output step and cross entropy loss for our architecture and for the rest we completed a parameter search using the expium data set focusing on testing 3 5 and 8 layers 8 16 and 32 neurons and relu and tanh activator functions 
a 5 fold experiment is used to determine performance and find the optimal architecture with 2000 backpropagation iterations 
the architecture is then compared to the other solutions to explore performance in accurately predicting assignee a future extension of our research might leverage nlp methods to form better matching among tickets for classification 
especially to reduce the number of features evaluated if we re able to map similar meaning words into a single feature we expect to reduce our overfitting and also make the features more linearly independent 
we ran the following experiments based on the http jumble expium com generated datasets 
a typical jira ticket has text body values in summary description comments and body sections 
experiments
the featurization step concatenates all text elements and unless otherwise specified as in 6 1 builds a multinomial event model with counts tracked for words that appear more than 5 times in a bag of words model assuming words are independently selected 
selecting the correct dnn marks the first stage of the experiments with the goal of improving predictions 
selecting neural network parameters is a challenging task so we tried out combinations to achieve better accuracy measures for our test sets 
dnns architecture selection
we explored cross entropy training error across different parameters of the neural network 
the following test parameters were experimented with and analyzed 
the focus on this execution is to observe the best minimization strategies during training that would have given lowest final logistic cross entropy loss 
the experiment was run with 2 000 iterations with 5 fold cross validation 
the results were a fairly surprising mix of high train and test tradeoffs and some of the algorithms suffered from local optima traps 
the experiment accuracy values are listed below as are some of the graphs related to the cross entropy loss per iteration 
looking at the results the choice for an architecture was not obvious because the small data set was usually over fit when we get high train accuracy 
the more interesting observations were on the output of graphs compiled with varying the different parameters and looking at the train loss per iteration 
depending on the parameters we see wildly varying patterns that need to be tuned for better accuracy and efficient computation 
it was surprising to see that taller and wider nets did not necessarily yield lower training loss 
the full list of graphs or the previous data can be found here 
in the end we selected the simplest high train accuracy neural network structure with the hopes that the larger dataset will fix the over fitting issue and also by picking the simplest architecture with the smoothest descent we are also attempting to reduce the even more non linear capabilities of bigger networks 
in this section we will compare executions of the above chosen neural network as it compares to naive bayes and an svm classifier with a linear kernel 
the different tables represent accuracy comparison on different subsets of the data 
classification accuracy
unfortunately we had computational hurdles when trying to run the svm classifier on larger values 
the highly non linear nature of dnn has allowed almost perfect 5 fold mean accuracy but that same benefit has heavily overfit the data as observed by the low test prediction 
the naive bayes model doesn t continue to improve that much with more data but we see that our neural network improves this is due to the higher non linear fitting capabilities of a deep neural network compared to naive bayes probability selection methods 
for smaller sample sizes the traditional approaches completely outperform because the deep neural network overfits data 
the low accuracy rates in predicting a developer make the currently tested experiments not viable for the primary triaging solution the results of our experiments resulted in poor accuracy on the test set because the dnn overfit the small dataset 
however the current solution still has a lot of opportunities for improvement 
in particular dropout implementation
the most important work is to implement dropout additional features from the jira ticket fields also provide opportunities for improvement watchers labels reporter hashed exceptions and so on may provide additional information for classification 
the contributions section is not included in the 5 page limit 
a new entrepreneurial venture at stanford is aiming to launch an athletic robot product 
the idea is to create the ideal golf caddy an autonomous robot that will carry a golfer s bags and follow him around the course 
this leaves the golfer to focus on playing the game and walking the course unhindered 
for older golfers this product will enable them to walk and enjoy the sport without dealing with a taxing effort of carrying around a golf bag this project focuses on the navigation aspects of the autonomous golf caddy 
we plan to focus on two problems classifying terrain seen by the visual sensors and detecting and tracking objects within the image 
machine learning will allow the robot to use a simple camera to identify and track the target golfer to follow identify untraversable terrain and identify areas of the terrain that should not be traversed e g 
putting green tee box sand traps while existing products are able to track golfers using bluetooth chips it is often desirable for to not follow the golfer into areas such as the tee box green or bunker or to avoid other obstacles along the way 
adding computer vision to the product will enhance the user experience 
computer vision techniques have been developed for the specific task of tracking people 
the pfinder or person finder algorithm incorporates a priori information of the nature of the human body 
using maximum a priori map estimation blobs or gaussian shapes are fit to the torso legs hands feet and face of humans cross country all terrain navigation has proven challenging for efforts such as those of manduchi et al 
another approach is taken by weiss et al 
to augment autonomous navigation by indentifying the type of terrain currently being traversed in this paper by felzenszwalb et al 
an important characteristic of the method is described is its ability to preserve detail in low variability image regions while ignoring detail in high variability regions 
the algorithm classifies an image into global regions drawing out boundaries for terrain and objects within the image 
it is able to detect boundaries based on variability of pixels and its surrounding region while also ignoring erratic boundaries by ingnoring variability in a more global region and thus segments images really well 
the robot uses solely a stream of camera images for its operations 
for the purposes of this project images were collected from test operation of the actual robot vehicle with the camera to be used on the prototype 
the prototype was manually piloted around simulating operation in service to a volunteer test subject golfer 
30 000 high resolution images were captured for use in this project some supervised data was also provided for the training of the classification algorithms used 
images were hand classified by color codes into the following categories sky clouds black these classifications were chosen with the intended algorithm in mind to have distinct features with the available pixel color and color gradient information 
however in the end only traversable terrain area needs to be identified which would consist of the traversable terrain or pavement classes 
thus only the accuracy with respect to these two classes is directly valued an example of a supervised training example is shown in
a number of machine learning algorithms will be tested to develop the image classification required for the product 
first however the classification objective must be clearly defined 
this segmentation will then be used with a controller and path planning algorithm to guide the caddy robot 
the most basic algorithm conceived was to segment a set of images into a number of categories based on the color values using the k means algorithm 
the segments generated can then be mapped to the desired segment groups 
k means based segmentation
however this algorithm is not robust to color variations by itself 
but after running basic k means a filter can then be extracted based on the pixel values from clusters 
this filter is then applied to the image by comparing all pixels and there euclidean distance in the color space rgb values from the selected cluster s color value and then selecting based on a threshold 
this results in filtering of traversable regions from the image 
one algorithm explored for classifying terrain is a two layer algorithm that uses a convolutional layer and a softmax regression 
classes are defined as explained in the datasets and features section 
terrain classification using a convolution and softmax regression
for the purposes of making the algorithm computationally lightweight enough to run in real time on a limited computer the images are drastically reduced in resolution from 720 x 1080 to 112 x 200 
segmentation can be vastly improved by looking at a small context around a pixel as opposed to merely the pixel itself 
to this end images were convoluted into a new data shape 
convolution
the image would normally be w h 3 
however by including values within r pixels of the pixel of interest the data is reshaped to w h 3r with the reduced image resolution of 112 x 200 a pixel convolution size of r 15 each pixel can be influenced by pixels roughly 5 of the frame width away 
a simple softmax regression can be used to map between convoluted pixel information to classes 
this must be trained with supervision but this is achieved with the hand classified learning examples 
the softmax regression linear step maps a flattened state vector of dimension whr 2 to a set of class probabilities for k classes 
we performed experimentation with two different classes of algorithms based on our classification goals 
bounding box generation for object detection and tracking k means clustering for image segmentation tasks
learning algorithm trials
another algorithm that was employed was an off the shelf object detector from tensorflow called mobilenet ssd 
we used a pretrained detector on our camera feed to obtain bounding boxes on the golfer which was able to effectively track as we traversed around the field while detecting other obstacles such as trees golf carts etc 
bounding box generation
we ran k means clustering and then selected one of the clusters of interest 
the image was then run through this and all the pixels that lied in close vicinity of the cluster were painted over with the same color 
k means based filter
this was able to segment the images into regions of interest 
we used scikit to implement the felzenszwalb segmentation algorithm 
this generated boundaries in the image and segmeneted the image into major blobs based on the boundaries 
felzenszwalb segmentation
these boundaries were effective in classifying out traversable and intraversable terrain 
adding pooling on top of this filter proved be an effective segmenting algorithm 
after training the convolution and softmax regression algorithm on the handful of images in the training set the classifier is tested on one of the many remaining images collected during field testing of the prototype 
a couple of notable examples are shown below showing interesting behavior of the algorithm 
convolution and softmax regression
in the context of the actual product the goal of the classifier is solely to identify areas of traversable terrain and cart paths to guide the cart 
whether the background is successfully classified into sky or trees is irrelevant 
thus the softmax regression is moderately successful 
the algorithm s viability has been demonstrated although further training examples are required to demonstrate high accuracy 
the computer vision algorithms explored with the project show promise in meeting the requirements of the autonomous golf caddy robot 
tracking the target person has been successfully demonstrated with use of the mobilenet ssd single shot multi box detector algorithm utilizing a neural network 
classifying the terrain has proved to be more difficult due to the subtle differences in terrain such as the difference between fairway and green 
for this purpose a number of approaches have been explored 
felzenszwalb edge detection proved to provide useful functionality by pooling classifications within detected edges 
a simple unsupervised k means segmentation algorithm also demonstrated limited performance in identifying traversable terrain 
however the convolution and softmax regression algorithm proved to be the most promising in identifying the relevant classes with robustness while still being computationally lightweight enough to run in real time with a limited processor 
however a more efficient means to create supervised data and provide more training examples must be improvised in order to expand the accuracy and robustness of the classifier 
both authors collaborated to collect data in the field with the product 
akshay gupta focused on the human tracking algorithm and implementing the bounding box tracking 
furthermore he investigated k means segmentation and felzenszwalb edge detection and associated pooling as approaches to classification 
nicholas gloria focused on the classification problem developing the two layer convolution and softmax regression algorithm and devising the means of generating supervised data the repository containing all code for the project can be found at the following link 
https drive google com open id 1t3cridiwy8aafzsrdrktjcicvb yk tb
team and i d like to work on a project related to my daily work 
this year we ve rolled out a feature to collect jobseeker s commute preference 
i introduction i am current a full time software engineer in linkedin jobs
a member can provide his exact address and preferences to commute e g 
driving v s 
public transit the time range of the commute as illustrated on the screenshot here 
for members who provided commute preference we are able to show member jobs within their commute preference range and recommend jobs based on their commute preference note that all commute preferences we collected from members today are organically input by member 
in order for us to provide more value to our job seeker from this feature we want to create a better coverage on linkedin members with commute preference 
after research i ve decided to scope my project to the following two sub problems predict if a user is a commute sensitive user and is willing to put commute preference in careers interest page 
we will reference as willingness problem in the rest of this paper predict member s expected maximum commute duration range by driving falling into different categories in minutes like l5min 60 min etc 
we will reference this as duration problem in the rest of this paper with these two problems resolved we will be able to leverage this data for precise feature promotion e g 
a popup box saying based on your activity you seems to be interested in jobs near you would you like to provide us your detailed commute preference 
or more directly we can leverage this predicted implicit data to improve job search and recommendation directly the input of our project would be linkedin member s basic information and job seeking related activity tracking data 
the output of our project will be a probability score in willingness problem and will be a list of score indicating the likelihood of the commute duration falling into different duration categories as a side note since i am dealing with linkedin s industry data and product please understand that i will hide some information for integrity whenever there is a need including dataset size some data distributions etc ii 
related work unfortunately the problem i am trying to resolve in this project is extremely specific to our product so there are no direct researches that i could leverage 
but it turns out that i got some inspirations on few other job seeker related studies within company done by other people 
the information i leveraged from those studies includes their usage of features and data source the models they are leveraging as well as the ways they are improving the model which i cannot give more details in this paper 
this is an industry level project where i am dealing with big data generated from our production 
i d like to underscore that there is significant amount of data engineering effort involved to make up the dataset ready for training 
iii dataset and features
since i am not able to share the code for this part due to privacy issue i will describe into details about my step to generate and selecting features 
all raw datasets i used for this project are persisted in hdfs which stores our etl ed database data as well as our tracking events sent from kafka 
for this project here are the datasets i leveraged standardized member profile data which includes member s derived location standardized job posting data which includes job postings derived location member s job activity tracking events including job view search apply save member s careers preference data which includes member provided preferred location and activity level standardized geo data which provides geo coordinate information latitude longitude given a geo location existing commute preference data organically provided by members all data i used for this project had been pre sanitized with our data team and had obfuscated all personally identifiable information the datasets i selected for this project is mostly coming from my product intuition coming from my day to day work as well as referencing to some similar research done within the company 
a datasets
it is obvious that the positive data could be generated from our existing commute preference data which we simply mark all members inside the data set as positive the negative data however is something we need to figure out how to build by ourselves 
we cannot use all members who did not provide the information as negative as a lot of them are simply not actively looking for the job and thus do not know about the feature 
b label preparation 1 willingness problem labeling 
based on the product intuition i decided to generate negative labels for members who had viewed the commute preference page but did not provide us information 
this approach makes sense in the way that if we are going to promote the feature with popup box or notification the negative data described here actually simulated the situation when member become aware of this feature and thus i believed it could capture our testing scenario 2 duration problem labeling we are currently storing categorized commute duration for members like 15 30 45 60 90 120 
i believe it is hard to capture member s commute duration preference in 15 minute granularity so i ve remapped the label into 
we can categorize the features into following two categories 1 member derived data i ve directly generated features of member industry and member job activity level from our derived data set 2 job activity distance data in general this category of features is generated from our raw data in the following steps 1 
retrieve all memberid jobid pairs from job view job apply job save tracking events 
c features generation process
2 
join and find out member s derived location from our dataset 
also join and find out the job s location 
3 
join the locations with our standardized geo dataset get the standardized geo coordinates for members themselves and job location 
4 
calculate the distance from member to job with the following haversine formula 5 
group the data with memberid and then build a member to vectors map for example member a vector containing all job view distances for member a 6 
since most models cannot deal with variant length vectors we will derive 3 features from the vector which is count average distance and standard deviation 
7 
apply steps 1 6 to all job view job apply and job save data 
apply steps 3 6 to member provided preferred location data and job search data as the location is directly associated with member in these two datasets 
as a result we generated 15 features here 
8 
specially for job search i ve excluded searches in country and state granularity as it s geo coordinate is kind of random in such a big geo entity 
instead i ve introduced two features country search count and state sarch count to capture this information 9 
at the end i standardized the numeric data into values with 0 mean and 1 standardized deviation with 
as you can imagine there will be a lot of missing values with my dataset as it is a norm that a member may never applied or proactively searched for a job etc 
i believed that it is a tradeoff between the model accuracy and the usefulness of the model as we will can also do the similar transformation for missing values in test data and enables the model to work for more people 
d deal with missing values
i decided to deal with the missing value in the following steps 1 remove all data rows missing job view data it does not make sense to predict for a member who never viewed a single job 2 populate job apply and job search average distance and standardized deviation with job view data if missing 3 populate job save and member preference location from the job apply data filled in step 4 generate a feature of is some action missing 0 1 based on the missing data populating resultthe above missing value populating algorithm is mostly based on product intuition 
there may be some better way to populate these fields and i will discuss in the future work section but note that this step is not required for xgboost algorithm i used which will be covered in the methods section 
after all the preprocessed data above i ve sampled a big enough amount of data as our full dataset and do a 80 20 split for training dev split while i am leveraging the dev set mainly for model tuning besides i ve set up a separate dataset with latest data as test set to report performance of different models 
this followed the intuition that we are predicting the future data with older data iv 
e sample and dataset split
methods for the two sub problems i am using the same set of features as well as similar set of machine learning algorithm but with different configurations 
before i actually implement any machine learning algorithm i noted that the label distribution of willingness problem is reasonably balanced 
the duration problem however the labeled data is highly inclined into label 0 and 1 and the model maybe skewed to predict 0 and 1 if i do not take some actions to resolve this issue i calculated a class weight with the following equation the intuition of this formula is to assign a higher weight for minority classes 
a deal with imbalanced data for duration problem
i then assign this class weight to the training samples according to the training sample class this actually transformed all my loss functions in our methods as a weighted loss which effectively penalized errors on minority classes 
we implemented the binary model for our willingness problem the logistic regression with l2 standardization optimized the following loss function for binary model for duration problem we implemented the softmax which replaced the hypothesis as and the loss function to optimize becomes 
b logistic regression softmax regression with l2 regularizations
i ve introduced neutral network to learn both problem with slightly different configurations for willingness problem i ve constructed a neutral network with 2 hidden layers with 30 and 10 neurons respectively 
in both hidden layers i am using relu activation function 
c neutral network
and i used sigmoid function as our output layer because i would love to get the similar format of results i had from the previous logistic regression 
i run the training process with 20 epochs 
the network structure and forward propagation formula can be represented as for duration problem i ve constructed a neutral network with 2 hidden layers with 50 and 20 neurons respectively and are again using relu as hidden layer activation function 
for output layer i am using a softmax activation function which works for multinomial classification problem 
i ve shifted my focus to non linear machine learning algorithms like decision tree i am using a gini loss for my decision tree defined as and the decision tree is constructed with the split rule which maximized the information gain ig in each split which is defined as n uvwxpy o rzha wxp i implemented decision tree for both willingness problem and duration problem 
it turns out that the decision tree model tends to overfit and after tuning the parameters i ve set down using minimum samples pruning rule with 100 minimum sample for willingness problem and 400 minimum sample for duration problem 
d decision tree
xgboost stands for extreme gradient boosting and has been a very popular decision tree ensembles algorithm used to learn structured data 
the idea of xgboost training is additive training 
e xgboost
in each step the algorithm fixes what have learnt and add one new tree at a time 
we can represent the prediction at each step t as to find the optimal tree to add in each step intuitively we need to optimize our objective in each step besides we need to tune the max depth of the boosting trees to avoid overfitting 
after experiment i choose max depth of 5 for each subproblem 
for logistic regression model decision tree model and xgboost model i ve tuned the parameters using my spitted dev dataset 
in general my approach to tune parameters are like 1 
a experiments
fix other parameters 2 
provide a vector of possible values i want to try 3 
train the model and report on dev auc performance 4 
pick the optimal one and repeat steps to tune other parameters 
i did not implement cross validation to tune features because my entire dataset is relatively big and i would rather spend more time on trying different models and resolve two sub problems 
i am also aware that i did not cover all permutations of possible choice of multiple parameters and just tuned them one by one which is the same reason of time limitation for logistic regression model i tuned the regularization parameter c and learning rate for decision tree i tuned the minimum sample on leaves 
for xgboost i tuned the maximum depth and learning rate for neutral network i investigated and experimented the tradeoff between a reasonable training time v s 
the complexity of model and decided to build a two hidden layer neutral network 
the number of neurons in each hidden layer is experimented with different combinations 
and i ve also chosen the epochs as i gradually increase the number until i saw no reasonable improvement 
unfortunately i could not drive a conclusion on the relationship between the dev result and the network structure based on my current knowledge 
as we can tell from the figures and result table it turns out that xgboost performs best overall while other algorithms also giving out reasonably well results 
here is some discussion i derived from results the features i selected are able to reasonably formulate the likelihood of people s willingness to provide us commute preference xgboost is again proved to perform pretty well in structured data and successfully avoided underfitting and overfitting without too much tuning on parameters i kept the missing values in dataset before i feed it with the limitation of page i will only plot the roc curve for xgboost from the result here we can note that all models are not performing very well in duration problem 
xgboost seems to perform best but it actually performs poorly on certain category and thus making it not as useful as the willingness problem 
here are some thoughts from me about the possible reasons of the performance the amount of data in label 2 90 120 is too low even though we deal with imbalanced data with weighted loss it is likely that the data itself is not rich enough to provide signal to classify this category the default setting of our commute preference collection feature is 30mins 
this could bias the source data as a lot of people just did not update this time even though they may really want to commute in other duration of time the quality of the latitude longitude data may not be precise enough to predict in this level of granularity but good enough to predict if they are commute sensitive 
in conclusion we got good result to predict if a linkedin job seeker is willing to provide us information about their commute preference with xgboost algorithm 
we should be able to leverage this result for feature promption as well as using it as another feature in job recommendation 
vi conclusion future work
however we did not result a reasonable result to predict member s potential commute duration time based on the features i produced as for the future work with enough time provided i see opportunities in multiple ways to further improve the model 1 better missing value handling 
given that there are some correlations between the feature columns e g 
job view and job apply we could potentially implement some regression algorithm to impute the missing values which takes the populated fields as feature and populate the missing fields 2 leverage rnn recurrent neutral network 
i am not flattening the job view apply save distance features by calculating metrics like count average and standardized deviation 
with my research i found that rnn is a model to handle vectorized features in a more natral way and i expect it providing better performance on model 
sentiment analysis of product reviews an application problem has recently become very popular in text mining and computational linguistics research 
here we want to study the correlation between the amazon product reviews and the rating of the products given by the customers 
we use both traditional machine learning algorithms including naive bayes analysis support vector machines knearest neighbor method and deep neural networks such as recurrent neural network rnn recurrent neural network rnn 
by comparing these results we could get a better understanding of the these algorithms 
they could also act as a supplement to other fraud scoring detection methods 
recent years have seen an increasing amount of research efforts expanded in understanding sentiment in textual resources 
as we can see the statistics from web of knowledge in however saying this to find and monitor opinion sites on the web and distill the information contained in them remains a formidable task because of the proliferation of diverse sites 
each site typically contains a huge volume of opinionated text that is not always easily deciphered in long forum postings and blogs 
the average human reader will have difficulty identifying relevant sites and accurately summarizing the information and opinions contained in them the objective of this paper is to classify the positive and negative reviews of the customers over different products and build a supervised learning model to polarize large amounts of reviews 
our dataset consists of customers reviews and ratings which we got from consumer reviews of amazon products 
we extracted the features of our dataset and built several supervised model based on that 
these models not only include traditional algorithms such as naive bayes linear supporting vector machines k nearest neighbor but also deep learning metrics such as recurrent neural networks and convolutional neural networks 
we compared the accuracy of these models and got a better understanding of the polarized attitudes towards the products 
so far there are a lot of research papers related to product reviews sentiment analysis or opinion mining 
for example xu yun deep learning neural networks is also popular in the area of sentiment analysis 
ronan collobert in this paper we want to apply both traditional algorithms including naive bayesian k nearest neighbor supporting vector machine and deep learning tricks 
by comparing the accuracy of these models we would like to get a better understanding how these algorithms work in tasks such as sentiment analysis 
our dataset comes from consumer reviews of amazon products besides to have a brief overview of the dataset we have plot the distribution of the ratings 
in 1 https www kaggle com datafiniti consumer reviews of amazonproducts
data preprocessing
due to the imbalance of our dataset we have tried data resampling in some of our experiments 
data resampling is a popular way of dealing with imbalanced data 
data resampling
in this project we tried to oversample the data of class 1 2 and 3 by repeatedly sampling those reviews because these three classes have far less samples than the other two 
therefore the original reviews of label 1 2 and 3 has been repeated 15 times in our training set 
however since there are many repeated samples in the training set it is easy for the model to overfit 
we have tried two types of features in the project 
the first type is a traditional method 
basically we build a dictionary based on the common words and index each word 
we set the threshold for the word dictionary to be 6 occurrence and ended up collecting 4223 words from our entire dataset 
then we transform each review into a vector where each value represents how many times the word shows up 
for this we actually tried changing the threshold and the length of the dictionary 
what we found is that the increase of the dictionary s length did not have too much effect on the accuracy another type of feature we used is the 50 d glove 2 dictionary which was pretrained on wikipedia 
for this part we basically want to take advantage of the the meanings of each word 
in this case we represent each review by the mean vector of 50 d glove vectors of all individual words making up the review 
as we will see in our result because of the way we represent each review the features got weakened and the distance between different reviews actually is not that accurate 
naive bayes is one of the most common generative learning algorithms for classification problems 
this algorithm assumes that x i s are conditionally independent given y which is called naive bayes assumption we also incorporated laplace smoothing in our model to make it work better 
the prediction of an example is given by the formula below with the first way of representing review texts it takes an array of non negative integers and models p x i y with multinomial distribution 
with the second way of representing review texts using glove dictionary the inputs are no longer non negative integers so we chose to model p x i y with gaussian distribution 
k nearest neighbor knn is a nonparametric classification method 
it has been widely used recently 
k nearest neighbor
when making a prediction this method fist look for the k n nearest neighbours of the input 
then it will assign the majority of that n neighbours class 
the distance between each neighbour is euclidean distance which is able to measure the similarity between each data point 
the equation above shows the mathematical representation of knn algorithm 
the general idea of knn is that if the inputs are similar to each other then the output would be the same 
in this project we have tuned the number of nearest neighbours k among 4 5 and 6 
linear svm is a mthod that creates a classifier a vector that separates the labeled datasets 
geometrically given two types of points circles and x s in a space it tries to maximize the minimum distance from one of the points to the other 
linear support vector machine
in other words it maximizes the margin 
the optimization problem that svm tried to solve is below it tried to find the w to satisfy the maximum margin problem and satisfy the separability constraint 
long short term memory lstm is unit of recurrent neural network rnn 
a common lstm unit is composed of a cell an input gate an output gate and a forget gate 
long short term memory
the cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell 
lstm networks are well suited to classifying processing and making predictions based on time series data since there can be lags of unknown duration between important events in a time series 
the configuration is shown as the in this project we used the lstm with 128 hidden units and then used a dense net with softmax as the activation function to predict these 5 classes 
the data has been trained for 20 epochs in experiments using lstm 
adam optimizer has been used to optimize the parameters the learning rate is 0 01 and the batch size is 32 
to prevent overfitting a dropout rate of 0 2 was set in the lstm layer 
the entire dataset of 34 627 reviews was divided into a training set of size 21000 60 a validation set of size 6814 20 and a test set of size 6813 20 with in general all models perform better with traditional input features than with glove input features 
specifically lstm generates the most accurate predictions over all other models 
the
we observed that knn required much higher computation complexity than naive bayes and svm during train time 
as in knn algorithm it needs to calculate the dis tance of all the evaluation data points and all the training data points which is more time consuming in addition the increase of the dictionary s length did not have too much effect on the accuracy 
one explanation is that when we decrease the threshold of the dictionary the length of dictionary will increase 
but the problem is that we only have less than 40 000 reviews 
if we think about it the number of data points is not that significantly larger than the dimension of feature space 
so the curse of dimensionality could the issue here the result using glove mean is worse than the method of normal word count 
the possible reason is that if we use the average the individual word feature will be weakened then the distance between different reviews will be inaccurate when it comes to lstm the result is a little bit better than other conventional machine method due to the bigger amount of the parameters 
we can also see from table 1 after resampling the training accuracy of lstm with glove has reached 85 6 
however the test accuracy is only 65 6 which means this model has overfitted on the resampled data since there are many repeated examples 
in summary we have tried two types of features 
for this two type of features we tried all the algorithms we mentioned in the model part including naive bayes svm knn lstm 
from the results we can see that our accuracy on the test set is the best when we use lstm on the first type of feature 
one of the main reason our accuracy is not high enough is because of the data imbalance 
we tried resampling and different weighting techniques that we got from the feedbacks of the audience during the poster session 
but that didn t help too much 
another possible solution we haven t tried is to find more data points from other resources 
we think that might help us solve the problem of data imbalance 
wanliang tan responsible for support vector machine algorithm and converting review texts to input features xinyu wang responsible for naive bayes algorithm 
xinyu xu responsible for the knn and lstm algorithm data preprocessing and resampling 
contribution
the spread of fake news through social media has been a dominant topic since the 2016 elections 
most work on fake news detection relies purely on text based models to determine article validity 
for example a lot of research has been done to train nlp models to classify articles and social media posts as fake news 
we aim to analyze the network structure of news propagation and leverage relationships between users and articles in order to improve on text based models to detect fake news a new approach to combating fake news is to study the spread of it from a network perspective where relationships between users and articles as well as between users and other users are important 
it is quite possible that the manner in which the news was shared could be indicative of how authentic the news is 
perhaps if a users shares more often this means that they have a higher percentage of shared articles that are fake 
maybe if a user has more followers that this suggests that they are more trustworthy 
it could be possible that articles shared multiple times are more likely to be fake 
these are all potential hypotheses focused on how the article is shared rather than the actual content of the article it could also be true that the content of the article influences how it was shared 
perhaps users who more often believe that a fake news article is real are drawn to similar uses of language or writing style 
because there is likely intersection in how the text and social features interact with each other we decided to run models which took into account both types of features for our models we experimented with three different types of inputs 
some models used purely text based features termed article context for input 
some of the models use features purely based on social network relationships termed social context 
some models used a hybrid of article context and social context features for input termed hybrid our baseline model was a naive bayes classifier using only article context as inputs 
we also implemented a logistic regression model that was tested on article context social context and on hybrid inputs 
we further improved the hybrid logistic regression model through iterative classification 
we also ran a shallow neural network model on article context social context and hybrid 
the output of all of these models were the fake real predictions for the articles in our test dataset ii 
related work jang et al 
showed a comparison between fake and real news propagation and found that real news reached a wider audience but fake news spreads through echo chambers and goes through a much higher number of iterations 
we use the fakenewsnet data repository released in august 2018 by shu et al 
the dataset contains 422 articles that were posted on twitter in 2016 with 211 articles labeled real and 211 labeled fake 
a data
these ground truth labels are drawn from both buzzfeed and politifacts article fact checking services each article in the dataset includes the headline text source images and other metadata 
in this project we focus only on the headline and text of the article 
we choose to ignore the source because we wanted the fake news detection task to be more difficult than simply learning which sources tend to produce fake and real articles i e 
articles from a source such as nytimes com are almost definitely real news 
however we see images and metadata as legitimate features to incorporate in future work abbreviated examples of fake and real articles are shown below real article source washingtonpost com date september 19 2016 title france becomes the first country to ban plastic plates and cutlery text france has apparently become the first country in the world to ban plastic plates cups and utensils passing a law that will go into effect 
the dataset includes what we term the social context of the articles being posted on twitter 
the dataset includes the anonymized user ids of 39 122 twitter users who interacted with the given articles 
for each user the dataset provides the set of articles that user shared on twitter ie user article interactions as well as how many times they shared each article 
it also provides each users set of followers ie user user interactions 
thus the social content portion of the dataset forms a graph where nodes are users and articles with edges between them 
a visualization of a subset of the dataset is shown in
we construct the three types of feature vectors we test across our models as follows 2 social context for social context we made use of certain information for each article including a how many times the article was shared b users who shared the article c users who viewed the article and d how many articles the sharer of the article has shared each article has one feature for each user where the value at the index of user i is a certain weight if the user has shared the article and a certain lesser weight if the user has been exposed to the article i e 
is a follower of someone who shared the article 
the weights we ended up using in our final model were 2 for sharers and 1 for followers 3 hybrid the hybrid features are essentially the article context and social context features appended to each other 
as a baseline we first decided to implement a naive bayes method using the text features article context 
this relies on the naive bayes assumption 
a baseline approaches naive bayes logistic regression
given a feature vector x where x i indicates the presence of the i th word in the article we assume which by bayes theorem allows us to predict labels by p y x p x y p y where we compute p y x for both y 0 and y 1 and pick the label which maximizes the quantity 
another baseline model we implemented was logistic regression model for our simplistic baseline approach 
in logistic regression we make predictions as follows where g is the sigmoid function 
we predict a value of 1 if p y x 0 5 and 0 otherwise observing the top words especially those identified by naive bayes there appears to be a stark difference in topics and content of the fake and real news stories in the dataset 
to improve on the baseline we incorporate the network structure into our logistic regression classifier using iterative classification techniques suggested by neville and jensen
b iterative classification
the dynamic model uses the hybrid feature vector and adds features that capture the relationships between articles which are not explicitly provided in the dataset 
given an article x we define related articles as the set r of all articles shared by users that also shared x 
c dynamic model
we add features for the number of articles in r that are real and fake as well as the ratio of real to fake articles 
in the training data we can easily compute these extra features and train our model by checking the ground truth labels of articles in the set r however at testing time we cannot check the labels of r thus we first use the regular model to compute predictions or soft assignments for the labels of the articles and then use these assignments to construct the feature vector for our dynamic model 
when we then make predictions on the test data our initial soft assignments may change which means we can recompute the feature vector and make predictions again repeating this process until convergence hence the name iterative classification 
we implemented a neural network in pytorch using the words from an article and its weighted title as input features 
the network has an initial linear layer that maps from all input features to the number of the hidden units 10 
d article context neural network
then the network has an activation layer which uses a rectified linear unit relu that outputs the maximum value of the input and 0 
the output of the relu activation is fed to a second linear layer that maps from the 10 hidden units to a single scalar value 
ultimately a sigmoid output layer calculates the probability that an article is fake 
if the output probability is greater than 0 5 then the article is classified as being fake otherwise it is classified as being real 
the nn has 10 hidden units with binary cross entropy loss and stochastic gradient descent with momentum 
all nn models had a learning rate of 0 001 and were trained for 1000 epochs 
we ran a shallow neural network on the social context features described in the data and features section 
the hyperparameters and setup were the same as for the article context neural network 
e social context neural network
the hybrid method uses the same setup as the previous two methods 
it combines the article context and social context features and runs the model on this updated feature array 
f hybrid neural network
the following table shows the results of the top 5 words found through naive bayes and logistic regression 
we observed these words in order to see if there was a stark contrast in the language that is used in fake versus real news articles 
a figures and tables
additionally the following graphs show the loss over the training epochs for the article context social context and hybrid neural networks 
for the hyperparameters we ended up using a learning rate of 0 001 and 1000 epochs 
iterative classification with logistic regression achieved the lowest test error 
we can see that logistic regression with hybrid features achieves a test accuracy of 83 26 and logistic regression with iterative classification achieves an accuracy of 87 7 thus the iterative classification framework accounts for over a 4 increase in accuracy while its performance was not the highest a neural network with purely social context identified fake news articles with 63 accuracy 
one of the main challenges of the project was understanding how to appropriately incorporate user user interactions into neural networks 
the training and testing were done on a mixed dataset of articles fact checked by buzzfeed and politifact creating a sparse matrix of user interactions between the sets of articles 
the hybrid neural network was created by combining the input features of the social context and article context nn but the expected lift in performance was not seen 
this may be caused by redundancies in input features and the sparseness of the input matrix 
future work includes improving the social context and article context neural network performance through additional feature engineering and additional tuning of hyperparameters 
we tried several different combinations of neural network layers but it would be beneficial to complete a more in depth analysis on which layers are suited for this particular problem additionally it would be beneficial if we could get access to a larger dataset to test our models on 
vii further work
with the relatively fakenewsnet dataset the models are more subject to overfitting and a large dataset would prevent this as well as provide an even more accurate representations of the social trends that are actually being seen in practice 
we believe we have done a good job of distributing tasks on this project 
neel implemented the logistic regression baseline and iterative classification 
viii contributions
meghana implemented the naive bayes baseline and the neural network structure 
anika implemented the social context features after doing a preliminary analysis on user relations along with the social context neural network 
the team together implemented the hybrid methods and spent time reading up on previous work and debugging challenges of the models together 
https github com neelr11 cs229
ix link to code
as the most popular multiplayer online battle arena video game pubg has attracted more and more players to join 
the game features the last man standing deathmatch mode in which one hundred players fight against each other in a battle royale 
players can also choose to match solo or with small team of up to four people 
in order to win the first place a k a 
eat chicken dinner players must choose appropriate strategies during the game and make every effort to survive till the end 
various gaming strategies may include collecting weapons and shields versus collecting sufficient medical packs engaging in every possible fight versus avoiding battles taking cars to relocate versus sneaking across bushes and forests etc given players statistics within a game we plan to predict their final ranks 
to be more specific we take the data from kaggle
a significant part of our work is feature engineering 
the source data provided by kaggle contains 4 45 million entries of training data with 28 features in each entry 
some entries in the data are invalid and a few entries are generated by cheaters 
we filtered out problematic data entries first then applied techniques to remove outliers e g players who had no move in the whole game but got kills who had unreasonable head shot rate or long distance kills 
next normalization of features to predict the rank of each player based on his statistic data we firstly considered linear regression algorithms 
besides the normal linear regression we tried lasso regression tree algorithm is another series of widely used algorithms that can be used as regression model 
we explored the random forest algorithm
the dataset comes from kaggle competition which contains anonymized pubg game stats formatted so that each row contains one players post game stats 
the columns are some features that have impact on the players finishing rank such as number of enemy players killed total distance traveled on foot in vehicle by swimming number of weapons picked up etc 
two example waveform plots showing distribution of damagedealt and walkdistance by players 
we first perform fully covered data cleaning 
some observations in our dataset have really weird characteristics 
the players could be cheaters maniacs or just anomalies 
removing these outliers will most likely improve results 
we inspect the data and figure out the types of players to be removed killing without moving anomalies in aim more than 10 roadkills more than 45 kills and 100 headshot rate anomalies in total travel distance anomalies in weapons etc we apply a few feature engineering methods to process the data 1 
add group statistic data as the final score is the same for all members within a group for each group in a match we get the mean sum max min value of each feature within the same group then rank the values within the same match to get the mean sum max min value ranking as a new feature for each group 2 
since the number of players joined in each match is different we first normalize feature values based on the amount of players joined in a match and then normalize the features by mean and variance 3 
convert categorical feature such as match type into one hot vector 
we have tried five most commonly used regression models 
linear regression was selected as our baseline model with a baseline mae score at around 0 09 and based on that we continued to try ridge regression lasso regression elastic net regression sgd regression 
regression models
ridge regression has the best out of box result with the help of aforementioned feature engineering likely because it handles multicollinearity data better ridge regression is a technique for analyzing multiple regression data that suffer from multicollinearity 
when multicollinearity occurs least squares estimates are unbiased but their variances are large so they may be far from the true value 
by adding a degree of bias to the regression estimates ridge regression reduces the standard errors 
decision tree are a popular tool in machine learning specifically in decision analysis to help identify a strategy most likely to reach a goal 
in this case we have applied random forest algorithm and lightgbm light gradient boosting machine algorithms 
tree models
lightgbm results in quite good mae score which shows significant improvement on linear regression models lightgbm is a new algorithm that combines gbdt algorithm with goss gradient based one side sampling and efb exclusive feature bundling 
i e thus the computation cost can be largely reduced 
more importantly goss will not lose much training accuracy and will outperform random sampling 
a mixture gaussian model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of gaussian distributions with unknown parameters 
one can think of mixture models as generalizing k means clustering to incorporate information about the covariance structure of the data as well as the centers of the latent gaussians 
semi supervised mixture gaussian
semisupervised mixture gaussian model adds a portion of labeled data into the existing mixture model to help algorithm learn accurately and converge faster 
below is the update rule for gmm 
our main motivation is to measure the accuracy of predicting players winning placement and primary metric is mae of predicted winning placement 
the winning placement is a decimal value ranges from 0 to 1 while 0 corresponds to last place and 1 corresponds to the first place 
our best result comes from lightgbm model with a mae of 0 0204 this result was ranked 57 in kaggle leader board as of dec 10 in our baseline attempt we feed only raw features into various models including linear regression ridge regression lasso regression random forest and light gradient boosting machine model 
then we tune our models by adding engineered features step by step and observe that models gradually improved 
the last step we perform is applying 5 fold cross validation to the data set 
please refer to the following in the light gbm model we used learning rate 0 05 
although we understand model could be trained faster with slightly higher rate we choose to use a conservative number just to make sure algorithm converges properly 
in order to reduce bias we introduced k fold cross validation and found out 5 fold performed better than 10 fold another part of our motivation is to categorize the different gaming strategies of top players 
some example strategies are camping hide in one place and wait for others to come actively fighting engage in more fight to collect weapons and equipment and escape avoid fights in order to survive till the end 
we expect players who use camping strategy have less total moved distance and high final rank on the other hand active fighter will have high total damage and best equipment we first trained two unsupervised learning models k means and mixture of gaussian and tried to observe the relationship between winning placement and strategy related features 
however we couldn t find obvious pattern in model predicted players categories 
in other words when plotting the strategy related features of a predicted class there is no clear insight in data distribution 
after further investigation we realized the problem is in our data 
the game data we used is post game statistics when means it doesn t have the power to describe what happened during the game 
given players choose strategies flexibly we cannot expect a player stick to one strategy throughout the game 
therefore we spend extra step in categorize players gaming strategies holistically 
by using semi supervised mixture of gaussian learned in class we manually assign 4 classes according to a few important features and observe if these classes can differentiate winning placement 
and we found the following patterns 1 
game prefer fighters 
the cluster labeled with most aggressive players has a mean winning placement over 0 95 while the most conservative players cluster only has winning placement around 0 4 2 
don t stay in one place 
although our model indicates keep avoiding confrontation does not increase final rank staying in one place will bring players troubles 
the cluster labeled no escape only has mean winning placement around 0 26 
with effective feature engineering and appropriate model selection we achive decent results and observe that lightgbm performs best in predicting final ranks 
moreover we explore some interesting gaming strategies which we will give out to pubg players as free chicken dinner 
for future work we will train the ai to play pubg and win 
with the results we have arrived at we will train the bots to maximize the exponentially decayed sum of future rewards with data produced entirely from self play that is the bots are playing against themselves to learn to play by the strategies that maximize their chances of winning 
by building ais that succeed in complex and dynamic games like pubg we believe it can bring humanity closer to the ultimate goal of making agis that function in the messiness of the real world 
https www dropbox com sh zuktbhf6h0fo0iz aadu5pcw2lk5evl5rwaikbmya dl 0
all members in our team performs extraordinary 
wenxin wei worked on data cleaning and training with regression models 
xin lu worked on feature engineering and training with light gbm model 
yang li worked on feature engineering training with random forest unsupervised learning algorithms and cross validation 
we use machine learning to build a personalized movie scoring and recommendation system based on user s previous movie ratings 
different people have different taste in movies and this is not reflected in a single score that we see when we google a movie 
our movie scoring system helps users instantly discover movies to their liking regardless of how distinct their tastes may be current recommender systems generally fall into two categories content based filtering and collaborative filtering 
we experiment with both approaches in our project 
for content based filtering we take movie features such as actors directors movie description and keywords as inputs and use tf idf and doc2vec to calculate the similarity between movies 
for collaborative filtering the input to our algorithm is the observed users movie rating and we use k nearest neighbors and matrix factorization to predict user s movie ratings 
we found that collaborative filtering performs better than content based filtering in terms of prediction error and computation time 
content based filtering makes recommendation based on similarity in item features 
popular techniques in content based filtering include the term frequency inverse document frequency tf idf weighting technique in information retrieval content based filtering has the advantage of being able to solve the cold start problem when there hasn t been enough users or when the contents haven t been rated 
however it is limited to features that are explicitly associated with the items and requires extensive data collection process 
in particular automatic features extractions are difficult and expensive for multimedia data such as images audio and video stream 1 
we use the movielens dataset available on kaggle 1 covering over 45 000 movies 26 million ratings from over 270 000 users 
the data is separated into two sets the first set consists of a list of movies with their overall ratings and features such as budget revenue cast etc 
after removing duplicates in the data we have 45 433 different movies 
the other dataset used is the user movie ratings which contains user id movie id and the user s 1 5 rating of the given movie 
we represent this data as a matrix where one dimension represents users and the other dimension represents movies the matrix is very sparse since most users have rated only a small portion of all the movies 
techniques such as nearest neighbor and matrix factorization are used to analyze this dataset we treat the task as a continuous ranking problem rather than a classification problem of thumbs up thumbs down because it is more flexible and encodes more information 
we can map ranking back to predicted rating and at suggestion time we will recommend the highest ranked items to users 
due to computational constraints we use a random subset of 100 000 ratings data in our project 
we randomly split the set of user movie rating pairs into an 80 training set and 20 test set 
we minimize the sum of squared error of between the predicted ratings and the actual ratings 
we run the baseline of content based filtering using tf idf calculates the weighted similarity between bag of words 
the importance of a term t is positively correlated with the number of times it appears in document d but the frequency of term t among all documents is inversely related its ability to distinguish between documents 
thus we calculate the frequency of word t in document d weighted by the inverse of frequency of t in all documents tf idf t tf t d idf d tf t d log d 1 d t d where d is the length of the document and d t d is the number of documents where t appears we calculate the cosine similarities between movies based on movie features 
we create two similarity matrix one using movie descriptions and tagline and the other using actor names director names and keywords 
we remove the space between first and last names to avoid confusion on different people with the same first name 
movie descriptions and taglines are often long and extensive whereas actor names are only one word combining them into one word vector would overweight descriptions and underweight actor and director names 
thus we separately calculate two cosine similarity matrix and combined them using a weight that minimizes training set error tf idf looks for the frequency of the exact word in a document and could not pick up on synonyms or similar descriptions so it produces very low similarity scores across all movies 
word2vec is based on a distributional hypothesis where words appear in the same context tend to have similar meanings 
to take the context of a word into account we use doc2vec next we apply k nearest neighbors as a baseline to collaborative filtering 
in item to item collaborative filtering we predict user u s rating of movie i to be the weighted sum of movie i s rating from the k nearest users based on their ratings similarity score where sim u v is the rating similarity between user u and v r vi is user v s rating of movie i and n k i u is the k nearest neighbors of user u s ratings 
similarly we can predict user u s rating of movie i using knn for item to item collaborative filtering adjusted for a given movie s average rating matrix factorization is another useful method in building a recommendation system where s is the set of u i pairs for r ui in training set and controls the extent of regularization to penalize the magnitudes of learned parameters and thus avoid overfitting 
bias b i b u incorporates global average bias from movie b i and bias from user b u we minimize the rmse by stochastic gradient descent 
the user and movie factors are initialized randomly from a uniform distribution and baselines are initialized to zero 
then for each user movie rating in the training set we update each parameter according to the error between the predicted rating and true rating 
for example the user factor p u is updated by p u p u r ui r ui q i p u where is the learning rate 
similar rules apply to all other parameters 
note that the entire factor vector for user u is updated 
after all the examples in the training set are seen one epoch has finished and we repeat the epochs until the change in training rmse converges 
alternatively instead of updating the entire factor vector for users and movies simultaneously we first update the first entry p u 1 and q i 1 by going through several epochs until convergence 
then we move on to p u 2 and q i 2 and so on 
we can also change how the use factors and movie factors interact by kernelizing the term q t i p u 
the other kernels we considered are radial basis function kernel rbf and sigmoid function throughout this report we use root mean squared error rmse 1 s u i r u i r u i 2 to evaluate the performance of the algorithm where r u i is the actual rating given by user u to movie i andr u i is the predicted rating 
for content based filtering we use the movie similarity matrices generated by tf idf and word2vec to predict an user s movie ratings formula 1 
to combine the two similarity matrices from tf idf we calculate their weighted sum 
evaluating the performance on a training set consisting of 80 randomly selected user movie rating pairs we see that the rmse is smallest for the weight w 1 0 7 w 2 0 3 as shown in for doc2vec we use the distributed memory version of paragraph vector pv dm model and include some noise words to increase the overall robustness 
we have experimented with various starting learning rates
we have explored both content based and collaborative filtering for building the recommendation system 
collaborative filtering overall performs better than content based filtering in terms of test rmse 
additionally content based filtering is computationally more expensive than collaborative filtering as it involves extensive processing of text features 
therefore collaborative filtering is preferred 
all codes are available at google drive link for future work we would like to address the skewed prediction caused by imbalance in the number of low ratings compared to high ratings 
we would also explore ways such as regularization to address the overfitting issue in knn 
additionally our recommendation system can be improved by combining content based filtering and collaborative filtering 
possible techniques include incorporating content features as additional features in collaborative filtering or vice versa decision trees and neural network 
we could also add in a time dimension to our model to capture the change in user preference over time 
lily cleaned the data conducted literature review and worked on knn and doc2vec 
tianyi cleaned the data reviewed literature and worked on tf idf and matrix factorization 
shujia reviewed literature coded up the matrix factorization algorithm with tianyi and made poster 
in this paper we propose a novel method for the task of poverty prediction utilizing natural language processing on latent embeddings of wikipedia articles as well as satellite imagery to predict poverty for geographical regions providing an alternative to on the ground surveys and nightlights estimation 
we demonstrate there are latent traits in the articles which correlate strongly with poverty 
this framework can be deployed across the globe and provides a successful and intriguing link between latent textual embeddings and socioeconomic applications 
elimination of extreme poverty is one of the foremost un sustainable development goals in this paper we utilize latent embeddings of wikipedia articles as well as satellite imagery to predict poverty levels of regions 
in short the input to our model consists of the embeddings of the k closest geolocated wikipedia articles to the coordinate of interest and for our final model a nightlight satellite image histogram of the region 
the output is a poverty prediction a continuous number between 2 and 2 
we use a variety of different methods to accomplish this task before arriving at a model that outperforms the current state of the art results 
lastly we provide a study of article embedding activations and what features the model is learning in order to better understand what is happening 
previous attempts to predict poverty in geographical regions have been limited to the use of satellite imagery 
in order to utilize latent wikipedia articles to predict poverty levels around the world we first obtained a corpus of wikipedia articles from the june 2018 dump parsed it into its constituent articles and extracted all those which were geolocated 
this process netted over 1 million geolocated articles 
we then trained a doc2vec model to train our models we specifically focused on five countries ghana malawi nigeria tanzania and uganda 
these are five of the most common countries for evaluating new model performance on and thus allow us to compare our model to benchmarks in the literature 
for each of these countries we trained a model on 80 of the points within it used the other 20 as our dev set and then evaluated the model on the other four countries 
this was repeated for each of the countries 
this approach is motivated by the fact that our nearest article and nightlights images have high spatial dimensions so primarily evaluating across national boundaries guarantees no train and test contamination 
also this cross boundary evaluation strategy and train dev test split is common practice for the space and is observed in much of the literature in this task
doc2vec svm regression for a baseline we created a simple support vector machine regression support vector regression uses the same principles as svm classification where input spaces are mapped into higher dimensional spaces using kernels essentially converting non separable problems to separable problems 
neural networks work by passing the input through a series of matrix multiplications in order to learn a complex relationship between the input features and the output 
baselines
these networks minimize the loss and update weights through a process called gradient descent finding the optimal set of weights for the task by backpropagating the derivative with respect to each of the weights 
to evaluate our models the primary metric we use is the squared person correlation coefficient we observed a steady improvement in r 2 values as we progressed from our baselines to our final architectures for error analysis we observe that often the points which the model receives the highest mse for are ones which come from areas where points with disparate ground truth values are tightly packed together such as urban areas where there are slums and wealthy places in close proximity to each other 
this makes sense since the spatial resolution of our input 5km x 5km image does not allow us to discern features at a higher resolution than its dimensions ie 
experiments and results
any two points within 5km of each other often share much of the same image and thus receive similar predictions this is a common issue with the nightlights approach 
to help us better understand wikipedia article embeddings especially the components that play key roles in poverty prediction we performed pca analysis on the first two principle components of wikipedia article embeddings 
more specifically we picked the article embeddings of the richest 33 and the poorest 33 points in the ugandan test set and compared their pca projections to the projections of school university hospital company and settlement articles that appeared in the test set see
in this paper we present preliminary yet state of the art results for poverty prediction using wikipedia article embeddings and nightlights histograms 
before this becomes usable in real world applications we plan to acquire wikipedia datasets with article creation dates in order to match them with appropriate nightlights imagery and explore using wikipedia articles written in different languages such as french as they may provide us with more africa related articles 
overall we feel our approach is a novel method that holds promise for future exploration and application 
the code for our models and error analysis can be found at the following link https drive google com open id 1zmozz7uxoe2esesrwlsdsak5x13lvznb
all members of this team contributed equally to the research presented in this paper 
specifically evan worked on obtaining and pre processing the dataset as well as developing and running our two main models 
zaid worked on developing and running the baselines as well as conducted error analysis 
chenlin worked on developing the second model and running experiments with all of our models as well as conducting error analysis 
graphs are inherently complex structures and learning suitable representations from these dynamic networks for downstream prediction tasks is a canonical problem with great practical implications 
indeed the generation of node embeddings that accurately capture the structure and high dimensionality of a graph may prove incredibly useful in the development of models for traditional prediction and classification tasks across nodes and edges 
in this work we focus on two critical components of representation learning on graphs the efficient and effective generation of node embeddings and the use of such embeddings for node classification and link prediction 
in particular our work aims to leverage recent developments of representation learning in hyperbolic space to provide two significant contributions to the field 1 
the development and empirical evaluation of a supervised algorithm to embed graphs in hyperbolic space providing significant improvements over recent unsupervised methods 2 
the development of a comprehensive multimodal framework to combine both euclidean and hyperbolic embeddings for increased representational power on node classification and link prediction tasks further improving results beyond current state of the art performance we provide an open source implementation of our findings embedding frameworks and visualization methods at www github com mananshah99 hyperbolic 
graphs are powerful tools to encode relationships between diverse objects and process unstructured real world data 
however while there are numerous benefits to such complexity the diverse nature of graphs has resulted in difficulty analyzing networks with varying size shape and structure 
furthermore conducting downstream prediction tasks e g 
regression or hierarchy generation from arbitrary nodes edges or substructures in a graph requires fixed dimensional representations of these structures a task which is made nontrivial due to the arbitrarily complex nature of networks 
the generation of embeddings for nodes in a graph provides an effective and efficient way to approach the analysis of graphical structures while retaining the diversity encoded in the graph itself 
precisely given graph g v e the canonical formalization of the node embedding task is to learn a function f v v such that f v x d for some metric space x s equipped with suitable metric s and dimension d a suitably learned embedding function f has a desired property of mapping similar nodes close to one another in the embedding space x d 
the resulting fixed dimensional embeddings for nodes can be readily used as input feature matrices for downstream tasks emphasizing the importance of properly capturing graph structure in embedding generation although numerous methods have been previously proposed to learn f v r d in both supervised and unsupervised manners recent works 
the problem of embedding nodes in graphs as high dimensional feature vectors has been explored in varying directions of which state of the art results have been observed by random walk based approaches deepwalk euclidean embeddings 
the first method to employ random walks for embedding nodes in graphs deepwalk generates node embeddings by treating random walks as sentences in linguistic models 
inspired by the widespread success of language models in learning latent distributed representations from words deepwalk relies on the intuition that random walks can be modeled as sentences in a particular language 
analogous to natural language modeling frameworks that estimate the probability that a word will appear in a sentence deepwalk estimates the probability that a vertex appears in a random walk by learning feature vectors f v v v 
for scalability purposes the authors use hierarchical softmax to approximate the conditional probabilities during gradient descent 
node2vec generalizes deepwalk by utilizing both the current node and its predecessor to identify the next node in a random walk 
by tuning two parameters p q r where the former loosely controls depth first transitions and the latter breadth first behavior node2vec is able to interpolate between different kinds of neighborhoods to better incorporate graph structure in embedding generation 
node2vec conducts a similar optimization procedure as deepwalk sampling vertices v v conducting random walks and updating feature vectors in gradient descent 
due to its ability to smoothly characterize networks according to both the homophily and structural hypotheses for neighborhood formation node2vec successfully improves performance over deepwalk across varied benchmarks hyperbolic embeddings 
hyperbolic space which uses non euclidean geometries is characterized by its constant negative curvature 
due to this curvature distances increase exponentially away from the origin and give hyperbolic spaces the name of continuous analogues of trees 
as a result hyperbolic spaces are critically relevant to model hierarchical data and distance in hyperbolic space can be used to infer both node hierarchy and similarity 
nickel et al hyperbolic embeddings excel in their encoding of both node similarity and node hierarchy which allows for unsupervised identification of latent hierarchical structures 
in spite of these benefits however the field of hyperbolic embeddings is relatively new and as such has significant room for improvement 
in particular all datasets used in
as the generation of hyperbolic embeddings is a relatively novel field in representation learning on graphs a critical problem with recent papers on hyperbolic embeddings chg miner 
email eu core 
datasets
in aggregate these two datasets represent graphs at either end of the spectrum of latent hierarchy presence while chg miner has no encoded hierarchies email eu core explicitly encodes hierarchies in its construction 
as both datasets have defined classes 2 classes of drugs and genes in chg miner and 42 classes corresponding to different organizational roles in email eu core node classification and link prediction can both be conducted on each graph 
supervised hyperbolic embeddings 
leveraging recent groundbreaking work on the generation of unsupervised hyperbolic embeddings we developed a supervised algorithm to generate node embeddings in hyperbolic space 
hyperbolic representation learning
in order to modify current generation of hyperbolic embeddings to incorporate node labels we alter the sampling procedure described in in particular our novel supervised loss function solely incorporates updates from vertices of different classes so thatis defined with the set of negative examples for u equal to n u v u v e c u c v u where c denotes a surjective mapping from node to class and d u v denotes the poincar distance function between nodes u and v as a result of this supervised modification embeddings are trained to simultaneously ensure that nodes of similar labels are embedded near one another in hyperbolic space and that nodes connected by an edge are encoded near one another as well 
since the resulting embeddings are mapped to an n dimensional poincar ball as in euclidean and hyperbolic embedding fusion 
in addition to the supervised learning algorithm we introduce the concept of hybrid embeddings that incorporate both euclidean and hyperbolic embeddings of a graph 
though a viable form of fusion methods that mathematically combine both types of embeddings eg 
hadamard product and cosine similarity are not used due to the scale differences between euclidean and hyperbolic space 
instead we simply concatenate euclidean and hyperbolic embeddings to maximize utility gained from each during downstream prediction tasks without utilizing mathematically incorrect combinations of vectors from different spaces 
to evaluate our generated embeddings we measure performance for two major network learning tasks node classification and link prediction 
while logistic regression on generated node representations is employed for node classification random forest classification on edge embeddings is utilized for link prediction node classification 
node classification and link prediction
in the node classification pipeline note that the feature vector for each node is its fixeddimension embedding 
with feature vectors for each node in the graph we implement logistic regression optimized using stochastic gradient descent 
in logistic regression our hypothesis has the form h x t x during stochastic gradient descent we minimize the loss functionfrom which we can take the gradient to derive the update ruleafter iterating until convergence we use optimized hypothesis function h to classify nodes given input feature vectors of arbitrary dimension 
specifically we split nodes of our graph into train and test sets for 5 fold cross validation utilizing h to predict across 42 classes for email eu core and across 2 classes in chg miner link prediction 
we leverage random forests in place of logistic regression for link prediction due to empirical performance improvements 
a random forest r x is a classifier consisting of a collection d of decision tree classifiers where d b and each tree d b x d b 1 2 b casts a unit vote to determine the most popular class for a particular input observation x 
given a training set x and corresponding class labels y we perform bagging b times each time selecting a random sample of fixed size from x and fitting a decision tree to the sample using a fixed size k random sample of the features 
when creating decision trees we aim to achieve maximum homogeneity in child nodes each time the data is divided into two parts where t p t l t r are parent left child and right child nodes x i is a feature variable and x i is the best splitting value for that feature 
hence we split at the optimal split value among all possible split values for all features 
to maximize homogeneity of child nodes we use impurity function t 
since the impurity of parent node t p is constant for all possible splits maximizing homogeneity of child nodes is equivalent to maximizing the change of impurity function t t p p l t l p r t r where p l is the proportion of data distributed to the left node and p r the right node 
therefore when partitioning the data at each node of a decision tree we solve the optimization problem arg maxin particular we use the impurity function t a b p a t p b t where a b are indexes of classes 1 2 
we subsequently split our graph into training validation and test sets which can be viewed as versions of the graph different time intervals where we wish to train on edges in time interval 0 and predict future edges to appear in time interval 1 
c and p a t is the conditional probability of class a given we are in node t for the link prediction task we generate x by taking the hadamard product of source and destination node embeddings to generate edge embeddings for each edge in the graph 
we generate positive training examples by sampling from edges that exist in the graph and negative examples from edges that are missing between nodes so that there is a perfect class balance between positive and negative examples 
with x and corresponding ground truth edge labels y we utilize the random forest algorithm to predict existence of edges 
in order to evaluate our proposed supervised hybrid hyperbolic embeddings on the aforementioned datasets in section 3 we performed 5 fold cross validation for node classification according to the accuracy metric and evaluated on test splits of edges for link prediction according to the average precision metric we can further evaluate our embedding frameworks by comparing the quality of visualizations in
our work develops a supervised hybrid hyperbolic embedding framework to approach embedding tasks for arbitrarily complex graphs 
we further generate models for node classification and link prediction provided nodelevel embeddings and evaluate our models on numerous real world datasets 
our results indicate that our supervised hybrid hyperbolic embeddings vastly outperform traditional methods on both node classification and link prediction indicating that leveraging graph hyperbolic structure alongside canonical euclidean frameworks provides significant benefits for overall performance 
specifically node classification results were bolstered by over 2 percent on both email eu core and chg miner datasets with the hybrid model indicating that both of our developed methods effectively learned representations characterizing the structurally differing graphs 
in the future we hope to extend our work to more diverse and large datasets to further verify the benefits of hybrid embeddings on differing graphical structures 
we further hope to identify more advanced methods of embedding fusion beyond concatenation 
manan shah developed baseline models supervised poincar embeddings the visualization pipeline and conducted experiments 
sagar maheshwari evaluated the developed models for both the node classification and link prediction tasks and conducted experiments 
the goal of this project is to come up with the optimal betting strategy for the game tichu 
tichu is a card game that includes elements of bridge poker and big two 
an essential element of the game is to correctly predict when you can go out first and make bets call tichu grand tichu bets to score points 
in this project we analyze the results from 15000 matches to come up with models that predicts whether the tichu grand tichu bets will succeed by looking at the starting hand 
with the trained models we then run simulations to compute the expected score of calling grand tichu vs calling tichu bets 
this allow us to come up with the optimal strategy of when to call tichu and when to call grand tichu 
to obtain a strategy that is applicable by a human player we reduce the input features to some basic features easily calculated by a human while retaining most of the prediction power 
this is done by taking the features with the largest coefficients in the logistic regression model 
the final result is that it is advantageous to call grand tichu bets when n ace 3 n dragon 3 n phoenix 3 n bombs 2 in the first 8 cards hand and it is on average better to call tichu bets when 2 n ace 2 n dog 6 n dragon 6 n phoenix 5 n bomb n straight n singleton small cards 7 
where n is the number of each pattern or card present in the hand 
the code for this project can be found on https github com hungiyang project tichu git 
the general goal of the game is to be the first to get rid of all the cards in ones hand 
two teams of two players play against each other to accumulate points and the first team to reach 1000 points wins the match 
at each round a total of 56 cards are being dealt each players getting 14 cards 
when the first 8 cards for each player are dealt players can look at the 8 cards and choose whether or not to call grand tichu which is making a bet of 200 points that they will go out first 
after all 14 cards per player are dealt there is another option of calling tichu which is making a bet of 100 points that he or she will go out first we are interested in predicting the game outcome based on the initial cards being dealt 
in particular we want to know the probability of making the tichu grand tichu bets based on the information of the initial cards of individual players 
with a trained model to predict the probability of making the bets we can run simulations to determine the best strategy of when to call grand tichu and tichu bets 
the owner of the online tichu server https onlinetichu com is kind enough to provide the gameplay data recorded from their server 
we have access to 14765 matches and 84127 rounds a match is the whole process of playing up to 500 or 1000 points and a round is each time cards are being dealt 
there are 20484 grand tichus and 38057 tichus bets called in this data set 2 1 
grand tichu success rate prediction the first supervised learning problem that we want to solve is to predict the grand tichu success probability given the 8 cards hand 
the input feature is the 8 cards hand in some kind of representation and the output of the model is the prediction of whether grand tichu is made being the first player to go out 
the second supervised learning problem is to predict the tichu success probability given the full 14 cards hand 
in the actual gameplay there are game mechanics such as card swapping and the option to call tichu anytime before playing the first card 
tichu success rate prediction
in our analysis we ignore these complications and just take the 14 cards after swapping as the input 
the input feature is some kind of representation of the 14 card hand and the output is the probability of making tichu being the first player to go out fig 
1 an example of a 14 card hand 
we want to predict the success rate of making tichu with an input hand like this 3 
methods and data processing 3 1 
algorithms we applied classification algorithms including naive bayes logistic regression random forest and boosting naive bayes assumes that the features hand cards or features are conditional independent given outcome making tichu or not where x i can be the number of some card or pattern and y is the outcome of making tichu or not logistic regression uses a sigmoid function to determine the probability of each outcome given a weighted sum of all the features 
the weight is fitted to get the largest likelihood random forests is an algorithm that combines multiple decision trees to get an estimator 
for each decision tree different bootstrap samples are used and at each decision node different subsets of features are used 
these procedure tends to reduce the chances of overfitting boosting takes a collection of weak classifier and weight their results in some way to create a stronger classifier naive bayes and logistic regression are our baseline model in which no hyper parameter tuning is required after the features are specified 
for the tichu classification problem our samples are biased strongly towards strong hands because in actual games players only call tichu when they are confident in going out first 
to make the model prediction better at evaluating weaker hands i add in the following cases also as training data for the tichu classification 
data processing and feature selection
the reasoning for this is that when an opponent calls tichu and no one on your team did the number one priority is to stop them from making tichu 
therefore all players on your team is on contending mode to go out first 
if you or your teammate succeed your hand can be considered a sample that can make tichu y 1 label 
if both of you failed they both hands on your team can be considered as hands that can t win tichu y 0 labels 
adding case 2 above in particular give us sample coverage of weaker hands so that we can be more confident about our prediction of weak hands 
for grand tichu classification since there is the card passing part that would be significantly affected by the whether grand tichu is called we only use the hand where grand tichu is actually called as training data 
in theory if the model is sophisticated enough and with enough data the hand input itself contains all the information 
however since we only have few tens of thousands of samples it is not quite sufficient to learn these connection on its own 
detect pattern as features
therefore we tried to search for playable patterns such as 4 of a kind royal flush straights three of a kind pairs etc and add them to the input vector below we list the patterns that we search for either grand tichu or tichu hand 
besides finding the model that gives the most accurate prediction we also want to have a model that is easy enough for human players to calculate during actual gameplay 
therefore we also looked at the importance of these features in our models and trained a model that only kept the minimal features that are easy to compute as a human without loosing too much prediction accuracy 
for grand tichu the noise is large and adding pattern detection essentially does not help the prediction accuracy 
below we list the results of combination of different subset of features vs different models 
grand tichu prediction
the metric we choose to evaluate our model is the auc which is the area under the roc receiver operating characteristic curve 
the roc curve plots the true positive percentage vs the false positive percentage 
it shows how the model performs under different decision threshold 
if the model is randomly guessing the auc will be 0 5 whereas if the model can classify all the samples correctly the auc will be 1 0 the basic way to represent the hand raw hand is a length 56 vector of 0 and 1 indicating whether the hand contains that card or not 
the compressed hand counts the number of cards of each value 
this way we encode the symmetry information of suits and can potentially help the models 
the full pattern are the 13 features listed in sec 3 3 in tab 1 the 4 features selected in minimal pattern takes the 4 features that has the largest coefficients in the logistic regression results from the full 13 pattern fit 
the four most important features are number of aces n a dragon n dragon phoenix n phoenix and bombs n bomb 
dragon and phoenix are the individually strongest cards followed by aces and bombs are the most powerful pattern in the game 
therefore it is not surprising that they have the largest coefficients in the logistic regression fit 
the coefficients for the four features for the minimal pattern fit are listed in tab 2 when the coefficient for n a is normalized to one 
it is interesting that dragon and phoenix are almost three times as important as ace for calling grand tichu 
we did not expect that from our personal gameplay experience we round the coefficients to the nearest integer to obtain a grand tichu index i g n a 3n dragon 3 phoenix 3n bomb that is easy to compute during actual game play 
if we train a logistic regression using only i g the auc is 0 62 and i g 0 87 correspond to 50 probability of making grand tichu 
this means that when your 8 cards hand has one ace there is above 50 chance to go out first if you call grand tichu in the
for tichu bet predictions we use a similar approach as for grand tichu 
however since the randomness is smaller in the tichu problem because the information of all 14 cards are available identifying the patterns becomes more useful 
tichu prediction
in the tichu prediction the minimal features we decide to keep are number of ace n a dog n dog dragon n dragon phoenix n phoenix bombs n bomb staights n straight and individual small cards n small 
the coefficients in the logistic regression when these features are used are listed in tab 3 coefficients for n a normalized to 2 0 the results of training with different model features combinations are shown in tab 4 below in the tichu predictions we see that the accuracy improves when the patterns are added as features 
the full pattern alone performs as well as full pattern plus hand as input 
the predictive power of just the minimal 6 most important input n a n dog n dragon n phoenix n bomb n straight n small is slightly worse than using the full pattern but still significantly better than just using the hand as input 
another interesting observation is that the models achieve better performance using compressed hand as input vector compared to the raw hand 
this is likely because the compressed hand encodes the symmetry of the 4 suits whereas the raw hand represents each card by a binary 0 and 1 and does not differentiate between card value and suit for ease of computation during actual games we round the coefficients of the 6 most important features to the nearest integer to create a tichu index defined as i t 2n a 2n dog 6n dragon 6n phoenix 5n bomb n straight n small 
training a logistic regression model with the tichu index i t we achieved an auc 0 83 and an i t 6 4 correspond to a 50 probability of making tichu 
therefore there is a positive expected return to call tichu when i g of the hand is 7 or greater 
to decide whether to call tichu it is sufficient to consider the probability of making the bet 
if the probability p tichu is higher than 50 than it is advantageous to call tichu 
simulation comparing grand tichu and tichu expected return
however when deciding to call grand tichu there is the added complication that even when your probability of making grand tichu p grand is higher than 50 the expected score for waiting and see the full 14 cards hand might be higher 
for example at 8 cards you might think the hand is not bad but at 14 cards you realize that there is no chance of the hand here means a fixed 8 card hand 
since different 8 cards hand with the same p grand can have different p tichu distributions we also run many realizations of 8 cards grand tichu hand to compute the expectation then we can compare e score grand p grand p and e score tichu p grand p to see whether calling grand tichu is better or waiting till 14 cards is better 
we then get grand tichu if p grand 0 5 p tichu p grand p 0 25we run 10000 realizations of 8 cards grand tichu hand and each with 100 realizations of 14 cards tichu hand 
for grand tichu predictions it seems that the uncertainty is very large and the individual strength of the cards give pretty much all the information 
for tichu adding the features of important connections between cards identifying patterns such as straights bombs etc significantly increases the prediction accuracy 
this suggests that with an even larger data set the more complicated models might be able to learn the connection themselves 
for the dataset we used models that are more complicated than logistic regression tends to overfit 
if we have access to more data models such as random forest and boosting should perform better than logistic regression in our simulation we evaluate randomly generated hands with our trained logistic regression model 
however the model is trained with data that human players would call tichu or grand tichu and the distribution might be different from randomly generated hands 
therefore when running the simulation the probability estimates for the randomly generated hands might be biased there are many game mechanics that we did not consider in our analysis 
for example there is card passing between all players which would on average increase the power of each hand and cause the distribution of cards to change 
also cards like 5 10 k dragon and phoenix are worth points by themselves and we did not take that into consideration 
another important game mechanics is called one two which is when both players in one team going out before anyone on the other team does 
all of these would change the optimal strategy and should be considered in future analysis 
also an alternative and maybe more straightforward approach is to train a regression model that predicts the score outcome instead of classification models that predict the betting outcome 
we build and experiment with various models for separating total electric energy usage kwh into usage by appliance in a given household 
we formulate the task of energy disaggregation as a regression problem for each appliance where we predict a continuous valued usage in kwh at each time step conditioned on the total energy used from m previous time steps 
we note that the challenge in this task is building a predictive network that is able to learn a mapping from r r n appliances 
we therefore experiment with a sparse coding model in which we attempt to learn signatures of each home appliance that together sum to aggregate energy signal 
we achieve the best results on the redd dataset plug wise meter data from 6 homes using a random forest model with 89 accuracy on the test set as well as our sparse coding model with 91 accuracy mentor akhila yerukola code repository https github com fl16180 disaggregate nrg
for years we have watched the sticker shock of utility bills affect many around us 
we know there is a meter at theback of the house that measures how much electricity we consume in total as a household and we receive a bill onceevery month 
friends and family remain in a dilemma as to what causes these bills to be so high many of the times 
we scrutinized our electric bills in detail looking for clues as to why but kilowatt hour was aggregated for the whole month 
listening in on calls my mom made to clark public utilities she inquired about what appliances specifically could be the main cause of our high bill 
although apologetic the call center agent could offer no help despite her best effort besidessome general energy saving tips like setting thermostats to 68 f use efficient lighting and so on 
too general for our project we experiment with and build several disaggregation models to separate total home building electricity usage by appliance 
we use the redd dataset and formulate the task as a regression problem predicting the energy usage time series of each of n appliances given an input time series of total building energy consumption 
by learning the signature for individual appliances we hope to build a generalized model for predicting usage percentage per appliance and give an accurate estimate of a given home s energy profile 
the development of machine learning approaches to energy disaggregation is a relatively new field of research 
to a large extent this is due to a scarcity of large scale public datasets with buildings that have been submetered 
for this reason research in this area is often highly centered around specific datasets 
in the past several years some small to medium sized datasets have been presented including the redd dataset used in our analysis 
because the redd dataset released in 2011 is one of the oldest and most well known it serves as a reference standard for algorithm development and testing for this reason we turn our attention to alternative approaches to energy disaggregation 
in 2010 a study by kolter batra and ng showed that a modified sparse coding methodology produced accurate disaggregation of whole building energy signals despite the promising finding few subsequent papers have applied sparse coding
we used the redd dataset for our analyses available from the data was downloaded in a raw form and processed using tools from nilmtk a python library designed for the processing of energy disaggregation data because we anticipated heterogeneity between houses we decided to train the models separately for each house 
thus for buildings 2 6 the period from apr 
dataset redd
19 2011 may 11 2011 was designated as the training set 
the period from may 12 2011 to the end was used as the validation set for the models fitted specifically on the training set from the same building 
then once hyperparameters were tuned over the validation set building 1 was saved as the final test set 
the final configuration of the models were trained over the period apr 
19 2011 may 11 2011 on building 1 and then the period may 12 2011 to the end was used as the final test set 
thus no tuning was done on building 1 which functions as the holdout set 
we explored if adding weather data would benefit our models 
since the redd data is collected from the greater boston area we obtained historical weather data from boston ma over the period apr 
weather
19 2011 june 30 2011 from the national centers for environmental information
suppose that for each house there are t observations of n appliances which we are trying to predict 
thus in this formulation we have n different target variables represented as y r t n with y i representing each appliance 
following the procedure of
we formulated the baseline as n separate regressions y i x 
we first attempted a knn model where we predict the continuous valued kwh usage of each appliance at each time step by finding the k nearest observations that are close to the aggregated energy usage at t i 
find the k nearest neighbors over all time steps t gives us a matrix of size t n k where n is the number of appliances from the training data 
we then reduce along the second dimension by taking the mean of the k observations to attain a prediction for our model 
we experimented with k 3 5 7 as k is ideally odd and tuned our choice of k by tracking accuracy on a validation set which consisted of 10 of our training set homes 2 6 with home 1 as the testing home 
we again run n separate regressions y i x 
however instead of a single predictor variable we constructed a 100 lag matrix x r t 100 where the first column is just x the second is x lagged by one observation etc since clearly a wide range of past values of the total home time series could be informative 
time series models
we added weather data to the above models simply by concatenating the matrices to get x x 
we constructed three models of increasing complexity 1 
ridge regression this minimizes y x t y x 2 
because lagged predictors are highly autocorrelated simple least squares regression would be unstable 
thus we use l 2 regularization to stabilize the model 
for each model we fit over the training set using 10 fold cross validation 
support vector regression in order to fit a higher dimensional feature space of the predictors using the optimal margin classifier we used the support vector machine with rbf kernel set at c 1 0 0 1 
specifically we use the gaussian kernel where can be infinite dimensional output 
random forest although the support vector machine can fit infinite dimensional feature space we used a random forest model because it is non parametric and an ensemble and capable of learning discontinuous patterns of features 
for example plateaus or troughs of the lag matrix over certain magnitude or duration could be highly informative of certain appliances 
100 separate decision trees were fit on the data and the mean of the trees was used as the prediction 
knn ridge svm and random forest were fit using scikit learn
modeling the problem of energy disaggregation as a sum of components where we learn a mapping r r n we are reminded of ica 
reading into literature we find that ng 
supervised sparse coding
et 
where we enforce sparsity on activation matrix a through regularization and also subject the l2 norm of b i 2 1 for each appliance i 1 n in training we thus perform 2 optimization steps much similar to k means where we first initialize a and b with small positive values and for each appliance first find the optimal a i and then treat a i as constant to optimize for b i subject to the constraints 
at testing time we store our trained dictionary of basis functions for each appliance b and recompute the optimization for activations 
we can then predict for each appliance using the dot product b i a i 
our primary evaluation metric was the fraction of total energy assigned correctly defined as n min n y n t n t y n t n n t n t n t where y n t is true value for appliance n at time t and n t is the predicted value 
the value ranges from 0 to 1 with 1 being perfect assignment we tuned the hyperparameters for knn and ridge regression over the validation sets from houses 2 6 see methods for details 
the other methods did not require any tuning 
refer to interestingly performance on the test set was high especially compared to the dev set which we believe is due to the data from building 1 being relatively more well behaved compared to the other buildings 
thus with or without fine tuning our finding lends support to the success of the modified sparse coding method 
as its pie chart shows it predicts each device class roughly equally well 
on the other hand the svm is less accurate overpredicting socket usage for example 
sparse coding also shows robustness to overfitting which may be related to the effect of regularization which lowers variance while introducing some bias 
it is interesting that random forest simply on a matrix of lagged time series data proved to be one of the top performing models 
that may be because it is an ensemble which increases robustness and reduces variance 
here we have explored several algorithms for disaggregating whole home energy consumption into individual appliances 
starting with low accuracy on our baseline knn model we have been able to find that non linear models such as random forests and sparse coding performed the best on the prediction task with 0 91 and 0 89 accuracy on our testing home 
we also experiment with data augmentation using limited amount of weather data and we hypothesize that adding a larger amount of metadata will help increase model accuracy 
seeing the relationship in previous time steps to current is non linear we hope to explore a few additional approaches including convolution and possibly gated recurrent networks with attention 
we also note that our model may not be general training with only 6 homes in a single city and thus hope to train our systems with a larger and more representative dataset 
ss and fl contributed equally to all sections of the project 
in particular samir worked on building the sparse coding knn and experimental neural networks while fred worked on building the data processing and visualization random forest multi task ridge regression svm 
both worked on data preprocessing and augmentation as well as visualization 
protein protein interactions are integral to most biological processes 
scientists would like to know the structure of these protein complexes in order to understand the processes they are used for 
unfortunately most protein complexes do not have an experimentally determined structure and these structures will not be solved given the current experimental techniques for structure determination 
therefore it is useful to be able to model the structure of protein complexes using the individual structures of each protein via docking simulation methods 
however numerous orientations can result from computational docking and so being able to predict which of the simulated complexes are the correct orientation would be essential in order to use the modelled structures 
as a result we developed models that are capable of predicting whether the binding orientation of a protein protein docking is correct 
our project can be formulated as both a regression and classification task 
our inputs are protein data bank pdb files that have energies and other information about a simulated protein complex as well as the position of all atoms in the complex 
we also have as labels the root mean squared deviation rmsd of all atom positions in the simulated complex to the experimentally determined structure of each complex 
a smaller rmsd signifies an orientation that is closer to the true binding orientation of the protein complex 
when we frame the problem as a regression task our svm regression model takes features from the pdb file to output a predicted rmsd 
previous work in the field has shown that an rmsd of 0 1 is considered very good 1 2 is good and 2 4 is acceptable thus we threshold our rmsd at 4 for our classification problem 
then our classification model either svm or 3d cnn outputs a binary prediction as to whether a binding orientation is correct 
we build our svm model after studying various related works of research such as pairpred which employs svm methods to predict whether a pair of residues from two different proteins interact with the advent of convolutional neural networks cnns in recent years we have seen no short of remarkable developments for their powerful performance in fields ranging from image recognition to bio structural analysis with constantly newer deeper and better performing cnn architectures 
in 2015 resnet 50 and resnet 101 architectures were introduced for their relative ease to optimize and their higher accuracy gained from considerably increased depth 
we too are inpsired by the multitude of recent endeavours turn to 3d cnn architecture for structure based analysis of protein and other biomolecules 
one such established literature documents enzynet a 2 layer 3d convolutional neural network classifier that predicts the enzyme commission number of enzymes based only on their voxel based spatial structure 
we also refer to established literature such as atomnet
our dataset is a modified version of docking benchmark 5 db5 
we split the 135 complexes we have from db5 into train validation and test sets 
each complex is only in exactly one of the sets with 70 of the complexes in train 20 in validation and 10 in test 
with high class imbalance due to our dataset containing significantly more negative examples incorrect binding orientations than positive we choose to undersample the negative examples 
therefore for each of the complexes we include all positive examples and an equal number of negative examples in the corresponding training validation or test set using an rmsd of 4 as the threshold for correct orientation positive 
ultimately the training set has 80 of the positives and of the used data the validation set has 15 and the test set has 5 
for our svm model we then extract four different features electrostatics van der waals buried surface area and solvation energy from each pdb file and normalize each feature by the mean and standard deviation of the training data for our 3d cnn we process the pdb files into cubic representations of the interface 
we split the interface into cubes of 10 cubic angstroms with 1 cubic angstrom voxels giving a value of 0 no atom at that location or a number representing the atom present 
to get the cube representations we first remove cofactors if present and randomly rotate the complex by multiplying all atoms by a rotation matrix with 3 randomly generated angles first around the x axis then y axis and z axis 
we then find the center of mass of the alpha carbons of all residue pairs that interact between the two proteins 
we then cluster these centers of mass into regions of 10 cubic angstroms and the center of each cluster becomes the center of the cube 
overall for our svm and cnn inputs our training set has 7812 samples and our test set has 1472 samples 
for our baseline approach we use a support vector machine svm which is a supervised learning algorithm used to separate data by calculating the margin between data points the functional margin can be thought of as a measure of confidence in correct hypothesis so we want it to be large in scale 
we use the following optimization problem where the solution is the optimal margin classifier we pass the four features we listed in the previous section all of which serve to describe the favorability of a protein complex interaction as inputs into the svm which then outputs a prediction of rmsd 
support vector machine svm 
we perform regression on rmsd values as well classification using a threshold of 4 0 previous related work has found that with 5 fold cross validation and a radial basis function rbf kernel the optimal c and values could be found using grid search over the following ranges to compromise between training error and margin c from 2 15 to 2 10 and from 2 10 to 2
incrementing by log 2
10
we additionally construct and investigate performance of a 3d cnn model on the cubic data we prepared as training input 
the model is inspired by cutting edge architecture resnet but utilizes 3d convolutions to better process and analyze three dimensional data a homogeneous highly modularized architecture resnext is constructed by repetition of a building block that aggregates a set of transformations with the same topology 
resnext 3d cnn
the model introduces group convolutions in resnext blocks as well as a new dimension called cardinality which is the size of the set of transformations an addition to factors such as dimensions of depth and width 
for a single example in the training set we optimize the weighted binary cross entropy losswhere p y i x denotes the probability that the network assigns to the label i w n p n and w p p n where p and n are the number of correct and incorrect cases of protein complex binding orientations in the training set respectively 
we evaluate the accuracy of predicted rmsd values of protein orientations regression using r squared statistic 
r squared the coefficient of determination is a statistical measure of how close the data is to the fitted regression line while at first we frame the problem as a regression task we later select a rmsd threshold of 4 0 and convert the problem into a classification task 
for evaluating classification performance we utilize roc auc score and f1 score 
roc auc score is the area under the receiver operating characteristic curve from prediction scores which is created by plotting the fraction of true positive rate sensitivity against the fraction of false positive rate fpr at various threshold settings
framing the problem as a regression task we implement an svm baseline 
we experiment with different svm hyperparameters and run a grid search for the optimal model performance 
from preliminary experiments we see that the rbf kernel outperforms alternatives such as sigmoid linear and polynomial 
we then experiment with different c and values for our rbf kernel svm 
c serves as a regularization parameter in svm trading off correct classification of training examples with maximization of the margin of the decision function 
the parameter defines the extent of influence of a single training example also known as the inverse of the radius of influence of samples selected by the model as support vectors 
we present our regression results train scores based on an average of 5 fold cross validation in
test r 2 c 4 32 0 4445 0 171
hyperparameters train
seeing that our regression baseline does not perform well we instead propose the reframing of predicting correctness of protein binding orientations as a classification task in an effort to improve our model performance 
we proceed to run a grid search for the optimal svm parameters c and values using rbf kernel svm 
support vector classification baseline
we present our classification results train scores based on an average of 5 fold cross validation in
train test f1 score lr 0 5e 3 bs 32 0 956 0 929 roc auc lr 0 5e 3 bs 32 0 981 0 954 to qualitatively evaluate our model performance we used pymol which is molecular visualization software to create 3d visualizations of protein dockings 
hyperparameters
we consider the problem of predicting correctness of a protein binding orientation docking 
when formulated as a regression task predicting rmsd values we have poor performance with our svm regression baseline 
thus in order to improve performance we reframe the problem as a classification task which achieved more promising results as expected 
this svm classification baseline seems to be overfitting on the training set so we consider a 3d cnn implementation that accepts 3d cubes of atom position data from the protein interface region 
using the model predictions on each cube we compute a weighted average over all cubes in a respective protein orientation interface and output a final binary value predicting if the protein orientation is correct 
upon empirical analysis our best model is a resnext50 3d cnn which has a f1 score of 0 929 that as expected outperforms our svm results 
we believe this model outperforms the resnext101 model perhaps because the latter tries to learn a complicated function on the data and thus overfits on the training set 
in the future we would also like to evaluate precision percentage of called positives that are true positives and recall percentage of positives that are called positive on the unbalanced dataset given our promising results with resnext 3d cnn in future work we would like to experiment with a multichannel 3d cnn and use more than just the atom type and position data to incorporate other attributes such as atom charge specific atom type such as alpha carbon 
we would also like to experiment with different ensemble techniques and try to outperform our results with more time it would also be fruitful to experiment with different ways of generating cubes to cnn input 
we want to ensure that as much information of the binding interface is captured by the cubic representation 
therefore it could be meaningful to vary how much overlap there is between the cubes 
it could also be valuable to randomly rotate each cube rather than just each orientation in order to help reduce overfitting 
lastly since more data is generally helpful we could potentially oversample positive data so that our original dataset is more balanced by using data augmentation via molecular dynamics which would allow us to avoid undersampling our original dataset
thanks to jo o rodrigues in the levitt lab for the dataset and advice as well as raphael townshend for his guidance on this project 
acknowledgements
all group members contributed to the svm experiments and the writing of the paper 
sarah also extracted features for the svm and processed the data cubes for the cnn while kaylie implemented the base code for model training and deployment 
all group members worked on creating balanced data sets for the cnn and then worked on the 3d cnn architecture implementation to ensure that all project work was evenly spread out 
flow cytometry is a method of single cell analysis where cells are encased in individual microfluidic droplets and run through a series of lasers 
the lasers excite either fluorescent proteins within the cell or antibody probes bound to cell markers and the scattering and intensity of the emitted light is recorded each cell has a distinctive cell signature comprised of how the cell scatters light which represents the size of the cell granularity of the cell surface and overall cell viability and the intensity of different colors of emitted light which represent the relative level of labeled proteins of interest that are present 
filters specific to a range of wavelengths collect light into discrete channels with photomultiplier tubes that convert light intensity into voltage readouts which can be stored by the computer flow cytometry experiments are ubiquitous within the field of cancer immunology since researchers frequently probe the effect of certain stimuli on gene expression levels protein expression levels surface marker expression etc 
of single immune cells since these properties often determine their downstream activity and function 
to analyze flow cytometry data researchers plot single cells on a 1 or 2 dimensional plot histogram dot plot respectively and gate on a specific cell population of interest by manually drawing linear polygonal boundaries 
importantly these populations usually follow a gaussian like distribution 
however these gates are often arbitrarily drawn which can lead to inconsistencies in data analysis if researchers are not fully aware of which populations they are selecting and how they are selecting them 
for example a positive gate that is too stringent might lead to many false negatives while a positive gate that is too inclusive might lead to many false positives 
furthermore while the use of flow cytometry is straightforward for populations of a single cell type researchers are often interested in collecting single cell data from mixtures of cell types in order to investigate cell cell interactions however gating flow data with mixed cells is challenging since unique cell markers must be present in order to differentiate cell types 
in co culture functional killing assays that mix car t cells a specific type of immune cell that is artificially engineered to target and kill tumor cells with tumor cells the levels of these unique markers often change due to cell cell interactions i e 
cells no longer look like the unmixed control populations 
as a result if two cell types are not distinctive enough in their fluorescent readout for a given channel their populations may overlap significantly making traditional gating impossible in summary there is a need for a more reliable gating strategy in flow cytometry data analysis that does not involve manually drawing boundaries in plottable dimensions to isolate populations of interest which is both limited in its effectiveness at separating semi distinct cell types and problematic in its arbitrary nature 
a machine learning approach while not entirely immune to researcher bias provides a more rigorous and consistent definition of gating cell populations of interest 
it follows that an unsupervised learning algorithm that utilizes data across all n channels is appropriate to identify clusters within flow data that represent a single cell type where no reliable ground truth exists previously researchers have trained unsupervised models to identify distinct potentially novel cell subpopulations within image 1 3 and flow 2 3 4 data 
however there remains an unmet need to unmix already known cell types in coculture experiments for single cell analysis after cell cell interactions have already changed their signature from that of the ground truth 
in this project i evaluated the ability of k means clustering and gaussian mixture models in combination with pca to isolate these single cell types and trained a semi supervised em algorithm to do so 
to train the model i utilized an unpublished flow cytometry dataset that i collected in lab from a functional assay with mixed primary human car t cells and k562 leukemia cells target cells 
i recorded 10000 events single cell data points per condition and ran each condition in triplicate 30000 total events per condition 
i partitioned the dataset to have 10000 examples 1 3 replicates for training the unsupervised learning algorithm and tuning hyperparameters and 20000 2 3 replicates for testing 
in addition to the co culture conditions i also took samples of unmixed human primary t cells and tumor cells to serve as controls for the ground truth labels 
all cells were stained using fluorescent antibodies in two colors anti cd19 apc and anti cd8 af405 and the car t cells express mcherry and egfp through a lentiviral vector 
with the forward scatter fsc h height fsc a area and side scatter ssc a area lasers each single cell is represented by a feature vector with seven channels of interest n 7 raw fcs files were converted to csv using an open source script provided by genepattern 5 and the broad institute at mit 
the csv files were then read and manipulated as dataframe objects using the python pandas library 
each row in the design matrix represents a single cell example and each column represents one of seven channel features from the raw input data 
derived features include pca projections of the input for visualization and clustering due to correlation between channels 
unmixed cells were used as the ground truth labeled dataset 
simple data preprocessing included shifting all values by the min and taking the log10 of color channels since the distribution of fluorescence intensity is log10 normal the distribution for scattered light intensity is gaussian like on the linear scale for pca the mean and variance of the dataset were set to 0 and 1 and the eigenvectors used for projection of the unlabeled dataset were also used for the labeled dataset 
in order to train the model to assign cluster labels to each cell i first reduced the dimensionality of the labeled dataset from seven to two dimensions using principal components analysis pca 
to calculate the covariance matrix then the projection of the design matrix to the dimensional pca subspace where is the array of the top eigenvectors 1 2 of sorted by the corresponding eigenvalues 
methods and implementation
following pca i qualitatively evaluated two unsupervised learning algorithms k means clustering and the unsupervised semi supervised em algorithm for gaussian mixture models on the ability to cluster single cell types in comparison to the manual gating strategy implemented by hand 
for k means clustering i initialized centroids at random training examples shifted the centroid to the average of the closest proximal training examples assigned these new centroids as labels to the closest examples to draw linear separation boundaries for clusters and iterated until convergence for the un semi supervised em algorithm i calculated the weights probabilities of each training example belonging to each of gaussian distributions in the e step and updated the parameters in the m step by maximizing the log likelihood with respect to each of the parameters iterating until convergence the supervision term for the labeled dataset is weighted by and assigning examples to the most probable gaussian 
qualitatively unsupervised em outperforms k means clustering in comparison to the manual gates set for t cells and k562 cells this is unsurprising since like hand drawn gates linear boundaries do not separate cell clusters as well as multivariate gaussian fits 
quantitatively the k means algorithm converges much quicker and with little correlation to principal component number or cluster number whereas for unsupervised em smaller pca subspace dimension and greater cluster number seem to slow algorithm convergence though this performance trade off results in improved clustering quality
i proposed a machine learning approach to automated gating of single cell types 
though mixed cells show large deviations from their unmixed counterparts through empirical testing i found that the inclusion of a very small weighted supervised term in the em algorithm successfully improved training performance based on clustering quality and convergence rate 
the clustering results on two test sets were highly variable one converged quickly with impressive clustering while the other was more lackluster most likely a result of pca instability 
performance might be improved by kernelizing reducing dimensionality with tsne or including a skewing factor within the gmm since many of these shifted distributions are skew normal
the goal of this project is to use sequencing data to identify transcription unit initiation and termination sites within a genome to determine which genes are expressed together 
although partially known identifying all transcription units in an organism can help create more accurate models of biologic behavior by better capturing interactions between coexpressed genes leading to higher predictive capacity 
unsupervised and supervised methods were used to identify structure from transcript sequencing data 
supervised learning methods performed better at identifying transcription start and stop sites and avoiding false predictions however they did not generalize well to the test set 
in bacteria many genes are expressed together on the same rna transcript called a transcription unit tu 
identifying all possible tus can play a role in better understanding biology as related genes are typically expressed together 
ultimately these transcripts are translated to proteins which carry out much of the functionality in cells so coexpression can have important physiological implications as these proteins interact with each other 
although many tus 3500 for e coli have been identified and compiled in databases 
the dataset comes from rend sequencing data in e coli from lalanne et al 
some processing of the data is required to better fit assumptions for the algorithms 
for unsupervised learning the first thing that must be done is to segment the reads into regions that contain multiple genes that are separated by other regions by areas of no reads 
the algorithms used will handle small regions of genes better than the entire genome at once 
an example of this segmentation is shown below in the bottom of
the first models tried were unsupervised learning methods 
although some transcription units are already known and well studied using unsupervised methods could reduce the bias of introducing prior knowledge that might be incorrect or fail to capture unstudied transcription units 
unsupervised learning
the first method implemented was density based spatial clustering of applications with noise db scan clustering 
this method was implemented with the scikit learn package
dbscan
this method was selected to provide insight into latent states of the system and implemented with the hmmlearn package 
after the poor performance of the unsupervised methods supervised approaches appeared to be better suited to the problem at hand 
both methods attempt to classify into three classes normal initiation and termination 
initiation and termination classes contain an annotated peak in the read data window 
starting with a simple implementation a multinomial logistic regression algorithm was implemented from scikit learn 
attempting to improve on logistic regression performance a fully connected neural network was implemented using keras 8 with a tensorflow 9 backend 
a sigmoid activation function was used for all of the hidden nodes with a softmax function for the 3 output nodes which correspond to normal initiation and termination classes 
various network architectures were explored with between two and four hidden layers containing 5 30 nodes 
with each hidden node weights and biases are learned through backpropagation with training data 
for each layer the weight matrix is multiplied by the inputs which then get added to the bias terms 
the result is then passed to the activation function to add nonlinearity and the result is passed to the next layer of the network 
the models were trained for 3 epochs as the training accuracy was sometimes low with fewer epochs although validation accuracy over multiple epochs was never checked 
with this dataset there is a large class imbalance between the number of normal cases and the number of initiation or termination locations 
because of this accuracy is not as useful of a metric and the models should balance a tradeoff between recall and precision where initiation and termination classes are grouped as positive results 
optimal parameters were selected based on the highest f 1 score across parameters tested 
equation 2 shows how f 1 relates recall and precision 
performance for each method with these selected parameters is summarized in for dbscan the parameters for the model and min points were varied and model performance was assessed with the validation dataset 
the optimal parameters were 2 and min points 15 
as shown in for hmm the data processing approaches were varied to find the best input 
this included varying the moving average window and including total counts instead of separate 5 and 3 counts 
the optimal value was surprising a moving average window of 1 which results in a poisson distribution instead of gaussian with separate counts input 
for this model there were not as many transitions as expected so exploring different transition probabilities might result in better performance 
for logistic regression several parameters related to data processing were varied 
this included oversampling the minority classes with smote the moving average window for read counts the size of the window of reads the algorithm was trying to classify and the pad which controls the location within the window that the peak needs to appear to be labeled a positive class 
the optimal parameters were no oversampling no moving average a window size of 7 with a pad of 3 so training data was labeled as initiation or termination only when the peak was directly centered in the window 
for the neural network implementation the same parameters were varied as for logistic regression in addition to the network architecture 
across the range of parameters 3 hidden layers with 20 30 and 20 hidden nodes respectively seemed to consistently work the best 
additionally a moving average window of 15 a sample window of 5 a pad of 0 and oversampling for double the samples provided the best f 1 score 
for both supervised learning methods many of the incorrectly identified peaks or falsely identified peaks were visually similar 
most of the poor performance might come from locations that are on the border between classes and could be annotated differently 
potentially incorporating additional biologic information in labeling initiations and terminations could improve performance and the hand annotation might not be as consistent as it needs to be 
although the results on the test data did not meet expectations there is some promise in the performance of supervised methods 
one way to improve performance would likely be adding additional training data 
on top of this additional feature engineering to better encode positional information and the knowledge that locations in the genome will likely be in the same tu as neighboring location could further improve performance 
the class imbalance might also need to be addressed and with additional training data the majority class could be undersampled while still having enough training examples to train all of the parameters in the model 
finally a few other methods could be explored 
a convolutional neural network might improve performance and be able to detect spikes as edges in 1d much like cnns can do with images in 2d 
recurrent neural networks also typically offer good performance for sequence data and could be a way to encode the positional information and relation between different locations 
all code and data is available on github www github com tahorst cs229 project
source code
genome wide methylation was first associated with future lymphoma by georgiadis et al 
in 2017 who found that epigenetic changes are already present in blood samples 2 1 to 15 9 years prior to diagnosis dna methylation is an epigenetic process whereby methyl groups are added to dna molecules without changing the dna sequence itself typically acting to suppress gene expression 
measures of fractional components of immune cells are derived from gene expression 
together these biomarkers provide insight into the differential expression of genes and the pathogenesis of lymphoma and were used as parallel inputs to our problem as on date it is not possible for medical experts to look at the data and predict the likelihood of a person having lymphoma in future 
as a result the bayes limit is currently unknown for this problem the input to the algorithm includes 1 the dna methylation levels floats across different genomic probes and 2 fractional components of immune cells floats representing the fraction of each component 
we use unsupervised feature reduction along with several supervised learning techniques logistic regression svms gda neural networks and random forests to output binary predictions of future lymphoma 
georgiadis et al 
tried to perform pathway analysis to identify the relevant genes and underlying biology pertaining to lymphoma feature selection is one of the main objectives in genomic data used for disease classification 
in fact the number of genes needed for discriminant analysis in disease classification is likely much lower than 50
since recall is an important parameter to understand the effectiveness of model based on discussions with our mentor in the department of bioinformatics we selected f1 score as the optimization metric for the classification model 
obtaining an accuracy of 50 is a satisfying baseline metric for the model 
iii objective
the data used in this study was obtained from stan 
the small number of blood samples large number of features in the dna methylation dataset and inherent biological noise present a set of challenges common across genomic applications of machine learning 
moreover dna methylation levels are correlated across gene probes 
b feature selection techniques
it is therefore desirable to capture the essence of the dna methylation dataset in a smaller feature space prior to applying supervised learning techniques 
two feature reduction techniques are used in parallel methylmix data provided and pca 
we then applied the following supervised learning models to the three datasets logistic regression gda svms neural networks and random forests application of the methylmix algorithm on the dna methylation dataset reduced the number of features from nearly half a million to 101 
the objective of the methylmix algorithm is identification of disease specific hyper hypo methylated genes given the success of pca as compared to methylmix we decided to see if the combination of the two would yield better results to visualize the data better and understand if the number of parameters can be reduced 
we applied pca to the dataset output from the methylmix algorithm retaining 95 variance with 49 principal components 
however drawing a decision boundary is not possible using the first two components of methylmix data 
further separability associated with methylmix data for the first 3 principal components was lower as compared to separability associated with dna methylation data 
logistic regression model works well as a baseline 
it is easy to implement and usually gives good insights 
a logistic regression
since we began with limited information about the distribution we felt the application of a logistic regression model was a good way to get started quickly and iterate upon 
this decision was further supported by the performance of logistic regression in conjunction with parameter reduction in cancer detection problems with similar data l2 regularization and ensembling techniques were used to address the relatively small size of the dataset and overfitting when training in a high dimensional feature space 
ensembling techniques used include traditional bagging as well as a more novel and linear approach in which we simply average the model weights for k fold training samples 
gaussian discriminant analysis is a generative learning model that models the probability of the data given the labels as opposed to modeling the probability of the labels given the data 
gda thus models attributes of the biomarkers for the disease versus healthy state and uses this model to predict future lymphoma given an unlabeled sample 
b gaussian discriminant analysis
gda models usually perform better if the distribution is gaussian in nature and if the number of examples is low 
since the number of examples is low in our case and since we did not know anything about the distribution we decided to apply gda 
our mentor has suggested the data likely has a bimodal distribution and that it is likely that non gaussian statistical models will perform better on any dna methylation data the model s parameters have been learned using maximum likelihood estimation mle 
using the mle parameters predictions are made on the test set to test the effectiveness of the model 
visualization of data plots using principal components gaussian and high dimensional polynomial kernels faced overfitting problems 
we then tried a linear kernel as in georgiadis et al which performed best 
c support vector machines
the performance was improved after tuning regularization and gamma margin parameters using validation data 
after learning the parameters the model was tested on test set 
since the logistic regression model gave good results we felt neural networks might extract deeper information and give even better results 
the number of parameters corresponding to neural networks are higher than logistic regression model and the number of samples available was limited so we decided to compensate by using fewer principal components 
d neural networks
sigmoid activation layer was used for the final layer relu and sigma activation functions were tried for hidden layers 
weighted binary cross entropy loss function shown in equation 4 was implemented with adam optimizer mini batch gradient descent using tensorflow and keras frameworks increasing the number of layers improved the training accuracy but ran into over fitting problems the model architecture was tuned to reduce variance 
further to combat the overfitting problem different regularization techniques such as l2 regularization for kernels early stopping learning rate decay and drop out were used 
the hyperparameters tuned include the weight in weighted cross entropy loss function threshold learning rate learning rate decay number of epochs activation function for hidden layers 
decision trees are another useful model for non linear decision boundaries 
however decision trees are high variance models prone to overfitting as can be imagined by a tree where there is a distinct leaf for each training example 
e random forests
random forests are useful techniques for bagging decision trees by training a bootstrapped sample on each tree and averaging these models 
we applied random forest models with gini loss function shown in equation 5 which similar to the cross entropy loss function for decision trees can help to maximize the information gain from one level of the tree to the next similar to our neural network model given the high variance nature of decision trees and the limited number of samples available we trained the model using fewer principal components as compared to other models the hyper parameters tuned include minimum leaf size and maximum features considered in order to reduce overfitting that was quite apparent 
application of adaboost improved the performance of the model by weighting misclassified examples during training in attempts of creating a stronger learner out of a set of weaker learners 
the best combination of model and dataset for lymphoma detection was logistic regression with l2 regularization on the dna methylation pca dataset with 59 principal components 55 variance achieving an f1 score of 72 with 69 accuracy 
a best predictor combination
all the classification models performed better on the dna methylation dataset where feature selection has been done using pca as compared to the dataset where feature selection has been done using methylmix algorithm 
b pca outperforms methylmix algorithm
gda works better if the input data is gaussian in nature 
gdas performance was improved when the input data corresponding to immune cells fractions database was transformed using box cox transforms
c immune cells fractions transform induces normality
overfitting was a common problem that most models faced in the methylation datasets 
ensembling techniques reduced the variance pertaining to the logistic regression model 
d bias variance analysis
the variance problem was bigger in models involving neural networks and random forests several regularization techniques were implemented to reduce this variance at the cost of reduced accuracy 
regularized logistic regression most likely finds the right balance in bias variance tradeoff and hence performed the best for this data set 
a larger number of samples and optimized feature selection technique may help to overcome this variance in the future 
logistic regression model gave the best results after the number of features has been reduced using pca techniques 
while neural networks could capture the information better and perform better on training datasets they ran into overfitting problems and several regularization techniques have been implemented to address this problem 
vii conclusion future work
similarly random forests performed nearly as well as logistic regression models but faced the overfitting problem 
logistic regression model likely performed better than gda on the methylation dataset because of the fact that methylation data does not follow a gaussian distribution if it is not transformed 
however gda performed almost as well logistic regression on the immune cells dataset which is smaller and may have an underlying gaussian distribution which can be revealed by de noising and power transformations future work should explore application of model and dataset ensembling techniques as they might reduce the variance and help in obtaining better results 
application of similar models and ensembling techniques on an available microrna expression dataset can also probably help to attain better results and obtain new insights as this is a related biomarker 
when the number of samples in the dataset grows over a period of time classification of lymphoma subtypes using a softmax algorithm will be an interesting problem to tackle 
ultimately we would like to map the principal components that were important predictors back to the corresponding genes for biological experts to understand the underpinnings of this disease 
large datasets of microscopy images of individual proteins are currently available but we lack an efficient and accurate approach to systematically classify the compartmentalization of each protein by now 
we are addressing this question by combining our biological expertise for feature selection with multiple neural network models 
we are able to successfully extract feature scoring matrices for the whole image dataset and achieve decently accurate predictions with our optimized resnet34 models 
proteins are key functional units in the human cell and correct localization of each protein is critical for their proper functions that together enable life 
hence a precise and systematic map of protein localization is highly desired 
thanks to advances in high throughput imaging biologists have generated massive yet continuously growing databases of microscopy images 
these images are generated at a far greater pace than what can be manually evaluated hence the need is greater than ever for automating cellular image analysis to accelerate our understanding of protein function interactions as well as their roles in human diseases in this project we are focusing on a kaggle competition human protein atlas image classification 1 
specifically the inputs of our algorithm are annotated microscopy images and we would like to generate a model to accurately predict protein localizations represented as integer labels based on the images 
to accomplish our goal we first use computer vision approaches to extract images feature selected based on biological expertise and then use multiple convolutional neural networks combined with our image feature scoring matrix to predict the localization of each protein 
previous attempts to tackle this problem have included numerous methods such as k nn classifiers support vector machines svm neural networks and decision trees 
more recently similar questions have been successfully addressed using deep convolutional neural networks cnns for singlelocalizing proteins in budding yeasts as concluded from the previous works convolutional neural networks have been given the most promising outcomes but are still relatively lower in accuracy when introducing the severe class imbalance and multi localizing proteins 
interestingly most previous studies were using either multi channel images or extracted features solely as the input for the algorithms
the dataset we are using is provided by kagglehuman protein atlas image classification 2 
in this dataset each sample is a microscope image of a type of cells that contain a certain type of protein 
the image is processed with 4 different filters resulting in 4 individual images the protein of interest green and three cellular landmarks nucleus blue microtubules red and endoplasmic reticulum yellow 
a visualization of our input data is shown in there are 31072 samples in the dataset for training and we split 80 of it as train set and 20 of it as validation set 
we also use affine and flip transforms to augment our data 
there are 11702 samples in test dataset and we can submit our predictions on it to kaggle to get a leaderboard score which is macro f1 
most of previous classification approaches were purely machine learning based hence losing the vast majority of the rich biological information 
as an optimization we utilize our expertise in cell biology and select 10 most crucial features in terms of the relative localization to different organelles overall properties 
feature extraction is implemented with opencv in python cv2 module 
features extraction procedure
we denoise the protein graph and make a mask for the nucleus component 
by laying out the nucleus component on our protein graph we could get great insights about whether this protein is in or out of the nucleus like shown in
overall properties
first we need to segment the blobs in green image to do so we first denoise and gaussian blur the image and computes the connected components 
contours are drawn around the connected component and are fit into ellipses 
information about individual protein segments
we use resnet34 based on residual network for image recognition 2 as our baseline and then add the calculated features to resnet34 as our own model because this is a multi label classification problem we decide whether an image belongs to each class independently and we use binary cross entropy as our objective function for each of the 28 classes 
residual network 2 is built on residual block which adds an identity mapping to outputs of original stacked layers 
when the optimal solution is closer to identity mapping than to zero mapping residual network could efficiently find optimal solution since it doesn t need to learn the identity function as a new one another method we combined with basic resnet model is to make the learning rate decay periodically 
residual network
this cyclical learning rate schedule method is proposed by leslie n smith 5 
the learning rate is calculated as cycle f loor 1 epochs cycle length when the learning rate is very low it takes a long time for the algorithm to converge 
but when the learning rate is too high the algorithm can also be hard to converge because it may miss the minimum point with large step 
therefore we make the learning rate increase and decrease in some specific period as shown in therefore residual network could help us learn more features about the image especially when the image has complicated features that are hard to recognize even for human eyes like the noisy protein images in our dataset 
we treat the last fully connected layer output from residual network as a graph representation of each image sample 
network architecture with features
after calculate our own data features we normalize all the features so that each of them 3 
our network architecture is shown in
we implemented our models in python and trained them on shared stanford wide gpu machines via sherlock cluster 
since this is a multi label classification task and each sample can belong to more than one class we decided to use the macro average of f1 score as our primary metrics to evaluate our models 
this metrics is in accordance with the score on kaggle leader board for each class let tp fp be the number of true false positive result and tn fn be the number of true false negative result then we have precision p tp tp f p recall r tp tp f n f1 score f 2 p r p r
metrics macro f1
classes represents the performance of our model 
this evaluation score ranges from 0 to 1 where 1 means the best prediction performance and 0 means the worst 
then the macro average of f scores of all classes 
we first trained both resnet50 and resnet34 for 20 epochs but we found that resnet50 would be badly overfitting as its validation set loss went as high as above 100 while its train set loss was continuously decreasing 
hence we continued with resnet34 
resnet34 resnet50
during this first experiment we found that 20 epochs are not enough for the model to converge so we switched to 40 epochs later 
we explored the distributions of train and test datasets provided by kaggle and found they are very different 
so we decided to use different threshold probabilities for each class when identify labels for the test dataset images according to its statistical information 
threshold
two different ways to initialize the weight for the 4th channel in the first conv layer a initialize it with all zeros b initialize it by copying the imagenet pretrained weights for the third channel 
with the second initialization our model reached a higher f1 score 
weight for yellow channel 4th layer 
this is because we are examine cells and pre trained first conv layer from imagenet are tuned to extract simple shapes which fits our task 
we tried to resolve overfitting issue by adding an additional 50 dropout layer before the last fully connected layer 
this did successfully resolve the validation train loss gap at the 40th epoch but we found that this new model did not completely converge at the 40th epoch and thus the kaggle score is slightly lower 
additional dropout
unfortunately we did not have time to train it with more epochs 
we tried tta to improve our f1 score on test set which does not change the model itself 
however we found that tta was not very helpful on the test set for this task 
test time augmentation tta 
in the meantime when we experimented on resnet we were also extracting our own selected features and comparing two resnet models with different test threshold and additional dropout without with our own features concatenated during our biological analysis on features we also found that the endoplasmic reticulum yellow channel is not very important in localizing proteins and it has many overlapping information with the red channel 
so we discarded this channel to exchange for more memory and computing power such that we can pass in the original 512x512 images instead of resizing them into 224x224 and to use additional dropout layer for controlling overfitting and use an additional fc layer for feature concatenation 
concatenate our features
we have developed several different deep learning models to solve the task of human protein compartmentalization 
we first experimented on pure resnet models and found that using different thresholds for test and train datasets and initializing weights for additional input channel with imagenet pre trained weights can significantly boost the prediction performance 
the highest macro f1 score we have achieved on test dataset is 0 403 
additional dropout layer can help mitigate overfitting but whether it can improve the final performance needs further experiment in the future the second part of our study is to extract our own features according to biological knowledge and concatenate them into resnet 
our feature scoring matrix however needs more fine tuning in order to boost the effect and future work is discussed in section 6 3 1 
kuangcong liu pre process data features 1 6 and design models for combining the resnet representation and our own features 
implement resnet34 model with our own features 
analysis on models zijian zhang design models for the pre processing process select the proper features for the scoring matrix and implement the training process of the resnet neural network and analysis wen zhou segment individual proteins and extract features 7 10 
implement the 4 channel resnet models without our own features as well as the train test procedure 
analyze on different models performance 
while traditional text to speech synthesis research usually separates text to speech process into multiple steps such as encoder decoder and wave synthesizer this process now could be constructed together to perform an end to end synthesis model 
in our project we applied word phoneme mapping signal filter and machine learning techniques support vector regression svr simple neural network and seq 2 seq with attention model to transform text to speech 
as a result our synthesis system could successfully generate a wav file by inputting a single text 
moreover the seq 2 seq model gained the highest mos mean opinion score of 2 5 which is determined by a group of listeners higher than other two baseline model 
audio signal processing and speech synthesis is a complicated process which includes text normalization tool encoder decoder and wave synthesis 
one of the most useful applications is generating speech from text 
with rapid development of deep learning researchers invent many end to end algorithms for real life problems which leads more innovative methods in solving speech synthesis problem 
there are several developed models which focus on speech synthesis tacotron from google
related work there were some past research in end to end text to speech synthesis 
ii 
char2wav project from montreal institute of learning algorithms proposed a new end to end model which included two components a reader and a neural vocoder 
the reader is composed of a bidirectional recurrent neural network rnn encoder and a recurrent neural network with attention decoder 
a char2wav
neural vocoder is a sample rnn to generate waveform file 
the model learns to generate speech from text and phoneme 
tacotron takes characters as input and raw spectrogram as output without knowing features as well as phoneme level alignment 
its model is consisted of encoder based on chbg module an rnn based attention decoder and a post processing module 
b tacotron from google
they proposed a chbg module consisted of 1d convolutional layer highway network and bidirectional gru to extract 
after learning the output goes into griffin lim synthesizer to generate waveform 
deep voice 3 project from baidu presented an innovative a fully convolutional architecture that includes encoder decoder with attention block and converter to transform text to speech 
the mechanism of the architecture firstly interprets textual feature into vocoder parameters and then input those features into different neural vocoders wavenet world griffin lim for waveform synthesis 
c deep voice from baidu
we used lj speech a public domain speech dataset consisting of 13 100 short audio clips of a single speaker reading passages from 7 non fiction books as our raw dataset 
before and after utilizing our machine learning models we did several steps of preprocessing and post processing of the dataset to optimize the learning process 
iii preprocess and post process of datasets
our input text raw data are sequences of words 
using the cmu dictionary input words are randomly mapping to either phonemes or alphabet to increase naturalness in generating speech which turns the input data to a 2d numpy array 
a input text
first dimension of the array is number of input texts and second dimension of the array is the number mapped by dictionary represents either phoneme or alphabet 
input audio files are treated as testing labels to train our model 
to digitize audio waveforms the audio file will first pass a fir filter which is an efficient tool to derive discrete frequency response of the audio 
b input audio
this process also help smooth noise in wav file 
then the output of the fir filter will secondly be passed into short time fourier transform stft structure so that the sinusoidal frequency and phase content of local sections of a signal as it changes over time can be determined after two filters the training label turns into a 3d numpy array with first dimension of the array is number of inputs second dimension of time steps which is depending on the time duration of the audio and third dimension of the amplitude of spectrogram with a fixed length of 1025 
due to the inconsistent lengths across 200 sentences we padded zeros to increase the length of all 200 sentences to length of the longest sentence 
with the same process we padded matrices of shorter wav files with rows of zeros so that the entire set of output arrays share the same dimension 
figure 1 spectrogram before after fir c zero paddling
however due to the randomness of phoneme mapping our program could not save the numpy in advance which means the whole preprocess of input text is necessary for each training 
after the system generates output spectrogram matrix from prediction we utilize inverse stft and inverse fir filter to transfer spectrogram back to audio and save the wave into byte with the method provided in scipy library 
d postprocess of output
we built svr model and simple neural network model as two baseline models as well as a complex seq 2 seq model with attention as advanced one 
the first method we used is a support vector regression svr models firstly the program generates a svr for each timestep so the total number of svr in our model equals to the number of time step after we preprocess data 
in each time step our training data requires the model to map multi input to multi output so we have a support vector multi regressor for training each input text matrix and an array of spectrogram output length 1025 
a svr model
for each time step the program takes the corresponding svr model and the input data to make prediction and then concatenate prediction together as final output 
in our experiments we tried a linear kernel and a polynomial kernel utilized in many natural language processing models for our svm models 
one problem for svr model is that the training time is too long since the model works with many separate models and a number of multi regressors which takes a long time to run 
to improve the problem we attempted another variation of svr model using incremental learning rather than retraining model every time with great amount of spectrogram data 
figure 2 svr model
the algorithm we chose from sklearn tool was online passive aggressive algorithm
a simple neural network is a model that consists layers of neuron and connections among them with weights and biases 
it consists three sections input layer hidden layer s and output layer 
b simple neural network model figure 3 simple neural network model
hidden layers are human determined shapes that can analyze the input data in different perspective and after all the hidden layers the output layer uses an activation function to squish hidden results to a limited range of values typically 0 to 1 
in our model we used sigmoid function as our activation function in this particular problem we reshaped the training labels to 1d arrays in order to match the neural network structure and after the model was validated we reshaped the prediction vectors back to matrices for the post processing process 
the input layer is the word embedding vector encoded from original sentences and the hidden layer is fully connected with same number of neurons as the input layer 
the output layer is a stretched 1d vector of the spectrogram 
we set mini batch to be 10 and trained 100 samples each time 
the third method we used is a seq2seq model with one embedding layer and one lstm long short term memory layer latent dimension is 512 as encoder and one lstm layer as decoder 
input data was first put into embedding layer to align the dimension with output wave matrix 
c seq 2 seq model with attention
and then output of embedding layer was feed into lstm encoder model 
the hidden state and cell state of lstm model were shared with decoder model 
lstm is a type of recurrent neural network rnn contains memory cell input gate output gate and forget gate 
when the input vector goes into the lstm cell it will be temporarily stored in the gates so that when future inputs come in the model can adjust the output by those temporarily stored values to prevent gradient vanishing or exploding with attention in lstm for text translation it searches all the information from the text and calculate its attention weight to determine the relevance of each word 
figure 4 lstm model diagram
in each step encoder ltsm keeps its output of the input sequence and train the input with attention 
thus the items in output depend on the corresponding items in input sequence with attention the attention information is stored in attention vector as shown below 
during the training process we train the model with two inputs text input and audio input the output is audio wave one time step later than audio input 
in the validation process the encoder model was feed with entire sentence 
figure 5 seq2seq with attention diagram
then internal states of lstm were shared with decoder lstm 
decoder lstm used the first time step of wave matrix to generate the whole wave matrix of the sentence 
the wave file generated from our svr model mainly consisted of disjoint words so that it does not sound like consistent human speech 
the wave file generated from this simple neural network does not sounds like consistent speech 
it was just a random combination of phonemes and words 
b simple neural network
although through the training process the loss function can be minimized down to the magnitude of 10e 4 when comes to the dev test data the result sounds not quite reasonable 
the wave file generated from this model sounds like human speech 
however inside the wave file several words were repeated many times which cannot be counted a complete sentence 
c seq 2 seq with attention
even though after training the loss function can be minimized down to the magnitude of 10e 3 when comes to the dev test data the result sounds not quite reasonable 
in our experiment we trained the model with up to 200 input data 
to do prediction the system firstly read a single input text and preprocess data 
figure 6 training loss after 250 iterations
our training data for the experiment is from lj speech database and our test text is an input string 
after the postprocess procedure of the model prediction the system stores the generated bytes in a wav file 
the training and test accuracy were not included because this regression problem involves timestep operation and comparison of each number in the matrix was not that meaningful 
then we asked a group of people to listen the wav file and to determine the quality score of our result based on consistency and naturalness 
the mean score standard listed in
according to the result of experiments all three training methods did not work well 
we concluded that this may because of the following reasons 
our simple neural network model was only a 4 layer structure and our seq 2 seq was 7 layer 
compared with tacotron a ten layer based rnn our models are not complicated enough to train our text input numpy with spectrogram numpy sufficiently because end to end model did not include any features of audio 
a complexity of problem
in our problem the output dimension is way higher than the input dimension so that with the limited knowledge we got from our input text we need to predict a matrix containing much more information that we cannot see from the input 
due to this fact it is kind of hard for us to choose an appropriate model and complete the end to end process using simple machine learning algorithms 
another problem is the training size for our models our training was up to 200 sentences 
for our system the preprocess part demanded a great amount of memory and training parts took long time since the model attempted to map each input numpy with a huge 2d output numpy 
b computation constraints
this raises up the issue that since we need to sweep all the data we loaded and find the longest sentence to complete zero padding those zeros are not efficient data and may take up too much memory in our dataset 
however if we sweep the entire lj speech dataset and use the same length every time it will contradict with the cmu dictionary mapping because we randomly mapped words to either phonemes or alphabets which turns out that each time the input encoded may not be same and that randomness will definitely improve our prediction quality 
thus 200 sentences were not enough for this system to learn and to fit randomness 
the size of training data for other existed models were much bigger than ours 
for instance tacotron uses 10 000 sentences in their training 
therefore all those factors were the possible error source of our training models 
the above results have proven that models like nn svm simple seq2seq don t work quite well for such complex problem 
conclusively human speech synthesis is an interesting but difficult task and we definitely need more complicated model for this kind of problem 
for future work developing a deeper model is the preliminary improvement 
another possible enhancement is ensembling different models 
moreover as discussed above larger size of training is also necessary for a more efficient speech synthesis system 
recent research has increasingly demonstrated the ubiquity and functional importance of intrinsically disordered proteins idps 
characterized by fluctuations through large conformational space idps engage in dynamic protein protein interactions ppis that have not been well understood through current structure based analyses 
we build on previous work on idp ppi prediction solely using sequence information and analyze the performance of various machine learning algorithms 
we achieve top performance on a previously published idp ppi dataset by using new featurization and data augmentation techniques 
however the results are difficult to interpret in terms of concrete protein pair characteristics that are favorable for interactions and more work still needs to be done towards improved feature considerations 
over the last two decades many algorithms have been developed to predict regions of disorder where there is no stable secondary or tertiary structure within protein sequences 1 2 3 4 
however less is known about how these disordered regions interact with other proteins 
such research is important for several reasons 1 a recent estimate 5 suggests that over a third of human proteins are intrinsically disordered and 2 these intrinsically disordered proteins idps have widespread roles in cellular processes such as cell signaling and regulation
many protein protein interaction prediction programs have been developed in the past utilizing various heuristic methods as well as standard machine learning algorithms such as support vector machines and random forest rf algorithms 
the labeled dataset was borrowed from perovic et al consisting of 90253 unique proteinprotein pairs where at least one protein was considered intrinsically disordered by the disprot protein disorder database 
each protein protein interaction pair was featurized by concatenating the feature vectors of its constituent proteins 
individual proteins were featurized based on techniques used by perovic et al 
and additional methods available in the protr 14 r package 
these features can be broadly classified into length independent features and length dependent features 
the length independent features describe compositional distributions such as included amino acid dipeptide and transition frequencies 
the length dependent features describe distribution of amino acid properties along the sequence including amphiphilic pseudo amino acid composition paac descriptors and several auto correlative measures 
in total this yields a 2449 dimensional vector for each protein thus a single protein protein pair is represented as a 4898 dimensional vector 
note that the dataset was also readily augmented since whether two proteins interact should not depend on the order of the proteins both orderings of concatenation of the individual protein feature vectors were included 
therefore the fully featurized augmented dataset was a 176548 samples by 4898 features matrix all data were normalized as z scores 0 mean 1 variance then visualized through pca plots to understand how well featurization separated the binary labelled data 
to reduce the feature complexity only the top 446 principal components corresponding to singular values 1 were retained 
finally the dataset was renormalized as z scores and split 60 20 20 into training validation and test sets 
the pca plots the non linear models tested included random forest rf classifiers gaussian or radial basis function kernel svms and neural networks 
rfs are ensemble classifiers that average a large number of relatively high variance decision trees here 10 100 each trained over a bootstrapped sample of the original data and a subset of the features here the top features 
enforcing maximum depth here 5 50 or unconstrained and minimum leaf size here 5 constraints further reduces variance 
the gaussian kernel exp 
interestingly the linear models consistently demonstrated less variance overfitting than the nonlinear models with the exception of the gaussian kernel svms which failed to converge within 50 iterations 
the maximum number of iterations was imposed due to time constraints and the quadratic time complexity of the algorithm used 
the results were not surprising given that the pca plots failed to show strong evidence of linear decision boundaries therefore suggesting an advantage for non linear models 
furthermore perovic et al 
had recorded their top performance with an rf model as well 
however the auroc achieved here 0 8268 was higher than that reported in their paper 0 745 which is likely due to data augmentation described previously unfortunately interpreting the performance result of the top rf model is difficult 
this is in part due to the ensemble nature of the rf model and even more so due to the pca dimensionality reduction step prior to training 
it is therefore almost impossible to concretely explain what protein pair characteristics are favorable for interactions versus non interactions 
the moderate prediction accuracies achieved through this project demonstrate large potential for improvement 
there are several simple extensions of the current project that deserve more attention 
first more or all of the original features could be considered rather than the dimensionality reduced set of 446 features along the principal components which was enforced largely due to time and computational constraints 
in addition more advanced neural networks may also be capable in identifying better nonlinear decision boundaries and by using more granular software packages like tensorflow or pytorch it would be possible to output loss gradients with respect to individual features to produce saliency maps thereby allowing improved feature analysis 
finally finer hyperparameter tuning would almost certainly yield better predictions on the training and validation sets 
many of the non linear models demonstrated significant overfitting which could be curbed through different regularization techniques stricter larger minimum leaf sizes for the random forest classifiers early stopping criteria and larger l2 regularization penalties alpha for the neural networks different data sources could also be incorporated 
the human proteome consists of over 20 000 proteins 
given that a third of them are predicted to be disordered and disordered proteins participate in an average of over 100 ppis each there are a lot more ppis that can be studied and predicted 
proteomes from other species especially well studied model species like mice yeast and fruit flies can contribute even more data 
many different databases have been set up to capture data produced by protein protein interaction experiments and computational analyses 
biogrid for example currently contains 353 521 human ppis finally new featurization strategies appear to be crucial to improving prediction 
current techniques of featurizing individual proteins and then concatenating their feature vectors as a representation of a potential protein protein interaction have been unsuccessful in producing visual separation of the binary data 
some possible future considerations include incorporating co evolution information and energy models 
specifically some disordered domains are known to stabilize upon interactions with other proteins 18 such information can be matched with predicted or known protein surface geometries to improve predictions 
even more recently a paper from september this year explores embeddings of protein complexes derived from ppi networks 
the 20 naturally occurring amino acids are the building blocks of all proteins 
these residues confer distinct physicochemical properties to proteins based on features like atomic composition size and charge 
here inspired by recent work in nlp we create vector embeddings of each amino acid based on their contexts of neighboring residues within folded proteins 
we then test the utility of these embeddings in a data scarce supervised task classifying amino acid mutations as neutral or destabilizing to t4 lysozyme 
one of the greatest open challenges in medicine is to accurately predict a protein s function or dysfunction from the primary sequence of amino acids that comprise it 
the ubiquity of full genome sequencing has made this challenge all the more salient as we now possess an abundance of genomic and therefore protein sequence data but are unable to experimentally assess how the thousands of mutations that are routinely uncovered manifest as changes to a protein s function 
we lack this ability because a experimental characterization of a single protein is costly and time consuming and b the space of protein sequences is unfathomably vast 
together these two factors conspire to yield a sparsely sampled sequence function landscape the high cost of experiments to determine protein function from structure makes predictive algorithms an attractive alternative 
while supervised deep learning algorithms perform well over complex high dimensional landscapes they also rely on ample labeled training examples 
until recently the field of natural language processing nlp was stymied by the same quandary computational biology now facesa preponderance of unlabeled sequences with little way to generalize across small sets of labeled data 
a breakthrough insight in nlp was that powerful models could be trained on smaller datasets providing their inputs words were featurized such that words with similar meanings had similar vector representations 
to capture the meaning of a word the idea of distributional semantics that you shall know a word by the company it keeps played a critical role 
leveraging the fact that words with similar meanings tend to be observed in similar contexts e g 
sentences documents several models have been created to learn meaningful word embeddings from unlabeled text most notably the watershed word2vec model taking lessons of nlp into account the obvious analogy for computational biology is to learn vector representations of amino acids that distill biochemical meaning from their context in proteins 
here we adapt the word2vec algorithm to accept a 3d context bubble of neighboring amino acids surrounding a target amino acid as input and predict the target amino acid identity as output 
the process of training this model which we call res2vec or r2v generates vector representations for each of the 20 naturally occurring amino acid residues the utility of which we validate on a protein mutation effect prediction task 
unsupervised methods to generate vector embeddings of words such as word2vec 1 and glove 2 have underpinned rapid advancement in nlp 
at their core these methods leverage the fact that words with similar semantic meanings tend to appear in similar contexts e g 
sentences documents 
the analogous problem in the fields of biochemistry and structural biology is to encode the biochemical meaning of an amino acid based on the contexts e g 
domains proteins in which it is frequently situated 
prior attempts have used this principle to create amino acid embeddings 3 based on abundant 1d primary protein sequences 
while these approaches yield useful embeddings which improve performance on supervised tasks relative to one hot amino acid encodings they disregard information about the 3d structural context of each residue 
the ability to predict a protein s 3d structure or foldfrom its primary sequence of amino acids has been a longstanding challenge in the field of computational biology 
to provide an unbiased benchmark for computational models addressing this challenge the critical assessment of protein structure prediction casp competition provides biennial releases of unpublished protein structures to guarantee that computational models can be evaluated against truly unseen data 4 5 
these releases have been further curated and extended by the alquraishi laboratory to create standardized rationally chosen training validation test datasets for problems relating to protein structure prediction 
in combination these resources amount to an equivalent of the imagenet dataset appropriately named proteinnet for protein structural data https github com aqlaboratory proteinnet 
to create our embeddings we employed the proteinnet datasets with the predefined training validation test splits as both sequence and 3d structural information are available for each protein in the database 
the positions for each atom residue in each pdb file a type of protein structure file are given in 3d space using a cartesian coordinate system 
however there exists no universal standard for the orientation of protein structures within the coordinate system of these files 
we therefore had to develop a reoriented coordinate system within which each context window could be examined while maintaining a consistent notion of orientation 
the 3d spatial arrangement of a specific amino acid s neighbors defines the physicochemical environment in which the target amino acid resides 
therefore not only distance but also the orientation of the neighboring residues is important to understand a target residue s context 
a target centric coordinate system
to provide an embedding model with information about the 3d context we devised a change of basis system such that the carbon of the target amino acid is at the origin with the three basis vectors positioned with respect to the nitrogen and carbonyl carbon of the target residue 
notation example matrix m vector m scalar dimension m index m assume a set of s one hot encoded symbols s 0 1 s s
e g 
words amino acids 
res2vec
the word2vec algorithm has two flavors that generate dense vector representations of a symbol skip gram sg and continuous bag of words cbow 
given a corpus of sequences e g 
word2vec architecture choice
database of text documents or proteins sequences either model will read through the sequences at each step considering a context window of symbols c 0 1 c s around a target symbol t however sg and cbow have different reciprocal tasks sg takes t as input and tries to predict c as output cbow takes c as input and tries to predict t as output 
canonically sg performs better on infrequent symbols while cbow is faster to train 1 
both models can generate useful embeddings but the cbow training task predicting amino acid probability from context is likely to capture more contextual information pertinent to the task of mutation effect prediction so we selected it for further development 
cbow must first summarize the c one hot vectors in c to create an input vector x r s 
traditionally cbow accomplishes this through simple averaging this approach removes any information concerning the position relative or absolute of the context symbols 
generation of weighted context vector x
while this may be acceptable for a small window over a 1d sequence we desired to provide our model with some notion of 3d position of the context residues 
we first decided to calculate the inner product of the xyz coordinates of each context residue s alpha carbon d r c 3 and a 3d vector of trainable parameters w 0 r 3 then pass the result through a sigmoid activation 
we reasoned that the resulting vector r r c should correspond to a learned pseudo distance which is then used to perform a weighted average of the context vectors to arrive at x we compared this sigmoid weighting to an alternative weighting scheme via 2 fully connected neural network layers parameterized by weights w 0 r 3 3 and w where is the relu function finally we also implemented an inverse distance weighting scheme where x was calculated aswhere d is the euclidean distance to the carbon of the context residue 
during training time we compared the performance of all four of the above weighting schemes and ultimately selected the fully connected neural network weighting 
given an input vector x the cbow model is similar to a softmax classifier consisting of a linear hidden layer followed by a softmax layer it first multiplies x by weight matrix w where 
cbow model
to selected a suitable model to generate the vector embeddings we performed sweeps across the hidden layer size and learning rate hyperparameters of the standard word2vec style model 
we tested hidden layer sizes ranging from 10 up to 1 000 hidden units and learning rate values ranging from 1 to 0 0001 
we found little difference in performance as measured by validation set loss when varying the hidden layer size 
to make direct comparisons between our embedding vectors and standard one hot encoding and blosum 6 empirical substitution vectors we ultimately decided on a hidden layer size of 20 in contrast to hidden layer size we found learning rate to have a large impact on model performance especially relating to convergence 
we found a learning rate of 1 to
the final set of weights before the softmax layer of the model provide the final embedding vectors for each amino acids and can be interpreted as carrying the biochemical meaning of each individual residue 
to gauge whether these vectors were encoding biochemically meaningful information we performed a principal components analysis of the matrix of embeddings 
results discussion
we note that the first two principal components seem to cluster the residues by important biochemical properties
the current performance of the model at roughly 9 accuracy is lower than might be expected 
to address this issue of model accuracy we would change the way that context information is encoded and fed to the model 
primarily we would reconsider the way in which the 10 closest residues are calculated 
currently the distance between residues is calculated as the distance between the carbons of each residue 
however given the great range of sizes for the side chains of the amino acids we would suggest changing the distance calculation to reflect the distance between the two closest atoms of the residue side chains 
this strategy would likely improve implicit encoding of spatial information and may have dramatic effects on contexts crowded with large amino acids residues 
a second change that may improve context representation is encoding of the directionality of the carbon carbon bond 
including this information would provide the model with the general directionality of the amino acid side chain e g 
whether the chain is pointed toward or away from the target residue that may be helpful in predicting the target residue despite the relatively low accuracy of the model the embedding vectors proved significantly better than residue identities alone at predicting mutational effects 
the performance of the embedding vectors matched that of the blosum62 6 matrix indicating that they may be helpful and informative for more intense mutation effect prediction tasks 
specifically given more time we would like to compare the performance of the one hot blosum and r2v encoding on a quantitative continuous effect prediction task 
such a task may be more sensistive to the implicit encoding of the r2v vectors and it might be possible to distinguish performance of the r2v and blosum vectors 
we note that the authors contributed equally to this work 
both authors conceived of and designed the initial implementation of this model 
both authors contributed substantially to the codebase necessary to download process clean and store the data as well as to define train and run the models and apply them 
finally both authors contributed to the drafting and revising of this report and the associated project poster 
breast cancer is the most common type of cancer in the united states the jupyter notebook that was used for this project can be viewed here goo gl rpgxci 
breast cancer is the most common type of cancer in the united states with an estimated 268 670 new cases expected by the national cancer institute in 2018 this score is comprised of six features diameters of primary tumor bed d 1 and d 2 proportion of primary tumor bed that contains invasive carcinoma which is dependent on overall percentage of carcinoma number of axillary lymph nodes containing metastatic carcinoma ln the diameter of largest metastasis in lymph node d met however because electronic medical records emrs tend to be unstructured and have missing data there was not enough information to calculate the rcb score 
in addition pathology reports were written in free text form so it was challenging to get a decently sized dataset even after using regular expressions for rcb features 
introduction and motivation
since there were barriers in creating labels with rcb a patient s response to nac was used instead 
patients were labeled as follows a partial no response meant that there were still some signs of cancer for the patient after nac at the time of their last check up and a complete response meant that the patient had no signs of cancer after nac at the time of their last check up 
these responses were evaluated by looking at a patient s overall ajcc american joint committee on cancer staging status in their emr 
the kaplan meier survival analysis curve was used to demonstrate the claim that patients with complete responses after nac will have a higher chance of survival in comparison to patients who did not 
this analysis measures the fraction of subjects living for a certain amount of time after a specified time patients who did not have had a death event before the last date of contact were labeled as censored observations 
kaplan meier survival analysis
to calculate survivability t the following probabilities were calculated t i is a time when at least one event happened d i the number of deaths that happened at time t i and n i the individuals known to survive have not yet had a death or have been censored at time t i 
this was calculated for two groups of subjects i e 
complete response vs partial no response 
this is shown in
as mentioned in the introduction previous approaches to predicting breast cancer patient survivability post nac involve using the rcb score and the fields used such as primary tumor bed area cancer cellularity and cancer positive lymph node count were correlated with higher recurrence 
however the unstandardized and incomplete nature of many pathology reports makes rcb difficult to calculate from reports in a database 
if the doctor does not record even one of the attributes necessary to calculate rcb then this value is impossible to find 
even if they do record all the inputs necessary the nature of pathology reports makes it very difficult to extract the meaningful information automatically other approaches for predicting breast cancer survivability have also been done but differ from our work in key ways 
for example delen walker and kadam have found success with decision trees however their data doesn t focus on nac patients and they use an arbitrary definition of survival our approach focuses on neoadjuvant chemotherapy breast cancer patients using patient response as a label proven to be correlated with survival through the kaplan meier estimator with discerning feature selection and using bootstrap random forests to avoid overfitting 
this differs from the few previous papers on using statistical ml methods to predict breast cancer patient survivability 
data was received from the oncoshare database for this project feature selection was a notable challenge since there were more than 200 features many of which were sparse missing information or not relevant for patient response pre diction 
thankfully the oncoshare database had a codebook detailing the specifics of each column of the emrs 
data and feature selection
however looking through the codebook more than a dozen seemed relevant for prediction models and this drew some concern for over fitting features 
therefore after some discussion with professor itakura oncologist from stanford school of medicine the long list of features was reduced to the following primary site location where the tumor originated laterality of the tumor side of the body in which the tumor originated tumors cell type tumor behavior malignant in situ benign or uncertain sequence of all reportable neoplasms during the patient s lifetime determined by the central registry estrogen receptor characteristics of tumor progesterone receptor characteristics of tumor actual number of tumors site specific information the type of diagnostic staging procedure
generally emrs are missing information or are sparse thus a large part of this project was dedicated to pre processing 
to pre process the data a combination of microsoft excel and pandas
after the data was pre processed the nac patient response predictions were tested using logistic regression k nearest neighbors knn and bootstrapped random forests logistic regression is a binary classifier and makes a prediction using the sigmoid function it was chosen as a baseline since it is known to produce reliable classification for binary data and is a good fit for this problem since we aim to classify patients with complete or not a complete response 
knn is a simple non parametric machine learning technique that classifies data based on a majority vote from its k neighbors because of this knn seemed like a good choice in balancing the bias variance trade off since the dataset was so small with a size of 340 
choosing machine learning models
in addition it made sense intuitively that patients who exhibited similar characteristics at the time of diagnosis might also have similar responses after nac lastly bootstrapped random forest was chosen 
bootstrapped random forest is an algorithm that utilizes decision trees to split on features depending on a threshold best splits are selected via gini impurity and each tree is run with bagged subsamples randomly sampled datapoints with replacement 
all trees are averaged afterwards all three algorithms were implemented using scikit learn
for training validation and testing the nac breast cancer patient dataset was split 80 20 for train test and then the 80 was utilized for 5 fold cross validation cv 
afterwards each model s performance was evaluated using the following metrics train test accuracy precision recall and specificity 
testing measuring performance
as stated in introduction and motivation the objective is to predict whether a patient exhibits a complete response or partial no response at the time of a nac patients last follow up which is determined by their overall ajcc stage in their emr 
to test this logistic regression
experiments results
since the averaged cv accuracy and the train accuracy from the 80 20 split were quite similar with a 2 margin the train and test accuracy from the 80 20 split will be used to compare the models in this discussion 
other information such as confusion matrices and train test accuracy plots can be viewed in the figures discussed in experiments results 
discussion challenges
looking across all models each algorithm performed relatively well 
bootstrap random forest performed the best optimal performance with a depth of 7 it had a train accuracy of 99 and a despite the good results that were seen across all models there may be some sampling bias since the dataset is so small m 340 
even though there was a strong correlation between patients with a complete response and higher chances of survivability as shown in
for this project simple supervised learning methods logistic regression knn and bootstrap random forest were implemented to predict whether a patient would exhibit a complete or not a complete response at the time of their last visit 
these responses were compared to the kaplan meier survival analysis curve and used to estimate their chances of survival after some specified time measured in days after some assessment bootstrap random forest performed the best and had high accuracy but because of a small dataset m 340 there may be some sampling bias 
conclusions and future plans
therefore if there was more time to work on this project we would take the following steps 1 
collaborate with pathologists radiologists and other healthcare professionals to gather more rcb data 
it would be helpful to have an rcb model to compare with our current model complete or not a complete response to see how well this model predicts survivability in nac patients 2 
utilize more robust natural language processing techniques that could potentially be used to process emrs 
again this would help create a reliable nac breast cancer rcb dataset which can be used to assess how accurate survivability of nac breast cancer patients is predicted 3 
use a mixture of rcb and emr features in our models and see how it improves the current rcb model hopefully with these changes this can help provide clearer conclusions about our model and how it could improve in the future 
as a group working on this project we contributed equally overall 
linda banh had large individual contributions in evaluating data features analyzing the model performance and writing the milestone report 
robel daniel had large individual contributions in literature research evaluating data features and putting together the poster 
preston ng had large individual contributions in preprocessing the data implementing each machine learning model and putting together the poster 
together we designed the pipeline for this project collaborated on what models were appropriate for the problem we were solving and put together this final report 
thank you to professor haruka itakura for her contributions to this project 
not only did she give us access to the breast cancer patient database in oncoshare but she also helped us focus on important patient features for our model designs 
we would also like to thank our project ta ethan steinberg for his help in guiding us on how we should pre process emrs and helping design ways to handle sparse data 
disease progression in individual patients is one of the fundamental questions in medical practice 
since many medical tests are either harmful or inconvenient to perform frequently it would be beneficial to develop a disease progression prediction method based on machine learning approaches 
in this project we focus on the study of the progression of motor impairment in children with cerebral palsy 
in particular gait deviation index gdi we have cleaned and preprocessed the gillette children s specialty healthcare dataset which includes around 6000 observations of children visiting a gait clinic and merged the dataset with the semls dataset which contains information of surgeries of each patient we have implemented soft longitudinal impute sli sparse longitudinal regression slr functional principal components fpca methods described by we have studied the effect of a surgery for each patient 
it turns out that after taking the surgery information into account the model could perform significantly better and explains around 40 of the variance which performs better than the current state of the art approaches 
the original data contains records of 12078 exams on 2904 patients mostly between age 3 18 
each exam record consists basic information e g 
walking speed cadence bmi height weight maximum knee flexion o2 expenditure and gait deviation index gdi for robustness of the result the following pre processing procedures were performed 1 
to avoid co linearity between the two legs of patients we consider only exams on left legs 2 
we consider only records with valid gdi age between 3 and 20 bmi body mass index between 10 and 30 and throw away the outliers 
to have enough data points to fit the individual progression curve we consider only subset of patients with 3 or more remaining records after previous steps the remaining data set contains 3106 exams on 777 patients 
some summaries of the data set are shown below 
the general question is stated as following 
let n be the number of patients 
problem formulation and related work
for each patient i 1 n we have n i observations y i 1 y i ni at time points t i 1 t i 2 t i ni where k be a set of basis for l 2 t min t max truncated to the first k elements 
we would like to estimate a set of coefficients w i r k so that y i j can be approximated by w t i b t i j 
the state of art approaches to estimating w i include direct approaches with functional principal component analysis direct approach has two major drawbacks to modeling covariance 
first overfitting happens when the number of observations n i for individual i is smaller or equal to the size of the basis k second similarities between curves are ignored while they could improve the fit potentially 
linear mixedeffect models can solve this problem conveniently by estimating the covariance structure and the individual fit simultaneously 
however they are not applicable unless the number of observations per subject is relatively large for we attempt to estimate k coefficients for every subject 
given the small number of observations we could still fit a linear mixed effect model in a smaller space spanned by functions with largest contribution to the random effect 
based on this low rank approximations are widely applied 
however due to their reliance on the distribution assumptions these models need to be carefully fine tuned for specific situations to avoid the potential bias caused by the assumption of an underlying probabilistic distribution in mixed effect models ref 
denote the set of all observed elements by pairs of indices as 
let p y be the projection onto observed indices is defined as the projection on the complement of the basis now is a t k matrix b 1 t t 
the coefficients we would like to fit is denoted by a n k matrix w 
therefore the problem can be formulated as a matrix completion problem heuristically speaking the target is to find a matrix w such that w b t y on our observed indices and thus we could impute the missing values of y by w b t 
the details of the previous heuristics can be found in the next part of this section 
as our model can be described as the direct way of finding such a w is to solve the following optimization problem where f is the frobenius norm i e 
the square root of the sum of matrix elements 
models without adjusting for surgery
however such approach consists two main drawbacks 
first if the number of basis functions k is larger than or equal to the number of observations n i which is the number of observations of individual i then the error could be reduced to 0 which causes overfitting 
second this methods ignores the similarities between the curves of different individuals which could potentially improve the model performance one of the standard ways to remedy these issues it to assume that individual trajectories can be represented in a low dimensional space by constraining the rank of w 
thus our optimization problem is now arg minwhere 0 is a parameter f is the frobenius norm and is the nuclear norm i e 
the sum of singular values 
ref 
inspired by this algorithm 1 in ref 
we refer to s x as the singular value thresholding svt of x 
the optimization problem above can be easily extended to multiple variables varying or constant in time that work together to characterize the progression of one disease x i is some n t matrices corresponding to the processes measured and x x 1 x 2 x p 
b i p b is a pt pk matrix with b stacked p times on the diagonal 
w is a n pk coefficient matrix that we want to fit 
this optimization problem can also be solved with algorithm 1 in ref 
the above two optimization problems both aim to reduce the dimensionality of the sparse observations 
in practice we would often want to predict the trajectory of one variable y gdi in our case with the knowledge of other variables x related to the same disease 
then the problem can be formulated as a regression of y on x arg minalgorithm 3 in ref 
finally combining the technique of dimensionality reduction and sparse regression we can predict the trajectory of one variable given some other covariates that are varying or constant in time 
we first solve eq 
2 using algorithm 1 in ref 
some preliminary simulations and data studies are presented in ref 
intuitively having a surgery would impact the progression of disease thus it is reasonable to build up a model which take surgery into account 
for each patient i our new model can be formulated aswhere the indicator function 1 s t equals 1 if patient i has received a surgery before time t and 0 otherwise can be interpreted as the average effect of a surgery among all patients and i is a random effect which can be modeled as a normal distribution with mean 0 and variance 2 in our case we could first regress y on the dummy feature 1 s to get an estimation for the mean effect after adjusting for the effect of surgery we could get a new matrix where each rows represents the adjusted gdi for patient i 
models after adjusting for surgery
then we could try to impute the missing values of based on all the methodologies described in part 3 2 
the code used can be found in https drive google com file d 1gkyf1iawjicrcvakksn fbi8qnfg1edo view usp sharing 
we first apply sli to the gdi data 
as a comparison fpca is also used to train the same dataset and generate predictions 
the baseline is just a naive prediction calculated by averaging all gdi data we have 
to estimate the accuracy and robustness we have performed the experiment 20 times at each time the data is randomly split into a training set containing 90 of the data and a test set containing the rest 10 of the data all models are trained on the training set and evaluated on the test set 
the averaged mean square error mse and the standard deviation sd of the mse on the test set are summarized in we can see that sli is better than fpca in this case and can explain 30 of variance from the baseline prediction defined as the percentage of reduction of mse from sli with respect to mse from baseline 
to further improve the prediction we include the surgery information which is expected to be highly correlated with gdi into consideration 
we then perform slr sli and fpca on the adjusted data obtained as described in section 3 3 instead of y and compare the results with the baseline as shown in on the other hand there is still a larger portion of variance remains unexplained this may be due to the sparse and irregularity of the data also it is important to note that in practice it is often hard to predict the disease progression precisely based on the current features of an individual 
but still there is room for improvement for example we could do variable selections to find the most relevant features or build more sophisticated models to capture the effect of a surgery some of the directions of future works are discussed in section 5 
the sli results using the original data can explain 30 of the error of the baseline 
after we include the effect of surgery the predictions of both sli and slr are improved and can explain up to 40 of the error of the baseline 
however even after we consider the effect of surgery the performances of sli and slr are still not as good as fpca we can perform feature selection to further improve our matrix completion based methods 
this can be done by forward selection or by using the top components from the dimension reduction of covariates as new features 
this project is in corporation with postdoctoral researchers dr ukasz kidzi ski and dr yumeng zhang from the department of statistics in stanford 
the gdi dataset is provided by dr ukasz kidzi ski 
the code we used is based on fcomplete https github com kidzik fcomplete git a r package written by dr kidzi ski 
dr zhang contributed helpful discussions and the code for data pre processing 
in the current study we approached the hit song science problem aiming to predict which songs will become billboard hot 100 hits 
we collated a dataset of approximately 4 000 hit and non hit songs and extracted each songs audio features from the spotify web api 
we were able to predict the billboard success of a song with approximately 75 accuracy on the validation set using five machine learning algorithms 
the most successful algorithms were logistic regression and a neural network with one hidden layer 
the billboard hot 100 chart this research is relevant to musicians and music labels 
not only will it help determine how best to produce songs to maximize their potential for becoming a hit it could also help decide which songs could give the greatest return for investment on advertising and publicity 
furthermore it would help artists and music labels determine which songs are unlikely to become billboard hot 100 hits 
the initial idea for this research project stemmed from a new york times article that used the spotify audio features to illustrate the similarity of summer songs machine learning is a popular research and industry tool to approach the hss question 
researchers have used convolutional neural networks another group of researchers used support vector machines svm to predict top 10 dance hits
a dataset of 10 000 random songs was collected from the million songs dataset msd tracks were labeled 1 or 0 1 indicating that the song was featured in the billboard hot 100 between 1991 2010 and 0 indicating otherwise 
next we used the spotify api to extract audio features for these songs the spotify api provides users with 13 audio features of which we chose nine for our analysis danceability energy speechiness acousticness instrumentalness liveness valence loudness and tempo 
the first seven features are represented as values between 0 and 1 by spotify 
loudness is measured in decibels and tempo refers to the speed of the song in beats per minute to account for artist recognisability we defined an additional metric the artist score 
each song was assigned an artist score of 1 if the artist had a previous billboard hot 100 hit and 0 otherwise 
we looked back to 1986 for this metric 
there is some inherent inaccuracy in this measure 
if an artist had a hit song before 1986 but not after they were given an artist score of 0 
to predict a song s success we used six different machine learning algorithms expectation maximization em logistic regression lr gaussian discriminant analysis gda support vector machines svm decision trees dt and neural networks nn 
we focused mainly on the accuracy of results but we report the precision and recall as well 
false positive predictions may be costly if a music label invests in a song that is unlikely to become a hit for an initial identification of clusters in the data we used the em algorithm assuming no labelled data then compared the clusters to the actual labels 
this algorithm creates clusters of the data according to a specified probability distribution 
in each iteration the parameters of each cluster are calculated and the probability of each data point being in each cluster is calculated 
we used a gaussian distribution with the following update rule we then used the semi supervised em algorithm with the labels of a randomly selected 20 percent of the examples 
this algorithm incorporates the known labels into the calculation of parameters as above for each supervised learning algorithm we split the data into training and validation examples using a 75 25 split 
an additional test set was not needed 
we tested the accuracy against both the training and validation labels 
lr and gda both fit a decision boundary to the data 
lr uses newtons method to maximise the logarithmic likelihood on the training set with the following algorithm gda fits a probability distribution to positive and negative examples and calculates the decision boundary that maximizes the logarithmic likelihood on the training set using the following equations we then used svm which creates a decision boundary based on the data points closest to the decision boundaries creating support vectors 
we maximize the lagrangian on the training set with respect to values of alpha as follows we used three different kernels linear radial basis function rbf and polynomial with notably different results dt creates a series of decision boundaries on the training set 
each boundary splits the data into two clusters within the current cluster at a value of a feature that minimizes the gini loss our final approach in this hit predicting problem was to use a neural network 
we used a neural network regularization with one hidden layer of six units and the sigmoid activation function 
the l 2 regularization function was applied to the cost function to avoid over fitting where w 1 is the weight matrix mapping the features to the hidden layer and w 2 is the weight matrix mapping the output of the hidden layer to the final output 
we used accuracy precision and recall on the training and validation sets to evaluate the performance of each algorithm the em algorithm gave a poor accuracy of 50 1 with predictions on data points matching poorly to their actual labels lr and gda yielded a reasonable accuracy of 75 9 and 73 7 against the validation data with similar accuracy against the training data indicating no overfitting 
the average cross entropy loss was 1 372 
the precision and recall on the validation set were acceptable 
the confusion matrix on the validation set shows that there are some false negatives meaning that songs that could potentially become hits could be unnoticed for the svm each kernel yielded reasonable accuracy on the training data but poor accuracy on the validation data indicating significant overfitting the dt algorithm can achieve full accuracy on the training data by creating closely spaced decision boundaries that split the data perfectly 
however this is likely to cause high overfitting with an accuracy of only 51 5 on the validation set 
we used random forests to correct the svm linear and polynomial kernels and dt against overfitting 
four sets of parameters were considered and the accuracy was recorded using 10 trials of 500 random samples was the most successful measure for each algorithm 
the accuracy on the training and validation sets were roughly equal implying that overfitting was reduced significantly 
furthermore to prevent overfitting of the dt we experimented with different maximum depths 
using a maximum depth of n the number of features gave the optimal result the nn gives similar accuracy to lr but interestingly generates significantly higher precision 
this shows the robustness of the nn prediction 
the regularization prevented overfitting 
the optimal number of epochs was investigated in order to achieve the highest accuracy 
we used lr and nn the most successful algorithms for further investigation 
we performed error analysis for both algorithms to determine the features with the greatest influence on predictions 
ablative analysis was used beginning with one feature and subsequently adding the features which provide the greatest improvement in accuracy until the maximum accuracy has been achieved features which reduced the accuracy of the prediction were subsequently removed from the analysis 
this provides a ranking of the features in terms of their influence on predictions 
the artist score proved to be the major feature for lr and danceability was found to be the prominent feature for nn next we investigated seasonal effects of the algorithms focusing on two periods summer months june to august and the holiday period november to january 
by training on songs released outside of the focus period and validating on songs released in the period we were able to identify whether the general trends in pop music in the period were different 
there was no difference observed for songs released in summer but there was a noticeable reduction in the accuracy when the algorithms were validated on the holiday set 
we can conclude that the features of a hit song are different in the holiday period we also investigated whether trends in pop music change over time 
we divided the data into subsets of fiveyear periods and split each subset into training and validation sets using an 80 20 split 
in most cases the accuracy on both the training and validation set improved implying that the features of pop music are somewhat unique to the time period of the songs release 
the period from 2000 to 2004 saw a worsening of both the training and validation accuracy compared to that computed over all examples and the period from 1995 to 1999 saw a decrease in the training accuracy
the analysis showed that lr and nn yielded the highest accuracy precision and recall of the algorithms tested 
svm and dt suffered from overfitting 
we would like to use more data to reduce the variability of results 
instead of using 4 000 songs we hope to include all billboard hot 100 hits taken from a longer time period and a similar number of non hits from the msd 
furthermore we would like to look into additional audio features such as duration which was not included in this project but has the potential to predict a songs billboard success 
a primary goal in biology is to fully understand the mechanisms by which dna specifically the coding sections or genes control all biological processes within every organism 
two main obstacles to this goal are i laboratory methods for definitively determining the function of a gene can require years and hundreds of thousands of dollars in research for even a single gene and ii historically independent research groups have not followed the same system for categorizing and labelling gene function making sharing data across group prohibitively difficult 
recently however the scientific community has come to recognize the gene ontology go hierarchy
training examples were constructed by combining pairwise interaction and microarray expression data for each of 5493 yeast genes all genes for which relevant data was available and labeled according to their go annotationssee
data collection and processing
the go is a directed acyclic graph where each node consists of a term name a unique alphanumeric identifier a definition with cited sources and a namespace to which it belongs 
the three namespaces are cellular component the parts of a cell or its extracellular environment molecular function the elemental activities of a gene product at the molecular level and biological process operations or sets of molecular events with a defined beginning and end pertinent to the functioning of cells tissues and organisms go annotation data from uniprotkb the training labels take the values j 1 0 1 
gene ontology labels
a gene is a positive example of both the most specific nodes to which it is annotated and all parents of that node 
this propagation accurately captures the hierarchical nature of the go increases the available number of positive training examples for each class and ensures that all training labels are hierarchically consistent by definition because go annotations are almost exclusively positive and typical machine learning algorithms require a balanced amount of positive and negative examples all examples not annotated to a particular node or any of its parents were labeled to be a negative example 
thus a gene that is annotated to a parent of class i but not to class i is considered a 0 example and is left out of the training set for the ensemble of svms for node i 
while barutcuoglu et al used a biogrid biogrid datasets each include a certain number of experience records each interaction type corresponds to a certain type of experiment 
each record includes the two genes interacting with each other using different types of labelling ncbi classification also known as entrez gene ids or formerly locuslink the publication that showed this interaction the taxonomy id the specific interaction type using the molecular interactions controlled vocabulary
biogrid data
dna microarrays are grids of microscopic spots containing dna probes subjected to various experimental conditions and used to measure gene expression levels 
microarray datasets are real valued matrices where the rows are genes or other dna snips and the columns are standardized conditions 
microarrays
the microarray data was retrieved from several sources
the method presented in this section is similar to the one presented by barutcuoglu et al 
it consists in using bayesian inference on separately trained svm classifiers 
statistical methods
first individual support vector machines are trained for each of the 95 chosen go nodes 
svm classifiers are machine learning methods that separates positive and negative examples with a linear decision boundary or hyperplane in a feature space 
svm classifiers on individual go nodes
the scikit learn implementation of svms was used and experiment with both linear and rbfkernel svms using various and c parameter values 
results of these tests are presented in section 4 1 
since a validation set was needed but the total number of positive examples for each class is relatively small bootstrapping random sampling with replacement is used to create 10 samples for each classifier each using 0 632 of the full set of training examples 
at test time each test example is evaluated only on the classifiers for which it is not part of the training set and labeled according to the median of the 10 predictions 
dev tests sets given few positive examples
after training the individual classifiers a bayes net representing the 93 node go hierarchy is constructed with two bayes nodes for each go node where i represents the svm prediction for node i and y i represents true membership in class i as illustrated by figure 2 p i y i is estimated with maximum likelihood estimation during svm training 
p y i 1 anychild y i 1 is assumed to be 1 0 before laplace smoothing since training labels are enforced to be hierarchically consistent i e 
bayesian inference
if a training example is positive for any child class it is guaranteed to be positive for its parent classes finally exact inference is performed using pgmpy
the results presented in this section show significant improvement compared to barutcuoglu et al both for the svm node classifiers considered separately and for the bayesian network made of all these nodes 
some biological functions reached an accuracy of 1 over the more than 1 000 test examples 
grid search was applied on the most represented go node go 0045944 positive regulation of transcription by rna polymerase ii to find the optimal kernel and the optimal penalty parameter for the error term 
heatmaps of these grid searchs are presented in the scores were computed using a 10 fold cross validation using scikit learn auroc represents the probability that a random positive example will be ranked higher by the classifier than a random negative example 
svm
given datasets with a small fraction of positive examples it is possible to achieve a high accuracy simply by classifying every example as negative and the auroc score is therefore a better measure of the performance of a classifier precision and recall were computed counting the number of correctly and incorrectly classified positive and negative examples since the implementation includes a bayesian network and since the number of positive examples is small compared to the number of negative examples it is mostly important to classify as many positive examples as positive 
indeed the bayesian network will have an easier time correcting false positives than false negatives 
therefore the kernel that maximizes the recall was selected i e 
linear kernel with c 1000 
using these values the precision was multiplied by more than 2 5 and the recall by 10 when compared to the results from barutcuoglu et al 
the average obtained accuracy over all the go nodes is over 97 7 and the average auroc is over 0 787 it is interesting to note that barutcuoglu et al 
used high c value and an rbf kernel 
while the high c value is to be expected given as they noted the high number of features compared with the number of examples the fact a different kernel was found to be the most efficient could possibly be explained by the updates to the datasets over the years more experiments were made giving classifiers used in this project more training examples barutcuoglu et al 
acknowledge noisy data as a reason for wrongly predicted examples in particular the choice made in this paper to include all types of interactions initially based on the fact that the interaction types singled out by barutcuoglu et al 
were obsolete seems to have had a very positive impact on the final results another explanation could come from the updates to the go classification as mentioned in section 2 1 the 105 go nodes selected by barutcuoglu et al 
correspond in the current classification to 95 nodes equivalent functions have been united in single nodes 
due to time and computational constraints as well as having to solve several bugs in the pgmpy code base the bayes net prediction code was only run on the subset of go nodes presented in the bayes net predictions universally improved upon both the accuracy and auroc score of the svm classifiers alone 
although the accuracy was already very high for the svms alone the auroc was not and the bayes net provided significant improvement 
bayesian results
this paper successfully reproduced the method developed in barutcuoglu et al 
an interesting future step would be to try to apply this model to both more complex species for example humans and less studied ones for example newly discovered species 
since the number of features was important com pared with the number of training examples it is possible that the model presented in this paper does some overfitting 
classifying genes from another specie would therefore probably require the model to be retrained even for the features share by this specie and the yeast while this model reduces the amount of experiments needed to determine the functions of all the genes of a specie it still requires a large amount of experiments 
especially for newly discovered species it would be interesting to determine the threshold in terms of the number of experiments required to achieve good accuracy i e 
the most significant features in order to limit the amount of biological experiments needed and thus reduce the cost and length of the process 
code https github com benoitpitclaudel cs229 laura selection of paper to replicate collection and processing of microarray data implementation of bayes net predict and all code in project py contributed to poster and writeup benoit collection and processing of go annotations collection and processing of biogrid pairwise interaction data svm experiments and parameter selection contributed to poster and writeup 
code and contributions
we will implement and compare machine learning algorithms to predict with high confidence the presence of a chronic condition myasthenia gravis which affects about 200 000 people in the us alone 
this should be helpful in eliminating the need for a painful and expensive singlefiber electromyography emg test and could potentially diagnose with a single anti acetylcholine receptor achr antibody ab test 
we have trained our algorithms on 22 co factors features commonly found with myasthenia gravis and could also predict the probability of being afflicted with myasthenia gravis given a patient history and a questionnnaire 
myasthenia gravis mg is a neuromuscular disorder that causes weakness in the skeletal muscles which are the muscles your body uses for movement 
it occurs when communication between nerve cells and muscles becomes impaired 
introduction to myasthenia gravis
this impairment prevents crucial muscle contractions from occurring resulting in muscle weakness 
we propose that with training set with the 22 cofactors and the myasthenia diagnosis in existing patients it would be possible for the system to correlate the occurrence of myasthenia with just a simple questionnaire and the achr and musk phlebotomy tests and avoiding the painful emg test 
the computational cost is mainly in the form of training which is incurred only once upfront 
once the algorithm is trained predictions can be produced in a fraction of the training time and would be beneficial to the patients 
the state of art in medical history diagnosis using machine learning has primarily been applied to imaging diagnostics in x rays and with tumour cell imaging it has not been studied on classification problems like diagnoses and specifically not with myasthenia gravis 
we have therefore assumed our baseline calibration with regression analysis which is standard in the field of medicine 
we propose that we can train the algorithms with a data set with 22 different co factors related to myasthnia gravis and therefore we can predict the occurance of myasthenia gravis using a combination of factors we thank dr srikanth muppidi muppids at the stanford neurosciences hospital for his immense help in gathering anonymous patient records for us 
we have a set of 10 056 data points with the above mentioned factors from a nih repository for myasthnia gravis 
we use a 95 5 train development set split to train and tune our model respectively 
as for the test set we use another separate set of 199 samples of anonymized data from patients only from the menlo park region california the following are the various features each data sample holds 1 
age age of the candidate is a factor in diagnosing myasthinia gravis as it potentially affects people in advanced ages 2 
gender there are studies that show that more women are affected with myasthnia gravis than men 3 
bmi a higher bmi may correlate to a host of health problems and may be a feature in diagnosing myasthenia 4 
years diagnosed with mg this field is 0 if a patient does not have myasthnia gravis but if diagnosed gives us an idea on the years they have been diagnosed 5 
achr anti bodies approximately 85 90 percent of patients with myasthenia gravis mg express antibodies to the acetylcholine receptor achr 6 
musk anti bodies useful for diagnosis of autoimmune muscle specific kinase musk myasthenia gravis 
second order test to aid in the diagnosis of autoimmune myasthenia gravis when first line serologic tests are negative 7 
presence of musk ab and achr ab this is a field we have generated to account for presence of both antibodies and therefore might indicate a higher incidence of myasthenia gravis 8 
seronegative around 10 20 of myasthenia gravis mg patients do not have acetylcholine receptor achr antibodies seronegative of whom some have antibodies to a membranelinked muscle specific kinase musk 
to examine mg severity and long term prognosis in seronegative mg compared with seropositive mg and to look specifically at anti achr antibody negative and anti musk antibody negative patients 9 
thymectomy surgical removal of the thymus gland may indicate a lowe incidence of myasthnia gravis 10 
sleep apnea a potentially serious sleep disorder in which breathing repeatedly stops and starts this is commonly incident with myasthnia gravis 11 
sleep apnea number this is an indication if the patient is aware of their affliction with sleep apnea 12 
non invasive ventilation support noninvasive ventilation niv refers to the provision of ventilatory support to the lungs without the use of an endotracheal airway 
it has emerged as an important tool in the treatment of acute respiratory failure this field measure if the patient has ever been put on an niv system 13 
niv number the number of times a candidate has been on niv support 14 
mg qol15 the mg qol15 is a brief survey completed by the patient that is designed to assess some aspects of quality of life 15 
ess the epworth sleepiness scale ess is a scale intended to measure daytime sleepiness that is measured by use of a very short questionnaire 16 
ess is greater than 10 this indicates a strong affliction to daytime sleepiness and therefore increased chances of being prognosed with myasthinia gravis 17 
psqi the pittsburgh sleep quality index psqi is a self report questionnaire that assesses sleep quality over a 1 month time interval 18 
psqi is greater than 5 this indicates a strong affliction to poor sleep rhythms due to fatigue and therefore increased chances of being prognosed with myasthinia gravis 19 
fss the fatigue severity scale fss is a method of evaluating the impact of fatigue on you 
the fss is a short questionnaire that requires you to rate your level of fatigue 20 
fss is greater than 36 his indicates a strong affliction to increased fatigue and therefore increased chances of being prognosed with myasthinia gravis 21 
mg adl the mg adl profile provides a rapid assessment of mg symptom severity it has been validated and shown to correlate with the qmg score 22 
mg adl bulbar subset score a short questionnaire to find out the fatigue in the bulbar and throat region this data was pre processed such that all text fields were transformed to take only numerical values for instance the gender field took 0 for male patients and 1 for female patients 
various algorithms were used on the training data with logistic regression as the baseline model since it is the most widely used machine learning algorithm to classify and predict medical data 
we will focus on the binary classification problem in which y can take on only two values 0 and 1 
the logistic model is a widely used statistical model that in its basic form uses a logistic function to model a binary dependent variable 
cross entropy loss is used to measure the performance of the model 
logistic regression models p y x aswhere g is the sigmoid function 
by making significantly weaker assumptions logistic regression is more robust and less sensitive to incorrect modeling assumptions 
when we have a classification problem in which the input features x are continuous valued random variables we can then use the gaussian discriminant analysis gda model which models p x y using a multivariate normal distribution 
the gda model makes strong modelling assumptions and when these assumptions are correct informally there is no other algorithm that performs better 
gaussian discriminant analysis
the advantage of using a cnn is that they often require only very little pre processing 
a 1d convolution was used across the 22 features of the input data 
the following cnn architecture was used 
trees have the capacity to learn highly complex data patterns depending on the depth of the trees 
this will always however lead to over fitting and hence increases the variance on the model 
random forests
thus random forests can be used which can be imagined as a method of averaging across these deep decision trees by applying feature bagging 
the bootstrapping procedure ensures that the variance of the final model is less 
moreover if a certain feature is a strong predictor of the final response then that feature will be selected in many of the decision trees 
we define three metrics that are very useful in calculating if our model performs well 
metrics
from our results we can see that the cnn model performed really well since it has the highest f1 score amongst all the other models 
the gda model performs slightly better than the logistic regression 
discussion of results
this can be expected since most of the features of the data are scores from questionnaires filled out by the patients themselves and hence the scores should be treated to contain some sort of noise associated with them 
most natural processes tend to be normally distributed and hence the reason why gda performs better than logistic regression 
random forests overfit the data and so the depth of the tree was limited to 2 the maximum features to 3 and the maximum leaf nodes 2 to prevent overfitting 
the cnn model however allowed for picking the features that strongly control the prediction since it used a convolution and max pooling layer 
some input features might affect our final prediction more strongly than others 
more often that not in the medical sphere medical records of patients are documents with very few of these features available 
future work scope
if we can narrow down the features that do not affect the final prediction severely we can successfully predict if a person has myasthenia gravis simply by looking at existing medical records and open up new doors for more tests that will help with a more certain diagnosis 
it can be noticed that the input features excluding the achr musk and seronegativity tests are side effects and related effects caused in a person suffering from myasthenia gravis 
thus if we knew the age gender and bmi of a person suffering from myasthenia gravis and the remaining features we could extend our model to predict the existence of other effects such as sleep apnea or if a thymectomy could help lessen the symptoms 
the team consisted of two members 1 
abhishek tapadar and 2 
asherin george anto george 
both the team members helped reach out to the professionals at the stanford neurosciences hospital in order to obtain the data required 
most of the discussions and decisions regarding this project were also done together abhishek tapadar contributed in writing code and deriving results from the logistic regression gda and cnn models 
he also helped in deciding upon the hyperparameters for the above models and the gradient boosted and random forests model 
in addition he helped in putting together the report for the various intermediate milestones and the final project report asherin george anto george contributed in performing the gradient boosted and random forests models 
he also helped in hyperparameter selection for the models abhishek tapadar was involved in 
he contributed in putting together the write up for the project proposal milestone poster presentation and the final project report 
chronic recurrent multifocal osteomyelitis crmo is an inflammatory bone disease affecting primarily children where patients present with bone pain and localized swelling 
since its first description in 1972 there have been over 500 documented cases 
clinically crmo affects mainly the distal regions of long bones including the femur and tibula during crmo treatment physicians use whole body magnetic resonance imaging wb mri to evaluate patient disease progression and response to treatment here we evaluate disease progression automatically using machine learning on pairs of images from mri scans 
due to the complexity of analyzing and generating high quality features for a full body mri we consider only subsets of an mri that contain clear images of the knee and long bones of the leg 
this simplifies the problem and enables classical machine learning techniques 
pairs of input mri images are classified as members of three classes condition improved condition persisted and condition regressed 
additional models for the binary class problem condition improved and condition persisted or regressed are also presented convolutional neural networks cnns that are used for many computer vision and mri processing applications require large datasets to avoid overfitting on the training set 
such requirements are unreasonable in many clinical settings particularly in rare diseases such as crmo where the number of known cases are in the hundreds 
instead we focus on careful data augmentation and methods such as cross validation and regularization to reduce overfitting 
we use custom methods to extract features from images applying classical models to attempt to make global predictions about an individual s disease progression based on an mri subset 
wb mris have proven to be a valuable tool in clinical and research crmo settings 
roderick et al 
a chronic recurrent multifocal osteomyelitis research
evaluated crmo treatment effectiveness by considering changes in visible lesions in wb mri
evaluating pairs of mri scans is similar to computer vision problems that attempt to recognize changes in scenes with applications to building and traffic monitoring
b scene change detection
machine learning applied to mri and other radiograph images is an active field of research
c machine learning applications to mris
data preparation wb mri scans from 45 patients were procured from the bristol royal hospital with scans averaging one year apart per patient 
a dataset containing the original radiologists assessment based on clinical data and mri readings was also procured 
iii 
the radiologist assessments were simplified to fit a three class model given two consecutive mri scans for a patient taken at two different dates a patient s condition with respect to crmo either improved i regressed r or persisted s 
as the scans represent a cohort of patients undergoing treatment with pamidronate therapy the data is skewed towards the improved class 
the mri dataset was provided in dicom format each scan consisting of approximately one thousand individual images depicting cross sections of the body 
from this initial data a pared down dataset was manually curated by selecting one to two representative images with clear views of the knee and long bones of the leg from each mri scan 
in some cases patients had no high quality representative images because all leg and knee images were extremely blurry or noisy these scans were omitted from the set leaving scans of 28 patients a list of date pairs and disease progression labels was manually curated from the information provided by the radiologist 
when possible labels for non consecutive scans were inferred to increase dataset size merging the images with the curated radiologist data resulted in 55 examples each consisting of a pair of images similar to figure 1 and a label of i s or r image feature vectors were extracted in multiple ways from a pre trained convolutional neural network producing 2048 length vectors and via a bag of visual words technique producing vectors of length where is the visual vocabulary size 
these 2 v v approaches are described in the methods section 
since initial models had high variance issues data augmentation was used to produce more training examples
a data augmentation
the augmented dataset was split to create 56 training samples and 25 validation samples 
a significant number of samples were placed in the validation set in an effort to realistically quantify the generalization error 
b data distribution
an additional small set of 7 test examples was held out as a means for assessing final model quality at the end of development 
the test sample was hand selected prior to augmentation and contains a discrete set of patients whose scans do not overlap with those in the development sets 
class distribution of train dev sets before and after augmentation 
we present two component approaches and an ensembling technique producing the prediction pipeline shown in
transfer learning is the process of using models trained in one setting for application to other problems 
the inception v3 model presented by szegedy c et al we used the convolutional base of the pretrained model and extracted features generated in the penultimate layer using tensorflow images were first normalized to match the training size of inception v3 299x299 by padding and resizing before being run through the network 
a transfer learning with inception v3 network
multiple approaches were tried to convert individual image feature vectors into a format to represent an mri pair 
predictor models were trained using two classes of models a softmax regression a generalization of logistic regression that applies to multi class problems by defining linear boundaries between classes 
softmax uses the multi class cross entropy loss function 4 1 with probability determined by the softmax function 4 2 overall architecture for proposed model b k nearest neighbors an item is classified by majority voting of k neighbors 
the bag of visual words on the resulting dataset we trained svm with radial basis function kernel and naive bayes classifiers 
for naive bayes a bernoulli event model was used and the feature vectors were transformed to a binary format with 1 indicating the presence of a centroid in the image and 0 indicating its absence 
b bag of visual words
then predictions are given by 4 5 and 4 6 4 5 4 6 was generated using scale invariant feature transform x kp sift both surf and sift rely on approximations for the laplacian of gaussians log an edge detection technique that applies a laplacian operator to a gaussian blurred image 
the gaussian blur is a low pass filter reducing noise and the zeros of the laplacian filter a second order differential operator indicate areas of rapid change characterizations of edges and blobs 
sift approximates the log with a difference of gaussians dog a similar technique which takes the difference of two images blurred by gaussians with different 
surf does the approximation using box filters 2 where a pixel is set to the average of its neighbors 
orb detects key points via a corner detector that uses heuristics on points within a given radius of a pixel after dropping low contrast points and edge points the remaining key points are transformed to vector descriptions computed by examining a window around the point 
ensembling methods were used to decrease variance by combining predictions of multiple weaker classifiers 
we used a simple voting ensemble to combine the individually trained models discussed above 
c voting ensemble
soft weighting was used which considers the probabilities produced by component models whereas hard voting considers only the predicted class 
component models and parameter tuning were performed on a 70 30 train dev split 
we used 5 to 7 fold cross validation ensuring sets were large enough to represent all classes in each round 
v experiments and results
the best performing models were re trained on the full training and development sets before final evaluation on the held out test set 
as accuracy is a poor lone indicator of success in small dataset problems we use several metrics f 1 score 
weighted average that attempts to balance false positives and negatives by considering precision and recall receiver operator curve roc 
plots recall against 1 specificity is a visualization of the discriminatory power of a model versus random guessing a straight line 
au roc the area under the curve summarizes the roc in a scalar between 0 and 1 
au roc closer to 1 indicates higher quality macro micro averaging 
used in multi classification problems macro average treats all classes equally while micro average weights classes based on class size 
we set a baseline by subtracting image brightness histograms creating length 256 vectors and training logistic regression 
this performed similar to random guessing 
a baseline
in the transfer learning approach we focused on methods for transforming individual feature vectors generated by the cnn into features that performed successfully when representing a pair of images 
two primary methods for combining data were tested feature concatenation where individual feature vectors were concatenated to form vectors of length 4096 and feature dissimilarity where we subtract the individual feature vectors giving a vector of length 2048 experiments were also conducted to process the initial data by performing data augmentation feature normalization and feature reduction removing features with 
b transfer learning model 
feature dissimilarity applied to video scene detection in
we trained naive bayes and svm with rbf kernel classifiers on the visual word datasets with vocabularies ranging from size 5 to 500 
hyperparameters for the svm were selected via grid search in 5 fold cross validation 
c visual bag of words
the highest performing hyperparameters for the svm models were found at c 10 
in naive bayes we 0 0001 experimented with both a uniform prior for visual world probabilities and with priors learned from the training set 
the best performing models during cross validation are summarized in
multi class both bag of words models were high variance achieving 100 per accuracy on the training set 
we augmented the dataset by generating 10 20 synthetic images for each training image created by applying a random linear warp and adding gaussian noise 
train dev test
this prevented the model from achieving perfect training accuracy but failed to improve cross validation performance this method consistently struggled to generalize past the training set 
despite key point density around interesting regions in the image visible as the green dots in the architecture diagram of
using the hypertuned parameters for models in previous sections we built custom pipelines to extract relevant features for each model and combine resulting predictions 
the same cross validation grid search approach was used to determine optimal weighting of the component models 
d voting ensemble
perhaps unsurprisingly the highest performing ensemble models gave higher weight to the cnn based model e g 
3 1 for multiclass 
although the confusion matrices on the validation set look the same for transfer learning and the ensembled method see interestingly visual inspection of the misclassifications showed that two of the examples in the multiclass and one of the examples in the binary class corresponded to images from a patient labeled s that clearly matched our intuitive understanding of the i class suggesting possible human error 
still some of the other misclassified examples were harder to interpret without crmo expertise more generally our simplified model of isolating the knees of patients inherently adds label uncertainty 
although crmo is most commonly found in the legs previous analysis on this cohort showed lesions throughout the body consistently all multi class models were unable to properly predict class s see although we made efforts to address model variance by ensembling decreasing feature size and performing data augmentation the majority of final models still struggled with high variance 
a validation curve for varied values of regularization parameters
our results are a promising beginning to research applying machine learning to crmo and we believe further investment in the small data techniques is worthwhile 
the most promising future work in improving our model lies in improving our features as experimenting with models and hyperparameters had relatively little impact on our results compared to feature selection 
vi conclusion and future work
the cnn transfer learning approach showed a level of success and we suspect it could be improved by retraining on the inception v3 network using a more relevant mri dataset rather than the original imagenet set 
such data exists for conditions like osteoarthritis and might be applied here to extract more appropriate features working closely with crmo experts is another promising path forward 
features based on rinbo developed with expert input could be information dense keeping both bias and variance low 
interpretable models such as decision trees could also make our models more useful in a clinical setting 
worked with pediatricians at the bristol children s hospital to acquire mri images and labels indicating whether a patient s condition improved stayed the same or regressed 
anna merkoulovitch
we thank dr athimalaipet v ramanan of bristol royal hospital for introducing us to crmo research and guiding us along the way dr chandrika s bhat for assisting in manually curating data collected from radiologists and the radiology staff for anonymizing and sharing patient mri images we also thank ta fantine huot for her tremendous amount of guidance and support throughout the project and ta suvadip paul for assistance in connecting with the uk based team and technical guidance 
at the state and the district level a great deal of debate and resources go towards funding public education 
there are a number of studies that estimate the returns of increased school resources to student achievement within the social science literature there are a number of different metrics upon which schools are judged 
some of the most common ways that outcomes are measured include standardized test scores graduation rates and the rate at which students progress to college 
given that although all of these are measures of how good a school is the mechanisms by which they are changed are likely different 
we try using a number of different models to estimate each of these outputs 
massachusetts has rigorous standardized testing the mcas that all students are required by law to participate in the tenth grade in the analysis of school performance that we found within the literature school specific models were limited to linear regression 
we seek to build upon this by developing more accurate models using a variety of more sophisticated techniques 
we seek to determine whether these models agree with the assertions from the social science literature about the effects of changing expenditures class sizes and teacher salaries 
in order to estimate the effect on the performance of a given school we approximate the limit definition of a derivative near the current levels of any of these explanatory variables 
there has been a considerable body of research by economists and policymakers looking into the relationship between school spending and outcomes 
a recent metaanalysis of 377 different publications investigating the relationship between schools funding and academic outcomes notes that irrespective of the the input feature class size teacher quality and expenditure per pupil between 10 and 20 percent of studies found a statistically significant positive correlation between 5 and 10 percent of studies found a statistically significant negative correlation and the rest were unclear
we combined data from a variety of sources to augment school level data sets with information about their surrounding areas 
in order to estimate public school performance we needed context about the income cost demographics and education levels of the communities they served massachusetts individual school level data was sourced from a kaggle dataset from the mass 
department of education we combined these school level inputs with data scraped from towncharts com using both beautiful soup 4 s html parser and selenium webdrivers throughout our analysis we removed a number of features we determined to be largely peripheral to the measures of success 
this was done in order to reduce over fitting 
most features removed were ones that theoretically should have no effect on student performance but were captured in our initial data process we transformed categorical variables into one hot dummy variables 
once we joined our school level data source by zip code with our scraped data source we split our full school data into elementary middle and high schools and ran our analysis on the 290 high schools in our sample that are public non charter 
as mentioned previously social scientists have attempted to statistically observe the correlation between school funding and student performance using unsophisticated methods such as ordinary least squares linear regression 
we hope to model this relationship using more sophisticated machine learning techniques 
after developing a suitably accurate closed predictive model we apply continuous intervention to attempt causal inference to isolate the effects of key input variables 
before attempting to fit predictive models to our dataset we visualized our data to see observationally if a correlation could exist 
to visualize such a high dimensional feature space in three dimensions we reduced our exogenous feature space to r 2 using principal components analysis on only those features unaffectable by policymakers 
a principal components analysis for data visualization
for example a feature such as classroom size would not be included in pca since we presume that classroom size could be affected by spending 
rather we only considered demographic features perceived to be outside the scope of increased scholastic funding we plot the two principal components of the demographic data along with average spending per pupil against one output metric attending college from
we began by benchmarking performance against the model utilized by many of the existing studies linear regression and achieved similar r 2 performance to that of the studies 
after observing significant training validation set performance disparity we attempted to reduce variance using regularization in ridge and lasso regressions 
b closed predictive models to predict student performance
though we maintained similar training set performance we saw marked improvement in validation set performance 
given the high relative dimensionality of the feature space with respect to the number of training examples we then experimented with several different ensembled tree based models on which we elaborate below 
we considered and tested more complex models including support vector machines and fully connected neural networks though abysmal performance on these models indicated an insufficient number of training examples for effective use of svms or nns 
models were implemented in python using a combination numpy and sklearn libraries
our most accurate models fell under the broader category of tree based models 
we experimented with a number of ensembling methods and combinations thereof including bagging and boosting 
tree based models
we sought to find robust models and prevent overfitting 
models included xgboost tree regression random forrest adaboost tree regression extra trees regression and bagged xgboost tree regressionwhich ended up being the most successful model of those discussed hyperparameter tuning increased model performance and reduced overfitting 
we capped tree depth in randomforest to 4 which prevented overfitting while remaining complex enough to predict accurately 
extra trees regressor extremely randomized trees performed best with a maximum tree depth of 5 and 120 estimators 
adaboost a metaestimator that uses errors in current predictions to slightly adjust tree weights performed best using 68 estimators 
xgboost with a tree depth of 3 achieved exceptional training performance but overfit badly on the validation set 
bagging multiple xgboost trees reduced overfitting and increased validation set performance 
across all bagged tree models we found greatest performance using approximately 100 trees we delve more thoroughly into the performance differences between various models in the results section 
using our most successful model bagged xgboost we used a technique known as continuous intervention causal inference concretely we find the slopeb 1 in the linear model p y x b 0 b 1 x which minimizes the cost functionusing data points generated by m in the small interval around x j0 
we interpret this slope as the direct causal relationship between an independent parameter and the output metricthe degree to which the output metric is affected by the input feature in question 
c derivative estimation and causal inference
we were able to observe significant gains in predictive accuracy using more complex models against the baseline of linear regression 
however even with more accurate predictive models we still observed weak correlation between government spending and student performance in line with existing studies on the matter 
we chart the performance of each model on its training set and an unseen test set 
in general tree based regression models significantly outperformed other regressive models in r 2 score defined aswhere is the model s prediction y is the ground truth output and is equal to e y 
a predictive model performance
thus a model that simply predicts the expected value of y would achieve an r 2 score of 0 0 and a model that predicts perfectly achieves an r 2 of 1 0 
note that using this version of r 2 r 2 y 1 a model can be arbitrarily bad thus generating an r 2 0 
we ran each of our models using four different ground truth metrics and we present the results for each below in table i using the college progression output metric we saw best test set performance with a bagged xgboost tree regression model which achieved an r 2 score of 0 67 a 0 24 improvement on linear regression in table ii using the graduation rate output metric we saw best test set performance with an extra trees regression model which achieved an r 2 score of 0 65 a 0 19 improvement on linear regression in table iii using the composite mcas score metric we saw best test set performance with bagged xgboost which achieved an r 2 score of 0 75 a 0 10 improvement on linear regression in table iv using the composite sat score metric we saw best test set performance with bagged xgboost which achieved an r 2 score of 0 86 a 0 08 improvement on linear regression 
output metrics this is likely due to drastic differences in metric scale relative to the graduation rate and college progression metrics which are measured as percentage values between 0 and 100 
composite sat scores by contrast fall within the 1000 2400 range 
we saw statistically significant performance improvements against linear regression in all output metrics though the most substantial improvement in r 2 value occurred using the college progression metric using bagged xgboost tree regression 
using the most successful model for each output metric we proceeded with the previously discussed causal inference model 
we conducted this analysis for all combinations of critical input features average expenditure per the approximated derivative for each of these combinations for a single school athol high school athol mass appears in table v below before analyzing these results we note that since our predictive models tended to have high variance and lessthan ideal performance the causal inference results should be taken as a proof of concept and not as definitive 
b causal inference results and implications
that said we do notice some interesting trends that corroborated results of social science research most interestingly average classroom size generally had weakly positive correlation with output metrics 
this runs counter to our initial intuition as we hypothesized that smaller classrooms would increase student performance 
in addition we notice weakly positive correlations between teacher salary and three of the four output metrics 
coupled with the average class size results for this school the recommended allocation of funds would be to hire better teachers at higher salaries as opposed to simply hiring more teachers to reduce classroom size 
we found that there were substantial gains to the status quo which used linear regression to be made in the accuracy of models predicting student outcomes by using more sophisticated models 
in particular tree modelsled by xgboost with bagging outperformed all other models 
bagged xgboost had r 2 values 0 24 higher for predicting college attendance 0 10 higher for predicting mcas scores and 0 08 higher for predicting composite sat scores compared to the linear regression baseline 
extra trees regression performed 0 19 better than linear regression in predicting graduation 
we hypothesize that these models were higher performing because they better incorporated the interactions between different variables a key factor in the complicated task of predicting school wide achievement we used these more informative models to perform causal analyses of the effect of slightly changing expenditures per pupil student teacher ratio and average teacher salary for all schools in our sample 
in a comprehensive literature review performed by eric hanushek 66 82 and 73 of studies of these respective effects have statistically insignificant relations in the future we would like to incorporate other states school data into our analysis 
building a framework to normalize test scores and researching laws to ensure that variables were comparable was outside the scope of what we could achieve in this project but we hypothesize that great gains to accuracy could be made with a larger sample set of schools 
we also believe that having year over year funding and test result data which we could not find for the schools in our initial dataset could provide additional insights into progress made by schools 
further with more accurate predictive models and more data we would like to solve the constrained optimization problem of reallocating existing education dollars to achieve whatever education goals the state may have which we attempted but found intractable given the time constraints of this project 
finally we could extend this analysis to elementary and middle schools rather than limiting it to high schools 
most discussion of project topics datasets and goals was between all three of us as a group 
however given each member s unique skills we often worked in complementary manners 
in all arenas different group members assisted their teammates with advice and debugging assistance isaac kasevich was primarily responsible for creating the data pipeline 
he cleaned the inputs and linked zip code data to specific school information 
he created the general framework by which any model we used was trained tested and validated 
isaac implemented the pca analysis and data visualization 
he also assisted with using the causal inference framework on a bevy of different models for explanatoryresponse pairs 
additionally isaac explored using convex optimization for budget optimization given our modelswhich did not make it into our paper zane kashner created the causal inference framework 
he created the process by which we estimate the local effects of small changes of certain inputs on a variety of measures 
zane played a part in the data process generating new features and with cleaning 
additionally he found the datasets that we used for both school level and zip code level information 
zane also created the framework for a two staged model linking spending to inputs dependent upon spending which did not make it into our paper ethan oro was in charge of implementing a number of models as well as the corresponding hyperparameter optimization 
in addition to this role he was the team member responsible for scraping the zipcode level data 
he also headed up the literature review of the existing research in this area 
ethan also investigated integrating illinois school data into our analysis which is not yet integrated in our analysis 
composing music is a very interesting challenge that tests the composer s creative capacity whether it a human or a computer 
although there have been many arguments on the matter almost all of music is some regurgitation or alteration of a sonic idea created before 
thus with enough data and the correct algorithm machine learning should be able to make music that would sound human 
this report outlines various approaches to music composition through naive bayes and neural network models and although there were some mixed results by the model it is evident that musical ideas can be gleaned from these algorithms in hopes of making a new piece of music 
this report will explore the various ways in which a computer can be taught to generate music 
having learning algorithms as a creative aid will offer great help for those seeking inspiration and innovation in their music writing 
this report will be approaching music generation in four ways one through a simple naive bayes algorithm and the others through neural networks specifically a vanilla neural network an lstm rnn and an encoder decoder model rnn 
for each algorithm we will utilize different approaches to data organization and music creation 
the naive bayes and the vanilla neural network will organize the notes temporally where a song is outlined by the specific time interval in which a note is played as each note will have a start and end time within a given piece 
for the lstm model a song will be outlined based by new note events where every time there s a new note a new vector is created 
the model also takes in one note at a time and outputs one note at a time 
for the encoder decoder model the song will be organized by chords where a song consists of just a series of chords with varying lengths 
moreover the encoder decoder model will take as input a sequence of notes and output a sequence of notes 
the input to our algorithm will be a note or a series of notes from a midi file 
we then use a neural network recurrent neural network an encoder and decoder recurrent neural network and a naive bayes approach to generate a new sequence of notes with the aim of making a good piece of music to test how successful the music generation is from the neural networks we will evaluate the prediction of the next note against the actual next note as a percentage score 
problem definitions
for the naive bayes approach however we will compare the generated music to a random flat distribution of notes 
we will therefore need to ask peoples opinions on to what extend our model improved the quality of the music 
this is a slightly unscientific approach of evaluating music but then again that is the nature of music 
there have been many attempts to make music using machine learning 
one of the first attempts made use of markov chains and transition matrices that define the probabilities of certain notes being produced 
this work was carried out by iannis in the past few years there has been an increased interest in machine learning created music 
ranging from aiva 1 which focuses on making complete soundtracks using ai to the magenta research project 11 which touches of many different aspects of music production in addition to these rather large scale projects on music the more successful approaches in music creation have been results of neural networks 
there have been smaller projects looking particularly at the midi build up of music 
daniel johnson managed to make a simple piece of music using a specially designed recurrent neural network rnn 7 and rnns have repeatedly been shown to give good and useful results in music generation 
drawing inspiration from these works we will also apply rnnalgorithms to model sequences of music moreover a recent transformer model has done very well in creating music that most resembles actual human performance 
a recent study by there have been far fewer attempts at making music using classification algorithms such as naive bayes and they show variable results 
the approach is usually combined with numerous strong assumptions and is thus not necessarily as versatile as the rnns 
10 
we had two main ways of obtaining data for our processing 
the first was to use free downloaded files of midimusic and then use music21 4 to decompose the files 
for the encoder decoder model that we ve implemented we focused on mainly classical piano music from the website www piano midi de because of the availability of midi files for classical music and the fact that the music for classical music is more standardized than that of another genre 
piano works from a variety of different composers such as bach brahms beethoven and mozart were used for a total of 771 songs 
we processed all the midi files using the music21 python package 4 and for matlab we used a similar written package 15 
each song was converted into a collection of chords where all the notes were chordified or in other words compressed into a single chord for each time step 
in other words at every new note event i e 
a new note was played a new chord will be created to result in that change 
moreover each chord was represented as a multi hot vector of length 219 
128 for the number of possible of midi notes 64 for the possible pitch duration which was derived from the given data 8 for the possible numerators of the time signature 4 for the possible denominators of the time signature and 15 for the possible key signature represented as number of flats positive or negative 
thus a song might consist of 5000 chords with varying note duration which is our note representation as previously mentioned contrasted to the temporal representation 
a total of around 700 songs were used for a total of around 400 000 notes and a standard 80 10 10 training validation test split was used for the lstm model the dataset consisted of 24 of romantic composer frederic chopin s etudes from https www classicalarchives com for a total of 57 minutes and 25 seconds of music and 22 290 notes 
due to the structure of piano music we decided to process notes or chords if there were multiple notes and rhythms of the both the left and right hand at each time stamp to train our model 
aggregating non classical music for the naive bayes and neural network model proved to be difficult for various different reasons ranging from copyright issues to general availability 
thus we have referred to another method for collecting data 
as outlined previously we have utilized four models of naive bayes vanilla neural network lstm rnn and the encoder decoder rnn 
we have named this a naive bayes like approach as only uses some of the fundamentals from the naive bayes we know 
in this algorithm we look at each press of a note as an independent variable even though we know that they are not 
naive bayes like
the purpose of this model is to make a distribution for which keys are pressed for a given chord 
therefore this algorithm will consist of two parts 1 
classify chords this was achieved by looking at the notes played by the left hand on a piano we made a program that makes a dictionary of all the unique combinations of three consecutive notes played by the left hand 
1 2 
classify notes now that we know the chords in a song the program will run through many pieces of music and find which notes were played by the right hand for a given chord in the dictionary 
after multiple songs we have generated a comprehensive distribution of p note chord 
we implemented a basic neural network nn for pattern recognition in music generation 
using a similar method to predicting the next word typed by a user in a text program we wanted to predict the next note played in the sequence 
therefore we gave the nn a vector representation of the previous note played its key signature the start and duration of the note as well as the previous 100 notes played a total of 104 entries 
we fed this into a neural network with 1024 neurons and asked it to predict the most likely note played among the 88 different possibilities 
as we will see in the next methods there is no direct memory component with this neural network but having the previous 100 notes as inputs serves as a proxy to the memory methods in the subsequent methods 
the most popular method for music generation the lstm helps us solve the problem of the regular neural network lacking memory or knowing how to relate or figure out data sequentially 
the main features of the lstm rnn compared to the regular rnn are the intput output and forget gates 
corresponding to the leftmost part of the center diagram in
lstm
to further explore the rnn architecture we have also implemented the encoder decoder model that is often used in natural language processing 2 
while still in essence an rnn the main difference is that it is a many to many model 
figure 5 encoder decoder model diagram
in other words this is a model that takes in a sequence of data and outputs a sequence of data 
the encoder takes in the input and translates it through the decoder for an output 
moreover instead of focusing on a very long term memory the memory of the algorithm is limited to the sequence that we feed into it 100 notes in this case 
we have also used gru layers instead of lstm layers the gru and lstm models are similar except that the gru has a reset r and an update gate z instead of the input output and forget gates of the lstm 
the forget and input gate are combined into a single update gate while the reset gate determines how to combine previous memory with the new input 
in terms of overall structure they are very similar but the gru is a much simpler model than the lstm and has shown to be quicker to train 12 moreover we used the approach of having each song be a collection of notes 
thus compared to the naive bayes model the data is not discretized by time and unlike the lstm model the data is not organized by new note events 
we will also use more features compared to the lstm model as the data will include information about key signature and time signature 
although it is likely that the neural networks will be able to learn the ideas of
we downloaded a pirates of the caribbean 9 film music compilation and found it had close to 10 000 notes played 
feeding this through our artificial neural network hilariously termed depplearning for the occasion it was trained over 115 iterations 
vanilla neural network
using 15 of the data for validation and 15 for testing the neural network showed the following development the network showed a 5 error on the training set but a 88 error on the test set which is a sign of overfitting 
when using the network to make music we heard that the same notes were repeated unnaturally many times and the song had no weight on harmony 
it is as if the network gives up in finding any pattern and instead returns the most played note in the selection the way we would generate the song after having trained the network is to keep the timings start time and duration of the original notes in the song and then only change the note played 
this further means that we have completely ignored the fact that more keys are played at the same time which results in a bigger error for the model 
if we would continue this project further then this would have been an important area of focus to qualitatively assess the the music itself we conducted a music turing test survey 
we let survey participants decide whether or not each generated piece of music by the algorithm sounded human or computer generated 
we also asked participants what they thought of the composing complexity of each song 
47 of participants thought that the music was human generated while 53 of participants thought that the composing level was intermediate or advance 
this is likely due to the model overfitting on the original song which resulted in a song that sounds familiar with the original song with a few differences 
to test this model we chose music with comparatively few utilized chords 
virginia which had close to 5 000 notes through the song showed a total of 40 different combinations of left hand progressions with four of these being very over represented 
these are the four chords that are repeated over the entire song 
after making the distribution over the notes we generated a new song where for each chord in the song the algorithm would pick notes from the probability distribution 
to test the accuracy of this model we compared the predicted key signature of the note to the actual key signature and we could obtain an accuracy of over 20 
however the point of this algorithm is to consider the harmony of the generated music 
this song sounded better in the form that we could hear the harmony a lot clearer a lot more than we would see in a flat distribution 
in our survey results 47 of participants thought that the music was human generated 
93 of participants thought that the composing level was intermediate or advanced 
this is again likely due to the overfitting of the model 
the training and evaluation loss dropped substantially in a similar pattern in first 20 epochs and finished at around 5345 and 8297 respectively after 100 epochs 
in parallel the average test loss was approximately 7986 
figure 8 lstm model loss
looking at the resulting sheet music from the algorithm qualitatively it is very difficult to determine whether or not the music was generated through an algorithm 
although there seems to be repetitions of certain melodies the notes were slightly offset so they are not exactly similar 
with regards to the survey results the lstm model had the most interesting results 
figure 9 music generated from lstm algorithm
only 36 of participants thought that the music was human generated but 89 thought that the composing level was intermediate or advanced 
also most people commented that they were able to distinguish the generated music because it was lack of dynamics 
from what we gather this indicates that the people thought that the music was relatively complex and coherent with typical musical motifs harmonies and melodies but the music lacked a certain human quality 
it would be interesting to study in future work what qualities in music lead people to think it to be more human 
the encoder decoder model was implemented on pytorch 14 
batch size of around 100 chords was used which is around 40 measures or so in a typical song 
encoder decoder
the encoder and decoder both consist of two grus with 512 hidden neurons each followed by a dropout layer with 5 drop rate to prevent overfitting 
however the decoder model uses teacher forcing with rate 5 to aid and speed up training 
the output of the last dropout layer in the decoder is then passed through a linear and a sigmoid layer that outputs the probability of each the occurrence of each pitch value note duration time and key signature loss was calculated through binary cross entropy loss where both the beginning of sequence and end of sequence tokens were removed to have sensible calculations 
the algorithm was optimized by the adam algorithm 8 with a learning rate of 1e 5 
the low learning rate and gradient clipping was utilized because we experienced exploding gradients in testing our algorithm the average training loss went down significantly throughout the epochs and lingered around 04473 but the average evaluation loss plateaued rather quickly and slowly started to increase in later epochs hovering around 7212 
this was closely aligned to the average test loss at 6773 
the initial dip in the evaluation loss shows that the algorithm did learn a little bit but the plateau indicates that there was a rather quick peak in how much the algorithm could learn and running more epochs would only result in over fitting 
the middling evaluation loss is evident in the music which consists of very repetitive sounding notes 
music was created by feeding sequences
in conclusion among the four algorithms that we tested the lstm rnn model performed the best 
coupled with organizing the data through new note incidents the lstm model was able to string together musical motifs with coherent melody and harmony 
although naive bayes and the neural network produced very musical ideas in song it was largely due to over fitting the data 
for the encoder model due to the discretization of the data most of its results were very repetitive notes that made some harmonic sense but was mostly unmusical 
in the future with more time it would be interesting to explore more memory focused models such as the transformer model while also incorporating note velocity from human recorded midi files to bring more life into the generated music 
with regards to the models simen worked on the naive bayes and the vanilla neural network jay worked on the lstm rnn and david worked on the encoder decoder model 
simen did majority of the work in constructing the poster and writing up the outline and milestone while david helped synthesize the write up final report 
jay conducted the survey on generated music and accumulated the results 
all code can be found in https www dropbox com sh ttzb502hheo9fst aacx3uqse csxnyp6bvamqzza dl 0 and you can listen to our generated music in these links https docs google com forms d 1w3rfi kevqqdzgk1cnvogcrlmhzajhhe6hcofkrkuxw viewform https bit ly 2zzsggd 
code and music
we present a hybrid siamese adaptation of the bi directional long short term memory bi lstm network for labelled data comprised of pairs of variable length sequences 
our model is applied for the purpose of auto grading of short answer questions 
we assess semantic similarity between the provided reference answers and the student response to that particular question 
we exceed state of the art results outperforming handcrafted features and recently proposed neural network systems of greater complexity 
for these applications we provide word embedding vectors to the bi lstms which use a fixed size vector to encode the underlying meaning expressed in a sentence irrespective of the particular wording syntax 
after this the time sequenced output of bi lstm layer is passed through an attention layer to give importance to different words of the sentences 
finally a fully connected layer is proposed to measure the similarity between the word vectors 
short answers are powerful assessment mechanisms 
many real world problems are open ended and have open ended answers which requires the student to communicate their response 
consequently short answer questions can target learning goals more effectively than multiple choice as they eliminate test taking shortcuts like eliminating improbable answers 
many online classes could adopt short answer questions especially when their in person counterparts already use them 
however staff grading of textual answers simply doesn t scale to massive classes 
grading answers has always been time consuming and costs a lot of public dollars in the us 
with schools switching to online tests it is now time that the grading also gets automatic 
in order to achieve this we start in this project by tackling the simplest problem where we attempt to make an machine learning based system which would automatically grade one line answers based on the given reference answers a typical example of the problem is as below question you used several methods to separate and identify the substances in mock rocks 
how did you separate the salt from the water 
the water was evaporated leaving the salt 
student 1 response by letting it sit in a dish for a day 
ref answer 
incorrect student 2 response let the water evaporate and the salt is left behind 
correct 
comparison of sentence similarity is a significant task across diverse disciplines such as question answering information retrieval and paraphrase identification 
most early research on measurement of sentence similarity are based on feature engineering which incorporates both lexical features and semantic features 
research has been carried around wordnet based semantic features detection in the qa match tasks and modelling sentence pairs utilizing the dependency parse trees 
however due to the excessive reliance on the manual designing features these methods are suffering from high labor cost and nonstandardization 
recently because of the huge success of neural networks in many nlp tasks especially the recurrent neural networks rnn many researches focus on the using of deep neural networks for the task of sentence similarity 
we chose the publicly available student response analysis sra dataset 
within the dataset we used the scientsbank part of the dataset 
this dataset consists of 135 questions from various physical sciences domain 
it has a reference short answer and 36 student responses per question 
total size of dataset is 4860 data points 
ground truth labels are available in the dataset whether each student response is correct or incorrect 
data pre processing including tokenization stemming and spell checking each of the student responses 
we used the pre trained glove embedding trained on wikipedia and gigaword 5 with 400k vocabulary and 300 features 
we split the dataset as follows 80 train 10 validation 10 test data 
we divided the auto grading task into 2 parts namely grading answers of already seen questions given a reference answer and grading answers of unseen questions given a reference answer 
the 2 nd case is of course more complicated as the algorithm hasn t been trained on the student responses for that question and is only working on the provided reference answer 
milestone summary
for the first case
our model composes of two sub models sentence modelling and similarity measurement 
in sentence modelling we use siamese architecture consisting of four sub networks to get sentence representations 
framework
each sub network also has 3 layers namely word embedding layer bi lstm layer and an attention layer 
in the similarity model we use a fully connected network and logistic regression layer to compute the correctness of the student response 
the complete model architecture is shown in figure 2 hybrid siamese network
the sentence modelling part is a process of getting a fixed length sentence vector from individual word vectors 
the aim is to get a sentence vector which can help in sentence similarity assessment 
sentence modelling
the word embedding layer maps every token of the sentence to a fixed length vector 
the size of the vector in our model is 300 which are pretrained glove vectors obtained from training over wikipedia and gigaword 5 vocabulary 
embedding layer
take sentence x x 1 x 2 x t as an example 
lstm updates its hidden state h t using the recursive mechanism asthe lstm also sequentially updates a hiddenstate representation but these steps also rely on a memory cell containing four components which are real valued vectors a memory state c t an output gate that determines how the memory state affects other units as well as an input and forget gate it and h t that controls what gets stored in and omitted from memory based on each new input and the current state where we obtain the final vector h i by concatenating the hidden states of both the layers 
bi lstm layer
thus a final concatenated vector is passed into the attention layer 
the attention mechanism can calculate a weight a i for each word annotation h i according the importance 
the final sentence representation is the weighted sum of all the word annotations using the attention weight 
attention layer
the similarity measurement model functions as a binary classifier for the learned sentence embedding 
our model is an end to end model which means that sentence modelling layer and the similarity measurement model can be trained together fully connected layer each output of our sentence modelling layer is a fixed size vector 
similarity measurement
we pass each of the student response reference answer pair into the fully connected layer to measure the similarity between them 
in this way we have 3 fully connected layers outputting 3 vectors for the pair wise similarity with the student response 
to evaluate the performance of our model we chose two metrics namely accuracy acc and mean square error mse 
a threshold of 0 5 is used on predicted probability for assigning the final labels 
assesment and loss function
for each sentence pair the loss function is defined by the cross entropy of the predicted and true label for training where y is the true label and is the output probability for correct response it is most easily interpret able as well as an apt choice for our task which is very similar to a classification task 
the hyper parameters used in our model were adapted from each model was trained for 50 epochs and batch size 16 
softmax activation function was used in the attention layer 
lstm was initialized with normal weights 
though we built various models permuting with cnn lstm bi lstm attention layer fnn and manhattan distance 
some of our best results are summarised in the table below 
our model is the best result obtained from these and it s architecture has been described above 
our hybrid siamese model achieved the highest accuracy 
the credit for this success can be given to the knn intuition 
the observation that correct answers given by student are very similar to correct answers given by other students has helped in achieving this increased accuracy 
also it can be seen that the attention layer creates a large increase in the accuracy of the models as compared to the ones without accuracy 
the ability of the attention layer to identify the weightage of each word according to its importance in the reference answer 
we studied the cases in which the model was misclassifying the student answers 
we found that there were two main causes for misclassification 
the model misclassified cases where the difference between the length of the student answer and reference answers was large 
we tried to overcome this by replacing the final fully connected layer with a cosine similarity measuring layer 
length of student answer
this led to lower accuracies 
therefore the fully connected layer is better than cosine similarity but we need to change some properties of the layers to get better results 
we believe that this problem can be solved by using a different attention layer which will enable the algorithm to remember the important words for longer time intervals 
next up we observed the model misclassified answers which were missing the keywords from the reference answers 
these student answers seem similar to the reference answer when we read it but are misclassified by the algorithm 
issue of key words
this could be a result of the attention layer giving extra weight to the keywords and not being able to identify a phrase which means the same as the keyword 
the example of the same is shown below modifications must be implemented in the attention layer such as changing the activation etc to make it more robust question what is the relation between tree rings and time 
our hybrid model with the intuition of knn beat all the other models on our dataset 
building upon our learnings from this project we would like to expand the analysis by training on run it on a larger unseen and out of domain dataset to gauge its robustness 
during the poster presentation we talked with a researcher who was interested in providing us with a much larger dataset 
we would also like to address all the issues we observed with our current model 
we will be trying out different attention layer to smooth out key word issue 
we would also consider adding better reference answers or better similarity detection mechanisms in the future 
we worked on each part collaboratively and didn t explicitly divided the tasks 
we both had equal contributions to literature review data collection writing code and report preparation 
this is the link to our code in github repository click here to access the github code
github link
nowadays on the internet there are a lot of sources that generate immense amounts of daily news 
in addition the demand for information by users has been growing continuously so it is crucial that the news is classified to allow users to access the information of interest quickly and effectively 
this way the machine learning model for automated news classification could be used to identify topics of untracked news and or make individual suggestions based on the user s prior interests 
thus our aim is to build models that take as input news headline and short description and output news category 
our data source is a kaggle dataset the data preprocessing consisted in combining some raw data categories that are very close for example arts and arts and culture education and college etc 
the hugh grant marries for the first time at age 57 
the actor and his longtime girlfriend anna eberstein tied the knot in a civil ceremonyin the following work we decided to only consider samples with description s size greater than 7 words 
moreover categories comedy and weird news were removed from the consideration 
all this preprocessing left us with total number of samples 113 342 and 25 news labels 
last step of preprocessing included removal of stop words as well as punctuation and finally stemming of each word 
first using the preprocessed news descriptions we created the dictionary of words 
the total number of unique words is around 40 000 
then we extracted the following word features for classification task word binary and word count features for binary and count features we used first 5 000 most common words to define the dictionary and then encoded the news descriptions as vectorseither as vectors of 0 and 1 for binary features or of word counts in the description 
word level tf idf scores for tf idf method we decided to extend the dictionary to the first 10 250 most frequent words 
moreover we combined the text from all the news belonging to that category and treated it as the one document 
thus our corpus of documents consisted of 25 documents one for each news category from which we learn tf idf representation and then we apply it both to train and dev set samples word embeddings word embeddings are a family of nlp techniques aiming at mapping the semantic meaning into a geometric space
in the first part of our work we experimented with traditional machine learning techniques naive bayes multinomial logistic regression kernel svm and random forest naive bayes with binary features we applied multivariate bernoulli model and with count features multinomial event model 
for each example we classify as arg max y p y n i 1 p x i y where we use map estimation for p y and p x i y while also applying laplace smoothing multinomial logistic regression we use the cross entropy loss with l2 regularization kernel svm we use a multi class svm random forest we used the gini measure g x m k p mk 1 p mk where p mk is the proportion of class k samples in node m 8 
we regularized each tree in terms of maximum depth in the second part of our work we focused on building the neural network models with word embedding features provided by the embedding layer of keras we trained several neural network models with one or two convolutional layers cnn and or recurrent lstm layer rnn cnn this a class of deep feed forward artificial neural networks that excel at learning the spatial structure in the input data by learning the set of filters applied to the data rnn this is a class of artificial neural network where connections between nodes form a directed graph along a sequence 
this allows it to exhibit temporal dynamic behavior for a time sequence 
we divided the data into train dev test split according to 80 10 10 
lastly we trained several neural network models 
for all models we observed that they quickly start to overfit the data often there is some combination of categories present in one news though it has just one true label in the dataset 
example 1 australian senator becomes first to breastfeed on parliament floor we need more women and parents in parliament said larissa waters 
here the true category parents was confused by the models with world news probably as it mentions australia and senator but the news is also about parenthood 
example 2 most u s 
troops kicked out for misconduct had mental illness 
the new report will likely add to scrutiny over whether the military is doing enough to care for troops with mental health issues this news belongs to the healthy living category whereas the models identify it as politics likely because the news mention us troops and military however it is mainly about health issues overlap of different categories we believe this may be due to the subjective assignment of the category upon news publication 
example how do scientists study dreams 
dreams are a compelling area of research for scientists in part because there s still so much to learn about how and why we dream 
this news belongs for some reason to healthy living category though it mentions a lot about scientific research so there is no surprise that all models identify it as category science thus often the model is able to understand some topic of the news but not may be the main onesometimes the true topic is more subtle or even implicit but there are some words in the news that are characteristic of other categories and as a result the model classifies it incorrectly 
these observations motivated us to compare top three labels predicted by each model to the true label of the example these results also shown in
application of tf idf method allowed us to select for each news category the words that are characteristic of this category 
then we extracted pre trained glove embeddings
visualization of word embeddings
we have built a number of models to predict the category of news from its headline and short description using methods both from traditional ml and deep learning 
our best model ensemble of four nn models achieves on the dev set 68 85 accuracy if considering top 1 label and 88 72 if considering top 3 labels predicted by the model 
it is interesting how this news dataset is extremely hard to classify for even the most complex models 
we attribute this to the subjectivity in category assignment in the data 
however in the future work we may also try to apply character level language models based on multi layer lstm or learn embeddings for the whole news descriptions as in doc2vec 
word vectors are typically computed by implementing distributional statistics but these word vectors cannot represent unknown words 
the ability to integrate word definitions with distributional statistics to create hybrid word vectors has the potential to improve performance on out of vocabulary tasks 
a baseline bag ofwords and sequence to sequence auto encoder were first iterated upon to obtain definitional word vectors that capture complementary information to distributional word vectors 
use of a sentence variational auto encoder to compute word embeddings was also explored 
preliminary results suggest that a combination of distributional vectors glove embeddings and definitional word vectors produced from an autoencoder provide an improvement for neural machine translation and warrants further testing 
word vectors are typically calculated as distributional statistics we take word definitions as inputs into various autoencoder models discussed in section 3 and use the hidden layer of the autoencoders as dense word embeddings of the definition 
these embeddings are used to test against standard benchmarks to understand the information captured and can then be combined with existing distributional word vectors in varying degrees to improve the performance of downstream nlp tasks 
there have been a number of prior attempts at deriving word vectors from dictionary definitions 
neural models using dictionary definitions and character level morphology this project is a continuation of previous work completed by andrey kurenlov tony duan aneesh pappu and rohun saxena at stanford university 
prior work began by building def2vec a mapping of embedded word defintions into a semantically meaningful space comparable to that of pretrained glove embeddings 
the def2vec team demonstrated the utility of def2vec in improving the performance of a neural machine translation model when the pre trained vectors vocabulary is limited however the def2vec team realized that definitional vectors alone are unable to perform as well as the highly robust distributional vectors in the nmt system this motivated the introduction of combined distributional and definitional word vectors hybrid distributional and definitional word vectors 
including both types of representation can capture different aspects of a given word s meaning and the integrated performance may outperform either individual model recently tom bosc and pascal vincent published their successful results on training a consistency penalized auto encoder cpae to capture semantic similarity better than distributional and current definitional word vectors 
the model uses a lstm to process the definition of a word and create a word embedding which is then used by the decoder to reconstruct a bag of words representation of the definition 
as a continuation of previous work we initially focused on two of the existing models in creating definitional embeddings an lstm baseline model which is composed of a multi layer lstm encoder and a simple conditional language model decoder with each output trained by cross entropy loss based on 1 hot vector over the entire vocabulary and softmax output can be seen as a simple classification problem and a normal seq2seq sentence autoencoder model with both encoder and decoder as configurable recurrent neural network 
after obtaining different word embeddings separately the intrinsic evaluation is completed as a series of word embeddings benchmarks comparing the lstm baseline model seq2seq model and existing glove word embeddings we finally applied our learnt word embeddings in combination with pretrained glove vectors to form our hybridvec embeddings and evaluated using opennmt as our extrinsic evaluation system 
we also explored the possibility of utilizing a more advanced variational autoencoder vae model in creating definitional embeddings 
the sentence vae is based off the decoder part will have two types of inputs the hidden state from the encoder and the sequence of output word embeddings corresponding to the word definitions 
it is a simple conditional language model where each predicted word is learnt through normal classification methods using softmax dimensional one hot vector and cross entropy loss 
for each definition of d in defs w the cross entropy is given by the total loss of all word definitions including multiple meanings of the same word is just the negative summation over all sentences it can also be interpreted as negative log likelihood loss nll in our approach we used different input and output embeddings which could have caused the model to overfit 
a unique word embedding matrix is an alternative way of implementation which can be explored in future experiments in order to minimize the distance between the definitional embeddings and the learnt word embeddings a penalty weighted by is applied on the l2 norm between the predicted word embeddings and the learnt word embeddings which gives the final loss function as v denotes the input embedding associated with word w if the penalty is large then after optimization we will end up with v very near to 1 3 in euclidean distance which makes the definitional word vector hold very similar meaning to the defined word itself 
the second model we explored to create word embeddings takes the form of a seq2seq autoencoder sae that respects the initial syntactic structure of the sentence 
given an input word w we look up its definition d w 
seq2seq autoencoder
each word of the definition is encoded through an embedding layer trained from scratch and then run through a 2 layer lstm encoder without attention to produce the dense representation h that represents the definitional embedding 
in the decoder another 2 layer lstm is applied 
the training loss minimizes the negative log likelihood between the predicted definitional word and the ground truth definitional word d for every position in the definition thereby constraining the definitional embedding to also learn the relative syntactic placement and relationships of the words in the definitions 
we only evaluated this model intrinsically because of its poor capability in representing the word meaning effectively 
our approach for machine translation is another seq2seq model with attention implemented through harvard s open source opennmt project
neural machine translation
for definitions we incorporated the datasets of previous work and employed data from the wordnet database lastly for the nmt task we used both the default 10k demo english german opennmt corpus and the yandex 1m english russian corpus which has one million aligned english and russian sentences
both the lstm baseline and seq2seq models are trained with the word vector dimension of 300 and hidden layer dimension of 150 such that they are comparable to glove 300d word vectors 
we implemented our model in pytorch paszke et al 2017 and trained using the adam kingma and ba 2014 optimizer for 20 epochs with a learning rate of 0 0001 and a batch size of 64 
training
the training was consistent with the prior work done in evaluating definitional and hybrid word vectors 
we evaluate the quality of the embeddings produced from our autoencoder models by using a third party word embedding benchmark test toolset word embedding benchmark web https github com kudkudak word embeddings benchmarks 
web is focused on evaluating and reporting results on common benchmarks such as analogy similarity and categorization 
similarity and relatedness 
these benchmarks are evaluated on similarity and or relatedness datasets that contain pairs of words and human annotated scores for each pair of words 
the predictions and the ground truth are ranked and the ranks are measured using the spearman s 100 metric 
quantitatively the word embeddings benchmarks for glove lstm baseline and seq2seq model in using t sne visualizations van der maaten and hinton 2008 of the test set embedding space allowed us to qualitatively explore the model performance 
the predicted lstm baseline definitional word embeddings tended to cluster in the feature space to a much smaller number of groups when compared to vocabulary size 
this is probably because word definitions mainly from dictionaries are more rigorously defined and lack the variation that exists in example corpora definitional vectors are unable to capture different types of texts describe clich s and idioms or pick up on context clues 
future studies should train definitions from a broader text source and try and incorporate different types of texts 
the glove embeddings however make use of feature space more efficiently suggesting glove vectors can grasp more subtle meanings of words in the real word 
the purpose of extrinsic evaluations in our work is to verify how useful definitional vectors are for downstream tasks 
in order to compare different performance impacts of glove and hybridvec we used the same glove 400k words as input vocabulary to generate embeddings for all the extrinsic evaluations 
extrinsic evaluation
we trained a translation model with opennmt py on two different corpora the 10k default opennmt demo english german corpus and the 3k validation sentences to make a quick observation on different pre trained word embeddings 
these quantitative results are shown in we compared nmt performance impacts between hybridvec and glove embeddings on the yandex 1m english russian corpus which has one million aligned english and russian sentences
using different input and output embeddings is a possible cause of overfitting within our models a unique word embedding matrix is an alternative to be explored in future experiments 
more time would also allow for the ability to finish testing the vae model 
word definitions mainly generated from dictionaries are more rigorously defined and lack the variation required to capture real word semantic information 
future studies are needed to use a broader text source in the training 
due to lack of computation resources the extrinsic training has not fully converged so the full exploration of hybridvec impact on downstream tasks is not complete 
definitional embeddings are fascinating although rarely studied by researchers for their potential in capturing complementary semantic information when combined with traditional distributional word embeddings 
we aimed to explore whether encoding this different source of information could add to the representational power of current methods in embedding words our experiments showed that an lstm baseline autoencoder approach is intrinsically successful at constructing word embeddings at a level roughly similar to distributional embeddings 
however examining the t sne visualizations showed that the learned definitional embeddings for the lstm baseline model tend to cluster in a small number of groups suggesting insufficiency in representing more subtle word meanings in natural language 
furthermore by observing validation datasets in intrinsic evaluation it is apparent that the overall feature space of definitional word vectors is much smaller than glove 
further study on these clusters is to be explored in future work extrinsic evaluation on both the opennmt demo corpus and yandex 1m corpus suggests that during training the perplexity and accuracy are both better than those of glove 
however the final evaluation on 5000 validation sentences diverges 
the bleu metric of our hybridvec vector is of a lower value for lstm baseline vectors 
given both the positive and negative impact on perplexity and bleu and the fact that none of the extrinsic experiments are trained to convergence the full utility of hybridvec remains to be seen 
special thanks to andrey kurenkov for general mentorship and guidance throughout this project as well as helping review our code and documents and thanks to the stanford cs229 teaching staff 
as a group working on this collaborated project we contributed equally overall 
haiyuan mei is responsible for improving the lstm baseline model and seq2seq model by contributing to the existing code base and getting the test results regarding the two models 
ranjani iyer is responsible for the initial model understanding and working on implementation of the vae model 
the goal of our project is to explore text complexity in the context of machine learning 
more specifically we will answer the following questions 1 
what features of the text are most relevant to this classification 2 
to what extent can machine learning methods be used to classify the complexity of a document 3 
how can we build a model to generate or transform text into different levels of complexity 
this project s outcomes have the potential of enhancing education immensely 
complexity classified documents allow students to find papers or conceptual explanations at understandable difficulty level 
generating or transforming text into simpler levels of complexity encourages more widespread knowledge approachable from different fields and backgrounds 
students gain the power to understand big picture ideas and ramp up the difficulty level as they see fit ultimately resulting in a more personalized educational experience 
there has already been some success in using ml for text complexity classification 
one paper from the university of washington
related works
we are using the weebit dataset
data feature extraction and selection
to preprocess the data we removed new line characters set all words to be lowercase and removed any disclaimers 
we also split the dataset into training validation and test sets 
for the following section let x represent one example of a preprocessed text 
word count and tf idf feature extraction were completed using sci kit learn word count we used a binary and a regular word count feature extractor 
we also experimented with changing min df and max df which represent the minimum or maximum document frequencies of a word in order to be included as a feature 
empirically we found that model performance was not very sensitive to the min df and max df parameters 
however the binary word counts option substantially increased accuracy 
in our analysis we set min df 5 max df 80 and tried both binary and non binary word counts tf idf tf idf extracts the word count weighted by a measure of inverse document frequency idf 
this diminishes the importance of common words such as a and the and highlights the importance of uncommon words 
however we found that the tf idf features gave worse performance than the word count feature extractor 
one possible reason for this is that tf idf creates feature vectors which are more similar in their topic meaning than in their structure 
in our task the topic meaning of the text may not be as important as the ordering and structure of the words 
we also added features for the counts of each part of speech using spacy
natural language features
we first wanted to find which of the natural language features were the most promising candidates for complexity classification 
adaboost another successful classifier was adaboost 
basic analytics on natural language features
given the relatively high results we obtained from using only average sentence length as a feature we expected an ensemble of basic classifiers to perform much better 
after tuning the number of classifiers and the learning rate adaboost achieved 79 7 on the validation set 
in other other algorithms we tried such as naive bayes or k nearest neighbors performed better than our baseline but did not have as much initial success as adaboost and logistic regression and did not seem to fit our problem as well 
for example they made questionable assumptions of independence or modeled complex documents as clusters which did not fit with our selected features 
one key drawback in our current representation of the documents is that all sequential information is lost in the feature encoding 
in other words any permutation of a document s words results in the same feature vector 
recurrent neural networks
however sequential relationships likely play a key role in determining the complexity of a text 
we did not use sequential encodings originally because models such as logistic regression expect fixedlength inputs 
our documents were of different lengths so using sequential encodings would require us to add padding to all of our documents 
however the high variance in document length made padding difficult to address this problem we will use a recurrent neural network 
the architecture of this neural network allows for arbitrary length inputs and have been successfully applied in nlp in the past 
for this model we encode each document as a sequence of pos tags instead of word embeddings with hopes that this will both allow the model to fit the data better and also generalize to unseen texts 
since there are complex and simple texts of any given topic we hypothesize that a more important factor in determining complexity is the grammatical structure of the sentence rather than the content 
encoding 
the encoding we chose was to represent texts as a sequence of pos tags 
model architecture
we also chose to replace the punct tag with the actual punctuation used in the sentence to help distinguish commas and periods so the algorithm may can learn the difference between a series of simple sentences vs compound sentences 
the final vocabulary consists of 46 pos tags and punctuation marks 
the first layer of the lstm is an embedding layer 
this is inspired by nlp methods which typically use word2vec or trainable embedding layers to represent each item of the vocabulary 
for our case the embedding layer takes in a element of the vocabulary and maps it to a em bed dim dimensional vector which is trained using the optimization algorithm 3 
lstm the embeddings are then put into a lstm model 
the tunable parameters of this step are n layers the number of lstm layers hidden dim the dimension of each lstm layer dropout the percentage of neurons that are deactivated in each lstm layer the lstm has an output for each value in the sequence 
to get a fixed length vector for the next linear layer s input we experimented with either using the output at the final value in the sequence or using the mean output across all values in the sequence 
however neither seemed to have any noticeable effect on our results 
the output of the lstm layer is then fed into a linear layer 
the purpose of this layer is to transform the output dimension of hidden dim to 3 as we are trying to predict 3 levels of difficulty 5 
linear
softmax finally the outputs of the linear layer are fed into a softmax layer which normalizes the outputs so that they can be interpreted as the probabilities that the text was of each of the levels 
we used the adam optimizer to train our model and used a randomized grid search to tune our hyperparameters 
some high level findings were that a learning rate of 0 01 converges much faster and often does much better than a lower learning rate 
we found that increasing hidden layers made the model strongly overfit the data decreasing performance on the test set 
while dropout helped with overfitting adding more layers still appeared to lower the test set accuracy even with dropout 
our optimal parameters were n layers 1 batch size 16 em bed dim 64 and hidden dim 128 with lr 0 01 after hyperparameter tuning we gained our best result pair of 80 3 on the test set and 86 4 on the train set using the lstm 
overall the lstm did roughly 1 better than both adaboost or logistic regression on the test set 
however our lstm notably only used natural language features and was still able to obtain better results than the previous classifiers using both natural language features and word count 
the final goal of this project was to generate texts of different complexity levels 
in this project we focused on grammar and sentence document structure as the primary determinant of complexity 
text generation
we employ a similar lstm model to before but for sequence prediction instead of classification 
given the sequence of pos encodings described in the previous section the model learns p pos t 1 pos t pos t 1 
we can sample from this probability distribution to generate sequences of pos tags 
pos 1 
the hyperparameters we selected for the generation model were n hidden 100 n layers 3 embed dim 64 lr 0 0001 
we found that this model was able to adequately learn in the different difficulty levels without being over complex taking very long to train as the training loss curves are shown in
let g be the trained model v be the vocabulary d v and x be a document 
we approximate g as where x t is the t th pos tag in the document x 
sampling
to sample a sequence of pos tags we sample from g to get a vector of probabilities normalize the vector to sum to 1 using our temperature parameter and then choose tags from the resulting multinomial distribution 
our temperature parameter controls the rigidness of the sampling 
low values for temperature result in more grammatically correct sentences and higher values encourage more diverse sentence structures 
finally we attempted to substitute word values back into our generated sequences 
since we did not have a methodical way to plug words back in we opted to randomly fill them in with the given text and then verify their class using our original logistic regression classifier more time efficient than our lstm 
using 100 randomly generated pos structures of each level we took 100 random texts and inserted words of each pos type to train and then attempted to predict the reading level of 20 more unseen examples 
we found that the classification accuracy dropped to 50 6 using our logistic regression model 
one generated level 2 sentence segment using our trained lstm det noun verb det noun adp det noun adp det noun below is a sample level 2 sequence of pos tags that we filled in using a level 4 text the giants founded the team with the help of the shelter a real level 2 text excerpt with matched pos tags underlined the giants founded the dog team with the help of a local animal shelter 
example generated sentence
overall our classification algorithms categorize the different levels of complexity in the weebit corpus with 80 3 accuracy on an unseen dataset using an lstm only requiring structural features with pos tags which is a significant improvement from our baseline or relying on average sentence length alone 
moving forward we plan to use more types of texts fiction biographical etc 
and add additional features like individual word complexity in order to better understand the content of a passage for classification though we succeeded in generating text from learned examples with our second lstm our generation model had a major weakness in finding sensical ways of re inserting words into our generated pos tag sequence 
furthermore even after generating these nonsensical sentences our original classification algorithm could not successfully classify the intended sequences with significant accuracy implying that there is a deeper influence on the actual content of a passage when determining readability 
in the future we would look for better ways of substituting pos tags or we may try fixing certain pos words to remove ambiguity in filling in parts of speech thus improving readability 
as applications and websites develop the trend to provide customized service for users building recommendation systems has gain more popularity and attention from businesses to improve user experience 
while a great number of recommendation systems for movies audios books or restaurants exist surprisingly there is hardly any for dishes 
as food lovers we intend to address this issue by exploring models for recommending chinese dishes we scrape data from a popular chinese recipe website app called xia chu fang cooking recipe 1 and implement word embedding algorithms and recommendation system algorithms to build our model 
the original input to our algorithms are user ids from xia chu fang and dish names users have saved in their favourite list 
we first preprocess our data and use word2vec on them 
we then generate ratings and apply collaborative filtering to build our recommendation system 
specifically we explore the skip gram model in word2vec to calculate dish similarity as well as apply the non negative matrix factorization and singular value decomposition methods in collaborative filtering to predict ratings on dishes 
limiting our dishes to chinese cuisine allows us to focus on constructing a more tailored recommendation system while still maintain a high practical value given the popularity and diversity of chinese cuisines 
we hope by doing this project we can tackle a less touched topic of using machine learning for dish recommendation and at the same time promote the greatness of chinese food 
multiple methods for building recommendation systems are discussed in literature 
in the natural language processing domain word2vec is a set of methods in word embeddings that produces promising results in recommendation systems
xia chu fang is a platform where users can publicly post recipes of dishes 
they can also search for recipes from other users and put them in their favourite aka starred list if they are interested in learning them 
as one of the leading recipe websites in china xia chu fang has over 1 8 million published recipes and 100 million users with at least 1 2 million registered users
the number of total dishes is an overestimate of the true number of dishes because a lot of them are duplicates with slight variations 2 resulting in sparseness of the data 
cleaning up dish names is quite challenging given chinese dish names different encoding and the complexity of the language itself 
dish name mapping
after trials and errors 3 we decide to utilize an online database of chinese dish names with their english translations 11 as a dictionary which has 1 871 unique keys representing recipe names 
using jaro winkler distance
unlike most of the other datasets for building recommendation systems our dataset does not have users ratings on dishes 
as a result we need to generate ratings in order to implement nmf and svd 
ratings calculation
we notice from the mapped data that many users put different versions of dishes in their starred list 
marking a dish multiple times implies that the user is really into this dish such that he or she is willing to save multiple recipes for it to try out different cooking methods 
this indicates that the user is more interested in this dish than a dish that is only saved once 
we therefore utilize this property and define a user s rating on a dish as this way we generate ratings ranging from 1 to 18 
we then normalize them by casting them to a number between 5 and 10 to minimize the effect of outliers and seperate it from the traditional rating scheme
we first try word embeddings on the original data without ratings 
among the various models in word2vec we primarily focus on the skip gram neural network model heavy notations aside 8 this merely calculates the probability of the output words being in the context of the given input word 
calculating similarity using word2vec
the model also returns a a similarity score for each output word 
the format
we then explore cf on our user dish dataset with our calculated ratings and compare predicted ratings generated by the nmf and svd approaches 
collaborative filtering cf 
to perform nmf we implement the python library surprise 9 
the predicted rating that user k has on dish i i where k and i are regularization parameters for user and dish both set to a default of 0 06 16 
non negative matrix factorization nmf 
similarly we exploit the surprise library if user k is unknown then the bias b k and the factors p k are assumed to be zero 
the same applies for dish i with b i and q i 
singular value decomposition svd 
to estimate all the unknowns we minimize the following regularized squared error and we perform the minimization using stochastic gradient descent with the following update rules k is the difference between the predicted rating and actual rating that user k has on dish i 
we use the default learning rate 0 005 and a regularization factor 0 02 see 10 theoretically let a be a real m n matrix with m n then we have a u v t where u t u v t v v v t in and diag 1 n 
the matrix u consists of n orthonormalized eigenvectors associated with the n largest eigenvalues of aa t and v consists of the orthonormalized eigenvectors of a t a 
the diagonal elements of are the non negative square roots of the eigenvalues of a t a and are called singular values
given our data size we split our dataset into 80 training set 10 development set and 10 test set 
this split results in 159 053 samples in the training set 19 881 samples in the dev set and 19 882 samples in the test set 
we train our models on the training set and calculate errors on the dev set for model comparison and obtain errors on the test set to examine model robustness 
the skip gram model from word2vec calculates similarity scores of dishes which allows us to directly output the context words with highest similarity scores as our recommendations of the input dish see
word2vec results
adopting the ratings we have calculated we run collaborative filtering on our training set through nmf and svd 
collaborative filtering results
to evaluate prediction errors across models we define the prediction error as the difference between the actual calculated rating and the predicted rating for the dish i e e i k is the estimated rating that user k has on dish i predicted by our model 
and we calculate the rmse of prediction accuracy defined as 
rmse of prediction error
ideally a good model would have a small rmse 
an alternative way to calculate error is using miss and recall 
if the estimated prediction error of dish rating e i k 0 05 we define it as a prediction hit and otherwise a miss
miss and recall
we fit our models on both the dev set and the test set and obtain the following values of rmse and recall in addition the test set rmses are very close to that of the dev set for svd whereas nmf has a larger gap between its test set rmse and dev set rmse although still reasonably close 
a similar observation can also be made for recall 
errors results
this means that both the nmf and svd algorithms are fairly robust and does not overfit but svd seems to outperform nmf in both accuracy and robustness as a result svd is selected as the winner and we provide
comparing the models we think that the skip gram model tackles the issue of a cold start and directly gives us recommendations based on the input keywords and other users s dish preferences in our database 
this allows us to bypass the issue that our dataset lacks ratings 
however it is not easy to make sense of these recommendations or find a good way to quantify its errors 
unlike the skip gram model we are able to conduct an error analysis on svd and nmf 
from the results we conclude that the svd model performs better at predicting ratings since it has lower rmse on the dev set 
its test set error is also closer to the dev set error compared to nmf which means it does not overfit and is fairly robust 
therefore it is the best algorithm for our recommendation system of chinese dishes we recognize that different results may arise from different datasets so there is still a lot of room for improvement 
for future work we can explore other recommendation algorithms such as doc2vec an extension of word2vec a hybrid system that combines both collaborative filtering and content based filtering a memory based algorithm and modelbased algorithms 
implementing these models will allow us to conduct a more thorough error analysis and improve the prediction mechanism 
we can also retrieve more data from xia chu fang or even other recipe websites given the computational power to investigate model stability and robustness 
moreover we can try to acquire more user and dish features and look for rating data on dishes so that we do not have to generate our own which might introduce bias 
finally we can also create a user interface where users can directly experience the dish recommendation system we build 
all students contributed fairly equally to this group project each putting more focus on certain parts according to their specializations 
yu zeng contributed more to data scraping word2vec and matrix factorization algorithm implementations 
yiting ji contributed more to literature review svd and nmf algorithm research error analysis project management and report write ups 
yogi huang contributed more to collaborative filtering research data compiling and poster write up 
our code can be found here https github com zengyu714 food chi 
music is becoming more and more easy to access through internet and musical apps 
with the increasing amount of music available digitally online there is a growing demand for systematical organization of audio files and thus a rising interest in automatic music genre classification 
moreover detecting and grouping music of similar genre is a keen part in music recommendation system and playlist generator there are indeed widely accepted rules in music theory that help human classification such as chords and rhythmic structures instrumental arrangement etc 
however in general musical content is complex and music genres are not well defined making it a challenging machine learning problem and distinguish it from many other classification problems the goal of this paper is to improve the performance of classical algorithms including logistic regression lr gaussian discriminant analysis gda random forest rf and support vector machine svm by combining them with a dilated convolutional neural network dilated cnn or dcnn 
the methodology is structured as follows 
we transfer the raw music inputs in wave format into 2d mfcc features at each timestamp 
as a baseline the 2d arrays are flattened and feature selected through pca 
this is then fed directly into the classical algorithms described above 
to improve prediction we pass the same 2d arrays into a 2 layered 1 dimensional dilated cnn and train it for reasonable performance 
the activation of each convolution layer are extracted as inputs to the classical classification algorithms 
all outputs of the models are probabilities from a softmax layer denoting the probability of the example being in a specific genre 
music genre classification as a branch of audio and speech processing has been studied by many 
a standard approach consists of two steps feature extraction and classification most of the genre classification studies focuses on finding the best set of temporal features transformations and filters that best represent the music 
ii related works
other studies have tried to use some ai machine learning techniques such as hidden markov model to classify music genres in this study we will build on top of the works done before and see if we can improve them by combining classical algorithms with neural networks 
the dataset we used is taken from gtzan using the librosa library in python the data is pre the figures above shows a heat map of classical and metal mfcc features 
as we can see there are already some interesting but subtle differences that we can see with our eyes 
iii dataset and representations
before any training experiments the dataset is split into train and test sets with a 80 20 ratio and they will stay the same throughout the whole project 
iv models
we investigated the performance of four classical classification algorithms softmax regression sr gaussian discriminant analysis gda random forest rf and support vector machine svm 
for the equations in this section we will always use x to denote the input feature and y to denote the labels 
a classical baseline models
moreover we will use x i to denote the training examples and y i to denote the true class for x i and i as the predicted class for x i for softmax logistic regression the model iswith the corresponding cost functionwhere 1 y i k is the indicator of whether a class actually belongs in class k and i h x i is the predicted probability of whether a class belongs in k for gda the model assumes that the class labels y is a multinomial distribution with 5 values with parametersis the multivariate gaussian with the density functionthe parameters k k and are calculated by maximizing the log likelihood of the given data a comprehensive review of random forests can be found in sections 9 2 and 15 1 in is the percentage of examples in class k in the region r in our implementation we choose a maximum depth of 7 
we observed that choosing a greater depth with allow us to fit to the training data with 100 accuracy but this will also result in high test error traditionally svms work naturally as two class classifiers 
in particular we are solving the following optimization problem for the multiclass classification problem with c classes we extend the two classifier by constructing c c 1 2 classifiers for each pair of classes 
then for each x i we choose its class by setting y i to be the class that x i appeared in the most number of times 
finally we note that in practice we use a kernalized version of svm where the kernel is given by the radial basis functionand is scaled inversely by the number of features and the standard deviation of the inputs 
this will map the input features to a high dimensional feature space allowing us to generate more complex boundaries as a baseline we flattened the raw 2d mfcc arrays into 1d vectors and feed them to the 4 above described classification algorithms 
the flattened vectors are of length 25800 
for classical algorithms a 25800 dimensional input can be quite high and susceptible to overfitting 
a preliminary idea is to apply principle component analysis pca to extract the top 50 principle components 
however we must take caution when applying pca since it does not always improve the test accuracy 
classification results for both the origin input and the input after pca is compared in the results section 
after we have established our baseline model we then trained multiple dilated convolution neural networks and compared results 
the main difference between a dilated cnn and original cnn is that each input unit is seen by a filter with gaps between them 
b dilated convolutional neural network dcnn 
a simple illustration for dcnn can be found in for optimization we use the categorical cross entropy loss defined bywhere is the weights in our neural network and b is the bias terms 
we optimize over and b using the adam optimizer which combines gradient descent with momentum and rmsprop we tried many models 
lastly and most importantly we will use the dilated cnn model as a feature extractor for the classical classification algorithms having our dcnn model trained we take the activation outputs of the two convolution layers flatten them and feed them into the four classical algorithms that we described above to summarize we will use 1 flattened mfcc vector 2 pca reduced mfcc vector 3 convolution layer 1 output light green in figure iv 2 and 4 convolution layer 2 output dark blue in figure iv 2 vectors as the input features of the four classical algorithms to better visualize the these high dimensional feature vectors they are reduced into 3d using pca and plotted in in terms of raw flattened mfcc features classical blues and pop music tend to stand alone by themselves but hip hop and metal are mixed together in the middle of the data points in terms of dcnn layer 1 features pop music separates itself away from the others on the right branch 
also on the left branch hip hop music now is separated from metal music although metal music now seem even more mixed up with all the other music genres although no dicision boundary can be seen seen directly from pca of layer 2 all of the five genres has a clear tendency of clustering 
c combining dcnn and classical algorithms
moreover compared with the other two features the points are more condensed 
it is worth noticing that these observations are based only on a 3d reduction of these features 
points could be well separated in higher dimensions even if the points seem not clearly separated apart in 3d 
after some hyperparameter tuning we decided to use the adam optimizer with standard parameters and learning rate 0 001 
here we investigate whether the number of layers in the model will impact on the performance of the dcnn by comparing a 2 layer model and a 3 layer model 
a dilated cnn modeling
both networks have trained with well selected parameters 
we used drop off in both with rate 50 to reduce overfitting 
the results are presented in as the table shows the training error of both 2 layer and 3 layer models are around 91 but the difference between train and test accuracy for a 3 layer model is larger than that of a 2 layer model suggesting that the 3 layer model suffers from overfit the data much more than 2 layer model 
here we investigated whether the filter size and pool size make a difference in our predicting performance of the model 
we iterated on only 2 layer dilated networks because they have the best test results from the previous part 
1 filter size and pool size 
the results are presented in we eventually settled on the best set of parameters of the the simplest model with 0 84 test accuracy since test accuracy start to flat out 
the confusion matrix of the model is shown below not surprisingly metal is the hardest one to identify while classical music can be identified with almost 100 accuracy
in this section we present the classification outcomes from the four classical algorithms lr gda random forest and svm with three different sets of input features the baseline model using flattened mfcc matrix and combined models using two convoluted layer output features the final classification accuracy of our baseline model and the combined model can be found in the performance of logistic regression and rf improved after pca and improved further with layer 1 and 2 of the dilated cnn features 
interestingly they out performed the dcnn which was used to extract the features 
b classical algorithms
we believe that there a few possible reasons 
first our neural network architecture might not yet be optimized for this problem since there is still a gap between the training and test accuracy 
second classical algorithms have much fewer parameters compared to the neural network so they may actually have a regularizing effect on the activations of the neural network thus helping us get higher test accuracy the column of results for svm is particularly interesting we see that svm performs extremely poorly when applied to the raw input 
there is a huge gap between the train and test accuracy which indicates severe overfitting 
this is likely because the kernalized svm has very strong predictive powers and will end up fitting very complex boundaries to the raw input in a high dimensional space that do not generalize to the test data 
however after pca svm performs even worse compare this with the three other algorithms whose performance increases after pca 
these results lead us to suspect that dimension reduction on the data does not help with the performance of svm 
however when we use the layers of the dcnn as input the performance of svm increases dramatically 
this indicates that the dcnn extracts low dimensional features from the raw input in a way that is fundamentally different from pca 
as we can see from the plots there is a blue dot among a sea of reds indicating that there was a classical music that had features extracted from dcnn that is very similar to pop music 
thus it made sense for lr and other algorithms to classify it wrongly as a pop music as you can see on the figure on the right 
we hypothesis that even though logistic regression have a slight regularization effect it is ultimately constrained by how well the dcnn extract the features 
we have successfully improved the accuracy of classical classification algorithms by using a dilated convolutional neural network as our feature extractor 
in some cases the performance of the classical algorithm can even exceed that of the neural network 
vii conclusions
in practice this will allow us to use a pre trained neural network as a feature extractor and improve both the performance and speed of classical algorithms 
we have also made a few interesting observations regarding the results in
there are several directions that would be interesting to pursue in the future 
in our work we used a dilated cnn as our feature extractor 
however since music data is inherently sequential other network structures such as lstm long short term memory and gru gated recurrent unit will likely achieve better performance 
moreover if we are able to train a much deeper network that attains high accuracy it would be interesting to plot the accuracy of a classical algorithm using different activation layers 
additionally we still do not have a full understanding of the results in
this project presents an implementation of named entity extraction for detecting attributes in the description of ecommerce products 
this problem is very important for ecommerce search and catalog building systems 
effective named entity extraction could significantly improve quality of search results in ecommerce retail system and so the experience of customers 
because description of products is provided in plain text form without any structuring this is also very challenging problem 
using as an example bestbuy ecommerce ner dataset we demonstrate the technology which includes feature extraction pipeline and trainig the model to recognize brands modelnames price and other attributes from the product description 
we provide a review of methods which are used for the information extraction 
in our project we focused on three methids svm gradient boosting trees and conditional random fields 
models we used were evaluated against the test set 
here we will determine some terms we will use in current report 
we define a product as any commodity exposed by a retailer 
product contains set of attributes where attribute is a named property of product which has some attribute value represented by one or several terms 
the axmples of attributes are brand color proce modelname etc we define product desciption as a set of attributes with corresponding values as a short example a p p l e w a t c h s e r i e s 2 g r e yhas following attributes brand apple category watch color grey modelname series modelname 2 let us mark product as p i as an anntribute and v i it s value the task will be to extract from text product description p 1 v 1 m v m also we consider that each product description is represented as set of terms x 1 x n 
we define our problem in the following way for each attribute i we need to find a function e i which will extract from product description attribute values v i which belong to i 
i e final cs229 project report 
2018in our current project we use the data set provided https www kaggle com dataturks best buy ecommerce ner dataset home and it s extension provided here https dataturks com projects dataturks demo 20document 20annotations both data sets has the same format and we joined it and used it as one extended data set after deduplication 
this joined data set has about 4000 records and 50 of these records are annotated tagged by experts 
the structure of the annotated record is represented below 
in this example we see that for the short description apple watch series 3 42mm from 339 expert annotated apple as a brand and watch as category 
c o n t e n t apple w a t c h s e r i e s 3 42mm from 339 a n n o t a t i o n l a b e l brand p o i n t s s t a r t 0 end 4 t e x t apple l a b e l c a t e g o r y p o i n t s s t a r t 6 end 1 0 t e x t w a t c h l a b e l modelname p o i n t s s t a r t 8 end 1 4 t e x t s e r i e s l a b e l modelname p o i n t s s t a r t 1 9 end 1 9 t e x t 3 l a b e l s c r e e n s i z e p o i n t s s t a r t 2 1 end 2 4 t e x t 4 2mm l a b e l none p o i n t s s t a r t 2 6 end 2 9 t e x t from l a b e l p r i c e p o i n t s s t a r t 3 1 end 3 4 t e x t 339 i e 
for annotated document the terms which expert marked as matched to some entity are mentioned in the annotation section of json entities provided in the data set brand category modelname screensize storage ram 
the frequency of the entities in the training set provided on graph below because of the provided distribution on current phase we focused on the most frequent entities brand category and modelname and train our algorithm to optimize metrics for these entities 
also we can see that as we shopuld expect brand are represented by smaller subset of words compare to category and modelname it matched to the scenario when one company produce several caterories of product and each category is represented by several models 
so we can expect different results of the extraction for these entities 
we specially would like to mention a data set for ecommerce which has about 2 millions of tagged product descriptions which also contains images and which was created benchmark the task of the attribute extraction for ecommerce https rloganiv github io mae this data set contains annotated ecommerce descriptions as well as annotated images of products 
ecommerce data set for attribute extraction benchmark
for result esimation we use p recission recall and f 1 metrics 
the definition are we calculate per entity metrics as well as total 
also we use accuracy to analyze classification of terms per entity 
one of the important aspect of the project was arrangement of the feature extraction pipeline 
we build feature extraction arround the concept of the extractor function 
feature extraction pipeline
so feature extraction pipeline is implemented as a collection of featureextractors 
each feature extractor is a function which is applied to the term and check if it s possible to generate the feature value for this term 
we follow the approach described in feature extraction pipeline operates per product description which is represented after normalization as x 1 x n our pipeline could be represented as a function f x 1 x n position f 1 f d which generate d dimensional feature vectir for term in position 1 n to descibe feature extraction we assume below w 0 x position and index of w is a relative index to the position below we provide a table with features we used with the short description 
note that we expect possibility of usage different set of features for the extraction of different entities so in our machine learning model each term in product description is represented by the set of features generated by feature extraction pipeline 
one type of features could be specific for current term for example is it numeric term or it consists only of letters is term started from capital letter and length of term 
another type of features is contextual value of feature depends on other terms in product description bigramm is the simplest example 
we use 2 classification methods for classification the entity for each token svm and gradient boosting trees implemented scikit learn package 
because we have a case of several classes multinomial classification we assign to the token an entity with the highest probability if it exceeded the threshold 
supervised classification approach
we assign threshoild based on the roc auc curve which we build for svm and gbt classifiers also we tried conditional random field to assigng labels for the tokens for svm we found that the optimal recognition has been provided by svm with rbf kernel and below we provid the results for different enttities 
paramaters and c for rbf were obtained via cross validation and are different for different entities below we provided an example of multinomial classification for svm classifier 
here tag is the original tag and tagpred is predicted tag 
for each category we explicitly provided the probability and the assigned category is calculated as category with maximum probability if it s exceed the threshold 
below we provide results for svm classifier rbf kernel 
we tried several other cores but rbf is the optimal one 
classification results for svm classifier
with cross validation we defined the optimal parameters and provide final results for training and test set below 
next we used the same approach but another classifier gradient boosting decision trees also from scikit learn package 
again we determine parameters using cross validation on training set and provide the results below for training and test set 
classification results for gbt classifier
com nltkdata conll corpora as a base line actually we use data set conll2002 provided by nltk and got with the same approach the following results for implementation we used crf library https sklearn crfsuite readthedocs io en latest 
the most probable label sequnce y for input sequence x is y argmax y p y x the same model we applied for attribute extraction of conll2002 data set https www kaggle 
in current project we applied supervised classification svm gbt and conditional random fields approaches to assign entities to the tokens from ecommerce product description 
in general we can see based on f 1 score that crf demonstrated better results 
we didn t use gazetteers as a source for additional features but we assume that it will be very strong signal which can significantly improve the results also we had very limited data set and we were not able to use other models like pretrained word embedding which could compensate small data set because we have modelnames which are very specific and most likely are not a part of pretraing set like glove 
because our data set is relatively small we were not able to solve the problem of overfitting but even on this data set we can see good recognition of brands it happened that svm training demonstrated the longest computation time and crf was the fastest one for future work we consider to apply approach described in
t he era of information explosion brings an increasing demanding on the ability to extract core message from billions of records of data 
sentiment analysis or opinion mining is widely applied to extracting and studying subjective information in texts 
by quantifying the opinions or attitudes in a large bulk of texts in a few minutes sentiment analysis has gained popularity in various business scenarios for retrieving customer responses 
in recent decades considerable progress has been achieved in sentiment analysis of english language 
at the same time a similar development comparable to the growth of market has not be seen in the scenario of chinese language the inputs are reviews about restaurant in chinese language 
the task is to classify each piece of review text into 4 classes not mentioned 
pang and lee svm has longer history with sentiment analysis comparing to the gradient boosting and lstm 
it is one of the most used classification methods in sentiment analysis due to its ability to generalize well in high dimensional feature spaces on the other hand although gradient boosting is one of the most applied off the shelf methods in general classification tasks surprisingly application of boosting ensemble method is not common in text classification 
a machine learning in sentiment analysis
ehrentraut et al recent years have seen a substantial progress in nlp tasks with neural network approaches 
lstm is popular in sequence modeling for sentiment classification because of its advantage against gradient vanishing or exploding issues in long texts 
a review of sentiment analysis in chinese language was given by peng et al peng et al 
also mentioned the different techniques for segmentation 
b chinese nlp researches
as chinese language does not have space between words it is necessary to use segmentation tools to extract words as the basic units of semantic meaning 
they summarized that jieba had a high speed and good adaptation to different programming languages 
for these reasons we decide to use jieba as our segmentation tool 
we use the data sets provided by ai challenger official
the main challenge in our project is preprocessing our data 
chinese language is difficult to accurately segment because of absence of space variant lengths of words and high flexibility of making new words 
we apply same preprocessing approaches to the training validation and test dataset 
with reasons presented in the related work section we use jieba cut for segmentation 
after segmentation we gain three lists of word lists produced by segmenting the lists of sentences 
we then train a word2vec model following the instructions in
the baseline model is provided by ai challenger official
lstm or long short term memory network is a type of recurrent neural network rnn to process sequence of information by feeding the output of preceding neurons to subsequent neurons 
unlike traditional rnn lstm networks are not vulnerable to gradient explosion
b lstm
or vanishing when dealing with long sequences of data 
this is achieved by forget gate input gate and output gate in each hidden unit 
fig 1 preprocessing flowchart
these gates decide how much information to let through and therefore can connect information with a wide gap between we build a many to one lstm model for each of the 20 elements 
with n the number of examples j the class id y the true label and p the predicted probability 
accuracy and weighted f1 score are the evaluation metrics 
svm classifier is one of the most preferred classification method among classic machine learning methods
we feed our xgboost models with the same input with svm models 
we use gridsearchcv function from sklearn package in python progaramming language to tune for parameters learning rate 0 01 0 05 0 1 and max depth 
d xgboost
after tuning on a subset of 500 training records and 100 validation records we choose 0 5 for lstm layer dropout and recurrent dropout 128 for number of hidden units 128 for batch size and adam for optimizer with learning rate of 0 001 1 of 0 9 2 of 0 999 
at the beginning we tried 50 epochs for all the elements and found most of them converge after around 14 to 15 epochs 
a results
we therefore decide to train for 20 epochs 
as we have 105k records in the training dataset batch size of 128 will make the training process comparatively fast 
adding different arbitrary class weights to different elements does not have clear improvement 
here is the plot
we use weighted f1 score along with accuracy because of the imbalance in class distributions which is prevalent across the elements 
the confusion matrix of element 13 space reveals the problem in xgboost model which also exists across all models 
b general discussion
class weights might be a good strategy though in lstm it does not show clear advantage 
on the one hand the situation
in general xgboost yield better results in terms of f1 scores and test accuracies 
our models improve from baseline model partially due to better preprocessing and partially due to better tuned hyperparameters 
vi conclusions and future works
apart from the aspects mentioned in the discussion section this task can be improved in the following ways 1 
collect data with higher label quality some examples are difficult to classify even for human beings 2 
improve the quality of language models with contextual representation e g 
bert 3 
moreover we might benefit from applying attention mechanism for long input texts 
the team members contribute equally to the project 
suofei was responsible for the training of baseline model data preprocessing and the construction and training of lstm model 
vii contributions
eziz contributed to the construction and training of svm and xgboost models 
two members both worked on poster making and report writing 
the code can be found at https github com suofeif cs229 project 
viii code
this paper proposes and evaluates preliminary models to produce musical style encodings with applications in music style transfer 
inspired by methods of neural style transfer
with the success of neural style transfer in this paper we consider musical genre to be directly correlated with style and as such attempt to learn a latent representation of it using both supervised and unsupervised learning methods directly from raw input audio 
concretely we investigate hybrid neural networks with both autoencoding and classification components to learn genre embeddings 
introduction and task definition
we evaluate our results primarily with the feasibility and interpretability of our embeddings when visualized using pca 
additionally we also look at classification metrics such as precision recall accuracy and model error to benchmark our models 
we primarily draw inspiration from previous work in neural style transfer for images 
in neural style transfer a common method of extracting a meaningful representation of style in an image is to use intermediate layers of a pretrained image classification network such as the vgg 19 with regards to the task of music genre classification we are motivated by promising work done by tzanetakis et al 
on the gtzan dataset
we started with the gtzan genre collection dataset which contains 1 000 tracks each 30 seconds long of 10 genres with a native sampling rate of 22 05 khz each of the original samples was in r 20 000 so we used average pooling with a pool size of 40 to downsample the dimensionality of our data which doubles as a regularization technique 
we ended up with an equal number of the four genres and 8000 examples in total 
each example was represented as a vector in r 500 
we chose a random split of 6000 1000 1000 for our train development and test sets respectively 
since our task was to learn encodings from raw data we did not use any explicit feature engineering 
for a classification model we initially implemented a basic two layer neural network with one hidden layer in r 128 and tanh activation 
our loss for one example is defined aswhere y r 4 is a one hot vector with a one in the component corresponding to the true class and r 4 represents the output of our classifier 
two layer neural network
we then implemented a vanilla autoencoder with a single hidden layer in the encoder and decoder as a baseline 
however this was extended to a deeper architecture as seen in the top half of
vanilla autoencoder
in our final model we combine the two approaches using the result of the encoder as input to a multi class classifier to form what we call a deep softmax autoencoder 
we theorize that this approach may reduce overfitting in the classification component because the classifier takes as input a vector in r 64 instead of r 500 
deep softmax autoencoder
to account for the combined model we modify our objective to minimize a weighted combination of reconstruction and softmax cross entropy loss aforementioned 
this is formally defined for one example in equation 3 
by encouraging the model to minimize reconstruction loss along with classification loss the model should be more likely to learn a latent representation of genre while retaining important information to reconstruct the original piece of music 
intuitively for both reconstruction and classification loss to decrease the encodings must both represent the original input and encode some information about its genre methodologically upon settling on this blueprint approach we ran consistent experiments to tune our hyperparameter values such as the number of layers and the layer sizes in our final model 
these along with our final architecture are reflected in
we divide our evaluation into quantitative and qualitative metrics 
we focus on measuring the performance of the deep softmax autoencoder architecture through precision recall and f1 scores 
in the qualitative analysis we visualize potential 64 dim and 4 dim embeddings using pca and discuss their benefits and tradeoffs 
from our baseline implementation of a basic two layer neural network as a genre classifier we saw a relatively low training and test accuracy compared to previous works as described in tzanetakis et al 
after implementing our deep softmax autoencoder we found a significant increase in training and test accuracy compared to the baseline two layer neural network 
in addition when examining the confusion matrix in
quantitative analysis
we notice in compared to the 4 dim encodings the 64 dim encodings have the potential to capture more subtle nuances within each genre 
these encodings serve different purposes particular tasks may require the expressivity of the 64 dim or the conciseness of the 4 dim 
qualitative analysis
in conclusion as shown in in the future we fundamentally seek to improve the interpretability of our latent representations 
specifically we plan to experiment with using these encodings for musical style transfer and evaluate our embeddings in an extrinsic task 
we also plan to interpolate components in our encodings to interpret the latent space 
we acknowledge limitations in our approach specifically in the trimming of the dataset and our avoidance of explicit feature engineering 
as such we hope to increase the number of classes in our dataset and expand our task brief to experiment with integrating mfccs and other forms of feature engineering to see if we can further inform our encodings generated from raw audio 
finally we are curious to see if replacing the autoencoder with a tcvae would help us learn disentangled representations of genre via a mutual information gap mig metric
vrinda and arjun worked on the initial data processing which woody then optimized 
vrinda and woody worked on the initial classification model and arjun worked on the autoencoder 
vrinda and woody worked on combining the two to form the deep softmax autoencoder before we all collectively brainstormed ideas ran experiments and evaluated results 
we all worked on this report collectively 
traditional semantic similarity models often fail to encapsulate the external context in which texts are situated 
however textual datasets generated on mobile platforms can help us build a truer representation of semantic similarity by introducing multimodal data 
this is especially important in sparse datasets making solely text driven interpretation of context more difficult 
in this paper we develop new algorithms for building external features into sentence embeddings and semantic similarity scores 
then we test them on embedding spaces on data from twitter using each tweet s time and geolocation to better understand its context 
ultimately we show that applying pca with eight components to the embedding space and appending multimodal features yields the best outcomes 
this yields a considerable improvement over pure text based approaches for discovering similar tweets 
our results suggest that our new algorithm can help improve semantic understanding in various settings 
determining the semantic similarity between texts is an important task in practical nlp 
new methods like doc2vec however with the advent of mobile devices we often have access to a wealth of passively collected information tied to any given piece of text such as the time and location at which the text was recorded in this paper
we build off of previous work that incorporates the context of a sentence to better determine semantic similarity between sentences 
staple models like term frequencyinverse document frequency tf idf struggle to incorporate contextual information 
instead we let two more recent methods doc2vec and contextual salience cosal guide our approach toward building better contextual understanding doc2vec is a method that learns continuous distributed vector representations for inputted text allowing it to better incorporate text ordering and semantics cosal computes the importance of a word given its context 
this is then used to produce weighted bag of words sentence embeddings thereby incorporating context into semantic similarity computations 
these contexts can also be small as cosal works well with as few as 20 sentences 
we tested our algorithms on the data world politician tweets dataset we preprocessed the data prior to constructing an embedding space 
we began by associating each tweet with its corresponding user location 
sample demonstration on twitter data
then using the geopy nominatim api we associated each location with corresponding longitude and latitude values 
next we encoded time as cyclical continuous features
we tested two different approaches to including multimodal data in semantic similarity computations 
each of them build on related work described in section 2 
our first approach was to directly improve upon the existing cosal algorithm 
we chose to work with cosal because we believed it to be most suitable for sparse mobile datasets 
modifying contextual salience
the cosal algorithm sends each sentence to a 50 dimensional embedding space using mahalanobis distance over the context 
it then computes the similarity score not adjusting for context in this method our approach was to modify this equation to take into account additional features such as geolocation and timestamp 
in general each input sentence s can be represented by n 1 features s s cosal s 1 s 2 s n where s cosal is the vector encoding produced by the cosal algorithm and each s i is an additional feature of the sentence for example in the twitter data set s 1 is the time at which the tweet was published and s 2 is an ordered pair representing longitude and latitude of where the tweet was published 
with these new inputs we proposed two potential new functions as improvements over sim cosal 
each d i is a distance function with two basic properties a b we have d i a b 0 1 and when sentences are closer in a certain feature their distance d i is smaller than the distance between two further sentences 
several candidate distance functions were tried for each d i such as and
defining distance formulas
using these equations a new similarity score was assigned to each pair of tweets 
the model was trained and tested with batches of 10 20 tweets smaller batches were necessary due to the difficulty of manually labeling all points the output of the model was an m by m matrix where each row corresponded to a tweet and each entry in the row was the ranking of similarity between that tweet and every other tweet 
loss function and parameter optimization
the loss function 5 calculated the difference between this matrix and the ranking matrix of the manually labeled data where y a b is the ranking of a tweet from the labeled data and a b is the ranking from our function 
this function was then minimized by varying 1 and 2 
this was done by manual gradient descent since the loss function is discrete there is no well defined gradient to use for traditional gradient descent 
we hypothesized that appending time and geolocation features to the doc2vec embedding space could induce closer semantic relationships 
recent work has suggested doc2vec works similarly to implicit matrix factorization we tested two ways of encoding time as a feature shown in we also varied the number of components for the reduced tweet embedding space to determine which number of components produced the most realistic semantic similarity metric according to similarity data labeled by stanford political science students finally we performed t stochastic neighbor embedding t sne as well as one over each pair of vectors in the twodimensional space to then minimize the kullback leibler divergencesover all datapoints using gradient descent 
pca and t sne
we performed gradient descent on our model with many different combinations of function sim and sim with different d i s 
we found that the loss function was minimized with the similarity function 1 s1 2 where a b is the geographical distance between locations of tweets in miles purely text based cosal similarity achieved an average loss of 32 80 
demonstrated increase in accuracy
our best model achieved an average error of 29 86 for a loss decrease of 9 0 percent 
in the next subsection we discuss the inherent flaws of this model 
that this method of modifying sim cosal does indeed better predict similarity between sentences but the level to which it can accurately do so is limited at a relatively low bar possibly less than 10 percent better than unmodified cosal 
limitations of this approach
by comparing against manually labeled similarity scores we found that reducing the original embedding space to 8 components prior to appending all features produced the most realistic semantic similarity metric as displayed in what could explain such a large improvement 
we saw that politicians often tweet about similar topics such as policy topics and sporting events at similar times 
pca component selection
our dataset also included tweets during natural disasters which led to many geospatially and semantically similar tweets 
furthermore it is also worth keeping in mind that labeled data was collected in limited quantities 
while it would be worthwhile to replicate this study with more labeled data our results provide compelling evidence for the incorporation of temporal and geospatial information in analyzing tweet similarity 
we applied t sne to compare the two embedding spaces visually 
these tweets are semantically similar both concern the u s debt ceiling crisis of 2011 and their authors share similar desired policy outcomes 
qualitative visual analysis using t sne
however they are not textually similar and hence are classified as dissimilar according to the distributional hypothesis taken by doc2vec 
on the other hand they are separated by small amounts of time and distance and hence are significantly closer in this new space we also tested our method on a small subset of tweets spanning the months prior to the 2016 election 
in this paper we introduced methods to incorporate additional features into traditional semantic similarity algorithms 
we found that reducing the dimension of the original embedding space and then appending additional nontextual features performed better than the original embedding space itself 
final thoughts
this also performed better than iterative minimization which we believe is due to the pca based approach working in more dimensions 
by acting on the level of the embedding space it incorporated multimodal attributes directly into the orientation of the tweet vectors 
on the other hand iterative minimization lacked spatial context and only acted on the final computed similarity score overall the success of our pca based model supports the hypothesis that multimodal data can provide valuable context for determining semantic similarity 
we would like to broaden our experimentation in collecting more labeled data 
we would also like to apply our algorithm in testing if tweets from local politicians differ from national politicians when controlling for location 
more broadly we would like to extend our results beyond the scope of political microblogging and apply it to other multimodal datasets 
in practice multimodal attributes are extraordinarily powerful and underutilized contextual markers and so may prove to be quite valuable in building nlp engines of the future 
all team members have contributed equally to this project 
peter hansel developed and tested our iterative minimization method 
nik marda conducted the literature review and data collection efforts 
william yin developed and tested our pca and t sne methods 
all team members contributed equally to writing this report 
flexible circuits are of great interest to hardware manufacturers 
wearable electronics that stretch with human motion are a prime application of this technology 
however achieving desired stretchability is difficult since transistor elements are relatively stiff 
stretching them beyond some threshold will irreversible damage the parts 
one solution is to place the transistors on a flexible polymer substrate which absorbs the required stretching deformation 
however a naive placement of the transistors on a large substrate is an inefficient use of space so we wish to optimize the configuration of the transistors to minimize the area of the circuit substrate 
we can use optimization to determine the configuration 
unfortunately this problem is non convex and the current genetic algorithm implementation is computationally expensive 
in a design setting where many circuits must be designed this computational expense is unacceptable 
this motivates us to use machine learning to predict a compact circuit configuration with minimal online runtime 
this optimization problem has been successfully performed for a three transistor circuit using particle swarm optimization pso 
during pso 60 particles are tested in each iteration 
problem statement
each particle is a circuit configuration with randomly placed transistor elements 
we run a mechanical finite element simulation on each particle where the circuit is stretched by 50 
each finite element simulation determines whether the transistors are damaged 
the particles are naively assigned to a random location on the design space looking for better placements and rejected worse placements 
the optimization problem is formulated as min area s t 
no overlapping transistors transistors inside domain transistor strain energy below thresholdthe
to apply machine learning to this problem we performed 1 000 optimizations to generate a dataset of near optimum configurations 
this section discusses the structure of the data and our collection procedure 
data collection
each particle swarm optimization was performed on a circuit with three transistors elements 
the twelve input features quantify the geometry and material properties of each transistor 
specifically each transistor has a specified width w k height h k young s modulus e k and maximum allowable strain max k for k 1 2 3 which are formatted in the dataset asthe eight output parameters include the optimized overall dimensions of the circuit w and h and the centerline positions of each transistor x k y k 
we used latin hypercube sampling to randomly generate our input features ensuring an appropriate span of the feature space where w h 0 2mm 2mm e 5mpa 50mpa and max 0 005 0 01 
this is implemented using the pydoe module in python 
simulations and data augmentation
we generated 1 000 optimized samples which took two weeks of wall clock time on 72 cores in parallel the 1 000 samples collected was the maximum feasible size for our available computational resources 
to increase the size of the dataset we developed an augmentation technique of the features and outputs 
to do this we permuted the order of each transistor s data 
for example one training sample can be permuted 3 
6 times since the data contains information for three transistors asthis gave us an additional larger dataset of 3 
1 000 6 000 samples 
we used both the regular data and the augmented data for learning to compare the effect of augmentation 
we first try predicting an optimized output configuration using a supervised learning approach considering that it is an easier benchmark to set 
we separate the data into 80 training set 10 validation set and 10 test set 
supervised learning approach
mlr is a learning technique that generates a model to linearly predict an array of outputs of formfor the ith observation and jth feature 
the mlr model was implemented using the sklearn module in python 
multivariate linear regression
after training we compute the mean squared error mse and absolute difference ad for both the regular and augmented dataset asthe difference in errors between the training and test data are small so mlr has low bias 
however the augmented data set appears to somewhat help prediction performance 
visualizing the predicted configurations gives additional insight into the shortcomings of mlr for our application 
the figure below shows three optimized configurations above and their corresponding predictions below 
the overlapping transistors in the predicted outputs are problematic since they violate the constraints integrated into the pso algorithm 
the outputs are generated by a nonlinear genetic optimization process and are therefore not a linear span of the inputs 
this led us to use mars a method better suited to modeling nonlinearities 
ren gibbons prajwal k a
multivariate adaptive regression splines
the figure of the predicted configuration shows overlapping transistor elements 
this is one limitation of a supervised learning approach there is no simple way to enforce constraints 
linear scaling
one workaround to obtain a valid circuit configuration is to linearly scale the configuration until no overlap is observed as shown below 
although this leads to a safer configuration as the substrate area has increased performing this procedure on the mlr test set results in a mean size increase of 802 over all test set entries compared to the optimized shape 
this is not a satisfactory result which motivates a reinforcement learning approach 
our optimization problem ideally falls into the category of continuous state space and continuous action space 
the problem has additional complexity in that the state space for the transistor coordinates changes each time the area decreases 
reinforcement learning approach
to circumvent these complexities we simplify the model by fixing the area and instead maximizing the factor of safety fs of transistors 
we define the fs as fs maximum allowable strain energy in transistor average strain energy in transistor in the rl algorithms positive rewards are given to the current global maximum state and high negative rewards for violated constraints 
the state with the highest reward is output as the optimized solution 
we found that the formulation of the reward function and initialization of the optimum value function are the most challenging aspects of developing this model 
the reasons for this are that the rewards for a particular state are not known a priori and the problem is non convex 
the simplest way to apply rl to a complex continuous problem is by discretizing it 
both the space and actions are discretized 
value iterations for discretized space
the optimal value function v is learned using bellman s equation the value iteration method was both implemented manually using the framework of cart pole problem from cs229 ps4 and using mdptoolbox package 
though the python module resulted in faster iterations the time taken for its initialization nullified this advantage initially high positive rewards were given to the current global maximum states 
but this led to premature convergence to local maxima 
so small positive rewards are chosen and the optimal value function is initialized to a slightly higher value so that other states are also checked for maxima 
random initializations contribute to uniform search over the domain 
a sample simulation by discretizing the space into 4 097 states and actions into 125 states produced an optimized configuration as shown below 
this method suffers from the curse of dimensionality with even a simple coarse discretization yielding 125 action states and 4 097 space states 
even though this algorithm predicts a fairly accurate optimized shape convergence requires over 4 000 iterations and 19 hours of wall clock time on the coarse discretization 
hence this is a feasible but not desirable method for this problem 
in this method the space is considered as continuous but the actions are still discretized 
in this method we approximate the optimal value function as a function of the states 
fitted value iteration for continuous space
initially linear regression is used for the approximation 
as this problem is non convex we found that linear and quadratic functions of the state weren t good enough for approximating the optimum value function over the whole domain 
so we are currently trying out different reward and state functions so as to get a converged solution 
we believe that this is a stepping stone towards formulating the rl for a continuous space and action problem for minimizing the area 
the goal of this project was to use machine learning to speed up a relatively slow genetic algorithm 
we developed learning models using mlr and mars 
both mlr and mars consider the output features to be independent of each other and depend only on the input features 
this assumption does not hold in this problem and therefore resulted in predictions with violated constraints 
the data we get by optimization is bereft of failed unsafe configurations 
thus supervised learning algorithms cannot learn about the safety of a resulting configuration linear scaling of the configuration to remove overlap is a possible solution for the problem of violated constraints 
however this technique results in a very high increase in area making it undesirable 
due to lack of data we couldn t implement recurrent neural networks rnn which considers the dependence between the output features by taking them as some of the inputs this urged us to look at reinforcement learning 
though value iterations by discretization considers all the constraints this approach may not be a good algorithm for non convex optimizations as the algorithm is time consuming and computationally expensive 
though we didn t find an optimal method for predicting configurations in this project we believe that we successfully evaluated the behavior of some of the ml techniques for such optimization problems for future work we wish to arrive at a good rl technique for this continuous space and action problem of minimizing the area of substrate 
if we can produce more data then a deep learning framework with rnns can also be tried 
finally we wish to apply these techniques for optimizing the design of large circuits with a large number of transistors which was our motivation for this project 
ren and prajwal worked closely together on this project meeting regularly to discuss ideas and our progress 
we developed the idea of the project together 
specifically prajwal wrote the majority of the python scripts for training and testing the data 
he also developed the rl algorithm 
ren worked on debugging the pso code pre and post processing the data and generating figures 
responsibilities for writing the paper and creating the poster were evenly divided 
the source code can be found here 
the readme md file contains a description of the various python files we used 
code base
reza rastak cee phd candidate author of pso algorithm used in this project offered advice and access to his code 
lithium ion batteries are deployed in a wide range of applications due to their low costs high energy densities and long cycle lives 
however long battery cycle life entails delayed feedback of performance often many months to years 
the high manufacturing variability of commercial lithium ion batteries makes this task challenging 
furthermore electrochemical models fail to capture the dynamics of degradation during operating modes of interest such as fast charging 
accurate prediction of cycle life using early cycle data would unlock new opportunities in battery development manufacturing and optimization the goal of this work is to predict the final cycle life 1000s of cycles using data from the first 100 cycles or fewer 
here cycle life is defined as the number of cycles before the capacity falls below 80 of the rated capacity 
we build predictive models using the elastic net random forest regression and adaboost regression 
our inputs are data including voltage capacity temperature and internal resistance as a function of cycle number 
early prediction of battery lifetime has been widely studied but has proven difficult 
harris et al 
in our previous work in this work our objective is to predict battery cycle life using only the first 20 50 cycles while achieving comparable or improved accuracy to our previous results which used the first 100 cycles 
we focus on both improving feature engineering and applying more advanced machine learning methods 
our dataset consists of 124 nominally identical commercial lithium ion batteries cycled under various fast charging conditions 
some properties of this dataset are displayed in our training set consists of 84 cells while our test set consists of 40 cells consistent with previous work iv 
feature generation because we have a relatively small dataset 124 cells careful feature selection is key to creating a successful predictive model of cell lifetime the most complex data source consists of high dimensional voltage data with 1000 features per cycle 
however features from this dataset are expected to be among the most predictive 
in addition to eliminating outliers a ransac surface fit is useful for generating features themselves 
we fit a 2nd 3rd and 4th order surface polynomial to find which parameters were most useful for fitting the surface 
we selected a simpler model as detailed in eqn 
1 where p 1 p 2 p 3 are fitting parameters or features x is the cycle number y is the voltage and z is the baseline subtracted discharge rate 
outliers here were more strictly defined as anything not fit to within 0 003 ah and the features generated with this method had relative success if we compare these voltage visualizations for a typical good cell and a typical bad cell in addition to the slope of the discharge curve vs cycle number we also explored a number of other features extracted from the capacity heatmap 
although some of these features are highly correlated with the cycle life they also have high correlation with one another 
while some features may be redundant representations of the same information adding regularization and pruning to our regression algorithm should mitigate issues caused by redundant features 
we build models with three regression methods in this work elastic net random forest regression and adaboost regression 
the first regression model we employed was elastic net regression in which the parameter vector is defined by the following equation elastic net regression combines the benefits of both 1 norm and 2 norm regularization in linear regression also called lasso and ridge regularization respectively 
the main difference between these two regularization methods is in the way they treat correlated features 
a elastic net
while 2 norm regularization will constrain the weights of correlated features in a more or less uniform way 1 norm regularization implements a winnertake all behavior in which one of the correlated features will be selected and the others will be ignored 
the advantage of lasso regularization is that it performs feature selection which can improve both model accuracy and interpretability when the dataset is feature rich 
adding the 2 norm penalty term to lasso regression has been shown to improve performance especially when the number of features is large compared to the number of data points we started by applying elastic net regression to our full dataset of 64 engineered features 
by investigating the relative weights assigned to these features by the elastic net we were able to select a smaller subset of 13 important features which we used for the remainder of our analysis 
we found that all three of our models performed better using this smaller set of features rather than the orginal set likely because having a reduced number of features helped to prevent overfitting the hyperparameters involved in elastic net regression are the overall regularization factor and the l 1 ratio which sets the relative importance of the 1 norm and 2 norm regularization 
in the scikit learn implementation of elastic net regression the algorithm performs stochastic gradient descent with cross validation to optimize over these hyperparameters automatically 
we optimized over the hyperparameters separately for each number of cycles 
we found that the optimum combination varied but the optimal value for tended to be around 0 01 indicating that the model was relying more on the 2 norm for our downselected feature set 
the optimal value of ranged between 0 001 and 0 01 
since our dataset is feature rich but contains a relatively small number of training examples only 84 we found that our models tended to be prone to overfitting 
we decided to try random forest regression because of its ability to reduce overfitting by aggregating results random forest regression averages the results from a number of decision trees 
b random forest regression
decision tree regression works by dividing the data recursively and assigning the mean value of the leaf training examples to all points contained in that leaf 
at each split it selects the optimal feature and split value to minimize the loss for a random forest with n trees the first step is to generate n different training sets of the same size as the original by sampling the original with replacement this process is known as bootstrapping 
the individual decision trees are then trained separately on their respective data sets 
to further decorrelate the trees only a randomly selected subset of the features are made available at each split decision 
finally after the trees are trained the output of the random forest is just taken to be the average of the predictions of the constituent trees 
because the predictions are averaged in this way the variance of the model can be greatly reduced by simply increasing the number of decision trees applying random forest regression to our data we swept over the number of trees and the maximum depth of trees 
we found that the results generally improved with increasing number of trees and increasing tree depth 
for our final testing we chose a model with 1000 trees and no limit to the maximum tree depth 
adaboost short for adaptive boosting regression is a popular regression method that relies on the idea of boosting in which the results of many weak learners are combined in a weighted average to achieve an accurate prediction 
the algorithm is adaptive because each training example is assigned a weight which varies over time 
c adaboost regression
if a weak learner performs poorly on a particular training example the weight of that example is increased so that the next weak learner will be more likely to fit it correctly 
decision stumps decision trees with small depths are commonly used weak learners in our application of adaboost we used decision trees of depth 3 as our weak learners 
as our hyperparameters we swept over the number of trees and the learning rate the factor by which the predictions of subsequent learners are discounted 
the optimal learning rate was found to be 0 1 and the model performance was found to plateau with approximately 1000 trees 
for each of these three learning methods we employed 5 fold cross validation to tune the hyperparameters 
this method allowed us to make the best use of our small training set 
d cross validation and scoring
we scored the models based on the achieved mean percent error which is a more important figure of merit for battery lifetime prediction than mean squared error 
we find that random forest regression most consistently achieves the lowest test error of our three methods 
our hypothesis is that random forest regression best captures the bias in our dataset without overfitting to outliers 
a prediction results
ultimately linear regression is limited in its ability to reduce bias while adaboost regression is highly sensitive to outliers 
in for all three methods the test error is consistently higher than the training error 
we attribute this result to three sources 
first our models may be overfit 
we attribute this both to suboptimal hyperparameter optimization as well as the specific attributes of the dataset i e 
high lifetime cells for adaboost regression 
second the test set was selected to maintain consistency with previous work 
however this data was generated from a separate testing batch of cells 
each batch is subject to small experimental differences such as temperature fluctuations etc 
we expect our models to be sensitive to these differences 
finally our sensitive error metrics are sensitive to our small sample sizes 
all three figures reveal the same general trend 
at low cycle number 20 30 the chargetime feature dominates the elastic net weights and importances with other features like the ir internal resistance features also contributing 
the chargetime feature is a feature related to the time required for the cell to charge from previous work we know that that cycle life is highly sensitive to charge time 
at low cycle number 20 30 cycles the changes in capacity as presented in at high cycle number 40 100 features such as log slope 2pt9v corr and log deltaq var are selected 
these features are derived from the capacity curve heatmaps presented in specific attributes of each feature importance map are also revealing 
for elastic net regression we note that for most cycle numbers nearly all features are selected 
we attribute this result to the initial feature screening that yielded a subset of predictive features 
another possibility is that all features are selected due to overfitting given the high variance of the results i e 
discrepency between train and test 
we also note that only four features were selected for the n 40 case which also corresponded to high errors of both test and train for random forest regression we find that while chargetime is most predictive at low cycle number the five features consistently selected are all derived from the capacity heatmaps 
these models consistently perform well at cycle numbers from 40 to 100 for adaboost regression we find that for a given number of cycles used in prediction feature importance is generally dominated by a single feature 
the overreliance on single features leads to models that do not outperform random forest regression 
machine learning techniques are a promising approach for early prediction of battery cycle life 
in this work we develop novel features that capture voltage data and apply state of the art learning techniques to reduce the number of cycles 40 required for prediction while achieving similar performance 12 mean percent error 
vii conclusions and future work
overall we find that tree based methods specifically random forest regression achieve high accuracy at low cycle number outperforming regularized linear regression 
we attribute this result to thewe have a few proposals for future work 
primarily we will try to reduce overfitting by performing a test train split over all batches and by using packages for automated hyperparameter optimization 
additionally we will expand our feature set by incorporating features from other sources of data 
components of the data set such as rest periods charging constant voltage holds etc 
applying principle components analysis on the voltage heatmaps and using the component weights as features is a promising path forward 
lastly developing a convolutional neural network to x ray tomography images taken before cycling could be used to detect manufacturing defects which we expect are strong predictors of cycle life we will also consider other use cases of early prediction 
classification of cells into low lifetime and high lifetime classes would be useful in preliminary screening applications in manufacturing and charging policy optimization we expect significantly fewer cycles on the order of 5 10 would be required for this application 
lastly we will employ this early prediction model in a reinforcement learning context to efficiently screen for fast charging policies with high lifetime 
all authors contributed to data exploration feature generation model development and report writing 
rational design of application specific materials is the ultimate goal of modern materials science and engineering nevertheless first principle calculations require tremendous amounts of computational resources one intrinsic property of substantial relevance in the screening of materials for novel applications is the material s elastic modulus which describes the response of the material to external forces 
furthermore it correlates with many of the material s mechanical and thermal properties our ml models work by performing a preprocessing step to generate a set of descriptive attributes as input features x using well known atomic properties we generate a list of chemical and physical descriptors 
the true labels of the model during training y are the elastic moduli calculated from first principle computations 
we show that simple ml algorithms can be used to predict elastic moduli with relatively high accuracy achieving a coefficient of correlation of 0 9 and low rmse 
to date several repositories containing materials data have been developed previous examples of ml models for materials properties are constructed from three parts training data a set of attributes that describe each material and a ml algorithm to map attributes to predicted properties 
different sets of descriptive attributes have been designed proposed
as shown in the elastic modulus distribution of the dataset is shown in
dataset and feature extraction
the features x of the ml models consist of 135 descriptive attributes 118 of which encode the particular chemical composition under consideration and 17 of which encode heuristic quantities developed using chemical intuition average atomic mass average column on the periodic
descriptive attributes
after generating a set of descriptive features x for the materials database the data was split into a 3939 537 632 train test dev set 
the x matrix was standardized to zero mean and unit variance using the training data 
we used three models available from the sklearn python package
ridge regression was used as a benchmark model and it consists of a linear model that minimizes the least squares loss while penalizing the size of the coefficients 
the normal equation is as follows where is the identity matrix of size 135x135 and is a pre chosen penalty term here 0 5 
ridge regression
mlp is a fully connected neural network nn consisting of at least three layers of nodes an input layer a hidden layer and an output layer 
briefly in a nn the input layer receives features x which are put into linear combination and fed into neurons in the hidden layer s where they are passed into an activation function in our case we decided to use a single hidden layer with a rectified linear unit relu activation function 
multi layer perceptron
the outcomes of the hidden layer s are then put into linear combination and fed into the output layer 
as common in regression problems we used an identity function for the output layer 
training of the nn is carried out through backpropagation which allows for minimization of the square error loss function 
rfr is a type of ensembling technique that fits decision trees on subsets of a dataset and uses averaging over the decision trees to improve accuracy of the predictions 
optimization of the number of trees and maximum depth was performed
random forest regressor
10 fold cross validation cv was used to evaluate model performance 
the advantage of cv is that all the instances in the dataset are tested once using a model that did not see that instance while training 
evaluation metrics
the elastic moduli database was randomly divided into 10 segments 
9 segments were used to fit the model and the remaining segment was used to test the model 
this procedure was repeated 10 times with different segments 
the evaluation criteria employed to evaluate the predictive performance of the models are the coefficient of correlation r and the root mean squared error rmse 
they are defined as follows where and are the predicted and target elastic modulus respectively and and are the mean of the predicted and target elastic modulus is the number of samples 
the coefficient of correlation is a measure of the strength of the relationship between the predicted and the measured values determining the accuracy of the fitting model i e 
r 1 shows a perfect positive correlation 
rmse is an error measurement with smaller error indicating a better prediction accuracy 
parity plots reflect the performance of the models
we have demonstrated promising results for predicting the elastic modulus of inorganic materials from a set of descriptive attributes which can be readily obtained for any chemical composition 
both mlp and rfr are suitable predictors given the chosen descriptive attributes there are many possible routes for future work 
the most straightforward route consists on predicting other elastic properties using the developed set of descriptors such as the shear modulus 
longer term approaches can focus on implementing advanced ensembling algorithms and partitioning the dataset into groups of similar materials to boost predictive accuracy 
the goal of identifying a simple model that describes observed data has been researched in the past by
system identification
the choice of feature selection in general is a difficult problem 
there are an infinite number of possible feature functions and it is impossible to know a priori which functions will comprise the pde 
generation of features
since the system identifier is meant to be an aide to researchers and scientists it is reasonable to ask the user to provide some amount of guidance to the algorithm without explicitly handing over a set of feature functions which the researcher presumably does not know to this end the system identifier relies on a domain specific grammar provided by the user that gives the rules for generating feature functions 
a grammar is a set of production rules that govern a language or a set of expressions 
each rule can either be non terminal in which the rule relates generic expressions together e g 
multiplication or terminal in which an expression is concretely defined e g 
the observed data 
each expression in the language can be represented by a tree of operators each of which is part of the grammar 
once a grammar is defined expressions can be sampled from the grammar with varying levels of complexity as defined by the depth of the expression tree 
a sample grammar and expression tree are shown in to produce the candidate set of features the user will specify a desired tree depth d and all possible expressions with depth d will be produced 
then this set of features is searched for expressions that evaluate to the same results and any such duplicates are removed 
the number of candidate expressions grows exponentially with the depth of the tree but fortunately most pdes that govern physical processes have terms that are only at a depth of 4 or less which makes the problem tractable see wikipedia list of nonlinear pdes 2018 
for the model systems all terms can be produced from an expression depth of 3 which for the grammar in figure 1a means that a total of 562 features will be considered 222 after removing duplicates 
the julia package exprrules jl was used to build the grammar and to sample expressions from it at the desired depth 
once the candidate feature functions are computed the next step of the algorithm is to select the best subset of features that describes the observed data 
in the next subsection an evaluation metric is presented that will be used to determine how good a subset of features is 
in the following subsection the algorithm that chooses the best subset will be described in detail 
the metric that was chosen to decide between models is the adjusted r 2 value of the fit 
the traditional r 2 value always increases when new features are added so it makes our algorithm susceptible to overfitting 
instead we penalized the addition of more features to the regression by defining the r 2 adj aswhere n is the number of features in the model 
note that when n 1 this expression reduces to the traditional r 2 value 
but when n 1 then the difference between r 2 and 1 is scaled by the number of parameters squared 
the third term in the expression was included for the cases when r 2 was very close to or exactly one as can be the case when dealing with noise free synthetic data 
if r 2 1 the second term vanishes and there would be no way to distinguish between models of different complexity 
thus a small value of is chosen to break ties in favor of the simpler model 
in the following tests 0 001 was used 
in general the algorithm to find which subset of features has the best r 2 adj value is very computationally complex 
to check all of the subsets of a set of n features requires 2 n linear regressions 
feature selection algorithm
the grammars and depths that would commonly be used would give rise to n 100 1000 features which makes the subset search intractable 
in place of the brute force search of subsets a form of forward search was implemented instead 
the workflow of this algorithm is shown in the inputs to the system indentifier are y the data that needs to be explained i e 
u t for the advection diffusion equation and the grammar defined by the domain expert which contains the observational data and the production rules to generate features 
the grammar is then used to generate many possible features with the expression depth controlled by the user 
the features are stored in an input matrix x which is cleaned of duplicate columns due to equivalent expressions from the grammar 
the columns of the input matrix x and the output vector y are normalized to a mean of 0 and a variance of 1 to avoid problems with differing scales and constant offsets 
x and y are then passed into the feature selection algorithm the feature selection algorithm starts with an empty set of featuresx 
each feature from x is added in turn tox so that a linear regression with y can be performed and the r 2 adj value computed 
the added feature is then removed and the next feature is added 
once all features in x have been tried the feature that had the best r 2 adj value is permanently added tox 
on the next iteration all features except the feature already used individually get added tox and the r 2 adj is computed for the resulting two feature input 
if there is no feature that caused the r 2 adj to increase then the algorithm stops and returns the set of features that are currently inx otherwise the new feature with the best r 2 adj gets added tox and the process continues the algorithm is also capable of returning not just the highest scoring set of features but the top k scoring sets of features 
this is achieved by running the feature selection algorithm k times and keeping a list of the sets of features that were returned on previous iterations 
these sets of features are skipped over on the next iteration and the next best combination is found 
searching through more possibilities increases the chances that the algorithm doesn t get stuck in a local optimum due to the greedy selection criterion 
the resulting complexity of this algorithm is o nk which is a significant improvement 
in order to evaluate the efficacy of the system identifier two test systems were selected for trials 
these test systems will produce data at varying levels of spatial resolution and with different amounts of noise 
sample data
the data will then be fed through the system identifier and the results will be compared to the actual underlying system 
the first test system is the unsteady 1d advection diffusion equation which describes the general transport of material in a fluid 
the system has the form u t d 2 u x 2 v u x with bcs u x 0 u 0 u 0 t u l u x t 0 the exact solution of which is given by van genuchten 1982 asthe solution is plotted in where is the constant density of the fluid u i is the i th velocity component and x i is the i th velocity spatial direction for i 1 2 
the first equation represents conservation of mass in the fluid while the second equation represents conservation of momentum 
the exact solution of these equations for flow around a circular cylinder of radius r centered at the origin is given in cylindrical coordinates as for the purposes of this project the velocities were converted into cartesian coordinates using the typical cylindrical coordinate transforms
in the real world experimental setting we expect the data collected to be noisy 
to model this we assume additive white gaussian noise in the measurements of u such that for a given noise level we have the standard deviation of the additive noise given by
adding noise
since the data we a dealing with comes from a spatio temporal domain it is required that we analyze the spatial derivatives of the data in order to build an accurate model of the system dynamics 
in an experimental setting the spatial derivatives of the data cannot directly be measured so instead we must compute the derivatives numerically using a central difference formula which gives the approximationwhere i represents the grid index in the spatial dimension 
numerical differentiation and filtering
the same can be done in the time dimension in the case of a noisy measurement taking the numerical derivative will amplify the noise more than the signal itself so we need a way to denoise the signal before taking the derivative 
one excellent way of denoising a signal is to use total variation regularization rudin 1992 which finds the signal u n such that the objective function e x n u n v u n is minimized 
x n is the noisy input signal and e x u 1 2 n x n y n 2 and v n y n 1 y n are the mean squared error and the total variation respectively 
the function imrof was used from the julia package images jl to perform this filtering on the sampled data and after each differentiation is performed 
the regularization parameter was set to 2 
the effect of total variation filtering on the numerical first derivative is showed in
the system identifier was run on both test systems with varying levels of spatial resolution spatial sample points and amount of noise in the data 
the parameters used for each system are shown in table 1 
in the trials for varying the spatial resolution the data was produced without noise and the number of sample points per spatial dimension was varied from 100 down to 5 
when varying the noise the spatial resolution was fixed at 100 points per dimension and the amount of noise was varied from 1 to 50 
for each configuration the system identifier was scored on if it induced the correct form of the underlying pde i e 
it chose the correct features and to what degree it computed the correct coefficients of the features 
the results of these tests are shown in we can see from the data that the system is very robust in the low data limit and is moderately robust to noisy data 
the correct form of the pde was determined for all spatial resolutions tested likely because the solutions are smooth enough that an accurate derivative can still be computed with largely spaced sample points 
the 1d advection diffusion equation was always determined correctly even in the presence of large amounts of noise and only the system parameters suffered in accuracy 
the 2d euler equation did not work as well with noisy data likely because the noise filtering was less effective in higher dimensions 
from this initial investigation we conclude that this system identification system can work for systems that are are unsteady nonlinear and have multiple dimensions 
identification is feasible in the low data limit and for moderate amounts of noise in the observed data 
the two main challenges of a system identifier are denoising and feature selection 
the use of a total variation denoiser and a domain specific grammar do much to alleviate these issues but more work can be done on improving the denoising filter when taking spatial derivatives as well as implementing better algorithms for producing candidate features 
genre classification is an important task with many real world applications 
as the quantity of music being released on a daily basis continues to sky rocket especially on internet platforms such as soundcloud and spotify a 2016 number suggests that tens of thousands of songs were released every month on spotify the need for accurate meta data required for database management and search storage purposes climbs in proportion 
being able to instantly classify songs in any given playlist or library by genre is an important functionality for any music streaming purchasing service and the capacity for statistical analysis that correct and complete labeling of music and audio provides is essentially limitless we implemented a variety of classification algorithms admitting two different types of input 
we experimented with a rbf kernel support vector machine k nearest neighbors a basic feed forward network and finally an advanced convolutional neural network 
for the input to our algorithms we experimented with both raw amplitude data as well as transformed mel spectrograms of that raw amplitude data 
we then output a predicted genre out of 10 common music genres 
we found that converting our raw audio into mel spectrograms produced better results on all our models with our convolutional neural network surpassing human accuracy 
machine learning techniques have been used for music genre classification for decides now 
in 2002 g tzanetakis and p cook in the past 5 10 years however convolutional neural networks have shown to be incredibly accurate music genre classifiers
we obtained all of our musical data from the public gtzan dataset 
this dataset provides us with 1000 30 second audio clips all labeled as one out of 10 possible genres and presented as au files 
from each clip we sampled a contiguous 2 second window at four random locations thus augmenting our data to 8000 clips of two seconds each 
since this data was sampled at 22050hz this leaves us with 44100 features for the raw audio input 
we restricted our windows to two seconds to limit the number of features 
we found that 44100 features was the perfect balance between length of audio sample and dimension of feature space 
thus after pre processing our input is of shape 8000 cs229 final report music genre classification 44100 where each feature denotes the amplitude at a certain timestep out of the 44100 
we also used 100 samples of un augmented data each of our cross validation and test sets we also experimented with pre processing our data by converting the raw audio into mel spectrograms 
in doing this we experienced significant performance increases across all models 
mel spectograms are a commonly used method of featurizing audio because they closely represent how humans perceive audio i e 
in log frequency 
in order to convert raw audio to mel spectogram one must apply short time fourier transforms across sliding windows of audio most commonly around 20ms wide 
with signal x n window w n frequency axis and shift m this is computed asthese are computed more quickly in practice using sliding dft algorithms 
these are then mapped to the mel scale by transforming the frequencies f by m 2595 log 10 1 f 700 
then we take the discrete cosine transform of the result common in signal processing in order to get our final output mel spectogram in our case we used the librosa library the resulting data can be visualized below hip hop metal reggae
in addition to pre processing our data as described above we also used principal component analysis to reduce dimension for two of our models k nn and svm 
pca is a form of factor analysis that tries to approximate the data by projecting it onto a subspace of lower dimension 
in order to do this we first transformed the mel spectrograms by normalizing their mean and variance 
in order to preserve as much variance as possible with m examplessince our data has mean 0 this is equivalent to taking the principal eigenvector of the covariance matrix of the data 
empirically we had best results reducing to 15 dimensions analogous to taking the top 15 eigenvectors of and projecting our data onto the subspace spanned by them 
we implemented this using scikit learn
we decided to first implement two simpler models as baseline measures of performance then progress to more complicated models in order to increase accuracy 
we implemented variants of k nearest neighbors support vector machine a fully connected neural network and a convolutional neural network 
in addition to mel spectogram features we used principal component analysis to reduce dimensionality for the input to k nn and svm 
after reducing the dimensionality to 15 features using pca see above we applied the k nearest neighbors algorithm 
predictions are made on a per case basis by finding the closest training examples to our test or cross validation example that we wish to classify and predicting the label that appeared with greatest frequency among their ground truth labels 
k nearest neighbors
through trial and error we found that best accuracy resulted from setting k 10 and weighting the label of each neighbor by distance explicitly denoting our data point as x let x 1 
then we choose weights w i for each i such thatfinally we return arg maxthe label which is most prevalent among x s 10 nearest neighbors when weighted by distance 
x 10 be the 10 closest neighbors to x those which return the largest value on x x i 2 where 2 denotes the euclidean distance between points 
this was implemented with scikit learn
after reducing dimensionality using pca we trained an svm classifier as well 
svms are optimal margin classifiers that can use kernels to find more complex relations in the input data 
since our data is not linearly separable this equates to findingwhere x i are examples j b are weights and biases c is a penalty parameter and 1 i is the functional margin for example i k r n r n r is a kernel function 
in a traditional svm this function corresponds to inner product between two vectors x j x i but in our case we are using an rbf radial basis function kernel this kernel also sometimes called the gaussian kernel corresponds to an infinite dimensional feature space is related to euclidean distance 
this function as a whole is often minimized using sequential minimization optimization 
we used a fully connected neural network as well with relu activation and 6 layers with cross entropy loss 
as the input to our model was 1d when using mel spectrograms we flattened the data 
feed forward neural network
our model is fully connected which means each node is connected to every other node in the next layer 
at each layer we applied a relu activation function to the output of each node following the formula at the end we construct a probability distribution of the 10 genres by running the outputs through a softmax function e z k to optimize our model we minimized cross entropy loss we experimented with various regularization techniques such as dropout layers and l2 regularization 
dropout randomly selects features to drop based off a specified constant and l2 regularzation adds a penalty term to the loss function in the form of i 2 i 
this was implemented with tensorflow
this was our most advanced model using 3 convolution layers each with its own max pool and regularization feeding into 3 fully connected layers with relu activation softmax output and cross entropy loss 
most of the equations can be found above and our architecture is visually presented below this approach involves convolution windows that scan over the input data and output the sum of the elements within the window 
this then gets fed into a max pool layer that selects the maximum element from another window 
afterwards the output is fed through a model described in section 4 1 
this was implemented with tensorflow and keras
the main quantitative metric which we used to judge our models is accuracy that is percentage of predicted labels which matched their true labels and our main way of visualizing the performance of our best model is through the confusion matrices as seen in we selected our hyperparameters based of empirical results and industry standards 
for instance choosing 4 by 4 window for our first convolution window was a result of seeing a similar window size work in other academic results and then fine tuning to meet our data specific needs 
we chose to use adam optimization for a few reasons 
working with audio time series data over 2 dimensions cause sparse gradient problems similar to those often encountered in natural language or computer vision problems 
we also felt our data was somewhat noisy and messy 
adam mitigates the sparse gradient problem by maintaining a per parameter learning rate and mitigates the noise problem by basing updates on a weighted average of recent updates momentum 
with adam our models trained more quickly and didn t plateau as early consult we also see that all four of our models struggle with over fitting 
we spent the most time trying to mitigate this issue on the cnn 
to do so we introduced three methods 
we played around with a combination of batch normalization dropout layers and l2 regularization 
we found some difficulty in using this as allowing the model to over fit actually increased our accuracy 
while we could bring the training accuracy and the test accuracy to within 05 of each other this would result in poor model performance 
thus we accepted our over fitting issue looking more closely at our confusion matrix we see that our cnn struggled most with the rock genre 
it only managed to correctly classify 50 of rock audio as rock labeling the others as mainly country or blues 
additionally it incorrectly classified some country as well as a small fraction of blues and reggae as rock music 
while it s not all that surprising that rock was a challenging genre a qualitative inspection of rock mel spectrograms implies that many rock music excerpts lack the easily visible beats that other genres such as hip hop and disco possess while our personal experience with rock music vis a vis the other genres tells us that rock is also missing distinctive traits such as high register vocals pop or easily audible piano classical or jazz 
additionally rock is a genre that both encapsulates many different styles light rock hard rock progressive rock indie rock new wave etc 
and heavily influences many other derivative genres 
however we were surprised that rock and country were so easily confused as opposed to rock and metal which would seem to rely on more similar instrumentation and tempo additionally we note that correct classification of jazz was less accurate than most other categories 
our algorithm falsely classified some jazz music as classical although never did the reverse of this 
we hypothesize that the more piano heavy jazz tracks may have been too close to classical music in terms of both tempo and instrumentation and may have been missing the saxophone or trumpet sounds and timbres associated with many other samples of jazz audio 
across all models using frequency based mel spectrograms produced higher accuracy results 
whereas amplitude only provides information on intensity or how loud a sound is the frequency distribution over time provides information on the content of the sound 
additionally mel spectrograms are visual and cnns work better with pictures 
the cnn performed the best as we expected 
it took the longest time to train as well but the increase in accuracy justifies the extra computation cost 
however we were surprised to see the similarity in accuracy between the knn svm and feed forward neural network in the future we hope to experiment with other types of deep learning methods given they performed the best 
given that this is time series data some sort of rnn model may work well gru lstm for example 
we are also curious about generative aspects of this project including some sort of genre conversion in the same vein as generative adversarial networks which repaint photos in the style of van gogh but for specifically for music 
additionally we suspect that we may have opportunities for transfer learning for example in classifying music by artist or by decade 
our group has collaborated closely on the majority of this project 
we worked together to find a project idea lay out the direction of the project and determine matters of data processing as well as implementation details 
the writing of the project proposal milestone report and this final report have all been a team effort we found that we never felt the need to explicitly delegate tasks 
for the poster arianna drove most of the design process but all three group members helped to provide the content however when it came to coding we realized that our team worked better when we could mostly code by ourselves 
derek was instrumental in setting up the environments and compute resources and together with eli took on most of the implementation of the neural networks 
arianna implemented the non deep learning models as well as spearheaded the data processing and feature selection aspects of our project 
in recent decades climate change has drastically influenced key characteristics and patterns of wildfire across the global land surface
much of the prior work in this field has focused on mapping burned area and fire severity after a fire has already occurred 3 4 5 
our dataset was assembled in the google earth engine platform 6 which allows for parallelized cloud computing on its large collection of geospatial data 
the primary data sources used were the moderate resolution imaging spectroradiometer modis aboard nasa s terra aqua satellites the global land data assimilation system product gldas and the climate hazards group infrared precipitation with station data chirps 
using remote sensing data ensures that our model can be easily be extended to other regions and times as long as similar remote sensing instruments are in operation at those times 
we computed a collection of fire relevant variables describing ecological hydrological and meteorological conditions from these gridded datasets for our study region of northern california and for the time period of 2001 2017 
these choices were informed by our review of the related literature 
for the purposes of training and validation we used a modis fire severity classification product developed from post fire spectral signals to engineer a binary fire no fire target variable 
logistic regression is a linear classifier that outputs a probabilistic prediction h x g t x where g is the logistic function g z 1 1 e z 
the model is trained to maximize the likelihood assuming that y bernoulli p h x 
logistic regression with forward stepwise selection
we used the glm function in r which uses the newton raphson method for optimization
in gradient boosting the n th model in the ensemble is fitted on the residual of the previous n 1 models 
the splits on each model are chosen to minimize the cross entropy loss y log p 1 y log 1 p at every split 
decision trees with gradient boosting
we used the xgboost r package 8 to fit 100 gradient boosted trees with a maximum depth of 2 
to avoid overfitting we chose the number of trees to use in our final model based on which number of trees gave the highest validation accuracy 32 trees when both climate and reflectance subsets were included 
we again used years 2001 2015 for training 2016 for validation and 2017 for testing 
a multilayer perceptron mlp is a feedforward neural network composed of an input layer receiving the signal an output layer making a prediction about the input in our case assigning a fire or no fire label and any number of hidden layers with non linear activation functions 
our mlp uses the relu activation function f x max 0 x and stochastic gradient descent for optimization which we implemented using the scikit learn python library 9 
multilayer perceptron
for classification it minimizes the cross entropy loss function 
the model consists of two hidden layers with 10 and 2 neurons respectively 
we used the years 2001 2016 for training and 2017 for testing 
we built machine learning models to predict the probability that a fire will ignite on any pixel on any given day given prior climate and land surface conditions derived from remote sensing data 
as described in section 4 the three models considered perform binary classification with cross entropy loss 
experiments results discussion
we used models that output a probability excluding methods such as support vector machines 
we trained each model on three distinct sets of features climate variables only reflectance variables only and both subsets we first considered logistic regression 
because the logistic regression models trained on each set of data had test accuracies of between 0 76 and 0 78 we found it more informative to compare their values of auc area under the receiver operating characteristic curve which plots true positive rate against false positive rate as the threshold for being predicted as a positive example is changed 
forward feature selection did not improve test performance we achieved an auc of 0 74 with all features but 0 68 with the four selected features 
this contrast suggests that the conditions that contribute to fire are too complex to be described with only a small number of variables 
however feature selection provides some insight into which specific variables are important 
in particular land cover classification 1 week gcvi 3 month swir2 and 1 week et were selected 
the fact that remote sensing indicators were chosen before climate variables suggests their abilities to capture the fire vulnerability state of the land system in a way that is not easily obtained from knowledge of the climate forcing 
this finding is confirmed by comparing the aucs of logistic regression models trained on the two types of variables 0 68 and 0 72 for the climate and reflectance subsets respectively 
reflectance alone can predict fire more accurately than climate alone but we achieve the best results auc of 0 74 when including both sets of variables 
our work demonstrates the vast potential for using machine learning techniques to better understand vulnerabilities to fire based on features of climate and the land surface 
we applied logistic regression with forward stepwise selection decision trees with gradient boosting and a multilayer perceptron to a robust dataset of ecological hydrological and meteorological variables derived largely from remote sensing 
conclusions future work
performance across models was similar though the multilayer perceptron using the full dataset both climate and reflectance subsets provided the overall minimum test error 
in general while we were able to predict non fire conditions more accurately than fire conditions predictions for the latter retained 75 80 overall test accuracy measurably better than the 73 that would be achieved with random guessing 
we believe that by coupling remote sensing assets with learning algorithms preemptive identification of fire risk with the goal of adaptive resource management is possible two future directions are particularly appealing to us 
we are first interested in using projections of future climate to infer expected wildfire dynamics in the region in the coming decades 
to address this we could leverage climate model outputs under the rcp 4 5 or 8 5 emissions scenarios which forecast climate change from the present to 2100 and modify our input variables to reflect these probable shifts 
we could then apply our models to the predicted data to assess future changes in fire risk over the next century 
the second future direction of interest to us involves predicting post disturbance effects in particular economic impacts or costs of damages on burned areas 
we could compile spatially explicit population infrastructure and economic data in order to extrapolate potential costs and damages from fires predicted by our model which would be useful for policymakers in conducting cost benefit analysis of fire prevention 
all team members contributed to the planning of experiments determination of project foci and writing of the report 
additionally all team members collaborated in determining dataset structure e g 
relevant variables and data sources analyzing data and creating the poster 
jake campolo prepared data and created map based visualizations caroline famiglietti implemented the multilayer perceptron tabulated outputs and led overall design choices natan holtzman implemented the logistic regression and boosted trees and analyzed interpreted and visualized project results 
our code for this project is available at https github com cfamigli 229 git 
over the last two decades the development of powerful small and cheap electronics has driven advances in increasingly versatile autonomous systems 
for example uavs have become increasingly viable platforms for many commercial and military tasks including package delivery power line inspection wildlife conservation building inspection precision agriculture and surveillance 
however as the demand for uav operations moves into more complex and high performance environments the autonomous control systems must also become more advanced 
beyond uavs this is also true for industrial and home robotics self driving cars commerical aerospace applications and more for uavs control systems have been developed using techniques from classical control theory e g 
pid control modern control theory e g 
model predictive control mpc and learning based control 
classical control techniques are widely applied in standard open source hobby autopilot systems because of their effectiveness and simplicity but are typically limited to small nominal operating regimes 
modern control techniques such as mpc introduce a notion of control optimality and can explicitly handle state and control constraints 
this makes these types of techniques attractive for high performance tasks but can end up being computationally constrained and often rely on dynamics models which are sometimes inaccurate a broad array of learning based control schemes for uavs have also been proposed 
some approaches use learning methods for high level task control such as converting image data into high level plans or even low level motor commands using convolutional neural networks while many techniques i e 
classical modern and learning based have been proposed for designing uav controllers additional work has gone into developing algorithms that augment existing controller designs to improve performance 
for example contributions similarly this work does not focus on controller design but rather on a modular component that can improve the performance of any given controller 
in particular a low level motor controller is proposed that takes the output of the primary controller and maps it to motor commands 
this low level controller is modular in the sense that it could be used with any type of primary controller and by using a data driven approach it has the potential to a posteriori correct for complex effects that may not be known by the primary controller 
as mentioned previously the proposed low level motor controller maps the output of the primary controller to motor commands 
for this work which focuses specifically on quadcopter control the output of the primary controller is assumed to be the desired acceleration along the quadcopter s thrust axis along with the desired angular acceleration about each of the three rotational axes 
ii problem description
for better physical intuition these outputs will be referred to as the desired net force f z and the desired net moments m x y z 
the kinematics and dynamics of the quadcopter system will now be discussed followed by a discussion on how the low level controller will be designed 
for a quadcopter the state of the system can be defined by its position orientation and their rates of change 
to identify position and orientation of a rigid body two reference frames are defined an inertial frame w that is fixed and a bodyfixed frame b that is considered attached to the rigid body 
a quadcopter dynamics
using the conventions from the quadrotor translational dynamics are given bywhere r is the position vector r x y z t m is the mass of the quadcopter g is the acceleration due to gravity w is an inertially fixed unit vector pointing upward and b is a unit vector fixed in the body frame pointing along the body z axis 
the scalar input f t corresponds to the net thrust produced by the propellers and the vector f aero encompasses the aerodynamic lift and drag that affect the system 
the rotational dynamics are given b where i is the inertia matrix defined in the body fixed frame and m x y z are moments that are applied about each body fixed frame axis 
the angular velocity is defined in the body frame as
as mentioned it is assumed that the primary controller outputs a desired net force f z and the desired net moments m x y z which will produce the desired motion 
these values must then be mapped to commands that will be sent to the motors 
b problem formulation
the challenge is that accurately determining the correct motor commands can be a complex task due to unknown effects from the aerodynamics of the quadcopter and propellers and the dynamics of the motors themselves 
for example consider the following scenarios 1 the controller commands the quadcopter to perform a vertical climb with some fixed acceleration 
2 the controller commands the quadcopter to land 3 the controller commands the quadcopter to fly at high speed at a constant altitude to a new location 4 the quadcopter battery levels are running low 
in the first scenario as the quadcopter speeds up the drag will increase and therefore the propellers will have to do more work to maintain a constant acceleration 
in the second scenario the propeller ground effect would cause the quadcopter to experience a higher acceleration for the same motor command as it gets closer to the ground 
similarly in the third scenario propeller effects would have a tendency to produce lift during high speed translational motion 
finally in the fourth case the thrust produced by a propeller could have decreased for the same motor command it would be desireable to have a low level motor controller that can account for these effects in order to make the quadcopter react as closely as possible to the desired motion from the primary controller 
to accomplish this a learning based approach will be used to generate a model g such thatthree different models are proposed with varying structure and inputs 
the set of possible inputs includes information about the state of the quadcopter that would be available online and primarily includes the angular velocity position r velocity and orientation 
from a practical perspective the structure of the models are also limited in complexity due to the runtime requirements 
at the very low level of the flight stack that this controller would be implemented operations must be performed on the order of hundreds of times per second 
the dataset is from flight experiments 1 of a pelican quadcopter manually piloted by an experienced pilot 
this dataset includes kinematic data i e 
position velocity orientation from a motion capture system and motor command data logged on the flight computer 
in total there are over 250 minutes of flight data comprised of more than a million data points from 55 flights 
which is simply the net force projected onto the body z axis and therefore captures forces from any type of effect 
in addition to the net force f z and moments m x y z two additional quantities are computed as possible features for the models 
these quantities are the axial velocity v a and transverse velocity v t seen by the propellers 
these quantities are computed as followsonce the data processing is complete the available variables that can be used as features include the position r velocity accelerationr orientation angular velocity angular acceleration and the velocities v a v t 
all of the data was also normalized using the emprical mean and standard deviation 
in this work three different models for the low level motor controller are proposed and they are compared against the standard model structure used in the open source px4 autopilot firmware 
in current open source autopilots the low level motor controllers map desired thrust and moment commands to motor commands based on a very simple model of how thrust correlates to motor commands for a given propeller 
this relationship is generally determined using static thrust stand testing which removes many of the extra effects that are seen in real flight scenarios the baseline model first uses a linear mapping to convert the net force and moments into thrust for each individual motor 
this linear mapping is given byor its inverse where k t is a thrust coefficient k m is a moment coefficient l is the distance between the propeller axis and the center of mass and o i is the output command for propeller i 
the output for propeller i is then related quadratically to the motor command m ithe models 7 and 9 can then be combined and the parameters combined to generate the full modelwhereand the square root is element wise 
when developing this model it is important to note that the square root is only defined for non negative arguments which must be considered in the training process 
three new models are now proposed in an attempt to improve upon the performance of the baseline model 
the first two models are variants of the baseline that incorporate additional features but keep the same structure and the third uses a completely different structure specifically a neural network the advantage of using the same model structure for the first two models lies in the inherent simplicity of the model structure 
b proposed models
as mentioned previously the models need to be evaluated at extremely high rates and so low complexity is desired 
additionally keeping the same structure and adding features will provide some additional insight into whether the model structure is inherently useful or not 
additionally in each model the linear mapping 1 baseline feature subset the first proposed model keeps the same structure as the baseline but includes a handpicked subset of the available features 
in particular the features that are included are the altitude z the velocities v a v t and the angular velocity 
these features will be denoted as the vector x 1 z v a v t t 
additionally second order polynomial features of x are included i e 
x i x j and x 2 i 
the combined set of features are denoted as x 
finally these features are used to augment the original mapping between o i and m isuch that the model becomes2 baseline features the second model is identical to the first model proposed except that all available features are used 
namely x 2 z r r v a v t t 
once again second order polynomial features of x are also included with all features denoted by x 2 
the model is then defined bysuch that the model becomes3 neural network apart from the initial linear mapping the neural network model deviates in structure from the previous models and includes more parameters 
for this model a bias term is also added to the initial linear mapping where b is a vector of bias terms 
then the individual motor outputs o are combined with the full set of features x 2 to become the input to the neural network 
the same neural network is used for each motor such that for a neural net f the network used has a single hidden layer with 10 nodes and relu activation and a linear output layer 
the baseline model and the extended baseline models with features were trained using mini batch stochastic gradient descent 
the learning rate and batch sizes were tuned for good training performance and the learning rate was decayed after each training epoch 
c training
a relu activation function was also added to the baseline and extended baseline models to ensure the square root argument was non negative during the training phase 
the neural network was trained using the adam optimization technique also with tuned hyperparameters and learning rate scheduling the training and validation datasets were the same for each model and were selected by randomly taking samples from the full dataset in a 60 20 20 split for training validation and testing 
for each case the mean squared error loss was used for training and for evaluation 
this loss is defined aswhere m is the size of the batch m is the data point and m is the prediction from the model parameterized by 
in each case the optimization algorithm is a first order method meaning that they rely on the gradient of the loss function with respect to model parameters 
for this work the entire training procedure was peformed using the open source deep learning platform pytorch the results of training each of these models can be seen in v results after training and selection the three proposed models and the baseline model were evaluated on the test dataset 
in addition to evaluating the performance of the models using the mean squared error loss score a more qualitative analysis can be done 
inherently the mean squared error loss trains a model to have small error and in particular penalizes larger errors worse than small errors 
a other performance metrics
but it is also interesting to look at the actual errors and their distributions 
the error for motor i is simply defined asover the test dataset the mean error and standard deviation of the error can also be computedthese quantities are of interest because it is desireable that the mean be as close to zero as possible along with a small variance 
if the mean is not zero it suggests that there is some inherent bias in the model 
it is also desireable to have a decreased variance because the hope is that the more complex models are able to better capture the effects causing the largest errors 
the mean and standard deviation of the error are computed for each model and the results are shown in the distributions of the errors can also be plotted to provide a more visual presentation of the information previously discussed 
these plots are provided in
the objective of this work was to create a better performing low level motor controller using a learning based approach 
where current low level motor controllers fail is in the ability to capture off nominal effects which can lead to poor performance in off nominal flight situations 
from the results of the proposed models simply augmenting the current methods to include additional features will not be sufficient to drastically increase performance 
however the neural network while only providing a moderate performance boost provides some promising results that suggest learning based approaches may still be useful in this domain 
as discussed the results suggest that learning based approaches to this probem may have potential but likely require more complex models 
some future directions will therefore definitely include exploring more expressive models such as neural networks with more layers or potentially even recurrent neural networks 
recurrent neural networks may be effective because the regression is fundamentally on time series data which suggests that a feedforward network is not utilizing all available information 
additionally the dataset that was used was not collected for the specific purpose of this project 
therefore it would be interesting to explore data collection methods that may provide better training performance especially if recurrent neural networks are used 
optical thin film systems are structures composed of multiple layers of different materials 
they find applications in areas such as solar cell design ellipsometry and metrology radiative cooling and dielectric mirrors 
the main property of interest is the transmission or reflection spectrum which exhibits a complicated dependence on the parameters of the thin film stack 
an open problem in optical design is finding a device that exhibits a desired transmission spectrum a process known as inverse design 
as a model system we will use unsupervised learning to analyze the transmission properties of a 5 layer stack of alternating glass and silicon layers across a wavelength range of 1000 2000 nm 
the input features are the layer thicknesses and discretized transmission spectrum 
we use a variational autoencoder vae to compress the features down to a latent space and then reconstruct the input 
since optical thin film systems are of great interest to the optics community there are numerous existing design methodologies 
among them are analytical methods the rely on intuitive descriptions of the underlying physics recently the has been a surge of interest in applying machine learning and neural networks to tackling the problem of electromagnetic design 
the first demonstration involved the design of spherical nanoparticle a problem very similar to thin films using a neural network nn 
we generate our training data using a transfer matrix model tmm of the optical properties of five layer thin film stacks 
in the transfer matrix model forward and backward propagating plane wave modes in each layer are coupled to the modes in each of the adjacent layers 
the layer i has refractive index and thickness 
the total transmission through the thin film stack can be written in terms of the plane wave field amplitudes as the matrices represent the coupling between modes at the interfaces 
whereas the matrices represent the phase difference after propagation through a layer 
they are calculated aswhere 1 and 
the procedure is shown schematically in for this study we limit the devices to five layer glass silicon glass silicon glass stacks in air 
using the transfer matrix code we simulate 100 000 random devices for the training set and another 1000 random devices for the test set 
the thickness of each layer can take values between 0 and 300 nm 
we calculated the transmission at 101 points between 1000 2000 nm 
in this range the refractive indices are approximately constant around 3 5 and 1 5 
the combined input feature vectors consist of 5 normalized thickness values and 101 transmission values for a total of 106 input features 
in order to study the interesting aspects of our data we train a variational autoencoder an unsupervised learning algorithm that allows us to compress the input data onto an underlying latent space 
however as a baseline we first attempt to apply principle component analysis pca in an attempt to achieve the same purpose 
in pca we compute the principal eigenvectors of the covariance matrix which is given bytaking the n largest eigenvectors ranked by eigenvalue allows us to reduce the dimensionality of the data from 106 to n this compression is strictly linear which means it is likely a poor model for the difficult to capture the behavior of thin film systems for that we turn to vaes the loss function for a vae consists of a traditional loss function in this case the weighted mean square error for reconstruction as well as a kullback leibler divergence regularization term where is the input is the reconstructed output are the network weights is the latent variable is the encoded distribution and is the standard normal distribution the vae implementation we use is based on a pytorch example by diederik kingma and charl botha
in addition to the reconstruction loss there is another metric of interest in this problem 
when the decoder reconstructs a device and corresponding spectrum from the latent space we need to ensure that the reconstructed device s real spectrum as computed with the transfer matrix method and the vae s predicted spectrum match 
the figure of merit for the accuracy of a batch with m samples is given by since the transfer matrix method allows us to fully describe the spectrum with only the five thicknesses as input we wanted to see if compression to an even smaller dimension was possible thus we optimized the network for three latent dimensions 
we compared the accuracy mse for different numbers of hidden neurons and mini batch sizes 
to understand the properties of the encoded latent space we systematically sampled the latent space on a threedimensional grid and decoded the sampled points 
the decoded spectra vary smoothly across the latent space 
the latent space variables appear to be strongly correlated with layer thicknesses we perform this for a batch of 100 random devices for a target spectrum 
some outputs are shown on the right of
we use a variational autoencoder to represent optical thin film stacks and their transmission spectra in a lowdimensional latent space 
we show that it is possible to represent thin film devices in a latent space distribution and to generate new devices and their approximate spectra by sampling from the latent space 
when the latent space is lower dimensional than the degrees of freedom of the thin film stack the latent space variables are strongly correlated with the more physically important parameters in the future a modified network architecture may be required to perform efficient inverse design 
in addition the accuracy of our model is likely limited because the latent space is lower dimensional than the problem 
while this reveals interesting behavior of the vae a more accurate model for the purpose of inverse design could be implemented by increasing the number of latent space dimensions 
our code is available at https drive google com drive folders 1knahigcb4oeyxg z8tpt1kt4notkd3ds usp sharing
john wrote the transfer matrix code 
evan ran the dataset generation 
john set up the framework for the vae code 
evan ran the training and sample generation 
john mapped the latent space 
evan implemented the inverse design 
density functional theory dft is a quantum mechanical modelling method that facilitates the prediction of ground state energies of atomic systems thereby yielding valuable insights into many important chemical and physical systems and processes dft is particularly valuable in the field of heterogeneous catalysis where we are often interested in the strength of chemical bonds between a catalyst surface e g 
metallic platinum and a molecular fragment e g 
ch2 
once bound we refer to this molecular fragment as an adsorbate the energy released by bringing these two species together to bond termed the binding energy is a function of the electronic structure of the surface and the molecular fragment 
for transition metal surfaces simple chemical bonding theory has been used to demonstrate the existence of a linear correlation between the first moment of the projected density of electronic states pdos of the surface and the binding energy of a given adsorbate this idea is summarized in
the initial phase of our project consisted of generating a database of periodic dft calculations using the open source code quantum espresso surface simulations consisted of performing a local optimization of the system s energy with respect to the atomic positions subject to certain constraints namely that the lower two of four atomic surface layers were fixed to simulate the bulk and the in plane position of the adsorbates binding atom was fixed 
dft provides the gradient of the energy with respect to these positions i e 
a data generation
the forces on each atom by solving for the electron positions using an approximate version of the schrodinger equation 
following this optimization the pdos work function and system energy were extracted 
next the resultant data was cleaned by removing systems that behaved unexpectedly upon structural optimization 
specifically we designed filters that look for cases of molecular fragment dissociation changes in molecular fragment coordination number and significant distortion of the surface 
we deemed these to be invalid examples and removed them from our analysis after cleaning our dft calculations we constructed a database for machine learning 
a database entry for our machine learning models consist of three valid dft calculations an isolated molecular fragment and surface and the combined system 
the keys of the database consist of the bulk structure composition surface facet adsorption site initial state and final state 
for example the first key represents the process of ch3 adsorbing at an empty site in an on top geometry on the 111 surface facet of fcc gold 
the relevant pdos spectra in this case are for an au surface atom and for the c atom in gas phase ch3 
the second key represents the process of h adsorbing on adsorbed ch2 sitting in a bridge site on the 0001 surface facet of hcp ruthenium 
in this case the relevant pdos spectra are for the c atom in adsorbed ch2 and an h atom in vacuum 
after data cleaning we were left with a database of 2000 examples on which to train 
the data generation process is summarized by the schematic in
the raw inputs to our ml models the pdos of the surface and the molecular fragment are continuous 1d scalar functions that can be discretized to arbitrary resolution 
based on the aforementioned success of correlating binding energies with first and second moments we will start by using as feature vectors the first ten moments of the pdos of both the surface and molecular fragment 
b feature engineering
next we train another set of models on the raw spectra with spacing 0 1 ev such that the feature vector is a concatenation of the surface and molecular fragment pdos 
in the case of convolutional neural networks we stack rather than concatenate creating a two dimensional array 
in instances where the molecular fragment binds to more than one surface atom the representative surface pdos is taken to be the sum of the pdos of every surface atom involved in the bond it is important to note that the pdos is computed relative to the fermi level of the calculation which represents the energy that divides filled and unfilled electronic states at zero temperature 
however when considering chemical bonding between the surface and molecular fragment it is also relevant to know where their electronic states lie on an absolute scale e g 
relative to vacuum 
the energy difference between the fermi level and vacuum for the surface is defined as the work function wf and we will also refer to this quantity as the work function in the case of the molecular fragment although this is an abuse of nomenclature 
both wfs are included in our feature vectors in addition to the information provided by the pdos iii 
methodology all models were trained via python packages scikit learn linear regression kernel ridge regression and random forest 
linear regression utilizes a linear model of the formwe can exactly solve for the parameters which minimize the squared error via the normal equations 
a linear regression lr 
kernel ridge regression can be viewed as an extension of linear regression with the addition of mechanisms to reduce bias kernel trick and variance regularization 
the kernel trick requires reformulating the training process to be purely in terms of inner products of examples such that individual feature vectors need not be explicitly represented 
b kernel ridge regression krr 
this allows the replacement of inner products with kernel functions that correspond to inner products in an arbitrary high dimensional space resulting in a nonlinear model when projected back onto the original feature space 
regularization is implemented to combat overfitting by adding the l2 norm of the parameter vector to the squared error lost function because krr is a distance based model it was important to normalize the data before applying the model 
we did so by subtracting by the mean and dividing by the standard deviation of each feature 
decision tree models divide feature space into discrete regions and for every example that falls within a given region the same prediction is made 
in the case of regression the prediction is the average of all training labels in that region 
c random forest rf 
the goal is to choose these regions such that the residual sum of squares is minimized bagging algorithms in which the predictions of many independently trained models are averaged are often employed with decision trees to combat overfitting 
the random forest algorithm further reduces variance by encouraging trees in the ensemble to be decorrelated by only allowing a subset of the original n features to be used in each tree 
boosting algorithms also fall under the category of ensemble learning algorithms where the predictions of multiple models are summed to achieve the overall prediction in the case of boosting these models are trained in series where specifically each new model is trained on the residuals of the sum of the previous models the boosting algorithm as written above minimizes the squared error of the total model 
so called gradient boosting abstracts away this choice of loss function and instead trains each successive model on pseudo residuals of the previous model which are simply the gradient of the loss function with respect to the predictions of the previous model 
d gradient boosting gb 
note the gradient descent form of the expression and the presence of the learning rate 
while these expressions are general in terms of the algorithm used to train each model decision tree ensembles are typically used which introduce typical hyperparameters such as tree depth 
such that the model update becomes 
convolutional neural networks are artificial neural networks that utilize one more convolutional layer 
convolutional layers differ from fully connected layers in that they perform convolutions of the input data with a relatively small kernel matrix rather than performing a matrix multiplication with a dense matrix whose dimensionality is similar to the input data 
e convolutional neural network cnn 
because the elements of the kernel matrix or dense matrix are parameters to be tuned the convolutional layer results in much fewer total parameters 
however use of a convolution operation requires the input data have some regular grid like structure which makes it a useful tool for image analysis or in our case spectra analysis our cnn was implemented using pytorch
as is the convention in the field of heterogenous catalysis we use mean absolute error mae to quantify the performance of our models defined as 
f error metric
to prevent model overfitting the labeled examples were split into train 70 dev 15 and test 15 sets 
training data was used to determine optimal model parameters and dev data was used to evaluate the model s ability to generalize to previously unseen data and to tune hyperparameters 
g train test splits
finally the model was judged by its performance on the test set we suspect that some of the examples in our dataset are highly correlated e g 
the same molecular fragment adsorbing on different sites at the same surface and we anticipate that a typical use case would involve making predictions on data that is unlike the data used to train the model 
in the domain of heterogeneous catalysis we might broadly consider cases to be similar if they involve the same surface composition and or the same reaction 
for the purposes of this work we group each example by its composition and reaction and then perform the train dev test splits such that the model will never be asked to make a prediction on an example whose reactants and surface composition it has already trained on 
as a concrete example if the model has trained on o binding at the ontop site of au 111 it will not be allowed to test on o binding at any site on any surface of au 
k fold crossvalidation which was used for hyperparameter tuning was also performed by making folds in this way 
alternative split schemes will be discussed later 
a using moments as features
while using feature vectors consisting of the first 10 moments of the pdos and the wfs gives reasonable performance we recognize that the spectra are not single distributions and are much more complex e g 
shown in in the future it would be of interest to investigate whether our model generalizes to more complicated materials such as alloys and metal oxides 
b using the full pdos spectra as features
furthermore we would like to experiment with other featurization techniques for the molecular fragments the pdos of which tend to be sparse with very narrow peaks 
a first approach may be to simply replace the pdos representation with a categorical variable indicating the molecular fragment identity 
we expect such a modification to greatly simplify model training 
however it also comes with an inherent loss in generalizability as it necessarily precludes the model s ability to make predictions of reactions that involve molecular fragments it has not seen before 
we are grateful to the instructors of cs229 andrew ng and ron dror as well as all of the teaching assistants for their help and support 
we also are grateful to our lab mate brian rohr who designed and built the convolutional neural networks and provided valuable guidance and advice 
finally we thank the stanford sherlock computing resources 
the application of machine learning ml techniques for understanding materials is a burgeoning field of research that has already seen success in academic and commercial settings 
for example ml methods have been used to improve estimates of electron wavefunctions from which a material s optical and elastic properties can be computed 
such research called high throughput computational materials design aims to identify materials likely to exhibit properties of interest e g metallic materials with exceptional tensile strength or superconducting ceramics 
1 once the enormous space of possible materials is restricted to a few such candidates each candidate is synthesized and tested in a laboratory a major challenge in applying ml to materials research is the vast space of possible materials 
further materials with similar compositions may exhibit wildly different observable characteristics 
training data exists for only some of these materials 
in addition it is unclear a priori which features are important to predict observable properties 
thus feature engineering will be crucial for the success of our project 
we will use ml to predict the electronic bandgap of inorganic solids 
roughly the bandgap shown in bandgaps are difficult to predict based on material compositions 
fig 1 energy landscape of silicon the bandgap is shaded 2
for example vo x can be either conducting or insulating depending on the value of x and the temperature 
yet obtaining accurate predictions of gaps is crucial to the development of improved electronic materials 
currently the best technique to predict bandgaps is density functional theory dft 
dft developed in 1964 by hohenberg kohn and sham makes ab initio from first principles calculations of material properties meaning it uses no phenomenological parameters 
multiple efforts have been made to predict electronic properties with ml 
these works motivated our use of the coulomb matrix as a feature encoding 6 7 and our choice of root mean squared error rmse as the error metric against which to optimize hyper parameters on our validation set 
our model is developed using jarvis joint automated repository for various integrated systems a database of materials properties that are computationally generated using dft 
the full dataset was randomly partitioned into 60 and 20 training and development splits for developing the models and a 20 testing split that is withheld and used to benchmark the algorithm s performance 
the large sizes of the development and test splits are necessary to adequately sample across the huge variety of material types it is crucial to develop a representation of the crystal structure information that simultaneously preserves the vital physics of the problem and serves as an efficient set of features for the machine learning algorithm 
we considered a number of feature representations one hot vector the simplest encoding we propose is a one hot vector representing a material s elemental composition 
the dataset cumulatively uses 94 elements so in this representation the n th element of a length 94 vector contains the fraction of the unit cell atoms comprised of element number n for instance the fictional compound h 1 al 2 is represented as the vector 1 3 0 2 3 0 0 since h and al are the element numbers one and three respectively 
this representation of features has the advantage that a neural network can learn about the properties of each element independently however the one hot representation has numerous disadvantages 
it lacks any structural data about the crystal which is physically vital e g diamond is an insulator while graphite is a conductor although both are crystals of pure carbon 
furthermore the one hot vector is a 94 dimensional vector such a large feature vector can result in a high variance model 
finally heavy elements such as eu are rare and only appear a few times in the training set 
therefore the model will be under trained with respect to a large proportion of the features resulting in a high variance model group one hot vector rather than introducing a new feature for each of the 94 elements we propose a group onehot encoding which uses an 18 dimensional one hot vector having one component for each group in the periodic table 
much of the relevant physics to material properties is contained in the number of valence electrons of elements a common quantity among elements of like group 
at the potential expense of increased bias this representation should reduce the model s variance relative to the one hot encoding as it associates rare elements with common elements of the same group coulomb matrix to create a more complex feature set we would like to add structural information about the crystals 
the real space coordinates of the atomic positions however are degenerate in choice of coordinate and therefore are themselves a poor set of features 
the coulomb matrix is a coordinate invariant representation of this information 
the coulomb matrix expresses the potential energy between pairs of atoms in the crystal which depends on their pairwise distance and atomic numbers 
the coulomb matrix c is a symmetric matrix defined element wise aswhere z i is the nuclear charge on the ith lattice site and r i is its position 6 
the coulomb matrix however lacks a specific information of the crystal s constituent elements and only describes the atomic charges 
therefore the neural net cannot learn properties associated with individual elements for instance ar and k have very different electronic properties despite their charges z differing only by 1 
a second drawback of the coulomb matrix is that its size is the number of lattice sites in the compound which varies between compounds 
we assuage the latter issue by choosing all coulomb matrices to be the a certain size n smaller coulomb matrices are padded with zeros and crystals with too many atomic sites are disregarded as the coulomb matrix is symmetric we include only its upper triangular entries as features reducing the number of features from n to n 1 2 
the upper triangular components are reshaped into a vector valued feature coulomb matrix extensions while the coulomb matrix is invariant in the choice of real space coordinates it is not invariant under permutations of its rows and columns although such permutations do not alter the material represented by the matrix 
we employed two methods to alleviate finally to present our models with the maximum amount of information we used an encoding including both the coulomb matrix and the one hot representations atom s groups as well as an encoding including both the singular values of the coulomb matrix and the one hot representation of each element to visualize the dataset we performed principle component analysis pca on the dataset under the element onehot feature encoding
we developed a pipeline to transform the inputs to our model the crystal structures of materials into useful predictions 
the crystal structures are first encoded into appropriate feature representations 
next a classification stage uses these features to predict which materials are metals and which are nonmetals 
the predicted nonmetals are then sent to a regression stage which predicts their bandgap the classification and regression learning models were trained on the training set and optimized using the development set 
optimization included testing different learning algorithms on each feature encoding and then tuning hyperparameters on the best performing encoding algorithm combinations 
to prohibit the regression stage from being biased on the performance of the classification stage the regression stage was trained and developed on the set of true nonmetals not the set of predicted nonmetals the latter inevitably includes misclassified metals and omits misclassified nonmetals 
the performance of the full pipeline was characterized using the withheld testing set we developed code to parse and encode the dataset in python 
we used a combination of self develop machine learning algorithm implementations written in python and methods from python s scikit learn package 
we classified materials as metals or nonmetals classification using logistic regression neural networks nns and random forest rf classifiers 
we predicted the size of the gap using linear regression nns and rf regressors 
we describe these models taking m as the number of training examples n as the number of input features x r m n 1 as a matrix of input features and y r m as the output vector of bandgap sizes 
denotes parameters that the model learns and takes different sizes in the different algorithms 
j represents the cost function and is a regularization parameter that prevents the models from over fitting the training set by enforcing that the magnitudes of the learned parameters remain small linear regression the hypothesis is linear in the input features h x x 
the cost function is j 1 2m x y t x y 2m 2 1 where r n 1 is a vector of parameters that the model learns with the first component being a bias term and 1 denoting a vector of all the components of excluding the bias term logistic regression the hypothesis is now given by a sigmoid which introduces nonlinearity h x y log 1 h x 2m 2 1 
nn neural networks are nonlinear models that can approximate high variance data 
they can be used for either classification or regression depending on the loss function 
the number of hidden layers number of neurons per layer and the activation function are all model choices 
for binary classification the cost is given by wher y a 2 is the output of the nn 
for real valued regression the cost function is given by j 1 2m i y i 2 2m 2 1 
we used neurons with rectified linear unit relu activation a g z max 0 z 
the output neuron of the nn classifier is a sigmoid and that of the nn regressor is a g z z random forests rfs are averages across independently grown trees and are less prone to over fitting than individual trees 
decision trees are classifiers that repeatedly partition the data into branches chosen to minimize the gini impurity i p 1 c 0 1 p 2 c where p c is the probability that an example in the branch is of class c rf classifiers take the mode over many individual trees 
regression trees choose branches to minimize the total variance within branches i v 1 2 s i j s x i x j 2 
rf regressors take the mean over many individual trees 
the parameter max features determines the number of random features considered when determining each split metrics along with the misclassification error we quantified the performance of the classifier by the area under the receiver operator characteristic roc 
the roc displays true positive rate versus the false positive rate as the classification threshold is varied 
we also considered the f1 score f 1 2t p 2t p f p t n where t f p n represents the number of true false positives negatives 
we quantified the performance of the regressor using the median normalized error and the rmse
all learning models for the classification and regression problems were tested on all feature encodings having finalized the full prediction pipeline we characterized its performance on the test set
the learning curve in both the classifier and the regressor would benefit from a larger training set 
the augmented coulomb matrix encoding increased the number of training examples by a factor of 10 but resulted in no performance benefit at a high computational expense 
incorporating additional dft databases into the training dataset may improve our predictions in literature 0 01 ev is generally used as the cutoff between metals and nonmetals 
however nearly half of the nonmetals in the dataset had gaps between 0 01 ev and 0 1 ev
our model s accuracy is sufficient to provide useful predictions for high throughput materials screening 
the leading performance of the element one hot encoding is surprising as it includes no information about materials crystal structures 
the group one hot encoding performed well at classification but poorly at regression 
we explain this discrepancy by noting that the valence of an atom which is given by it s group is important for predicting metallicity but that the value of the bandgap depends on electronic potentials information that the group one hot encoding lacks 
adding the singular values of the coulomb matrix as features supplies this information and correspondingly increases the regressor s accuracy 
much of our model s error came from errors in the training dataset for incorrectly labeled examples with gaps between 0 01 ev and 0 1 ev 
to correct for this issue we propose to augment the dft generated dataset with experimentally obtained values for small bandgap semiconductors these examples would be heavily weighted in the training algorithm we propose future work to add features representing element s electronic attributes such as the orbital character of their valence electrons to increase the complexity of the features 
further to better represent structural information we propose to use a unsupervised learning such as the mixture of gaussians model to transform the atomic positions into categories of crystal structures based on their symmetries 
including these features will allow a neural network to discriminate between categories of materials 
nowadays solving the energy challenge by harvesting the energy directly from sunlight through photosynthesis has became an attractive way 
many efforts have been made to find materials that can output chemical fuels using solar energy 
recently biv o 4 came out as the most promising material for solar energy conversion and environmental protection 
the chemical reaction of biv o 4 can be used as a photoanode that oxidizes solar water to o 2 in the photoelectrochemical cells 
based on that purpose the surface physicochemical properties of biv o 4 has been carefully examined in order to improve the performances of biv o 4 based photoanodes 
according to g l 
li s study subsequently identification of 111 facet of biv o 4 becomes important 
the traditional method would be using electron backscatter diffraction ebsd to determine crystal facets however this method is restrictive and not quantifiable 
in addition there exists limited numbers of research on facet detection especially for biv o 4 
therefore this project is focusing on the identification of 111 facet of biv o 4 from sem images 
our dataset contains around 3000 samples including approximately 60 training data and 40 testing data 
each sample is a 160 215 pixel grey scale sem image which was generated from lab experiments these sem images were labeled into either positive or negative 
positive label means the sem image contains 111 facet and negative label means no 111 facet is detected 
in order to prevent error from data labeling the labeling process was guided by a phd student who leads the experimental side of this project 
labeled sem images were saved into either positive folder or negative folder and the algorithms could read the label of images based on the folder s name 
due to the high reaction tendency of 111 facet exposure of this facet requires complicated chemical reaction which makes the sem images difficult to obtain 
therefore this project was facing a difficulty caused by this relatively small dataset 
data augmentation
to solve this problem data augmentation which includes cropping scaling and flipping mirroring became an essential part of this project 
as for cropping different sem images have different working distance which is the distance the beam is focused at this leads to different image scale and resolution 
therefore high scale images were cropped into 9 pieces and low scale image were cropped into 4 pieces see
another challenge came with this project was the shape variation of
edge detection
hog is one of the most common techniques to extract features for object detection especially for human detection 
hog represents each image by a feature vector by calculating distributions of intensity gradients which preserves the information of local object appearances and shapes to a large extent 
hog
since we aimed to determine the existence of 111 facet by detecting the shapes of biv o 4 in our project we chose hog as as one of the candidate methods to extract features 
we firstly used hog to extract features from sem images 
after calculating hog a two dimension image was converted into a one dimension vector 
svm with hog
we then input the vector into svm and determined whether it was positive or negative 
we mainly considered two kinds of kernels of svm including linear kernel and gaussian kernel whose formula are shown in equations figure 5 svm with hog
the most common method for image classification is cnn 
therefore in this project we used the architecture as shown in sigmoid
shallow cnn
deep neural networks are of great importance for many image recognition tasks and we also tried using the resnet to train our data 
kaiming he et al proposed the resnet framework to solve the degradation problem and ease the training process of deeper neural networks 3 
resnet
compared with standard neural network architectures resnet has shortcut connections performing identity mapping and recasts the original underlying mapping h x into f x x see
as illustrated from the result shallow cnn with dropout achieves the highest test accuracy 
it could also be observed that both svm and shallow cnn overfit slightly 
as for the performance of resnet transfer learning on pretrained resnet results in better performance 
l2 regularization might be helpful for controlling the overfitting of the cnn training process however it also leads to a drop of test accuracy 
therefore using only feature dropout is the better choice for shallow cnn 
resnet we tried several numbers of layers for resnet and we found that prediction accuracy for our data did not benefit much from deeper network might because we only have small dataset or this network structure is not suitable for our data 
shallow cnn after tuning parameters shallow cnn obtained the test accuracy of 87 which is the best accuracy among all models 
thus this structure might be more suitable for our data comparison svm method has fewer parameters and simpler structure compared with neural networks 
thus it computed much faster than neural networks but the accuracy was much lower 
compared with resnet shallow cnn had better control on the entire learning process 
therefore by tuning the parameters it can get higher accuracy data during our labeling process even though it was guided by experienced people in this area there are still some error sources such as low quality of sem images different viewpoints and sizes of particles and various shapes of particles 
these led to ambiguous determination of 111 facet which might influence the accuracy of the models 
in the future we intend to improve our project mainly in application aspect and algorithm aspect 
in terms of application we will expand 111 facet detection to other kinds of facet detection 
and we will transfer classification problem into regression problem which is to determine the proportion of target facet 
in terms of algorithm principal component analysis pca will be added between hog and svm in order to avoid overfitting 
additionally more feature descriptors such as sift surf and orb will be considered 
for the two cnn models shallow cnn and resnet we will use heatmap to detect the data concentration of the algorithms and see why shallow cnn performs better than resnet 
based on the result of heatmap parameters and architectures of models mentioned above will be further modified to improve the performance 
this project is completed with the conscientious and cooperative effort of three group members wanling liu zixi liu and jiyao yuan evenly in terms of idea generation model establishment code deployment as well as report writing 
we would like to express deepest appreciation to thomas mark gill for his data and guidance 
code at https drive google com drive folders 1f7lffumve tvuwqbp7jalkabglupok8f usp sharing
the wide field imager wfi on athena x ray observatory is a detector system which will include on board processing algorithms designed to reduce the particle background
there are some existing works to understand model and reduce the wfi background with classical algorithms without using deep learning method 
one of the approaches is to reject hits which are next to a pixel whose deposited energy is more than 15 kev in summary these existing works all tried to find features manually to distinguish between particles and x ray photons 
in contrast my project aims to discover features using deep learning neural network which is the main difference 
the data i analyze are from geant4 an input sample to dbscan records the energy levels of the pixels on the detector in one frame which is a 500 by 500 grayscale image 
before sending the single cluster figures into the neural network i map every pixel value through the heaviside step function 
in other words i assign a constant value to every non zero pixel 
this preprocessing is very useful because it turns out that the prediction accuracy becomes more than 99 in this case while the prediction accuracy will be only about 70 if the neural network runs on the single cluster figures directly 
the intuitive reason for this is that the energy of a photon is most probably much weaker than that of a particle which results in that the neural network does not pay much attention to photon pixels the feature in the algorithm is just the original input 
in this project i use dbscan an unsupervised learning algorithm to cluster the activated pixels and use deep learning algorithm to identify whether each cluster contains photons or not 
methods 
the ultimate goal of this project is to identify whether each activated pixel belongs to a photon or a particle 
but it is not practical to run a neural network on the whole detector image 
the goal of this section is to cluster the activated pixels based on their connectivity where each cluster will be processed by the neural network in the next section after comparing several unsupervised learning algorithms i found that dbscan densitybased spatial clustering of applications with noise algorithm 1 
classify each data point core point a point that has at least minp ts neighbor points within its radius 
border point a point within the radius of a core point but has less than minp ts other points within its own radius 
noise point a point that is neither a core point or a border point 
if one data point is within the radius of a core point these two points are considered directly density reachable 
larger clusters are formed when directly density reachable points are chained together 
after the preprocessing mentioned above the input 50 by 50 matrix will be processed by the neural network the first layer of this neural network transforms the images from a matrix of 50 by 50 pixels to a 1d array of 50 50 2500 elements after the inputs are flattened the network consists of a sequence of two fully connected neural layers 
the first fully connected layer is a 256 nodes relu layer 
deep learning
the second and last layer is a single node sigmoid layer which returns the probability that the label of this figure is 1 
this process can be expressed as equations the final prediction accuracy of the network on the validation set is 99 1 what s more i have also tried cnn network 
but the accuracy is only about 50 which is much poorer than the fully connected network 
in my experiment for dbscan i set 3 and minp ts 3 in which case the algorithm clusters activated pixels pretty well 
i created 1000 500 by 500 detector images from geant4 and sixte databases randomly for test each of which includes 50 particle events and 50 x ray events left panel of in order to evaluate the error of my algorithm i compare each processed image with the corresponding perfect image generated together with the test set but no particle included 
experiment 
the performance metric in this project is defined as where n output only n perf ect only represent the number of non zero pixels only in the processed image and the perfect image respectively n perf ect represents the number of non zero pixels in the perfect image 
the simulation turns out that the average of the metric over 1000 test samples is equal to 97 2 for the purpose of giving an example 
in summary i use dbscan unsupervised learning algorithm to divide the raw image into clusters for the convenience of later processing 
then i use deep learning neural network to predict whether a cluster includes useful information and reject useless ones 
conclusion and future work 
the experiment shows that my algorithm performs very well on simulated data one natural future work is to run my algorithm on real data collected by other satellites 
what s more in my project i do not modify clusters containing both particles and photons for the sake of not losing x ray information 
therefore one another possible future work is to deal with those clusters very carefully to eliminate particle noises while keeping photons 
i would like to thank prof steven allen s lab for the use of the geant4 and sixte databases as well as dr dan wilkins for immensely useful discussions and advice code https github com max snow reducing athena particle background
the three major fossil fuels petroleum natural gas and coal combined accounted for about 77 6 of the u s primary energy production in 2017 
in fact at current usage levels oil will run out in 53 years natural gas in 54 and coal in 110 
clearly there is a need to move towards a society that refrains from exploiting unreplenishable natural resources consequently the world is evolving in the direction of renewable energy sources 
while solar power and wind energy are rapidly gaining acceptance across the globe a limiting factor to these resources is their dependency on climate conditions 
in order to evaluate the efficiency of the aforementioned renewable sources we built a machine learning model to predict solar and wind energy outputs at a given location using local weather data 
this in turn allows us to determine the optimal renewable energy source for a given location the input to our algorithm is weather data for a particular location 
we then use one of four supervised learning models linear regression an svm an artifical neural network and a generalized additive model gam to output the predicted solar energy output per unit area of the solar cell and or the predicted wind energy output per unit area swept by the wind turbine 
in general previous attempts at solar and wind energy output predictions can be largely grouped into three categories those utilizing artificial neural networks those using linear formulas and those using nonlinear formulas 
cammarano et al 
finally yang et al 
keeping the objective of predicting solar and wind energy outputs using weather conditions in mind there are three datasets relevant to our project 
the first is an online weather api note that it is necessary to synchorize these datasets 
in particular to construct a solar training example that uses certain weather values as input features and the solar production as the output value we used the coordinates and time component of the solar data to request the relevant weather data 
similarly we again used the coordinates and the time component of the wind data to harmonize the wind dataset with the weather api the weather api response is represented as a json object that includes weather data for zero or more stations situated in the geographical region defined by the request 
data for each stations is typically indexed with respect to a list of timestamps that fall in the time period defined by the request 
the solar data is also in the form of a timeseries discretized into 5 minute intervals over a full year 
to process the solar dataset we decided to average the energy production over a day and utilize any weather timestamps that fall in that range to serve as the input features 
we also normalized these features by subtracting the mean and dividing by the standard deviation since they have widely differing ranges pressure values of 10 5 pascals are very common while temperature is generally within the range 50 100 celsius 
in the end we had 962 162 examples which we shuffled and divided as 98 1 1 into the training validation and test sets finally the wind data is presented as the average energy output for wind turbines over an year so we average the weather data over the same year to use as the representative input features 
once again normalization of inputs was key due to the same reasons as given above for the solar dataset 
in this case we had 29 315 total examples which we shuffled and divided as 70 15 15 into the training validation and test sets 
for each of solar and wind energy we implemented four machine learning models linear regression svm artifical neural network generalized additive model that were trained validated and tested independently 
in the next few sections we describe each of these models in greater detail 
we used linear regression with mini batch gradient descent as our baseline model to train a set of parameters for our four features discussed previously 
at each iteration the update rule for the j th parameter using mini batch gradient descent is where is the learning rate and m is the batch size 
essentially the linear regression algorithm finds a hyper plane in high dimensional space that minimizes the root mean squared error of the predictions and the actual labels 
at each step in the gradient descent we update our parameter according to the computed gradient which further reduces the loss 
this is repeated until convergence to give us the optimal parameters 
we also built an svm model using the gaussian kernel and l 1 regularization 
svm is a reasonable model because there can be many hidden complex and non linear interaction between the weather features which cannot be captured by a simple linear model 
in particular because we only have four raw input features the projection of our data into an infinite dimensional space using the gaussian kernel is extremely powerful and can reveal valuable insights into the behaviour of the data in class we learned about using svm in a classification setting but in our model svm is used to perform regression 
as such there is a slight change in the constraints of the optimization problem which is shown below 
in order to train the model parameters we would like tosubject to the constraintsthe idea of svm is to find a hyper plane in high dimensional space that fit the data which maximizes the functional margin 
as it is extremely improbable for that all the data points to lie exactly on the hyper plane we will ignore the error for points that lie within a small fixed distance of their true values 
and if the data is non linear cannot be modeled by hyper planes we project the data into a higher dimensional feature space using kernels where the projected data becomes linear 
in this case we used the gaussian kernel to project the data into an infinite dimensional space 
we also implemented a fully connected neural network to model our data 
we decided to use a fully connected neural network because the features are all highly interconnected and the connected layers allow us to model their unseen interactions with one another 
artificial neural network
moreover we decided to use relu as the activation function of the output layer because the energy output must be non negative deep learning and neural network is a powerful tool because it enables us to capture complex relationships between the features that we may not be able to reason without the aid of machines 
each of the links between two neurons models some hidden dependency between them 
while fully connected layers may sacrifice run time it weighs the influence of all of the neurons in the previous layer when performing forward propagation 
and in order to find the parameters bias and weight matrices of the neural network we use a method called back propagation 
similar to gradient descent at each iteration we update the parameters by taking the gradient of the loss with respect to the parameters 
finally we conducted some research and found that a generalized additive model may model our data well 
similar to general linear models glm a gam assumes that y follows some exponential family distribution 
generalized additive model gam 
however instead of trying to fit the data to some set of parameters the goal of gam is to model the output as a sum of functions of the inputs 
that is we want to find 0 and smooth functions f 1 
the use of arbitrary smooth functions f i allows us to model arbitrary relationships between the input features and the output 
f m such that gam is a compelling model because it segregates the effects on the output from each of the input features 
its additive nature also allows us to visualize individual relationships through dependency plots as shown in the results section 
in this particular case our functions f i s are all single variable whose domains are a single input feature 
in gam the smoothness of the functions f i can be adjusted using the number of splines which is a way to estimate functions smoothing the functions allow us to capture the general trend while eliminating noise or fluctuations which can negatively effect our predictions 
the details of the estimation will not be discussed but the greater the number of splines the smoother the functions will be 
generalized additive model
we played around with the spline numbers and found that num splines 4 20 4 100 for solar data and num splines 5 18 10 30 for wind data produces the highest accuracy 
the four components corresponds to the features wind speed air temperature relative humidity pressure 
since we have designed multiple regression models the primary metric we use to measure the performance of a model is the average error 
in particular we utilize the root mean square error rmse as our primary metric 
note that the rmse is defined as where m is the number of testing examples y i is the true output of example i and i is the predicted output of example i by the model 
it is however interesting to note that while the neural network achieves low training error for wind energy it has a relatively high test error 
this indicates that the neural network is overfitting to the training data in the case of wind energy 
similarly note that while the training error is fairly low for all models in the case of solar energy each of the models ends up having a much higher test error relatively 
this indicates that the models are once again overfitting in the case of solar energy 
this could be so since the training set is so large while the validation and test sets are relatively small 
for this purpose we introduced regularization into our code but it did not have a significant impact most interesting perhaps is the performance of the generalized additive model gam 
from partial dependency plots allow us to gain a visual intuition on how features affect the output value 
for instance by looking at the solar output vs relative humidity graph in
over the course of this report we have described the set up and utilization of four distinct supervised learning models in order to predict renewable energy outputs per unit area given the weather data for a particular location 
during the training and testing process gams produced the best performance for both solar and wind energy albeit with close competition from neural networks in the former category and svms in the latter the success of gams might very well stem from the fact that they allow us to model arbitrarily complex relationships between individual features and the output 
this might also serve as justification for the neural network s performances as it also allows us to glean and model increasingly complex interactions between the input features 
meanwhile the fact that the svm with its gaussian kernel did not produce stellar results for solar energy suggests that the selected input space is reasonably representative of the features that affect solar energy output 
at the same time close competition from the svm and its derived features in the case of wind energy leads us to believe that we may need to utilize additional features in the case of wind energy since the relationship between the chosen features and the output definitely does not seem linear given more time and resource we would like to develop more complicated models that perhaps combine the advantages of the models we have discussed in this paper 
at a higher level we would also like to extend our application to even more renewable energy sources 
the end goal here is to determine the most efficient renewable energy source for any given location by utilizing not just weather data but also information like proximity to resources water bodies for hydro power generation for instance 
we introduce a novel approach to solving pde constrained optimization problems specifically related to aircraft design 
these optimization problems require running expensive computational fluid dynamics cfd simulations which have previously been approximated with a reduced order model rom to lower the computational cost 
instead of using a single global rom as is traditionally done we propose using multiple piecewise roms constructed and used with the aid of machine learning techniques 
our approach consists of clustering a set of precomputed non linear partial differential equations pde solutions from which we build our piecewise roms 
then during the optimization problem when we need to run a simulation for a given optimization parameter we select the optimal piecewise rom to use 
initial results on our test dataset are promising 
we were able to achieve the same or better accuracy by using piecewise roms rather than a global rom while further reducing the computational cost associated with running a simulation 
improving the design of aircrafts often requires solving pdeconstrained optimization problems such as maximizing the lift drag with respect to some parameters 
here is an optimization vector containing parameters that we want to optimize 
it is also common practice to have a lower bound lb and upper bound ub on this vector to find the optimal we must update it iteratively running a computational fluid dynamics cfd simulation at each optimization step 
in figure
fluid flow problems are governed by nonlinear partial differential equations pde 
solving these equations using cfd techniques such as finite volume method is equivalent to solving a set of nonlinear equations where is the set of parameters for our simulation and w is the unknown vector of dimension n w p r n called the state vector 
high dimensional model hdm 
specifically a row of the state wris represents a property of the fluid flow such as pressure at point i of the cfd mesh 
thus the cfd mesh has n points 
in unfortunately this problem is very expensive to solve when n is large as it is in the case of solving cfd problems where n is in the order of thousands or millions 
in order to solve cfd problems faster a reduced order model rom can be used in order to approximate the hdm where v gl p r n n denotes the global reduce order basis rob and w r p r n denotes the new vector of unknowns called the reduced state 
substituting eq 
reduced order model rom 
4 into eq 
now the least squares problem to solve is min wrpr n rpv gl w r p q q 2 2 6 
to build a rom we first need to find the global reduced order basis rob v gl 
this is done by solving the non linear equation 2 for many optimization vectors 
global reduce order basis rob v gl
thus given a specific vector i we can define the solution state vector therefore for a set of k optimization vectors t u k 1 we solve 3 and we get a set of state vectors twp i qu k finally we perform a singular value decomposition svd on the matrix m to compute the global rob v gl here v gl is computed by only selecting the first n columns of the matrix u and therefore v g l p r n n 
thus the global rom has dimension n and can be used in the entire domain d wp q v gl w r p q p d 10 
in this project we propose creating multiple piecewise roms in the domain d each having smaller dimensions than a global rom would 
these piecewise roms do not need to be accurate in the entire domain d but only in limited region of the design space d by using machine learning techniques we hypothesize that we can improve quality of the reduced order basis rob within a given design space d then using these piecewise roms will allow us to solve even cheaper least squares problems than 6 whilst maintaining a similar or better level of accuracy relative to the hdm to do this we must group the precomputed solutions twp i qu k 1 into multiple clusters and then create multiple piecewise reduced order basis tv i u c 1 where c is the number of clusters 
piecewise roms in the design space
for instance choosing two clusters in figure 5 we can see a schematic comparison of a global rom versus 2 piecewise roms built by clustering twp i u k 1 into 2 clusters 
on the left all the training solutions tw i u 10 1 computed solving 2 using t i u 10 1 are used to create v gl and therefore a global rom 
on the right we first cluster the training solutions tw i u 10 1 into 2 clusters and then we construct 2 reduced order basis v 1 and v 2 
v 1 is built using the solutions computed using the parameters t 1 5 6 7 10 u and v 2 using t 2 3 4 8 9 u 
as such the global rom uses v gl p r n n 
the two piecewise roms instead use v 1 p r n n1 and v 2 p r n n2 respectively where by construction n 1 n and n 2 n therefore the first piecewise rom makes the following approximation using v 1 wp q v 1 w r p q 11 and the second piecewise rom makes another approximation usingtherefore by using either 11 or 12 we can solve min wrpr n rpv i w r p q q 2 2where i indicates which piecewise rom i is used 
using this method with piecewise roms gives rise to two machine learning problems that we must solve 1 
given multiple precomputed solutions tw i u k 1 how do we cluster them most effectively into tv i u c 1 2 
given an arbitrary which piecewise rom tv i u c 1 should we use to best represent the hdm in the next section we describe the methods we have implemented for addressing the above problems 
our proposed methodology to solve a pde constrained optimization problem operates in two phases an offline phase and an online phase 
in the offline phase we cluster precomputed training solutions from which we build our piecewise roms that are used in the online phase 
overview
in the online phase we query multiple during the optimization process 
for each queried i we need select which piecewise rom v i to use 
then we run the simulation to compute wp i q and lift drag p i q 
since our goal is to break our domain d into smaller sub domains we believe clustering the training points based on the euclidean distance between features will be most effective 
we have applied three algorithms in order to implement this k means expectation maximization to fit a gaussian mixture model and agglomerative clustering in terms of clustering features we have considered using b a features as they provide more information on the physics problem we are trying to solve and obviously lift and drag are both related to the fluid state 
clustering
the number of training points used when constructing roms are relatively low when compared with other machine learning problems 
additionally we do not have ground truth values for our classifications and only are able to determine how well our algorithms performs after doing a rom simulation 
therefore we have chosen to evaluate two simple methods nearest centroid and multinomial logistic regression as our algorithms for performing classifications 
for nearest centroid we simply select the piecewise rom whose clustered points have the closest centroid to the queried point while multinomial logistic regression is trained using a cross entropy loss and the labels output from clustering during the offline phase 
since during the online phase we will not have access to the lift or drag for a given query point we are only able to use as a feature for classification 
in order to to create a train validation test set we sampled the parameter domain d r 3 to compute 90 solutions twp i qu
design of experiment
for all of the following experiments we define our error to be the difference in lift drag calculated with a rom and lift drag calculated with the hdm 
we refer to mse as the mean squared error across our test points and max error as the highest percentage deviation from the lift drag calculated with the hdm 
for our first set of experiments we determine the best parameters for our methodology given our design space 
specifically we use the validation set to determine the clustering algorithm
model parameter experiments
clustering features number of clusters for each experiment we ran three tests each with 20 training points folded from our total 50 training points and test the error on our validation set 
in subsection 4 3 we investigate using a predictor to automatically determine our parameters without having to run any simulations on a validation set first we tested for the best clustering algorithms fixing our classification algorithm to nearest centroid the number of clusters to 4 and clustering features to t lift drag u 
classification algorithm
finally we tested the effect of using different numbers of clusters 
we used k means for clustering nearest centroid for classification and t lift drag u as the clustering features 
with our optimal clustering classification algorithms and features derived from subsection 4 1 and clusters of size 2 and 4 we tested the accuracy of our methodology versus a global rom approach for calculating the lift drag the objective function of the optimization problem 
for the test with two clusters we show the offline clustering phase in figure 12 
global rom comparison
in practice users would not want to have to use a validation set to determine the best clustering parameters as the time required to do this may outweigh any efficiency savings from using piecewise roms 
therefore a predictor for a reliable set of clustering parameters is necessary for real world applications 
predictor for rom accuracy
we tested different cluster scoring methods including silhouette score
from our results we see that the k means and agglomerative clustering perform somewhat similarly compared to the gaussian mixture model 
this makes sense as the points used in the offline phase are not necessarily gaussian distributed while k means and agglomerative clustering less strong of an assumption 
as for clustering features the difference between feature sets is relatively small 
this makes sense as for many points in the design space our optimization vector will be highly correlated with the lift and drag we are also able to see an interesting trade off when it comes to the number of clusters used 
from the results we can clearly see that the error decreases with the number of cluster sizes 
this is sensible because as we increase the number of clusters the number of points are assigned points used to create each rob decreases decreasing the accuracy of the rom approximation 
however as the number of points used to build the rob decreases so does the computation cost of running a simulation with the corresponding rom 
therefore the number of clusters used should be chosen on a per application basis where the user would select the number of clusters corresponding to the acceptable error overall we can see that the our proposed methodology is superior when compared with using a global rom 
we can see that we are either able to get a much higher accuracy than the global rom with a similar computational cost related to the rom size or we are able to achieve a similar accuracy with half the computation cost of the global rom with regards to predictors for parameter selection we can see that all three cluster scoring methods show some indication that they could be used as a predictor for cluster rom accuracy at least for our design space 
silhouette score and the calinski harabaz index may be slightly more correlated than the davies bouldin as the distance between points on the edges of clusters are reflected in their scores rather then only accounting for the distances between cluster centroids 
however more rigorous testing is needed especially we do not know if it will generalize to other pde constrained optimization problems 
in conclusion we present a novel approach to solving pdeconstrained optimization problems by utilizing multiple piecewise roms 
this approach has proven to be both more accurate and more computationally efficient than using a single global rom 
it shows particularly strong promise for time constrained applications with high dimensional design spaces 
in these scenarios the global rom would need to be very large in order to be accurate across the whole design space and thus it might not be able to meet real time deadlines 
piecewise roms on the other hand can be more efficient and thus able to meet the timing constraints we would like to continue testing the performance of our approach in more realistic higher dimensional design spaces 50 60 parameters 
for this project we chose a limited design space due to time constraints as running tests in higher dimensional design spaces is naturally more computationally expensive and takes more time 
we would also like to continue research on predictors for clustering effectiveness as this is a key component for this approach to be practical in real world problems 
the first part of this project was discussing and creating a new methodology for solving pde constrained optimization problem 
this was a significant part of the project where both forest and gabriele discussed on the optimal approach to take 
to implement this methodology gabriele wrote code to build and run roms from a set of training points in addition to writing code to generate the data to start the experiments 
forest was responsible for implementing the machine learning algorithms from external libraries as well as automating testing 
both gabriele and forest contributed towards research and decision making for the use of machine learning techniques in this project in addition writing routines to output and post process results for analysis unfortunately the code must be run on a super computer with many external libraries from the stanford aeronautics astronautics department 
we have included a zip file containing the only the code written for this project available at https drive google com file d 1bp4iw6rir cn3hxwl58cf pi5xppsi4w view usp sharing
humans are generally good at categorizing and organizing music 
we may create novel playlists containing songs that may seem very dissimilar according to any baseline similarity measures 
it is this novelty and deeper level of understanding that we attempt to learn in this work how do users put together novel playlists 
music listeners create playlists based on a multitude of strategies and intents 
many users put similar sounding songs together some make playlists solely from one artist others generate heterogeneous mixes spanning multiple genres eras and soundscapes 
clearly the problem of playlist classification increases in difficulty as playlist moods become less concrete and or separable 
to tackle this problem we will be using spotify s api to gather audio artist and genre data for selected playlists 
with a more carefully curated and specialized fingerprint of each playlist we hope to teach an algorithm to understand what makes that user and their playlists special 
the playlist lengths in the dataset 
for example in the toy dataset section 3 m 1044 
each song is labeled with an integer y i c representing one of the c output classes playlists 
in the toy case c 13 
in general we would describe our problem space as non radio playlist continuation to distinguish from real time radio based solutions 
there are a few core concepts in our approach 
first we attempt to model the theme of a playlist with minimal assumptions about the user 
second the model should be scalable and generalizable to any user and any song on a service we use spotify 
third the model should be novel and specific to each user 
these concepts are informed by three fundamental pieces of literature in the space which conclude the following individuals apply a wide variety of methods and reasoning for how and why their playlists are created model playlists using random walks on a hypergraph of song nodes especially due to large data these implementations would not be fully appropriate for our application however they introduce important core concepts and algorithms that we have borrowed such as hybrid feature sets both collaborative filtering and contentbased features 
for our first study we started with data we perceived to be most separable a toy set of c 1 13 spotify curated playlists country by the grace of god spread the gospel for a total of m 1 1044 tracks 
the first experiment is an idealized fully supervised setting 
dataset features
we also conduct a second study generalizing the most successful model on real user data to truly challenge the models for each playlist we used spotify s api to pull tracks audio features per track and genre tags per artist in the dataset spotify has developed 2095 genre tags spanning from broad ex 
pop or r b to extremely granular ex 
australian dance and big room 
these audio features and genre tags are created with spotify s internal machine listening and analysis capabilities acquired mostly from echonest for both experiments we split the dataset 80 20 80 train and 20 test 
in the future we may explore data augmentation techniques to expand the size of these datasets most of the audio features ex 
danceability energy and speechiness are measures between 0 1 
however key takes on an integer value 0 11 loudness is a float 60 0 measuring average decibel db reading across a track and tempo is a float representing beats per minute bpm 
as such we explore the effects of applying the standard preprocessing step of subtracting the mean and scaling by the variance of each feature 
this ensures that the relative scale of these features do not negatively skew the results 
a summary of our results is in section 5 
the models we elected to compare are perceptron regression with one versus all binary classification for each class various svms polynomial sigmoid and rbf kernels and two neural network architectures the perceptron update rule given a sample prediction h x i is given bythis update rule comes a generalization of the gradient descent update on the negative log likelihood for logistic regression 
svms attempt to find a boundary which maximizes the geometric margin between classes 
specifically denoting the geometric margin we look to solve the following constrained maximization problem w 1 i e find the separating boundary given by w and b such that all samples are a distance of at least away from the boundary 
the true algorithm solves the dual form of this primal problem as the problem as stated above is non convex 
the library used for the regression and svms is scikit learn python s machine learning platform 
lastly a neural network can be thought of as a sequence of linear transformations non linear functions applied to an input set of samples x r m n to obtain a prediction vector y r n 
y a h a 2 where w i is a linear transformation a i is a non linear activation function applied element wise to its argument and h denotes the number of hidden layers 
the neural network attempts to learn a weighting of the input features w i s that best classifies the output 
the neural networks were implemented in pytorch though we test 6 models architectures in this work we have elected to focus most of our efforts on the neural networks to solve the problem 
it is difficult sometimes even for humans to discern exactly what holds a playlist together 
furthermore the relationship between song features and playlists will not lend itself well to geometric separation in a feature space further complicated by the fact that one data point song may have multiple labels belong to multiple playlists 
the complexity of this problem indicates that less complex algorithms may not be suited to solving the problem effectively 
for all tests below we took test accuracy to be our primary metric of success 
ta correctly classified total of samples many models achieved high training accuracy but unless the test accuracy matched this level the model was likely overfitted 
we compared perceptron with polynomial sigmoid and rbf kerneled svms with and without the preprocessing step of rescaling the features on the toy dataset 13 spotify curated playlists 
we performed k fold cross validation with k 5 
regression svms
the results are summarized in without performing the preprocessing step the accuracy of the results takes a significant hit 
on preprocessed data rbf kerneled svm reliably performed the best achieved the highest test accuracy 
tuning the penalty parameter on the error term we obtain a final test accuracy of 0 80 0 05 
the precision recall f score and support results are tabulated in note that the playlists celtic punk and spread the gospel achieved 100 precision by the tuned svm all corresponding tracks in the test set were correctly classified into these playlists 
this demonstrates the efficacy of these methods on more unique genre specific lists 
kitchen swagger on the other hand is a harder playlist to classify as it does not match any one genre 
we considered many parameters of our network number of hidden layers activation functions sigmoid relu identity softmax logsoftmax with vs without l 2 regulation number of iterations stochastic batch full gradient descent loss function mse vs nll etc 
we ultimately found that a neural network minimizing nll loss via full gradient descent with one hidden layer of c neurons and a logsoftmax output layer performs best on the toy set 
train test 2 identity sigmoid 0 91 0 77 1 sigmoid 0 89 0 82 with the architecture finalized we tested on a handful of real users again training on 80 of their playlists and testing on the remaining 20 
the results are summarized in
hidden layers
train test a jacob s playlists b myles playlists we see that certain users are more challenging to classify than others ex 
myles vs jacob 
user
in some ways the observed results are not surprising 
we have observed that a relatively minimal neural network does a better job at classifying playlists than perceptron and svms do 
this is understandable as the nature of curating a playlist is rather subjective and playlists are not always separable 
even so the neural network struggled against a real user set 
there are multiple factors that play into this larger labeled datasets 
in many instances the user data we were training testing on consisted of only hundreds sometimes tens of samples 
training on only 80 of this dataset decreases this number further 
this is not enough information to train a neural network classifier on nor to gather meaningful test results on given our limited features 
in the future we may either select larger user playlists or consider applying data augmentation techniques to increase the size of our set segmentation 
it is easier to classify a song into a playlist when the number of output classes c is small 
this concept is known as segmentation different algorithm 
it may be the case that even neural networks are not the best option for this problem though literature in the field suggests otherwise more track features 
we have built a classifier with only audio features and one hot vectors of genre tags for each track 
we are missing vital data such as artist info release date etc section 7 
playlist curation and music recommendation is an art that trained music analysts and disc jockeys have perfected over decades 
with the rise of massive listener data and the wealth of statistical and algorithmic developments we have begun to see a rise in computergenerated playlists and mixes 
conclusion future works
at this stage however there is massive room for improvement 
we have attempted to build a playlist classifier using audio features and genre tag data from spotify s public api node2vec 
through spotify s api we have access to relatedartists data each artist comes with a list of 20 similar artists 
we have gathered this information and built a related artists graph where each node represents an artist and edges link like artists 
using snap
in the future we hope to integrate this into our framework taking advantage of large free corpuses such as wikipedia and music website scrapes 
google has published an open source library for computing vector embeddings of words 
results from echonest show that nlp understanding of the text around music are highly effective in music classification for example the stevie wonder vector may be near the words soul michael jackson piano or motown in the vector space 
word2vec
candidates for word2vec representations in our data include artist names user generated tags both from our users and from open sources like acousticbrainz genre tags playlist titles song names etc fasttext 
facebook has developed the fasttext library for document classification 
we hope to try using it with the variety of texts that we hope to collect including the wikipedia pages for our artists user generated tags genre tags and a variety of other textual metadata from track information artist information and more 
in this case a playlist can be considered a document category and we will attempt to classify song documents as belonging to the document category 
the data used for this problem comes from spotify s web api 
kevin obtained the necessary authorization with spotify automated the request process for all playlists tracks and feature data and built a postgres database from scratch via sequelize an object relational manager orm for node js 
member contributions
once all the data for the toy set was in the database the duo implemented the baseline perceptron svm tests on the toy set using scikit learn 
amel designed and tested our neural networks using pytorch wrote script to get pca visualizations 
kevin took on advanced data collection building a related artists graph and computing vector embeddings of each node using node2vec 
understanding fluid flow in porous media at the microscale is relevant to many fields such as oil and gas recovery geothermal energy and geological co 2 storage 
properties such as porosity and permeability are often calculated from laboratory measurements or direct imaging of the microstructure 
however due to acquisition times and experimental costs it is difficult to evaluate the variability due to rock heterogeneity 
instead researchers often use statistical methods to reconstruct porous media based on two point or multi point statistics recent advances in deep learning have shown promising use of generative adversarial networks gans for rapid generation of 3d images with no a priori model
gans are made of two components 1 a generator g that creates a synthetic training image and 2 a discriminator d that tries to differentiate between the synthetic and real training image 
as the gan is trained the generator tries to create more realistic training images to fool the discriminator while the discriminator tries to become better at classifying real label 1 vs fake label 0 images 
a generative adversarial networks
z is the latent space vector sampled from a normal distribution so g z maps the latent vector to the image space 
x is the data from an image real or fake and d g z is the probability that the generated image is real 
the two competing objectives result in the following value function researchers have shown that using a deep convolutional network in the generator and discriminator models can improve synthetic image generation and training of the network 
the overall guidelines for training a dcgan usually involes 1 using strided convolutions instead of pooling so that energy resources engineering stanford university the network can learn its own pooling functions 2 using batch normalization to improve gradient flow 3 removing fully connected hidden layers and 4 using a specific set of activation functions in the generator and discriminator explained further in the methods 
the main objective of this project is to investigate the accuracy and feasibility of generating 2d 3d sandstone images through training a dcgan model 
while this has been already studied in the literature it is a relatively new field with many ongoing areas of interest such as ways to improve training stability image quality and incorporating grayscale and multiscale images this project first aims to successfully create and train a 2d gan before eventually training a 3d gan 
ii objective and scope
we can then evaluate how modifying the gan architecture affects the loss and accuracy of the generated images 
once trained these images would then be able to be used as inputs into digital rock physics calculations of properties such as permeability and capillary pressure 
understanding how permeability is affected by variations in porosity and connectivity is necessary in many research areas involving fluid flow through porous media 
the dataset was obtained from micro x ray tomography scans of a bentheimer sandstone 
the original data is a greyscale representation of the 3d solid and has been processed and binarized into the pore and solid phases 
the full dataset is 512 3 voxels large with a voxel size of 3 06 m 
in order for the training image 64 x 64 voxels to capture an adequate area the image was downsampled to 256 3 voxels with a voxel size of 6 12 m 
for data augmentation subvolumes were extracted every 16 voxels to yield 36 864 training images 
initial tests were also done on a larger dataset of 72 728 images and yielded comparable results 
to reduce training time we used the smaller training set for the majority of our tests 
we used a deep convolutional gan dcgan which uses convolutional transpose layers in the generator and convolutional layers in the discriminator as our network model
iv network architecture
to train d and g we used two different loss functions 
we first use the binary cross entropy loss function model dcgan 1 training is performed in two steps 1 train the discriminator to maximizewhile keeping the generator fixed and 2 train the generator to minimizewhile keeping the discriminator fixed 
v training loss function
in practice due to the effect of vanishing gradients it is easier to maximizeconvergence is theoretically reached when the generator can generate a distribution p g x that equal to p data x which corresponds to a discriminator output of 0 5 
further details about the training steps can be found in we also investigated the effect of using the wasserstein distance as the loss function instead model dcgan 2 the primary advantages of using the wasserstein loss are that it can prevent mode collapse in the generator and allow for better convergence 
the wasserstein distance measures the distance between two probability functions and the discriminator now becomes a critic that evaluates the wasserstein distance between the real and synthetic images 
the distance is calculated by enforcing a lipschitz constraint on the critic s model either through weight clipping or a gradient penalty where is the gradient penalty coefficient and is set to 10 for our model 
the objective of using a dcgan with our dataset is to recreate a pore network image with similar morphological properties as the original porous media 
to evaluate the accuracy of our model we use a set of morphological descriptors known as minkowski functionals 
vi morphological evaluation metrics
in 2d there are 3 minkowski functionals that describe the surface area perimeter and euler characteristic 
the area is simply the percent of pixels that are labeled as pore n pore divided by the total number of pixels n total area n pore n total finally the euler characteristic in 2d describes the connectivity of the surface and is defined as the difference between the number of connected regions solid and the number of holes pores a region with a negative euler characteristic will have more holes than connected regions which indicates low connectivity across the area 
a region with a positive euler characteristic will have more connected regions and therefore a high connectivity across the area which can allow for fluid flow for evaluation after the model is fully trained we create 64 realizations of 100 2 pixel images using the generator and randomly select the same number of images from our training set 
the generator outputs images with values between 1 1 
the images are normalized filtered using a median filter with a neighborhood of 2 pixels and binarized using otsu s method 
an example of the final synthetic image used for evaluation is shown in we next show the effect of varying different model parameters during the training process 
we were able to successfully implement and train a 2d dcgan model to generate reasonable images with similar morphological properties to the original rock sample 
however it is still unclear if the generator is accurately capturing the underlying probability distribution of the real data 
viii conclusion and future work
further investigation could involve using the wasserstein loss as it is a measurement of the distance between two probability distributions 
while our model using the wasserstein loss did not perform as well there have been extensive studies on ways to improve gans and dcgans and only some of the suggestions have been implemented here the ultimate goal of this work is to create a 3d gan to create 3d pore networks for use in digital rock physics 
the major challenge when scaling from 2d to 3d is expected to be in the computational train required to train the 3d network 
therefore it is still important to understand the underlying architecture in 2d and knowledge gained from this project will be invaluable when constructing the 3d model 
thanks to tim anderson and prof anthony kovscek for their guidance on this project 
part of this work was performed at the stanford nano shared facilities snsf supported by the national science foundation under award eccs 1542152 
ix acknowledgements
project code can be downloaded by clicking here stanford google drive 
x project code
to provide a spacecraft with such pareto optimal autonomous planning capabilities is a huge challenge 
in this project the solution approach to the problem combines machine learning both reinforcement and supervised and numerical multi objective optimization 
solution approach using machine learning
in particular assuming a value for the nea dynamics parameters an heuristic multi objective optimization algorithm is used to generate a pareto front describing the trade off offered by various motion plans according to two conflicting cost functions 
the two cost functions to be minimized provide metric of 1 the control effort required to realize the motion plan j v 2 the inverse of the quality quantity of scientific output perceivable through realization of the motion plan j science 
the pareto front obtained relies on the assumption of the nea dynamics parameters when these parameters are changed different pareto fronts are obtained 
to identify a specific point on a pareto front which corresponds to a specific trade off between the two conflicting costs the multi objective problem can be scalarized
a spacecraft sc capable to autonomously plan its motion while accounting for conflicting mission objectives in a pareto optimal way would permit to accomplish complex mission tasks around highly uncharacterized celestial bodies such as near earth asteroids neas 
the two most common conflicting objectives of a space exploration mission are the maximization of the scientific output and the minimization of the control effort i e 
the propellant required on board 
if the targeted celestial body is sufficiently known and has a predictable orbital environment both numerical and analytical tools can be leveraged on ground to design spacecraft motion plans that account for the trade off 
on the contrary if the celestial body and its orbit environment are largely uncertain all plans elaborated on ground may fail dramatically when implemented in space 
a clear example are missions around neas having relevant dynamics parameters i e 
mass shape rotation axis orientation and gravity coefficients largely uncertain 
in these missions a spacecraft should be capable of autonomously plan its motion when an updated knowledge of the nea environment is provided by the sensors and the navigation filter 
in addition the generated motion plan should account for the trade off science output vs control effort in an optimal sense 
the multi objective optimizer used is the multi objective particle swarm optimization mopso which is shown in literature to provide high level performances in terms of time of convergence and full reconstruction of the global pareto front
dataset generation
the neural network nn weights have been trained on the training set using a mean square error mse loss function 
the nn hyperparameters have been tuned and optimized according to the performances provided on the development set
this project explores the use of neural networks nn to provide a spacecraft with autonomous multiobjective motion planning capabilities around a near earth asteroid nea 
the trained nn has been shown to provide interesting but still moderate accuracy results 
conclusions and way forward
to improve the performances the first way to follow is to enlarge the dataset 
in addition future work will explore ways to leverage information about the covariance of the estimated state that the navigation filter outputs 
in this sense a possible way to go is to reformulate the problem as a stochastic markov decision process 
i have developed this project advised by prof simone d amico 
the topic of this study has been motivated by the on going research project autonomous nanosatellite swarming using radio frequency and optical navigation developed at stanford s space rendezvous laboratory in collaboration with nasa ames research center and sponsored by nasa sstp small spacecraft technology program 
the increasing deployment of distributed energy resources e g 
solar electric vehicles in power distribution systems will result in greater uncertainty in power demand 
one method to mitigate this uncertainty and maintain grid reliability is to enable more control of demand side resources through demand response programs 
demand response dr is a reduction or shift in power consumption relative to baseline behavior during peak loads or high prices 
while dr programs have historically focused on the industrial and commercial customers residential dr programs are expanding 
these programs are run by utility companies or third party aggregators and generally focus on control of specific types of residential appliances such as air conditioners or pool pumps the effectiveness of a dr program depends on the power consumption patterns of consumers and their responsiveness to prices or incentives 
power consumption at the household level is extremely volatile and can vary significantly from one household to another given heterogeneity in consumer behavior and the stochastic nature of exogenous variables e g 
weather patterns 
direct targeting of consumers with behavior patterns well suited for dr would be highly beneficial and cost effective for utility companies 
past research has focused on using smart meter data for consumer segmentation to identify households with similar power consumption patterns using unsupervised learning kwac et al 
utilized a combination of adaptive k means and hierarchical clustering to develop a load profile dictionary from a dataset of 220 000 consumers in ca
the objective of this project is to segment consumers based on their appliance level power consumption with respect to three factors consumer 1 availability 2 variability and 3 flexibility 
in contrast with previous studies we focus on appliance level power consumption 
iii project objective
availability refers to the tendency of a consumer group to consume power during periods of peak system demand when a utility would be most interested in curtailing power consumption 
we evaluate consumer availability using unsupervised learning to cluster consumers into groups with similar temporal use patterns for each appliance 
variability is associated with the consistency of power consumption patterns 
consistent use patterns typically result in more accurate power demand forecasts which improve the effectiveness of a dr program 
we evaluate variability by using unsupervised learning to cluster load profiles into discrete groups and analyze the entropy of the distribution of the load profile assignments for each consumer 
flexibility refers to the willingness of a consumer to shift power consumption from a peak load period to an off peak period 
we use supervised learning to predict the responsiveness of consumers to changes in price based on household characteristics and features extracted from power consumption profiles 
appliance and meter level real power consumption data was obtained from the open source pecan street database for the availability and variability analyses twelve months of minute level data from 2014 2015 and 2016 were used for the training validation and tests sets respectively 
availability analysis was applied only to deferrable loads which are appliances that are user initiated 
iv dataset and features
the power consumption of these appliances is primarily dependent on consumer use patterns 
we extracted the start times of each appliance use event from the raw data using thresholding heuristics based on changes in the moving average of power consumption 
a multinomial distribution of the start time over the hours of the day was fit for each home and appliance using maximum likelihood estimation with laplace smoothing 
given an extracted set of appliance start times s s 1 
for hierarchical clustering we used agglomerative methods with the ward variance minimization algorithm the gmm was modeled as a mixture of 24 dimensional multivariate gaussians each with tied covariance matrices 
23 such that equation 1 is always defined 
this was required since the 24 features are elements of a discrete probability distribution and are thus not independent of each other 
a availability
the model was trained using expectation maximization both probabilistic models were each run 10 times for 2 14 clusters and the assignments with the lowest intra cluster variation as measured by symmetrized kl divergence were selected for each of the analyzed cluster sizes all four unsupervised learning methods were implemented using the sci kit learn three different metrics were used to evaluate the performance of each method 
suppose the training and validation set both contain n consumers and k clusters are obtained from both sets such that cluster c t 0 
supposec j is the cluster assignment associated with consumer j 
k from the validation set contains n c d consumers 
we define the availability of a cluster of appliances during hour h as the mean power consumption during hour h for the entire cluster 
the increase in availability from consumer segmentation is the maximum availability over all clusters divided by the availability of all of the appliances in the entire datasetwhere is the set of all time indices associated with hour h and p j t is the power consumption of consumer j at time t in the test set 
an effective consumer segmentation algorithm should yield a large increase in availability the completeness score where n c t c d is the number of consumers assigned to cluster c t in the training set and cluster c d in the validation set 
this analysis assumes that consumer power consumption patterns remain similar between the training validation and test sets such that a perfect clustering algorithm would recover identical clusters finally we used the intra cluster variation of the samples as a measure of cluster quality 
the kl divergence as defined in equation 1 was used a distance measure between samples 
the elbow method
the variability of consumer power consumption patterns was also analyzed using unsupervised learning 
first k means clustering was used to cluster the 24 hour power consumption profiles of all homes for each specific appliance into k load shape types 
b variability
this resulted in 365 load shape cluster assignments for each home and appliance 
the distribution over these load shape types q j r k for each home and appliance over the entire training set was calculated using laplace smoothing 
the entropy of q j gives a measure of the variability of consumer use patterns 
three methods were compared for predicting the responsiveness of the power consumption of each consumer to changes in electricity price linear regression with recursive feature selection k nearest neighbors knn regression and random forests with recursive feature selection 
the tuning parameters for each algorithm were selected to minimize the mean squared error mse in the validation set 
c flexibility
tuning parameters included the number of features for linear regression and random forests the number of neighbors for knn and the number of estimators and the maximum tree depth for random forests 
in this project we developed methods for segmenting residential consumers based on their appliance level power consumption with respect to their availability variability and flexibility 
results indicated that segmentation using unsupervised learning methods can increase load availability for dr programs by up to a factor of two 
hierarchical clustering applied to the start time probability distributions of deferrable appliances produced the most consistent performance 
results indicated that cluster assignments can vary significantly from one appliance to another and can differ from the cluster assignments obtained by only analyzing the total power consumption of each home 
this highlights the importance of performing consumer segmentation based on appliance level power consumption data 
power consumption variability was assessed by calculating the entropy of the distribution of load profile types for individual consumers identified using kmeans clustering 
results indicated notable differences in the variability of power consumption of different appliance types and segments of the population which could be exploited by a dr program provider 
we tested three different supervised learning approaches for predicting consumer responsiveness to electricity prices and found that low variance models such as linear regression paired with recursive feature selection resulted in the lowest test error future work may investigate incorporating additional variables such as day of the week and season into the availability analysis 
expanding the analysis to a larger dataset may provide more insight into the generalizability of the results code for this project can be found at https github com ebuech cs229 
lily buechler performed feature extraction on the raw data implemented k means clustering and hierarchical clustering for the availability and variability analysis implemented the three supervised learning methods for the flexibility analysis and contributed to the poster and report zonghe chua implemented the gmm and lda models and performed the cluster number selection analysis using the elbow method and silhouette scores on all the unsupervised algorithms for the availability segmentation 
he also contributed to the poster and report 
in this project we seek to take a time series of current amplitudes collected by by phononsensitive detectors called qets quasiparticle trapping assisted electrothermal feedback transitionedge sensors and identify the start time of the pulse 
currently our techniques for registering a pulse are not accurate and often classify detector noise as a pulse 
this issue is significant because the separation of signal and noise is critical to the success of the experiment and is easily generalized to any other detector of this type 
furthermore the problem of processing data to find pulses and characterizing them occur in many other fields were signals need to processed therefore it has the potential for a wider range of uses 
we implement various machine learning models and found the most success with pca fcnn 
the cryogenic dark matter search cdms research group seeks to directly detect the most frequent form of matter in the universe dark matter 
to do so we study the behaviour inside semiconducting crystals at cryogenic temperatures 
when a dark matter particle or another form of radiation interacts with the crystal a cloud of electrons and holes is produced at the interaction site 
these charges are then drifted through the crystal by an applied electric field and produce phonons that are collected by phonon sensitive detectors called qets quasiparticle trappingassisted electrothermal feedback transition edge sensors 
once the a signal pulse is received we seek to determine the start time of the interaction from the raw data 
we use logistic regression a shallow fully connected neural networks fcnn linear and kernelized principle component analysis pca with fcnn and convolutional recurrent neural networks cnn rnn 
dr andrew watson s dissertation
while we dont have enough real data 1 to train on we have do have a monte carlo simulation of our experiment built with g4cmp 2 
combining the results from simulating with real noise we created our dataset as represented by
dataset creation
in input features were traces with two channels a i r 2048 2 
for the logistic regression and fcnn we flattened the trace to a 4096 dimensional vector 
for the cnn rnn we kept the shape of the trace 
for pca fcnn model we flattened the traces and used pca to find the first 1024 principle of the components pcs using 20 of the training set which explain 89 83 of the variance 
we decided to try pca because we plotted the correlation matrix of the traces 1 by real data we mean data that is produced by the physical detector instead of by the monter carlo simulation 2 https github com kelseymh g4cmp 3 each simulated event has an associated trace for each channel 
a trace is a time series defined by an array of 2048 values where each value represents a current measured by the detector 
radial basis kernel and 1024 pcs 
we tried this too because we thought the relationship between the projected features could be non linear 
we represent a trance example input as a i r 2048 2 the flattened version as a i flatten a i r 4096 the true value of the time as t i and the prediction ast although for applied purposes we are more interested in reporting the mean absolute error mae 
we begin the training modelling phase by fitting a logistic regression as our most basic baseline 
we use a i s as inputs and t i discretized into one hot encoded vector with 1048 classes as outputs 
baselines logistic regression and shallow fcnn
as a secondary baseline and to get a sense of the power of neural networks on this task we make a fcnn with one layer with 512 nodes 
we perform pca by finding the eigen basis of the correlation matrix c and then finding the projections by where v k t is the k th principle 
we then feed these projections into a fcnn explained in the next subsection 
pca fcnn
we perform the kernel trick on linear pca as follows this was the structure used
kernelized pca fcnn
base on the suggestion of ta ashwini ramamoorthy we implemented a long short term memory lstm neural network as this type of nn is especially suited to dealing with time series data 
we built the model with two layers of max pooling with a stride of 4 followed by two dense hidden layers of 256 and 16 nodes respectively 
training the lstm proved to be much slower than previous methods 
on the other hand the lstm predictions would converge withing a relatively smaller number of epochs less than 20 so little training was required 
unfortunately despite trying different structures of the model we were unable to attain an mean average error with this method that came close to that of pca fcnn 
the results table represents the average mae scaled to 2048 the total number of bins 
from this project were able to develop a methodology to construct an effective tool that we might use as part of physics experiment to determine the start time of pulses measured by our detector 
after constructing our dataset from our monte carlo simulations we trained logistic regression fcnn cnn lstm a standard pca fed into fcnn and kpca with a radial basis kernel fed into a fcnn to predict t i and had the best test set mae with the standard pca fcnn method 
our goal was to get to a mae around 1 or 2 however the lowest we ever got on training was 4 
this is likely due to the fact that the pulses are so noisy which is why we chose this challenging problem in the first place 
an important insight from this project was that more complex models dont always produce better results as can be seen comparing the lstm cnn and kpca fcnn with the pca fcnn 
another lesson we learned was that producing the dataset and preparing it for training can be the most time intensive step 
finally while we didnt accomplish exactly what we set out to do we are content with out results and will continue improving on them 
if we had more time we would try other models tune hyper parameters more methodically keeping track of all results and use a larger dataset with more examples per energy and also a wider range of energies 
we would like to thank to chin yu for suggesting this project and providing key insights about machine learning the northwestern supercdms team for providing the mc simulation and the sample noise traces from which we generated our dataset and finally the cs299 tas and professors for giving us the opportunity to do this project and teaching us all the material 
we present an unsupervised machine learning model for computing approximate electromagnetic fields in a cavity containing an arbitrary spatial dielectric permittivity distribution 
our model achieves good predictive performance and is over 10 faster than identically sized finite difference frequency domain simulations suggesting possible applications for accelerating optical inverse design algorithms 
inverse design problems computational design of structures by specifying an objective functionare pervasive throughout physics especially in photonics where inverse design methods have been used to design many highly compact optical components the iterative fdfd simulations although exact can be computationally expensive and scale poorly with the design dimensions 
for many applications an approximate field solution is sufficient 
a machine learning model which could quickly compute approximate electromagnetic fields for a dielectric structure could reduce this computational bottleneck allowing for much faster inverse design processes 
we were able to find a small body of existing work related to this problem 
shan et al 
lagaris et al 
our model computes approximate electromagnetic field solutions for a specific type of scenario formalized here 
suppose we have a perfectly reflective d dimensional 2 cavity of length l with an electromagnetic source at the center 
problem summary and approach
the cavity contains material forming an arbitrary spatial distribution of dielectric permittivity x 
discretizing the cavity into pixels of size l the permittivity at each point in space can be expressed as a vector of size n l l d 
given an input permittivity vector and knowledge of the source location the model outputs an identicallysized vector e pred representing the electric field amplitude at each point in space 
the cavity scenario was chosen to impose dirichlet boundary conditions of e 0 at the cavity edges ensuring the electric fields are standing waves and thus real up to a global phase 
the structure of the model is loosely analogous to a generative adversarial network pred 
the second part is a discriminator 5 which computes the maxwell residual of the predicted field as d providing a measure of how physically realistic the generator s outputs are 
in both cases the loss of the total model is
maxwell s equations govern the dynamics and propagation of electromagnetic fields in materials and form the foundation for classical electromagnetism 
where e b are electric and magnetic fields at a given point in space and time are the permittivity and permeability of the material t is time is charge density and j is current density 
unsupervised training with maxwell residuals
in a nonmagnetic electrically neutral linear material such as many cases of interest 0 0 and these equations can be simplified to where h b 0 is the magnetizing field 
in a steady state frequency domain solution such as the ones found with fdfd methods e t ee i t so t where is frequency 
we can combine if the electromagnetic field is polarized say with e z polarization then e e and j j at each point in space 
we can then vectorize this such that e and j are the electric field and free current amplitudes in the direction and is the dielectric permittivity at each point in space 
if we have a model which takes in a permittivity distribution and a source term j and returns a predicted field e pred then we use eq 
4 to define the maxwell residual l m as the maxwell residual provides an element wise measure of the physical realism of the predicted field e pred a measure of how far the predicted solution is from satisfying maxwell s equations at each point 
if the model can sufficiently minimize l m then it can produce solutions which approximately satisfy maxwell s equations at each point and thus are approximate global electromagnetic field solutions for the system described by and j 
this training does not require the model to ever see the exact fdfd field solution the outputs it attempts to replicate and is thus unsupervised 
we found that when trained or more precisely overfit to predict the field of a single permittivity distribution virtually any network architecture would allow the predicted field to converge to the true field given enough training time 
for more on this see section 4 1 
model architecture and implementation
the challenge was finding a network architecture with the correct structure to capture the generalized transformation e when trained on a large number of possible permittivities we tested many different network architectures for this project 
purely convolutional architectures like the ones used by ref 
our final network architecture employed a hybrid convolutional dense deconvolutional approach and is shown in during training the network outputs the maxwell residual l m e 
dropout layers with p 0 1 and relu activations are present after every layer except the last one 
our model was implemented using pytorch
as an initial experiment we trained the model to predict the field of only a single input using the maxwell residual method described in section 3 2 
the evolution of the predicted field as the network is trained on a sample permittivity is shown in
experiments 4 1 fitting to single
for the main experiment in this paper we trained a model with the architecture described in to evaluate the results of the trained model a test set of 10 4 new permittivity samples was generated using the same generation procedure 
the model was run on each of these inputs the loss for each sample was calculated average loss of 8 8 10 4 and the results were sorted from best to worst 
training on permittivity datasets
example good best 10 10000 typical middle 10 10000 and bad worst 10 10000 field predictions from the test set are shown in the first three panels of finally we tested the model s capability to generalize to inputs outside of the training distributionthat is permittivities representing a different set of structures than the ones generated for the training and test sets 
as an example the predicted field amplitudes for a sample where each point in space has a permittivity value randomly chosen between vacuum and silicon is shown in the last panel of
in this paper we presented a machine learning model capable of computing approximate solutions to maxwell s equations over an arbitrary dielectric permittivity in a cavity 
the model was trained using an unsupervised approach where it learned to minimize the maxwell residual of its predicted fields thereby maximizing the physical realism of the solutions 
our model demonstrates good predictive performance and is over 10 faster than comparable fdfd simulations suggesting applications for accelerating optical inverse design algorithms for future work we would like to implement a complex valued model to solve the more general problem of predicting fields outside of a cavity environment 
our choice of the cavity problem was driven primarily by pytorch s lack of support for complex tensors 
in the project repository we have an initial implementation of this which explicitly parameterizes e and e although this approach was only mildly successful 
we would also like to explore using our model for dimensionality reduction especially for 2d and 3d problems 
we were able to achieve a 1 16 dimensionality reduction with our model applied to a 32 32 2d input of permittivities by adjusting the network parameters to force a 64 value chokepoint in the middle dense layers of the network 
this figure is present in the poster but omitted here due to length constraints 
this could force the model to learn more efficient representations of the relationships between permittivities and fields 
we would like to thank shanhui fan sunil pai and tyler hughes for several illuminating discussions relating to this work all source code used for this paper is available at https github com bencbartlett neural maxwell 
trained model parameters were too large to include in the repository but are are available upon request 
since the 1970s the seismic bridge design process has gone through a great change from capacity based design to performance based design and the performance of bridges has become a great concern to engineers 
the safety margins of the contributive factors vary from case to case and the trends are still unclear partially if not all because of the change in the design logics in the past decades 
therefore having an on hand trained model on bridge performance prediction would be to some extent helpful for knowing how well badly an existing bridge would perform in a future earthquake as well as guiding the design of a new bridge to survive a future earthquake in this project we are trying to train a prediction model for bridge performance under earthquakes with supervised learning 
the inputs to our algorithm are the age of a bridge the magnitude of the earthquake and the distance between that bridge and the epicenter 
we then use logistic regression quadratic discriminative analysis and k nearest neighbor classifier to output a predicted performance categorized to be positive damaged or negative undamaged of the bridge under the given earthquake 
prof kiremidjian s paper greatly inspires our interest on data driven earthquake engineering 
when trying to select the related features we refer to a comprehensive study of u s bridge failures from mceer technical report 
we also learn the advantages of bootstrap resampling in managing unbalanced data from dupret and koda s paper the problem we encountered is actually a sub problem of other projects on application of machine learning in earthquake engineering in the previous cs 229 projects 
in this project we implement some similar methodologies such as knn but with a different approach to select the optimal k value 
the data collection is a big challenge for us because we have limited records of bridge failure due to earthquakes 
therefore we make an assumption in advance if an earthquake occurs near the site of a bridge but there is no record showing that the bridge is damaged then this should be identified as an example of no damage negative examples we first search for bridge failure records in previous famous earthquakes in the reports published by the u s government find the site location of the bridge and mark it as a positive example 
next we search for other earthquake records happened in bridge s history near the site location on usgs earthquake search catalog and mark them as negative examples 
finally we split the dataset into training 70 and test 30 sets because of the intrinsic scarcity of positive examples our dataset is unbalanced 
the positive tonegative ratio is about 1 10 
to mitigate this problem we use bootstrap resampling with replacement to up sample the positive class and generate training sets with more reasonable positive to negative ratio 
in our project this ratio varies from 0 2 to 1 0 the raw features selected for the model are the age of the bridge the magnitude of the earthquake and the distance between the bridge and the epicenter all of which are continuous variables 
for future work we plan to explore more features continuous and discrete such as the material type the structure type the annual average daily traffic etc 
the goal of the algorithm for this binary classification problem is to predict the correct or more likely bridge performance given data on the earthquake and the bridge itself we use logistic regression quadratic discriminative analysis qda and k nearest neighbor classifier knn to train the model respectively and independently 
the logistic regression updates a set of parameters that maximize the log likelihood 2 
quadratic discriminative analysis qda qda is one of the most commonly used generative models where we assume that the results from each class are normally distributed 
in binary case with means 0 1 and 0 1 the log likelihood can be computed as 
the k nearest neighbor classifier considers k neighbors of the current data point compares the number of positives and negatives within the k neighbors and outputs the label i e 
positive or negative of the one that has a greater number than the other 
k nearest neighbor classifier knn 
to determine the optimal value of k we use a cross validation set and plot its accuracy with respect to the value of k from 1 to 10 as shown below based on the figure shown above the optimal k value in this case is 2 
the following training and testing accuracy are the average results from 5 cases with different positive to negative ratio after resampling since bootstrap resampling is a randomized sampling algorithm we run 100 iterations for each model and plot both training and testing accuracy vs numbers of iterations with different positive to negative ratio as tabulated below 
algorithm pos neg 0 2 pos neg 1 0
there are a few observations from these plots 1 
the training accuracy is relatively stable in terms of number of iterations and only fluctuates about 2 5 for each algorithm 
knn
the accuracy decreases as the size of resampling increases 
however since the test set is also unbalanced a very high testing accuracy may not be meaningful as it always tends to predict the result to be negative 
additionally simply resampling the size does not truly add any valuable data point into the data set so the scarcity of positive examples is not improved 
the testing accuracies of knn for a positive to negative ratio of 1 0 appear to be a straight line or closely which indicates that knn algorithm tends to make the same predictions for random input of positive examples 
this also makes sense because in case of 2 a larger positive example size will make the output of knn more stable as it only uses the nearest 2 neighbors for prediction from all the plots and tables shown above qda seems to be the most accurate one among the 3 models we choose for this project 
also its accuracy does not decrease too much as we expand the size of our positive class 
the testing accuracy is even more stable than that of qda always around 80 but it behaves poorly when making predictions on the positive class 
in this project we pre process the raw data set with bootstrap resampling and implement 3 supervised learning models on the training set 
during training process we observe that the accuracy decreases as we increase the size of resampling 
however since the test set is also unbalanced a very high testing accuracy may not be meaningful as it always tends to predict the result to be negative for future work we expect to expand the size of dataset more explicitly to increase the number of positive examples to run tests on other generative models and to implement multi class classification so we may obtain a more meaningful and practical model as we desired thinking about the nature of the problem helps us understand the observations above 
in a civil engineering perspective seismic performance of structures is highly uncertain and most of them are hard to predict 
to view this study in a broader scope it may be observed that there are usually lots of different constraints on features data size physical meanings of results etc 
in civil engineering scenarios which may impact the practicality of machine learning in such kind of studies 
xiao and ziyang came up with the topic and scope of the project together 
we both engaged in data collection and feature selection 
ziyang mainly ran the algorithms to train the model and test the model 
xiao and ziyang interpreted the results and made observations together 
ziyang prepared the proposal and the milestone whereas xiao made much of the poster and the final report 
the link to the github repository containing all the datasets codes and output are given below https github com jzy95310 cs229 fall 2018 final report
we are motivated by the opportunity of using machine learning on novel vr based mental healthcare datasets to improve understanding and diagnosis of mood and emotional disorders 
recently researchers have established a relationship between degree frequency and type of head movement and the degree of clinical depression experienced by an individual we are given the time series data gathered from oculus sensors for 148 engage study participants 
the time series represent two channels of yaw pitch and roll values of the participant s head location 
this is our input data and our output labels are binary true if the participant s gad7 a score for level of anxiety value is above 10 false otherwise 
we featurize this data in two ways 1 summary statistics across time and 2 a 30 point discrete fourier transform 
we feed both of these input featurizations to three different classifiers logistic regression naive bayes and decision tree 
so in total there are six classification nodes 
the outputs of these 6 classifiers are fed into an ensembled learned weights voting node and the output of that is our final prediction 
we compare the efficacy of this approach for several weighting threshold schemes against a convolutional neural network using the unfeaturized time series as input and also against predict 1 and predict random baselines finding our best model to improve upon baselines 
to our knowledge this is the first time the task of psychological disorder prediction using machine learning has been explored for time series head movement datasets gathered from virtual reality experiences 
that said we drew inspiration from discussion with members of the panlab and chose machine learning methods based on their success with similar tasks 
we chose to use a convolutional neural network in hope that it captures patterns that hand crafted features do not 
a study done by hoppe and bulling proposes a convolutional neural network for learning featurizations for classification tasks on eye movement data
the data we have available to us is head angle data recorded over time by two sensors of an oculus vr 
the data is recorded at 0 2 6 and 12 month sessions and there are five types of virtual reality experiences during each session 
dataset and featurization
an example of the time series yaw pitch roll data for a single vr experience can be seen in in our experiments we attempt to predict the gad7 score an indicator of anxiety based on the feature vector discussed above 
computing these features requires that we have the associated score labels and the tracking data for each experiment type for a given participant and month v gad7 148 is the number of pairs of participant month for which we have the required data 
because of the small number of examples we use a hold one out cross validation scheme and for testing we set aside 30 test examples 
this leaves us with a training matrix x gad7 and a label matrix y gad7 whose rows correspond respectively to the feature and label for all p m leftover for training 
in our experiments x gad7 has 118 observations and 120 summary statistics features or 360 frequency domain features depending on the featurization scheme used 
we compute summary statistics on both the time series and the differences between subsequent elements of the time series as our features each piece of head tracking data is a matrix t whose columns are the roll pitch yaw gathered by sensor 1 concatenated with that of sensor 2 
now we compute a difference matrix then for every column of t we compute the mean and variance and every column of d we compute the sum and variance 
summary statistics
we concatenate these statistics across all experience types to form a feature vector for each participant month pair 
we use a 30 pt discrete fourier transform dft computed on each time axis for our second featurization 
the n point dft is defined as follows where x k is called the discrete fourier transform of the sequence x n 
discrete fourier transform
x k can be thought of as periodic with period n or as of length n hence it is called the n point dft and for our purposes is used to compute n features there exists an aptly named algorithm called the fast fourier transform fft which is a computationally efficient implementation of an n point dft 
we use numpy s fft implementation to featurize our time series head movement data across each each channel s yaw pitch and roll time axis 
the primary challenge of our task beyond uncertainty in featurization due to the novelty of the task is data scarcity 
we have only 118 participants on which to train our systems and a complex phenomenon to model 
we pursue three broad avenues to tackle these challenges 1 simple classifiers with different inductive biases and trained off of each of our two featurizations 2 ensembles of simple classifiers with vote weighting determined through random search and 3 a small convolutional neural network with pooling across time simple models we train multinomial naive bayes classifiers decision trees and logistic regression classifiers on each of our 2 featurization types for a total of 6 simple classifiers 
in short the naive bayes models each observation as having been generated by sampling the class and then sampling all features independently given the class the decision tree iteratively splits the data to minimize the gini loss based on individual parameter differences and the logisitic regression learns a linear combination of the features to minimize the difference between the true and predicted probabilities of anxiety disorder for each patient 
we ran manual search in development deciding to use a max tree depth of 5 and a regularization precision of 1 for our logistic regression ensemble weighting through random search because of the high variance nature of the data and the different inductive biases in each of our featurizations and simple model architectures we hypothesized that an ensemble of models may improve over the performance of any individual model 
we start with a simple majority vote ensembling baseline 
to leverage the intuition that 1 the individual simple models are not of the same quality and 2 the tradeoff between recall and precision may be controlled through the voting threshold for predicting anxiety we run a random search on our development set to find high quality model weights and decision thresholds for precision recall and f1 specifically for each of the 6 simple models we draw a value from a gamma distribution with parameters shape 2 scale 1 and then normalize the weights by the sum of all 
we chose this distribution because it should give some variation between model weights without deviation in extremes 
for the threshold we sampled from a uniform between 4 and 6 as we found that sampling from a greater range led to degenerate results like extremely high recall by predicting all participants to have high anxiety 
for each sampled set of hyperparameters we ran all models 5 times using hold one out evaluation and averaged the scores 
we then picked the set of hyperparameters for each of f1 precision and recall that led to the best development result to run on the test set 
this model is visualized in
for our second model we consider a small 1 dimensional convolutional neural network that uses the unfeaturized raw head movement data across the 6 channels of roll pitch yaw for both the right and left sensors 
we felt that a 1d cnn model was well suited for our data given that we were working with time series data where the exact time of head movement may not be as important as the amount or speed of movement in short intervals for predicting anxiety levels 
1 dimensional convolutional neural network
our cnn architecture consists of five layers 
the first layer is a 1 dimensional convolutional layer followed by an average pooling layer a relu activation layer and two dense layers that also have relu activations 
we included dropout as a form of regularization as well and chose the dropout rate during our hyperparameter search on the development set 
this model s layout is visualized in
because the split of high anxiety to low anxiety participants was roughly 20 to 80 we report precision recall and f1 score across all experiments 
in our random search we sampled 50 ensemble configurations 
a few interesting patterns emerged 
ensemble weighting through random search
the top recall ensemble simply chose to ignore all models but the naive bayes on summary statistic features 
the top precision ensemble as might be expected used the highest threshold of the three at 60 of voting weight required to predict high anxiety 
the top f1 ensemble assigned almost all its weight equally across the summary statistic models 
a convolutional neural network layer passes the same feature detector across all spatial steps of the data and in our case uses a pooling function to aggregate features across all timesteps 
for our cnn we found that the set of hyperparameters that resulted in the highest f1 score on the development set was a filter count of 16 a kernel size of 10 and a dropout rate of 0 5 and ultimately used this choice of values for our final model 
1d convolutional neural network hyperparameter tuning
we report numbers on cnns trained only the first 400 timesteps of the data as the results were robust to the number of timesteps used 
one interesting takeaway from our hyperparameter search was that smaller models tended to do best reflecting small data problem but our smallest models started to degrade as well perhaps signaling limitations of raw head movement feature format with so little data 
our results are summarized in our ensembles underperform our expectations perhaps due to the high variation in model quality and the bias of our random search to being close to uniform 
however while they do not in general outperform the individual models we do see the desired behavior of each weighted ensemble f1 precision recall tending to bias towards that metric while not sacrificing too much on the other metrics 
test set results
finally our cnn model underperforms all baselines despite hyperparameter tuning to attempt to avoid overfitting 
we expect that this is because the small amount of data and somewhat abstract problem make joint feature detection and generalization infeasible finally we conducted qualitative error exploration on our development set 
we sampled two highanxiety participants one of which our naive bayes classified correctly and the other incorrectly 
qualitatively the examples seem quite similar in the amount of movement as seen in
in this work we explored head movements a noisy signal in the medical domain which we confirm to be useful for predicting patient anxiety disorder 
we faced an inherently small data problem since controlled participation in a vr experience is costly to collect 
discussion conclusion
as such we focused on featurization and model comparison to determine what features and methods are promising for evaluating anxiety through head movement 
our best model improved on an informationless baseline by 10 2 points f1 a modest but potentially useful result when combined with other predictors of anxiety in a hypothetical future system 
by ensembling models and running a random search on the ensemble voting weights and decision threshold we were able to control the tradeoff between precision and recall but not improve upon the f1 score of individual models a mixed result 
for thoroughness we compared our featurizations and simple models to a low parameter cnn finding as we expected that the cnn underperformed models with hand crafted features 
we hypothesize this was due to the rather small data setting 
our findings suggest that head movement data has signal for predicting anxiety disorder and suggest that future work may leverage richer representations of each patient in combination with head tracking to improve predictiveness and eventually improve professionals ability to care for patients 
in this paper we explore the applications of machine learning to sports betting by focusing on predicting the number of total points scored by both teams in an nba game 
we use neural networks as well as recurrent models for this task and manage to achieve results that are similar to those of the sports books 
on average our best models can beat the house 51 5 of the time 
last may the united states supreme court legalized sports betting 
while it is still up to every state to pass legislation on the issues many have already been working on bills 
as tracked by espn eight states have already legalized sports gambling with two more states having signed bills that will legalize gambling in the near future 
with its ruling the supreme court paved way to the opening of whole new legal and thus taxable market of size 150 billion according to the american gaming association in this project we will explore the applications of machine learning in the field of sports betting using a case study of the national basketball association 
more specifically we wish to design sets of models that predict betting indicators for nba matches 
out of the three main indicators overunder money line and point spread we focus on the over under 
in other words we predict for every nba game how many combined points the two teams will score 
ideally we would be able to come up with an estimate for this indicator that is more accurate than that of some betting platforms and use this to our advantage to place bets 
the paper predicting margin of victory in nfl games the paper football match prediction using deep learning 
the data collected so far can be classified into two groups betting odds data and game data which consists of both data describing team performance and player performance we found odds data on sports book review online a website that compiles betting data for every nba games since the 2007 2008 season 
the website offers a downloadable excel file for each season 
using a short script we were able to retrieve all the necessary target variables and the corresponding betting odds offered 
the betting indicators scraped were the following i over under the total number of points scored in a game ii spread the number of points by which the home team wins or loses iii money line a number encoding the amount of money won from placing a bet on the winning team of a game in the rest of this project we focus on the total number of points scored and we use the over under data collected to solely evaluate our models and do not include the above data in our features for game data we retrieved data from basketball reference using fran goitia s nba crawler
using the retrieved data from basketball reference we build a featurized dataset 
for every matchup between two teams we decide to look at both team s past three games 
feature building
we included simple features such as points scored points scored against or total rebounds and also more complicated metrics such as offensive rating and plus minus 
in order to account for opponent strength we also added each opponent s season averages in all above metrics 
finally to account for player fatigue we added the number of days since the last game as well as the distance traveled see we trained a random forest as a baseline before moving on to more complex models 
this problem is similar to the user item rating prediction information about the outcome of previous games between other teams can inform our predicted output for the current matchup 
to leverage this we used singular value decomposition we hope that the rows of matrices u and v capture information about teams at home and away respectively and we then predict u i v t j total points scored in a game between teams i home and j away where b k is the k th row of matrix b 
like the collaborative filtering model the neural network captures information from the outcomes of previous games between other teams as during training the network is provided the results of previous games as input along with the identity of the two teams involved in the game 
however the neural network has an advantage over collaborative filtering in that it is also able to take features of both teams involved as inputs 
therefore it can draw on not only the outcomes of previous training examples but also the offensive rating of each of the teams involved over their past three games etc passing in the feature set created in 3 2 we train a neural network with fully connected layers and relu activations 
the input data is flattened into a size of 1524 features 
the relu activations are used for ease of training and to reduce the likelihood of gradient vanishing see
given that games are played sequentially we decided to use an lstm to process the past three games one by one 
as described in long short term memory for our task we implemented an lstm model with a fully connected layer at the end to output the number of points scored by the two teams for the desired game see
long short term memory network lstm 
after tuning our models on the validation set to minimize validation mse we found the following architectures to perform best ii neural network we use four fully connected layers reducing the input of dimension 1524 to size 500 then 100 then 20 then finally a 1 dimensional output that predicts the overunder of the desired game see iii lstm we use one layer with hidden dimension 20 and a fully connected layer at the end see we evaluate the performance of our models primarily through the mean squared error mse between the over under value predicted by the model and the true point total observed in the game 
the over under values predicted by the sports books can serve as a benchmark as calculating the mse between the sports books predictions and the observed point totals gives us a sense of how well our models are performing relative to the books 
in addition we can also calculate the percentage of the time that our model would have correctly predicted that the true point total was either over or under the over under number provided by the sports books we found that the neural network correctly chooses over or under around 51 5 of the time while the collaborative filtering beats the line 51 of the time on average 
due to the rapidly changing nature of the nba it is difficult to acquire sufficient training data that reflects the way the game is currently played 
this can be seen most prominently in the increasing frequency with which nba teams are attempting and making three point shots 
according to the nba teams around the league combined for around 15 000 three pointers made in the 2009 2010 season and broke the 25 000 mark last year 2017 2018 when we were training the models we found while while we had data available starting from it is worth noting that the collaborative filtering model which doesn t use any team features significantly outperformed the random forest 
this shows the high variance in our data as well as the strong seasonal trends that a model needs to encompass in order to be accurate on this task on we achieved a test mse of 369 84 for our best model the neural network 
this value is higher than the mse of the sports books predictions which is 320 70 but is relatively close to the level of accuracy of the books 
when considering that nba teams are scoring roughly a combined 200 points per game mse values of over 300 from even sports books may seem high 
however this is simply a reflection of the high variance nature of nba games where anything from injuries to players to a strong shooting night or the decision of a coach to rest his star players after building a lead can all lead to huge swings in the overall point totals of a game 
the variance of the data was also reflected in the need for a relatively large weight decay parameter while training the neural network to prevent overfitting encouragingly when placing bets against the sports books on games in the test dataset using the neural network the predictions made were correct with respect to the actual outcomes 51 5 of the time as mentioned earlier 
however note that in the real world the fees associated with betting mean that successful long term betting patterns need to be correct at least 52 53 of the time 
in the future it would be interesting to explore if directly predicting whether the true result of the game is over of under a sports book s over under prediction would lead to higher levels of accuracy in this secondary metric 
overall we were able to achieve encouraging results using our models as our best predictions rivaled the accuracy of sports books 
indeed our neural network had a mse of 369 84 compared to the 320 696 mse of sports book 
on average our model was able to pick the correct side of the over under 51 5 of the time in the future there is definitely potential to improve the model through augmenting our data set 
first of all the odds lines offered by sports books themselves offer a large amount of information on the potential outcomes of games including data that indicates the direction the odds lines are moving in the hours before the game 
it is entirely possible that the odds lines would indicate that the sports books are very good at predicting the outcomes of games involving certain teams but tend to skew in some direction when trying to predict the outcomes of games involving other teams 
in addition while our current models only use team level data incorporating player level data could add additional layers of nuance while accounting for the impact of injuries or player fatigue further exploration of model architectures could potentially improve our results as well 
due to the similarity of our current problem predicting betting indicators from the home team and away team to classical recommendation systems predicting ratings for a given user and item we could definitely explore adapting algorithms used by netflix amazon and others for recommendation 
in this project both vishnu and alexandre contributed equally to the writing of this report 
alexandre completed around two thirds of the data retrieval cleaning process built the collaborative filtering model and the lstm network 
vishnu completed the remaining third of the data retrieval cleaning process built the baseline models as well as the neural network 
the rest was achieved jointly by both members 
several prominent pathologies such as cerebral palsy parkinson s disease and alzheimer s disease can manifest themselves in an abnormal walking gait 
abnormal gait is characterized by irregular patterns in step length step cadence joint angles and poor balance 
introduction and background
early detection of these abnormalities can help in diagnosis treatment and effective post treatment monitoring of patients a comprehensive metric used to assess the extent of gait pathology is the gait deviation index score or gdi the current gold standard method of measuring gdi marker based motion capture places a set of reflective markers on body segments and tracks their trajectories over time 
sessions with patients can last multiple hours and cost hundreds or thousands of dollars 
a potential less expensive and less time consuming alternative is to analyze video captured by commodity devices i e 
mobile camera phone using machine learning algorithms to predict gdi previous attempts have been made to predict gdi from monocular video footage using a projection of joint centers onto the two dimensional plane of the camera 
in this project we leverage cutting edge computer vision tactics to extract three dimensional features from each frame of video 
by stacking processed frames into a video sequence relevant spatiotemporal features can be modeled for gait characterization 
thus the project goal is to use monocular video footage to predict gdi score with lower root mean squared error rmse than existing methods 
our work builds on the efforts of many machine learning scientists who developed models to extract spatiotemporal features from video as well as biomechanists who have analyzed human motion to help physicians assess neuromuscular pathology 
a leader in this field is lukasz kidzinski a member of stanford s mobilize center who was our advisor for this project 
we met with lukasz throughout the project for guidance on data processing and model generation 
in 2017 lukasz and his team used a temporal convolutional network built on videos processed through openpose to predict gdi a critical component to our analysis is the refeaturization of images into a spatial representation of human pose 
specifically we leveraged the densepose algorithm which converts red green blue images toeach pixel in an image to one of thousands of surface locations on a modeled human mesh 
densepose builds on prior work in human pose estimation most notably the skinned multi person linear model our models and experiments were motivated by researchers who have used machine learning to extract spatial and spatiotemporal features from video 
ibm used a cnn with a multi layer perceptron to classify images into one of many types a guiding work for extracting spatiotemporal features from images was harvey s blog post
our dataset comprises of 3 000 videos of patients walking in a room at gillette children s specialty healthcare center for gait and motion analysis the videos have a resolution of 640x480 and are 25 frames per second 
each frame is processed using densepose which maps all pixels of an rgb image to the surface of a modeled human mesh
densepose right 
each pixel is assigned to a corresponding point on a human skin mesh the processed frames consist of three channels i part index u and v coordinates and are passed as inputs to the different learning models 
figure 1 a sample rgb image left processed by
in the case of a model with a temporal component 10 outputs i e 
processed frames are concatenated in sequence per training example before running any initial experiments substantial work was performed to process data 
this included running densepose algorithm on top of thousands of videos and organizing them into folders 
subsequently the folders were assigned a gdi score based on the corresponding examid from a joined file of physician assessments 
due to the massive data volume and limits of memory we did not leverage the entire dataset in our experiments 
we typically accessed 500 1 500 videos depending on the model s computational demands 
as such though we considered using video slicing image mirroring or other data augmentation techniques we decided not to implement these as generating additional augmented data was not necessary 
patient videos are transformed into i u v channels by densepose and are then passed as inputs to our model which we call gdi net 
gdi net is a machine learning algorithm with spatial and temporal components to predict a patient s gdi score 
methods and experiments
gdi net was implemented in keras an open source neural network library which runs on top of tensorflow environment our initial implementations were quick and simple 
we built a linear regression model implemented in scikit learn package using 5 frames of 480x640x3 resolution for each training example as the model complexity was gradually increased a sole spatial component was added to gdi net architecture 
we trained the spatial component which consisted of vgg16 an off the shelf cnn architecture or a custom 2d cnn with 3 834 training frames of 480x640x3 dimension 
the model was validated against 951 examples to quantify performance on unseen examples the vgg16 model is pre trained and only the last layer of the model was replaced by a linear function and trained to perform our regression task 
the motivation behind using the pre trained weights was to investigate the possibility of transfer learning 
since collecting patient data and building custom models is expensive and cumbersome transfer learning is desirable and could reduce the lead time of any application development 
after receiving advice from cs229 course assistants at the course poster session we also ran a vgg16 model in which all weights were re trained a challenge in executing vgg16 was that it requires an image with a standard size of 224x224x3 as an input 
since the densepose output has a resolution of 480x640x3 the outputs had to be cropped before being passed to vgg16 
the crop was made in a manner to preserve as much information as possible i e 
by selecting pixel values where patients are most likely to appear in the frame however some loss is inevitable densepose outputs were passed whole to the custom cnn 
the inspiration behind the cnn s architecture was mahendran et al 
s model that leveraged a cnn to predict the pose of a vehicle within a continuous regression framework 
in addition lukasz kidzinski provided instrumental guidance for making important architecture choices in the cnn although the spatial component identifies human poses in a frame it cannot track the pose trajectories over time 
to detect temporal characteristics a temporal model either an lstm or 1d cnn was added to gdi net 
the input to a temporal model had 2 920 training examples of 10 frames of 480x640x3 resolution concatenated into a single array 
hyperparameter tuning was performed to further optimize models mainly tuning learning rate batch size and dropout 
the majority of this effort focused on tuning hyperparameters for our most promising model the cnn with lstm 
the complete architecture of the highest performing model is outlined in the results and discussion section of this paper the immense size of the dataset and the desire to concatenate frames sometimes led to issues with memory overload 
in those scenarios hyperparameters such as batch size and kernel size were adjusted to avoid memory limits 
further for computationally expensive models we leveraged sherlock a high performance computing cluster available to stanford university affiliates to reduce run time
one of the interesting findings from our experiments was the relatively poor performance from using an off the shelf model 
the initial goal in these experiments was to exploit transfer learning to build complex networks with millions of parameters that are pre trained 
a summary of experiments and results is shown in
we used vgg16 a model that is readily available within the keras api and has reported success with image classification tasks frame specific models that only captured spatial features of a given frame did not perform well 
this is expected as gdi is largely determined by trajectories of body parts and individual frames do not hold temporal information describing how the patient moves over time 
as such we spent most of our time and effort experimenting with spatiotemporal models the best performing model combined a 2d cnn on each frame and an lstm to capture temporal patterns 
the learning curve regression plot and detailed architecture are outlined in
in this study we implemented a machine learning approach to develop a cheap and automated way to assess gait abnormalities 
the results especially that of a cnn with lstm model are very promising 
we are excited by our model s potential to help diagnose and track the progression of neuromuscular disease although the cnn with lstm model was able to achieve a low rmse error analysis should be performed to investigate the deviations between the model and ground truth 
once the nature of these errors is understood the model and the architecture can be fine tuned for better accuracy an option to enhance our architecture is to implement 3d convolution blocks instead of a separate spatial and temporal component 
we hypothesize that it may perform better than the spatiotemporal model as it can capture lower level features in time and is not affected by the way data is passed from the spatial component to the temporal component in our experiments we observed a growing gap between training error and validation error as training progressed which suggests overfitting 
although we attempted to mitigate this using dropout more could be done to generalize our model 
future experiments can focus on reducing model complexity or incorporating l2 regularization an interesting approach we would like to implement is building a classification network by bucketing gdi scores to the nearest integer 
using a softmax layer we can take the probability weighted sum of bucket values to determine a scalar gdi score 
in other words we can train the network as a classification task but derive the scalar gdi score using the appropriate weighted sum of the softmax output 
this option also opens the opportunity to use off the shelf classification frameworks for our task our capacity to experiment was constrained by memory overload issues 
the efficient management of memory and resource utilization would allow for more rapid experimentation 
chunking memory swapping or simply accessing machines with larger random access memory can be applied to address this issue future experiments should explore refeaturizing the processed densepose outputs to global x y z coordinates 
this would allow us to manually engineer additional relevant features such as knee flexion angle that are expected correlates of gdi score 
we can further compress our data by considering only the x y z coordinates of the most relevant body landmarks as movements of the hip knee and ankle are particularly important for gait analysis 
this process would require the manipulation of densepose outputs to a customized smpl human body model the potential for future work is enormous as we have just scratched the surface of densepose s capabilities 
a determined effort can lead to the development of a robust reliable and low cost alternative to analyzing human gait 
adam gotlin collaborated closely with the stanford mobilize center and other project partners to secure data and share knowledge 
he orchestrated meetings with advisors and experts and helped identify best practices for time series and movement data 
he led efforts on building temporal cnn models apurva pancholi spearheaded initial experiments and was involved in data preprocessing 
he owned data transfer from the mobilize center to our project team 
apurva developed a custom cnn that was the primary spatial component for our highest performing model 
umang agarwal led data processing and consolidation 
he owned efforts to read and interpret densepose source code in order to generate gdi net model inputs 
he led efforts to exploit transfer learning and contributed to tuning neural network models to maximize performance 
code for this project can be found at https github com agotlin cs229dp
using words can be limited when communicating across cultures and literacy levels 
drawing images is a shared communication method that can bridge those divides 
if successful this model can be applied for a variety of interesting tasks including a new search interface where someone can draw what they need and search for it or an app where a language learner can draw an image and get the translation immediately 
these applications require computers to understand our quick line drawings or doodles 
our goal is to develop an efficient system to recognize labels of hand drawn images from google s quickdraw dataset 
the input to our algorithm is an image 
goal
we use logistic regression support vector machines svms convolutional neural networks cnns and transfer learning to output a predicted class 
deep learning has proven to be very successful in general image classification 
past imagenet 2 1 1 
image recognition
transfer learning 
the idea behind transfer learning is that we can apply knowledge from a generalized area to a novel task while pre trained imagenet models have been widely used in transfer learning for other natural image classification tasks they have not commonly been used for handdrawn images 
however researchers lagunas and garces have successfully used transfer learning with vgg
we are working on a relatively new dataset released under two years ago with a focus on efficiency 
our project could be seen as within the domain of image recognition 
however doodles only consist of lines and have two colors black and white which could render complex features learned by image recognition models useless 
we want to focus on not just accuracy but also efficiency 
efficiency i e model size training time is critical to allow deployment of the system in real life applications yet it has not received sufficient attention in research 
we used the bitmap version of the data 
each drawing consists of 28 by 28 raw pixel inputs with values from 0 to 255 
we took advantage of the fact that each image has only two colors black and white to binarize the pixels for a more compact representation 
to make training more tractable on modest computing resources we elected to work with a subset of the data 
the classes selected were chosen randomly from the overall pool of classes and were fixed throughout our experimentation 
the number of examples per class is 20 000 
we picked 3 10 and 50 classes to train 
for each of the classes we split our data into training validation and test sets with the ratio of 80 10 10 
broadly speaking we looked at two classes of algorithms for our task traditional machine learning approaches and deep learning techniques 
the link to our github with our code is here https github com jervisfm cs229 project 
4 1 1 
logistic regression 
classical machine learning
for our baseline we used logistic regression a simple and fast model to train the log likelihood for our logistic model where h x 1 1 e t x 4 1 2 
support vector machine 
support vector machines are optimal margin classifiers and the optimization objective for these models we explored using support vector machines with various kernels to find empirically the kernel most suited for the task of doodle classification 
the types of kernels we experimented with are linear rbf radial basis function polynomial and sigmoid 
we started out with a cnn a natural candidate for image recognition given the convolutional layer s ability to capture spatial dependency 
a key insight is that since a doodle is a simple image some components of the cnn may be unnecessary 
deep learning 4 2 1 convolutional neural network cnn 
by identifying and removing these layers we developed a compact model that is both fast to train and still accurate 
the cnn architecture is given in
even with a simple cnn we noticed that training a deep learning model from the ground up can be time and resource intensive 
since a doodle is also an image we explored if it is feasible to transfer knowledge from winning imagenet architectures to our specific problem of doodle classification via transfer learning 
we used four different baseline models namely inception v3 vgg mobilenet and resnet50 from the imagenet competition and extended them for doodle classification figure 4 
more specifically we added a global spatial average pooling layer after the original architecture followed by a dense layer of 128 units with relu activation and finally a softmax layer for classification 
we used stochastic gradient descent with an adam optimizer to fine tune the added layers 
due to constraints on computational resources we optionally finetuned the top two layers of the original network 
for logistic regression and svms we ran these models locally 
due to our local machines computational power constraints we ran the cnns and transfer learning models on google cloud on a virtual machine with 320gb of local disk 12 cores of cpu 64gb of ram and an nvidia p100 gpu with 16gb of memory 
efficiency we measured training time in seconds 
we used google cloud deep learning machine image
logistic regression was implemented using python s scikit learn framework putting these numbers in context for the 50 class dataset a classifier that randomly guesses the class would have expected accuracy of 2 
we see that the logistic regression classifier did relatively well that said there was still room for improvement and we performed error analysis next to understand the types of errors that the classifier was making 
looking at the confusion matrix we can see that logistic regression performs relatively well 
the diagonal of the confusion matrix carries the most weight indicating it often makes the correct prediction 
we notice that in the wrongly classified regions the true label banana is highly misclassified with hockey stick 
this is expected as a hand drawn banana is very similar to a hand drawn hockey stick as seen in figures 6 and 7 
thus this experiment suggests that in order to predict hand drawn doodles contrary to our initial belief we may need a more sophisticated model instead of a simpler model because the quality of the drawing may not be very good 
to our surprise the svms performed worse than linear regression overall 
however this could be due to the fact that we have not done extensive hyperparameter tuning for svms among our different choices of kernels the rbf kernel performed the best followed by the polynomial kernel with degree 5 then the linear kernel 
the sigmoid kernel performed the worst with an accuracy equitable to assigning a category at random this result is consistent with what we expected 
the accuracy corresponds to the complexity of the feature space with rbf corresponding to an infinite feature space and polynomial and linear having fewer features 
although the sigmoid corresponds to a higher dimensional feature space its corresponding kernel matrix is not guaranteed to be positive semi definite 
therefore if the chosen parameters are not well tuned the algorithm can perform worse than random
we tested four different variations of a basic cnn using binarized and non binarized data 
a base v1 model included two convolutions and two max pool layers from which we then iteratively simplified as shown in we find that the best performing cnn on the larger classification task of 50 classes was v2 which attained a validation set accuracy of 81 83 with the confusion matrix below 
there is a close correlation between the training accuracy and validation accuracy 
testing this v2 model on the test set we obtained a final accuracy score of 81 87 
this shows that our trained model is able to generalize well to unseen data in general simple cnns performed well for our task 
with fewer classes the accuracy is roughly the same when taking away layers but the training time decreases 
with more classes accuracy decreases with fewer layers as expected but the lowest accuracy is still significantly greater than the accuracy found using logistic regression 
we ran transfer learning with weights pre trained on the imagenet dataset with four different architectures vgg a model proposed in the imagenet 2013 challenge which was widely used due to its simple architecture consisting of only repeated units of convolutional layers followed by max pooling the results of transfer learning experiments are discussed below we noticed however that the best performing model on the quickdraw dataset vgg is also the one with the simplest architecture 
this suggests that more complex architectures such as the inception module used in inception v3 or the residual block used in resnet50 may not be beneficial for the problem of doodle classification 
transfer learning
since doodles are simple drawings only using the classic convolutional and max pool layers may be the best 
this also corroborates our earlier finding with cnn where our simple cnn built from the ground up did well on the quickdraw dataset 
in terms of training time mobilenet performed the best 
this is expected since the model is optimized for efficiency and has the smallest number of parameters 
overall the trend in training time follows the number of parameters in the base model which is expected 
the exception of inception v3 whose training time is the largest despite it having the second largest number of parameters of all the four models 
this was due to us additionally fine tuning the parameters of the two top most layers of inception v3 
this was because transfer learning with inception v3 was performing poorly in terms of accuracy and we wanted to see if further tuning hyper parameters would help 
overall we find that optimizing the model by reducing the number of parameters will help with reducing training time which further supports our initial push for simplifying the models to achieve higher efficiency 
our project aimed to recognize the meaning of doodles a critical first task in order to build any system that uses hand drawn images for communication 
we focused on doodle recognition with an emphasis on efficiency in conjunction with accuracy 
after implementing logistic regression svms cnns and transfer learning and analyzing our results we found that a simplified cnn was best for the task balancing both accuracy and training time 
we also found that for simpler images such as doodles using classic architectures such as a combination of convolutional and max pool layers can outperform complex architectures for future work we would further develop our most promising approach by performing more extensive experiments to determine the effect of each layer in the cnn 
we would also explore using transfer learning as a fixed feature extractor for logistic regression our fastest model 
given more time we would also love to explore working on efficiency in conjunction with smaller datasets each team member contributed equally to this project 
in this paper we are exploring the generation of depthmaps from a sequence of images 
compared to similar projects in the field we have decided to incorporate both spatial cnn and temporal lstm aspects in our model by creating convlstm cells 
these are used in a u net encoder decoder architecture 
the results indicate some potential in such an approach 
hardware progress has enabled solutions which were historically computationally intractable 
this is particularly true in video analysis 
this technological advance has opened a new frontier of problems 
within this expanse we have chosen the classic problem of depth inference from images 
specifically given a sequence of images captured over time we output depth maps corresponding one to one with the input sequence 
as a spatiotemporal problem we were motivated to model it with convolutions spatial and lstms temporal the input to our algorithm is a sequence of images 
we then use a neural network u net encoder decoder architecture with bi convlstm cells for encoding and convolutions and transconvolutions to decode to output a predicted depth map sequence 
as we deal with sequences of images this process is many to many where for each input image we output one depth map 
solutions to the above problem would enable 3d world generation from simple video input with applications from vr to robotics 
while there are hardware approaches to depth determination problems such as lidar or multiple lenses software solutions provide flexibility in their application 
after researching this initial problem in depth we became familiar with literature on depth maps their algorithms and datasets 
this presented itself as a sensible path forward as it seemed simpler and better scoped 
in depth
this area is a classic one with not only history but ongoing and recent progress 
concerning depth maps there are various families of problems single image to depth map depth map alignments from sparse to dense but given the background research we d done on the image depth map sequence we were naturally drawn to the most similar problem from a sequence of images generate a sequence of depth maps there are many reasons to be excited about such a problem especially as the interest for spatiotemporal models is booming 
for us however we wanted to learn about rnns and cnns and as space time lends itself to natural conceptions of convolutions and recurrent networks we proceeded down that path quite excited to apply modern rnn and cnn techniques we were both disappointed and relieved to find extremely relevant literature depthnet while there some people praise cnn to the detriment of rnn we wanted to explore this avenue further 
in pursuit of this approach we have our own opinion as will be discussed at the end 
it is fitting to begin with paper that introduced the core unit of our model convolutional lstm a machine learning approach for precipitation nowcasting we chose depthnet there are many great people and great ideas 
in the search for a dataset with both picture and depth map we have decided to use the kitti dataset
descriptive overview
first we organized the data and store image sequences in subfolders as it seems to simplify and speed up the training
convlstms are more than a convolutional layer into an lstm layer they convolve on the hidden state and the input together 
this functional difference has led to some speculation as to the merits of one over the other where convlstms sometimes prove more effective as in very deep convolutional networks for end to end speech recognition the specific math for a convlstm is where refers to a convolution operation and to the hadamard product 
convlstm bi convlstm
we picked this model to optimize for simplicity of approach while maintain sophistication of the model s capacity 
the u net structure is symmetrical and conceptually simple and the main complexity is within its subcomponent bi convlstm 
architecture u net encoder decoder
this subcomponent could be tinkered without alteration to the whole 
the inputs to our network are 5d tensors b n c h w where b refers to batch size n to sequence length c to channels h to height and w to width 
per layer the number of filters and therefore output channels of that layer increase during the encoding phase starting from 3 rgb and decrease during the decoding phase finishing at 1 depth 
we use relu activation functions for each encoding layer and at the last step of the decoding phase 
skip connections in the u net structure pass forward outputs to later layers concatenating with the output of the directly previous layer 
see figures 2 3 for greater details 
for this project we are using two separate machines with both a recent nvidia gpu 1080ti and p100 
our current implementation of the model uses pytorch 0 4 1
we used multiple metrics for training and evaluation purposes based on properties distinct to each function 
specifically rmse irmse mae imae and a custom scale invariant loss 
rmsewhere d i log y i log y i for the i th pixel n the number of pixels and 0 5 succinctly rmse is standard and easy to implement while providing comparison against other models 
it is distinct from mae in that rmse significantly larger than mae indicates large variance of error distribution frequency finally a1 a2 and a3 metrics are accuracies that represent the percent of pixels that fall within a threshold ratio of inferred depth value and ground truth depth value 
a refers to a base and 1 2 3 refer to powers of that base ie a3 is the most lenient and a1 the strictest 
these accuracies are independent of image size and therefore ideal for baseline comparison 
also whereas losses provide an unintuitive metric of goodness and progress accuracy is more comprehensible 
multiple a values indicate the distribution of inferences 
we are comparing ourselves most directly to depthnet and other kitti competitors with the corresponding loss measures 
the current two leaders are dl 61 dorn and dl sord sq 
baseline measures
comparing sequences of length 1 to 6 we see that the 6 outperformed the 1 on every metric 
this implies that the lstm does provide utility to analysis and that over time information holds clues to depth 
this somewhat justifies the theory behind the model and is consistent with the results of previous teams such as depthnet 
we presented a different approach to image to depthmap implementation 
using jointly a u net architecture and convlstm cells we tried to incorporate both spacial and temporal elements to insure consistency when generating depth maps for sequences of images i e 
video there are several areas to continue our work here 
we d first like to train for an extra week and see if we continue to progress towards convergence 
first increasing sequence length 
while we limited ourselves to a length of 6 we are curious as to the impact a sequence of 600 would compare 
second data processing 
there are many transformation alternatives to be played with 
we especially would like to train on bigger image sizes if we had more time and compute 
there are also multiple ways to iterate over the data as for loss functions we used many of them for evaluation and it d be interesting to explore if any of those is better than our custom loss for guiding the gradient and training 
beyond that we would like to play with kernel size and the number of filters per layer as there are interesting questions in the optimal number per layer 
expanding in creativity how would increasing the number of encodingdecoding layers affect performance 
we did not nearly approach overfitting problems 
more dramatically what are the effects of u net 
if we were to remove the skip connections how would we perform 
other advanced techniques invite exploration 
pyramids for convolutions attention for lstm drop the rnn and just use a 3d convolution perhaps use a 3d convolution inside a convlstm multiple timesteps to each lstm timestep 
there are many possibilities 
while everyone has done some of everything the largest contributions from each to any particulars may be described as follows 
john has researched the literature designed the models and assisted manuel with several of the models and metrics 
appendices contributions
manuel implemented the final model and metrics and ran multiple training runs 
geoffrey has set up the infrastructure for preprocessing data with various transforms saving and loading models and worked on metrics 
existing studies have shown that excessive alcohol drinking can impact the normal structural development of brain anatomy during adolescence in this project our goal is to design a classification model to predict if a subject is a heavy drinker based on their resting state fmri data stored as blood oxygen level dependent bold signals 
after pre processing the inputs to our models are parcellated fmri data as bold signals as well as patient demographic information age sex scanner type 
we then used each model logistic regression svm deep learning to output a predicted classification of the patient as a heavy drinker versus non heavy drinker 
as more and more neuroimaging databases become publicly available machine learning models are becoming increasingly useful in functional neuroimaging classification 
over the past decade there have been several attempts to leverage machine learning on fmri data to classify neurodegenerative diseases or different tasks 
these fmri classifications are often compared to traditional manual classification methods using clinical behavioral data such as the dsm iv criteria for psychological disorders 
the earliest experiments we found mostly relied on support vector machines svms or linear classifiers which achieved accuracies between 69 92 
chanel et al 
svm and linear classifiers
used svms for classifying austistic spectrum disorder asd from both task based and resting based fmri
recent and current techniques for classifying on fmri data seem to take more advantage of recurrent neural networks rnns which naturally lends itself to time series data 
chen and hu developed a rnn based model that was able to identify individuals by their fmri functional brain fingerprints with up to 94 accuracy
for alcohol abuse classification little work has been done with machine learning models on fmri data with most analyses being done statistically or using basic regression models these papers vary on their feature selection and validation methods 
based on the results presented by these works a mixed cnn rnn based approach seemed promising due to the nature of our data which consists of resting state fmri only 
alcohol abuse and our problem
as a result we chose to build a cnn rnn based model to classify heavy drinkers as well as compare the performance against other neural networkbased models as well as regression and svm based models 
our original dataset consists of fmri scans from 715 adolescents young adults from the ncanda database 
the scans measure the bold signal from each brain region per second over t 269 timesteps 2 2 seconds frame 
preliminary visualization of the data such as time series plots of individual patients
fundamentally bold signals were normalized by z score to reduce variability between patients 
moreover there was significant class imbalance within the dataset 122 17 heavy drinkers out of 715 
pre processing and derived features
after setting class balance 50 50 our dataset size was reduced to 244 patients because we were limited by the size of our dataset it was imperative for us to make smart use of our data 
because neural networks have a high tendency to overfit we needed a proper dev set in addition to train test 
we took a random sample of 5 of our dataset 13 taken 231 leftover and set it aside as our dev set which will not be used during kfold cross validation of the test training data 
we decided on 5 of the dataset because we wanted to keep this set size small but still enough to not be overly skewed towards one class based on the cumulative probability of the binomial distribution likelihood of getting 3 or less of a single class is 17 
for 10 fold validation the remaining 231 samples were split 90 10 resulting in 208 training and 23 test samples 
other features included demographic information sex age and scanner type 
train test dev total
to classify heavy drinkers versus non heavy drinkers we implemented a series of models based on related experiments found in the literature 
for our baseline we utilized logistic regression 
for our main exploration of the project we experimented with deep learning models 
we briefly attempted support vector machines as a foil for deep learning 
logistic regression a common binary classification algorithm utilizes the sigmoid function also known as the logistic function 
incorporated with linear prediction parameters and the features of x the classification prediction is given by the following probability distribution whose log likelihood is given by the following due to our low number of features 25 when using derived features from ica parcellated data we used newton s method for convergence 
newton s method requires the hessian of the loss with respect to the features to be calculated which is impractical for high dimensional data 
for fewer features newton s method has the benefit of converging quickly which also allows us to use batch gradient ascent 
newton s method update rule is given by the following 
support vector machines svms map a given set of features to a higher dimensional space so that nonlinear classifications can be made 
as opposed to logistic regression which minimizes functional margin defined by the following equation support vector machines seek to minimize the geometric margin which is defined by the following equation in doing so the convergence of the algorithm takes the norm of the parameters into account and essentially the parameters become invariant to random meaningless scaling 
this is important in allowing the parameter change to be small enough for the algorithm to converge appropriately 
as various svms have been used on fmri classification models in the literature we ran 4 svms with different kernels 1 linear 2 polynomial degree 2 3 sigmoid and 4 radial basis function rbf 
finally we chose to primarily use deep learning in our most promising model as we saw an analogy between our data image processing and natural language processing 
convolutional neural networks cnns are used frequently in image processing to recognize patterns that can be anywhere throughout the picture in our case the fmri data is a time series data for different brain regions 
we are trying to recognize patterns of brain activity at any time point throughout the time series along multiple channels brain regions we tried using 1 d convolution each region as an input channel time series for convolution 
convolutional neural networks
formally the equation for computing 1 d convolution looks like the following where o represents the output of the convolution m represents the m th filter output in the convolution i represents the axis for time series b represents the bias term in the convolution w represents the weight matrix associated with a given output filter x represents the input data and n represents the n th input channel brain region in this case 
note convolution above assumes stride 1 
our most promising model
our deep learning models
in general for all models because we used 10 fold cross validation as described in the data section to get an accurate measurement of the measurement of our model small dataset means cross validation is imperative we had to make sure to set aside a dev set that will not be seen during cross validation but will be used as a means for adjust for the hyperparameters 
this was the best way to properly adjust hyperparameters without being biased by the very data that we eventually train the model on 
for logistic regression we either used fmri data in addition to all demographic information or with all demographic information excluding age 
as expected of newton s method the algorithm converged very quickly 1 6 iterations depending on the threshold for convergence batch gradient descent 
logistic regression hyperparameters
the criteria for convergence c was measured by the magnitude of change in the parameter c d 2 2 where the threshold for convergence was determined manually using the dev set 
we started with a value of 100 for and decreased by a factor of 10 and observed the effect on the average difference in accuracy between the train set and dev set over the k fold validation the dev set never enters the train or test set 
the average difference in accuracy was invariant to the changes in see chart below 
in other words 1 iteration was enough for meaningful convergence of the algorithm and that any additional iterations will not cause significant overfitting 
therefore we left the value of at 1e 5 a value used in class 
1 1e 1 1e 2 avg train dev accuracy 16 16 14 17
using sklearn s package for svms
support vector machine hyperparameters
for the deep learning models which was implemented through keras with theano backend to that end we established three conditions for exiting iterations 
as the model was training the model was to exit iterating if 1 t rainacc devacc t rainacc devacc 20 t rainacc devacc t hreshold the difference in accuracy between train and dev sets is fairly small and they both have reached a threshold value of our interest 0 6 better than random guessing 2 t rainacc devacc t rainacc devacc 20 t rainacc m ax t rainacc devacc the difference in accuracy between train and dev sets is getting too large the train accuracy is too high 0 65 and it is greater than the dev accuracy 
deep learning hyperparameters
suggests that the model is beginning to overfit 
the train accuracy is high 0 65 while the dev accuracy is low 0 55 
suggests that the model is beginning to overfit as for window size stride and output filter dimensions for cnn and output dimensions for rnn we manually judged based on the rate of training and changes in accuracy differences between the train and dev set to have window size 5 stride 1 output filter dimensions 5 and output dimensions for rnn 5 except for when rnn was the only layer present 
3 t rainacc m ax devacc m in
to measure the final performance of all our models we utilized accuracy and f1 scoresince we balanced our classes accuracy gave us a good indications of how the models performed 
however f1 score was a way to formally take both recall what proportion of correct predictions was for heavy drinkers over non heavy drinkers 
performance evaluation metrics
and precision what proportion of heavy drinker predictions was correct 
into account 
logistic regression with our derived features and demographics performed the best
generally svms were not our major focus 
purely based on data we obtained while adjusting for the threshold hyperparameter we saw that all svms either underfit or overfit 
to prevent overfitting we were able to adjust our threshold but we may need to explore other hyperparameters kernels relevant to svms the bulk of our work went into attempting deeplearning models 
as we initially experimented with different hyperparameters one of the first things we noticed was the tendency to underfit or overfit 
we had to make sure to have a dev set we could test on such that we could determine window size output filter dimension and output dimension for rnn 
we also attempted using regularization to no avail as well as drop out 
our most important use of the dev set was to know when to drop after an epoch 
to ensure the model trained enough but not too much we reasoned that the best place to stop was when both the train and dev set accuracies were fairly close and above a certain threshold or when the two accuracies were getting too far apart 
in doing so we were able to avoid overfitting 
however all in all we saw that we could not yield any significant improvements in accuracy f1 score 
overall although we were able to control many of the hyperparameters to prevent under overfitting and adjust the number of layers we found that deep learning models did not perform very well 
this is either because our overall architecture was fundamentally flawed inclusion of demographics is crucial or we were simply lacking in data to be able to make meaningful distinctions as indicated by aggressive guessing of a single class 
as for svms they were not our primary focus for developing our models so we weren t able to see their full potential on the data 
nonetheless our most preliminary work suggests they will have similar issues as our deep learning models 
although we do have our most successful model baseline logistic regression using derived features we also found that removing age as a factor reduces its efficacy suggesting that even logistic regression with our derived features was not any more successful than our other models moving forward there are multiple things we would like to try for our future directions 
as mentioned above we simply may not have had enough data to be able to pick up sensitive features 
to circumvent this issue we can use transfer learning which is commonly used for model development on medical images due to relatively modest sample sizes 
this involves applying learned parameters from other large datasets ideally those that combine images and sequence data as our fmri dataset does and training our own final few layers to use features detected from larger data but predicting for or own another important direction is to incorporate demographics as features into our deep learning models 
although we were limited by the fact that all of us had just learned to use keras and didn t have the time to incorporate multiple data inputs we saw how important demographics can be 
moreover we have not normalized for an natural biological changes in brain function as an individual ages 
including demographics may potentially drastically change our models performances another idea would be to look into different parcellation methods for pre processing the data craddock with more regions etc 
as currently there is no consensus on the best parcellation method of fmri data lastly we can look into making our svm models more robust for our classification problem 
for this project we kept most of the default scikit learn parameters except for tolerance as exploring svms was not the main focus of our project 
however fine tuning additional parameters as in poly sigmoid rbf kernels or considering additional custom kernels may help with the issue we had of either extremely underfitting or extremely overfitting 
all team members contributed equally to writing this project report 
joseph noh researched models and built the frame work to use through keras theanos 
he was primarily responsible for pioneering different methods models 
yong hun kim tested code for different model combinations and hyperparameters and recorded the resulting data 
cindy liu generated a large number of images tables for data and result visualizations implemented the svm models and created the model diagram we would like to acknowledge qingyu zhao for providing the pre processed data and serving as a mentor for the project giving us guidance on our set up and answering our questions about the data 
we would also like to thank tas atharva parulekar and raphael townshend for providing guidance and advice during project office hours as well as professors andrew ng and ron dror for teaching cs229 this quarter 
using a training set provided by the pacific earthquake engineering research peer center we build a classifier to label images of structures as damaged or undamaged using a variety of machine learning techniques k nearest neighbors logistic regression svm and convolutional neural networks cnn 
we find that when compared to classical machine learning techniques the performance of a cnn is best on our data set 
we evaluate the mistakes made by our classifiers and we tune our models using information gleaned from learning curves 
we find that our best performing model which uses transfer learning using inceptionv3 trained on imagenet with an added fully connected layer and softmax has a test accuracy of 83 
the pacific earthquake engineering research peer center has provided image datasets that can be used to classify structures in terms of damage the input to this project consisted of 5913 images 
of this set of images 3186 images were labeled as undamaged or 0 54 and 2727 images were labeled as damaged or 1 46 
each image includes 224 by 224 eight bit rgb pixels 
we split the images into the following sets 90 for training 2870 undamaged and 2451 damaged 
46 are damaged 10 for validation 316 undamaged and 276 damaged 
47 are damaged 
we decided not to set aside images for testing because of the limited number of samples although if we were to eventually submit to an academic journal we would need to be more rigorous in this regard we used several models three classical machine learning models several variations of two deep learning models mobilenet and inceptionv3 convolutional neural networks and one model which combined classical and deep learning techniques the primary output of our classifier models is the accuracy as determined by the number of correctly predicted images over the total number of predicted images 
there are few references on image classification of damaged buildings 
one good survey paper on structural image classification is
minnie ho intel corporation minnie ho intel com
google llc jatron google com a few examples from our dataset are shown in
jorge troncoso
we normalized the images so each of the 224x224 8 bit rgb pixels x was in the range 1 1 
this was done by setting each pixel x to 128 1 
figure 1 example structural images
for k nearest neighbors logistic regression and support vector machine we also scaled and flattened the pictures before feeding them into the models 
we built six models three classical machine learning models k nearest neighbors with k 5 logistic regression support vector machine with rbf kernel two deep learning models mobilenetv1 0 and inceptionv3 convolutional neural networks and one model which combined classical and deep learning techniques support vector machine based on activations earlier in the inceptionv3 network 
the performance of each of these models is summarized in the results section 
in k nearest neighbors an unlabeled vector is classified by assigning the label which is most frequent among the k training samples nearest to that query point 
in logistic regression we use the sigmoid function to estimate the probability that an image belongs to a certain class 
this sigmoid function is parametrized by a vector which is obtained by maximizing the log likelihood 
our logistic regression model included l2 regularization with 1 0 
due to the suboptimal results achieved by this model we did not spend additional time tuning the regularization parameters 
during training support vector machines try to find the maximum margin hyperplane that divides data points with different labels 
supports vector machines can also efficiently perform non linear classification using what is called the kernel trick implicitly mapping the inputs into high dimensional feature spaces our support vector machine model performed non linear classification using the radial basis function kernel which is defined by the formula below 
2 we set the penalty parameter c of the error term to 1 0 and the kernel coefficient for the rbf kernel to 0 001 
due to the suboptimal results achieved by this first model we did not do further tuning of these parameters 
mobilenetv1 and inceptionv3 are two convolutional neural network cnn architectures designed for image recognition tasks 
mobilenetv1 is a lighter lower latency neural network designed for use on mobile devices while inceptionv3 is a heavier architecture which tends to achieve better performance 
mobilenetv1 and inceptionv3
since we only had a few thousand images training these networks from scratch would surely cause overfitting so instead we downloaded pre trained versions of these models using tensorflow with weights optimized to classify images in the imagenet dataset froze the weights of most of the layers of the pre trained networks and trained a new fully connected layer with a sigmoid or softmax activation placed on top of each of the pre trained networks 
this is a common technique used in machine learning known as transfer learning
since our dataset was quite different from the imagenet dataset the features extracted at the top of the inceptionv3 network were probably not optimized for our application so we thought we might be able to achieve better performance by building an svm classifier based on activations earlier in the inceptionv3 network which contains more general features this was achieved by feeding the pretrained inceptionv3 network all of our images computing the output of the 288th later for reference the inceptionv3 network has 311 layers and using these outputs as features for an svm classifier 
here we also implemented model selection to find the optimal kernel coefficient gamma of the rbf kernel as shown in we used a google cloud deep learning vm instance for many of our simulation runs with tensorflow optimized for an nvdia p100 gpu and intel skylake 8 core cpu using intel mkl and nvidia cuda 
support vector machine based on activations earlier in the inceptionv3 network
we discovered an instance optimized for nvdia was faster on cnns but an instance optimized for intel was faster for sci py all of the code used in this project including many experiments whose results we did not include in this report due to lack of space is available in our github repository https github com jatron structural damage recognition 
the performance achieved by each of our models is summarized in the it is not surprising that the models based on cnns performed the best since the parameters could best take advantage of the spatial information in the images 
we note however that the mixed network svm plus inceptionv3 also did well after tuning the kernel coefficient gamma of the rbf kernel we were able to achieve 75 validation accuracy and 95 training accuracy with this model 
experimental results
as mentioned earlier we had applied transfer learning in tensorflow to baseline inceptionv3 model originally trained using imagenet adding a fully connected and softmax layer similar to
bias versus variance
we find that by using 4000 images for training 1000 images for testing on the retrained inceptionv3 model discussed in 5 2 we obtain the following test confusion matrix 448 86 117 349 
after performing the prediction we checked manually through several hundred images to determine patterns in correctly predicted images false negatives and false positives 
misclassified images and data augmentation
examples of misclassified images are depicted in
we retrain the model from 5 2 using keras but this time we remove the top layer of the inceptionv3 network flatten the output of the penultimate layer add a fully connected layer and softmax activation 
we find that we are now overfitting
experiments with inceptionv3
we conclude that a variation of a convolutional neural network performs best on our dataset 
furthermore while bias can be managed by training more parameters layers of the cnn we must be careful not to add so many parameters that we overfit 
conclusions and next steps
however overfitting can be also managed by adding random images to data in terms of future work and next steps more controlled experimentation can be done to manage bias and variance 
we could improve validation accuracy by better managing the data correct mislabeled images add images similar to the false positives or negatives cropping irrelevant features understanding differences in texture or pattern vs damage and accommodating wide angle versus close up images 
furthermore other techniques such as ensemble averaging could perhaps lead to better performance 
we acknowledge sanjay govindjee who alerted us to this problem 
the guidance of fantine huot and mark daoust are also gratefully acknowledged 
autonomous fly by feel vehicles motivated by the supreme flight skills of birds a new concept called fly by feel fbf has been proposed to develop the next generation of intelligent aircrafts 
to achieve this goal stanford structures and composites lab sacl has developed a smart wing which embeds a multifunctional sensor network on the surface layup of the wing collected from a series of wind tunnel tests with different flight states the dataset explored in this study includes conditions of angle of attack from 0 to 15 degrees incremental step of 5 degrees and conditions of airflow velocity from 0 to 25 m s minimum incremental step of 0 5 m s 
discussion and future work features references
60 000 data points are collected from every piezoelectric sensor for each flight state 
we perform data augmentation in the time domain by splitting 60 000 data points into numerous segments as samples 80 samples are used as training data 10 are used as validation data and the 10 else are used as testing data with uniform distribution among each flight state goal minimize misclassification rate 1 1 
the gini index categorical cross entropy convolutional neural network architecture 1 
results of the decision tree algorithm indicate that mean and standard deviation of signal magnitudes and power spectrum are key features 
decision tree
when velocity interval becomes smaller features from different sensors are required to guarantee higher classification performance 
linear models work well with manually designed features 
feature selection improves linear separability of the data 
the convolutional neural network shows comparable performance by feeding in only standardized signal segments 
it is demonstrated that the convolutional neural network can be trained to capture important features from the original signal directly 
we are going to develop a regression model in the following 6 months 
discretized flight state has constrained application if the resolution is not sufficient and high resolution requirement with limited data also poses difficulties for classification 
we hope to train a regression model to provide an accurate estimate of the flight state for example aoa 9 8 airflow velocity 24 3 m s which would be more of practical use than specifying an approximate range of aoa and velocity 
1 f p 
kopsaftopoulos r nardari y h li p wang b ye f k chang experimental identification of structural dynamics and aeroelastic properties of a self sensing smart composite wing in proceedings of the 10th international workshop on structural health monitoring stanford ca usa 1 3 september 2015 
2 x chen f p 
kopsaftopoulos q wu h ren f in this problem a large feature pool from both the time and frequency domains is created to obtain enough useful information from the raw signal data 
we split total data into 80 10 and 10 for training validation and test dataset respectively 
there are 4 743 training samples 522 validation samples and 522 test samples 
artist identification is the task of identifying the artist of a work given only the image with no other metadata 
many paintings have unknown or highly contested artists and experts in the field need a long time to learn the styles of various artists before being able to analyze paintings 
additionally even with significant expertise artist identification can be difficult because one artist s style can vary wildly from painting to painting 
with machine learning we can provide experts with baseline estimate to reduce necessary time and effort as well as making artist identification more accessible to those with less experience additionally this task was chosen because it is a fairly straightforward image classification task and we want to compare conventional machine learning techniques for image classification to more recent deep learning techniques 
specifically in this report we will compare the use of feature extraction with an svm to the use of a cnn 
our dataset contains 256 x 256 x 3 color images of paintings for the svm we extract features from these images as the input for the classifier while we feed in the raw image to the cnn 
for both models the output is a predicted artist 
the metrics we chose to compare the two methods are accuracy training and inference times and ease of implementation 
as previously stated artist identification is often a task done by human experts such as curators in museums art historians and other collectors 
however recent times has lead to a marked increase in computational methods for artist identification much of previous work on this task involves exploration into feature extraction and the subsequent application of a classifier like an svm 
blessing and wen uses features including dense sift hog2x2 ssim and texton histograms to classify works using an svm with 85 13 accuracy approaches using deep learning have also been very successful on this task 
viswanathan explores the use of three different cnn models demonstrating that features from imagenet are generally applicable to artist identification and thus showing that transfer learning can be very useful for this task most prior work has worked with datasets involving relatively few classes blessing and wen only used work from 7 artists
our data is obtained from the dataset for the kaggle competition painters by numbers 
obtaining the data
a set of 6 image descriptors were explored as features for the svm gist descriptors hu moments color histograms sift keypoints histogram of oriented gradients and haralick textures 
each of the 64 possible feature combinations was explored by concatenating feature vectors obtained from each method horizontally 
after exploring the combinations thoroughly the 3 most useful features were found to be the gist descriptors hu moments and color histograms 
gist descriptors are a set of 5 attributes that represent intuitive properties of a scene naturalness openness roughness expansion and ruggedness 
these attributes were determined by aude oliva and antonio torralba hu moments are a set of 7 polynomial combinations of image moments that are defined as to be scale shift and rotation invariant 
scale and shift invariance are ensured by using the central moments to account for shift and scaling by the 0th moment to account for scale 
rotation invariance is obtained by the specific polynomial combinations of moments from ming huei hu the color histogram is a simple set of 3 histograms representing the distribution of color appearances in the image 
a histogram for each color was computed by quantizing the color values for each channel into 8 bins and counting appearances of each quantized color 
the histogram has no representation of the spatial distribution of the colors only their appearance each feature combination was scaled using min max scaling so that each element was in the rage 0 1 
for input to the svm pca was applied with 100 components on the training features to determine the top 100 principal components 
test data was projected onto the same principal components prior to prediction 
examples of feature extraction can be seen in for each class where a one vs rest classification scheme was used so a classifier for each class was trained to identify that particular class 
for each class a distinct svm finds the margin that best separates the data in one class from the data in the rest after the rbf kernel is applied while minimizing the allowable misclassification terms i 
as a deep learning approach to image classification the cnn is a neural network that consists of several layers of different types including convolutional max pooling relu activation dropout dense fully connected and softmax 
each convolutional layer is a set of learnable 2d filters which are applied to input data by the 2d cross correlation operation 
in max pooling layers data is downsampled by a set factor n by choosing only the max value in each n n square to propagate forward 
the relu activation function is a unit ramp function f x max x 0 that allows for nonlinearlity in the network 
in dense layers input data is flattened into 1d vectors multiplied by a matrix of learnable weights and added with a learnable bias 
dropout removes a percentage of activations to help prevent overfitting 
finally the softmax layer computes the class probabilities for the data for our cnn we base our architecture on the winning submission to the kaggle painters by numbers competition j e fj where l i indicates the loss for example i y i is the correct class for example i j is one of the potential classes and f k indicates the score for class k according to the cnn 
the architecture is shown in
our svm model is implemented with the scikit learn package to choose the best hyperparameters features and c for our svm we ran a grid search and 3 fold cross validation across 10 5 10 5 and c 10 5 10 5 using the training set for each feature combination 
the model for each feature combination with best and c was then tested on our validation data and the best was chosen as our best model 
the cnn was also trained multiple times using different architectures adding and removing convolutional and max pooling layers and the best model was chosen using the final validation accuracy after training 
the metric we used for judging our models was accuracy on the test set 
our dataset was overall reasonably balanced so this should be a good measure for how our models perform on these classes 
performance
we found that the 3 most useful features in classifying artists were the gist descriptors hu moments and color histograms 
from the results tabulated in we can see that compared to even the best svm results the cnn is superior 
it achieves higher test accuracy with less noticeable overfitting 
looking at
to compare inference time fairly inference for both models were performed on the ec2 instance 
the results show that cnn took an order of magnitude less time to run inference while achieving higher accuracy 
training and inference time
the bulk of the inference time for the svm can also be accounted for by the feature extraction of the test data however so if feature extraction were accelerated the svm might achieve similar time results 
judging from our current results however the cnn is superior in inference each blue data point in
in terms of ease of implementation the cnn was superior all it required was setting up the initial architecture and feeding in the raw images 
in contrast implementing the svm required a lot of overhead in researching potential features correctly extracting these features from the images and then tuning different combinations to be used with the model 
ease of implementation
we explored the task of artist identification using a dataset of 7462 paintings from 15 artists and compare the performance training and inference time and ease of implementation for both the classical method of feature extraction with an svm classifier and the deep learning method of a cnn 
our best result came from the cnn with an accuracy of 74 7 in comparison to our best svm using gist features and hu moments with an accuracy of 68 1 for future work we would like to address the overfitting in our svm models 
despite the tuning of the regularization parameter our svms are overfitting heavily to the training data this may be mitigated by other regularization techniques such as early stopping we would also like to investigate additional features such as classemes given our rather small dataset we believe that transfer learning would have a lot of success on this task 
training our cnn with a large dataset such as the imagenet dataset and then fine tuning for artist classification with our smaller dataset can help our model gain knowledge about object recognition which has already proven useful in features for our svm 
the initial dataset and implementations of the cnn
our code can be found at www github com jchen437 artist classification 
many present day facial recognition systems focus on making verification or identification facial feature invariant 
while these systems are highly effective for fully automated tasks manual facial recognition is still required in numerous real life scenarios that involve comparing against id photos 
category computer vision
our goal is to build a system that is capable of transforming faces to include or exclude glasses and beard 
such a system should be able to handle a wider range of facial features with small modifications 
a few network structures have been tested for this purpose and we have found that cyclegan 1 is the most capable compared to other vanilla gan systems 
generated images from test set are presented and their inception scores 2 are analyzed 
details regarding characteristics of these generated images are also included in our discussion 
potential future improvements could involve making our system more generic or introducing semi supervised learning to expand usable data sources 
source code for this project is available on github 
there have been significant improvement in our capability to identify and verify human faces over the past few years 
device makers are already taking advantage of such development by equipping their latest phones and tablets with ai co processor and powerful image processing algorithms 
however the recent trend has mostly focused on making facial identification and verification invariant to facial features 
these works certainly help machine recognize human faces however most humans are interested in seeing people in the natural state without any facial disguise a system that can recover undisguised faces could be helpful for criminal investigation 
in particular witnesses should be able to make use of these processed images to identify the criminal among a series of id photos which typically include no disguise or in person among a number of held suspects 
people utilizing online dating apps could also utilize this system to reveal the real person behind facial disguise a feature that many find useful we build on current work related to gan based style transform methods that are commonly employed for applying facial disguise 
recent works have demonstrated much success in related areas we train our generative neural network using a facial disguise database from hong kong polytechnic university
the seminal paper on gans was first published in 2014 where g tries to minimize this function where an adversary tries to maximize it creating the min max optimization problem min g max dy l gan another related work that builds on top of the traditional gan is called the cyclegan
finding an appropriate dataset is one of the most important task for this work 
unfortunately due to privacy concerns and inherent difficulties in obtaining ground truth associated with human faces
iii dataset a data sources
we expect neural networks to produce better results when faces are intelligently selected from the images 
cropping out faces help the neural network select area of interest and reduces input size 
b pre processing
with a reduced input size the network can spend resources on applying feature transformations and on identifying features 
for this project we used image sizes of 64 64 to decrease demand on gpu memory our dataset from hong kong polytechnic comes with cropped images 
celeba in contrast contains too much background for the dataset to be generic enough for a variety of tasks 
we made use of opencv s haar cascades
neural network used for our purpose are much more sophisticated than typical generative adversarial networks that deals with mnist datasets 
multiple network structures have been attempted and their differences will be presented in the results section 
a vanilla gan structure
given the theoretical background of generative adversarial neural network as discussed in section ii a vanila gan can be roughly represented by the project milestone has demonstrated that vanila gans have limited capability in terms of both beard removal and facial feature reconstruction 
this as described in section ii can be tackled by introducing coupling implemented as a cyclegan like structure shown in the forward generator g maps disguised faces to original faces whereas the backward generator f maps original faces back to disguised faces 
we apply adversarial loss functions to both gan s in addition to the adversarial loss functions we have an additional cycle consistency loss to preserve the individual identities through the generation process such that our full objective would be where is a hyperparameter that controls the relative importance of the two objectiv losses we tested cyclegan using relatively simple network structure 
however the cyclegan structure has two sets of generator discriminator pairing effectively doubling the size of the network 
structure of both pairs are the same as presented in taking the exponential makes it easier for us to compare the values 
our experiments are conducted on google cloud vm instances with nvidia k80 gpus 
this setup significantly speeds up the training process compared to running on cpu only machines decreasing discriminator training time from over an hour to less than a minute and generator training from 10 to 15 minutes per batch on a simple cnn structure to a few seconds we built the training infrastructure using keras 
v experiments a experiment environment
in addition we have developed a generic infrastructure that is capable of handling difference generators and discriminators in a plug and go fashion 
this modular infrastructure has significantly lowered overhead associated with experimenting with a wide range of network structures 
our custom code referenced vanilla gan implementation from
we present generated images from different networks that we have experimented with 
b results and discussion
simple convolutional neural network as shown in fig 5 generated images are a lot smoother than that from multilayer perceptron 
b 
there is also traces of beard mustache region being modified by the generator network 
fig 4 generated images from multilayer perceptron
also the generator seems to be brightening columns near nose where mustache typically appears 
c residual convolutional neural network residual networks are supposed to be better in retaining characteristics of the original image 
since this network also contains more convolutional layers the result shown in fig 6 has slightly higher quality than images generated using simple cnns 
fig 5 generated images from simple cnn
these images have far less bright dark bars 
d cyclegan a relatively simple cyclegan structure is implemented for this work 
this is because cycle gan consumes more than twice the memory compared to its vanilla counterparts 
fig 6 generated images from residual cnn
expanding our network to support colored images also significantly limits complexity of the network 
nevertheless cyclegan produces high quality images as shown
clearly with the introduction of reconstruction and identity loss the generated images are of much higher quality 
not only that irrelevant features are modified our reconstructed images look almost identical to the original verifying that the reconstruction losses are highly effective plot of losses for cyclegan running the beard and glasses modification task is presented
fig 7 generated test images from cyclegan
all images presented here are faces of male 
this is because training the network with female faces introduces makeup to modified faces 
fig 8 model losses of cyclegan
for example removing beard adds lipstick regardless of gender 
similarly removing sunglasses frequently adds eyeshadow or eyeline 
another interesting phenomenon we notices is that old celebrities tend to get clear glasses whereas younger celebrities tend to get sunglasses 
though the network handles most images reasonably well we have noticed that it is still struggling with removing opaque sunglasses 
this difficulty is expected because image with opaque sunglasses provides little information about wearers eyes 
the algorithm has nothing to construct the eyes from 
instead it puts a generic eye in place of sunglasses which often look out of place 
this effect is observed among images in which glasses hide significant portion of eye brows 
reconstructed eye brows in those cases are of dubious quality since this project is generative in nature there is no accuracy to evaluate 
inception score is perhaps the more appropriate numerical metric to include for the experiment 
inception score of all tested networks are presented in table vii 
since inception score for cifar 10 images
this project successfully identified a neural network structure to perform the task of modifying facial features 
although results of this work focuses exclusively on beard and glasses the same infrastructure can certainly be used for other features in the future we would like to build a generic infrastructure that is capable of handling any facial feature 
vi conclusion and future works
it would also be helpful to make the training process semisupervised 
this will allow us to include other datasets that do not have relevant tags 
mirza s
our team has divided work evenly based on each team member s technical background and course load 
to be more specific jingbo worked on pre processing and testing neural network models boning worked on building various neural network models and meixian focused on plotting and writing reports poster 
vii distribution of work
traditional off the shelf lossy image compression techniques such as jpeg and webp are not designed specifically for the data being compressed and therefore do not achieve the best possible compression rates for images 
in this paper we construct a deep neural network based compression architecture using a generative model pretrained with the celeba faces dataset which consists of semantically related images 
our architecture compresses related images by reversing the generator of a gan and omits the encoder altogether 
we report orders of magnitude improvements in the compression rate compared to standard methods such as the high quality jpeg and are able to achieve comparable compression magnitudes to the 1 quality jpeg while maintaining a much higher fidelity to the original image and being able to create much more perceptual reconstructions 
finally we evaluate our reconstructions with mse psnr and ssim measures compare them with jpeg of different qualities and k means compression and report compression magnitudes in bits per pixel bbp and the compression ratio cr 
standard lossy image compression techniques such as jpeg and webp are not data specific i e 
they are not designed specifically to handle individual datasets in which the images are semantically related to each other 
introduction and related work
hence these techniques do not make use of the semantic relations among the images in a specific dataset and do not achieve the best possible compression rates 
this has led to a growth in the research towards deep neural network based compression architectures 
these models tend to achieve orders of magnitude better compression rates while still maintaining higher accuracy and fidelity in their reconstructions 
another challenge in constructing a generative deep neural compressor rises from the fact that gans lack the encoder function 
the generator network of a gan can map from the smaller dimensional latent space to the larger dimensional image space but not the other way around 
in compression language this means that a gan can give us a decoder but not an encoder 
addressing this issue is the work of
our main contribution in this paper is the introduction of this novel method of latent space vector recovery into the compression literature 
accordingly we construct a compressor using solely a pretrained gan generator omitting the encoder altogether 
we refer to this method as gan reversal throughout the paper 
compression is done via training a vector in the latent space which is further compressed with bzip2 a standard lossless compression scheme 
decompression of images is simply done with a forward propagation of the latent vector through the gan generator 
to the best of our knowledge we are not familiar with any other literature that uses this gan reversal scheme for image compression furthermore the mentioned paper of
a gan consists of two networks called the generator and the discriminator 
during training the network tries to minimize an adversarial loss function 
models and methods
to achieve this the generator tries to create images that cannot be differentiated from true images while the discriminator tries to correctly classify images as real or generated 
the discriminator is constructed just for the training purposes and is discarded after training 
as stated in the introduction the remaining generator network maps from a low dimensional latent space to a higher dimensional image space 
for the formulas below we will refer to the latent space as the z space and refer to the image space as the x space 
it is not an inherent feature of a gan network to perform a mapping from x z 
however this mapping is exactly the necessary encoding step of compression 
in this compression architecture we encode images to the latent space based on the work of the specific model construction is as follows first we either train a gan or acquire the pretrained generator of a gan that is capable of generating images of a specific domain such as human faces 
the weights of this generator network which exactly corresponds to the decoder of our compressor are kept frozen 
the gans that we use for image generation are from a specific category called dcgans introduced by because these mentioned loss functions are pixel wise distance metrics they have limitations in terms of outputting perceptual images and recovering the edges in the original images 
this motivates us to use perceptual similarity metrics for our training 
one of such metrics is the well known structural similarity index ssim 
for two aligned windows x and y from different images this metric is defined as note that this function takes the neighboring pixels into account and it is a more perceptual metric than pixel wise metrics 
another remark is that ssim value increases up to 1 as the images become more and more similar so the corresponding loss function to be minimized is we minimize these loss functions with respect to the latent vector z via stochastic gradient descent 
note that we actually know that the unknown latent vector was sampled from u 1 1 
thus after each iteration we can clip the vector to stay in this range 
another important remark is that this gan reversal training is non convex and we cannot guarantee to recover the same latent vector after each training 
multiple latent vectors can indeed map to the same image but for our compression purposes it does not matter which latent vector we recover as long as its corresponding image is close to the original 
in our initial experiments we use a gan architecture implemented according to the work of for the test set and for an unbiased evaluation of the models and baselines used we used a test set of 10 images from the celeba dataset face centered and resized to be of size 128 128 pixels 
the images in the test set are not outputs of the gan generator unlike in
experiments and dataset
in this paper we are aiming to design a better extreme compression scheme with two main objectives the scheme must achieve higher compression rates than other standard lossy image compression techniques and the reconstructed images must still be of high perceptual quality and true to their originals 
to assess how well our scheme is doing based on these objectives we have to use suitable metrics for image quality the metrics that are relevant to our work and that are most used in the literature include the mean squared error mse peak signal to noise ratio psnr and structural similarity index ssim 
metrics used and baseline models
while mse seems like the easiest metric to use it is actually not as indicative of perceptual or textural quality as the other metrics to measure how much compression our scheme is achieving we use the bits per pixel bpp metric which conveys the magnitude of the compression scheme by indicating the average number of bits needed to represent one pixel 
to put the numbers in context a non compressed image that represents each color channel of the pixel using one byte has a bpp of 24 
however for fairness before reporting the bpp we losslessly compress the images from any scheme similar to what we do with the latent vectors in our gan reversal approach 
therefore the bpp of the uncompressed png the other baseline we investigated was jpeg optimized which is a popular lossy image compression scheme 
we used two values for the quality parameter of the jpeg schem 10 and 1 
note that for the purposes of this paper where we are aiming for extreme compression our main objective is beating the jpeg 1 baseline in the ssim metric which corresponds to better perceptual quality while still having a comparable compression ratio cr 
the main future goal should be to improve the loss function used for latent vector training even further 
this will enable us to build compressors that achieve more perceptual reconstructions with higher fidelity 
the additional challenge will be to build an automated process for the compressor 
since the compression procedure utilizes a gradient descent based training scheme there are significant parts of compression that rely on human observation such as the hyperparameter tuning running other training sessions with differently initialized random vectors for improvement and picking the best perceptual output among all reconstructions 
for a practical compressor all these processes must be automated 
we would like to thank kedar tatwawadi and shubham chandak for their very helpful discussions and comments 
behavioral cloning the model car is equipped with a mono frontal wide angle camera capturing 120x160 rgb images which are used for the training and testing inputs for the autopilot 
in addition the model car s steering angle and motor throttling values are used for the classification labels so the autopilot can estimate and output the best steering angle and throttling output given an input image in the testing phase 
bojarski et all 
1 have shown that it is possible to use a cnn based supervised model to drive a car 
the work has used three frontal cameras using the middle camera as the main source for the agent s inputs and using the side cameras to compensate the car s shift and rotation movements 
it basically has relied only on the frontal captured images to classify the right steering angle to keep the car on the track 
this modeling is quite simple to come up with a decent performance if it is trained with sufficient amount of data 
however the biggest problem is that once it encounters a state which it has not seen before it is very easy for the agent to drift away significantly figure 1 behavioral cloning trajectory drifting image from where l s is the 0 1 loss of with respect to in state s as to optimize the bound of this approach the same work has suggested data aggregation which can achieve the cost j to be bounded linear to the trajectory t as shown from the following theorem from the same work letting n to designates the number of iteration to perform the data aggregation section 5 3 if n is o ut then there exists a policy 1 n such that
our model car has a mono wide angle camera a servo controlled steering and a thrust motor
donkey car
the primary inputs for the training and testing are the 120x160 rgb images captured from the frontal camera classifying the best matching steering angles and the throttling values 
the agent has performed about total 200 wraps of running on the indoor track in multiple sessions each wrap equaling to capturing about 520 images their corresponding steering angles and throttling values 
the training and validation is performed with split ratio of 0 8 0 1 0 1 between the training developing and validation sets 
the the image on the right in the same figure shows an input image super imposed with a masking to show the image segments activating the cnn most where p p s designating the distribution of states visited by the expert 
cnn autopilot
in order to improve the baseline autopilot policy i have employed data aggregation also specific to this project s in the iteration i is achieved by the expert manually modifying the actions by its best estimation 
for a specific example 
data aggregation
the table in the first row of the table shows the failure rates of each individual policy i without merging all the datasets from each iterations that is not performing d d d i for the next training dataset 
the first column shows the failure rate of the human driver 
experiment results
1 is the first policy trained with the training data collected from the human driver 
2 is the first policy which is generated by manually modifying the misbehavior using 1 
3 is the next iteration policy generated from 2 the second row shows the case of the dataset being merged as d d d i 
only the first iteration is conducted stopping early as the agent failed to track from the start with the given policy the also if i had more time i would have tried modeling the policy through a reinforcement learning model other than an imitation learning tried here 
even though i don t have any quantitative data to support after training the autopilot many times it seems like confirming the fact that the dependency of current state to its past states plays very important role for the agent s robustness section 2 
launching a real world agent using a reinforcement learning taking it out of the simulated environment will pose very interesting challenging problems 8 references
we introduce a novel technique for data augmentation with the goal of improving robustness of semantic segmentation models 
standard data augmentation methods rely upon augmenting the existing dataset with various transformations of the training samples but do not utilize other existing datasets 
we propose a method that draws images from external datasets that are related in content but perhaps stylistically different we perform style normalization on these external datasets to counter differences in style 
we apply and benchmark our technique on the semantic segmentation task with the deeplabv3 model architecture and the cityscapes dataset leveraging the gta5 dataset for our data augmentation 
the task of semantic segmentation is a key topic in the field of computer vision 
recent advances in deep learning have yielded increasingly successful models intuitively semantic segmentation should depend only the content of an image and not on the style 
indeed the style of an image captures domain specific properties while the content is domaininvariant 
we choose to focus on the deeplabv3 model flipping
the cityscapes dataset collects a diverse set of street view images from 50 cities in germany and surrounding countries 
some examples can be seen in the gta5 dataset consists of screenshots of the open world sandbox game grand theft auto v the game is a particularly apt content domain as it is set in a large metropolitan city where street view style images are possible 
further the gta5 images have pixel level image labels that are compatible with those of cityscapes however they required some additional preprocessing 
the cityscapes and gta5 datasets have a difference in their representations of ground truth 
in particular the cityscapes dataset encodes class labels with a grayscale image where each pixel s grayscale value represents the class label 
on the other hand the gta5 dataset encodes class labels with an image where the pixel color represents the class label 
this difference is displayed below in
we selected the well known deeplabv3 architecture for semantic segmentation and used a popular pytorch implementation https github com jfzhang95 pytorch deeplab xception 
deeplabv3 uses a pre trained resnet 101 model as its backbone but adds two additional modules an atrous spacial pyramid pooling module and decoder module designed specifically for the task of semantic segmentation 
it utilizes cross entropy loss 
cross entropy loss is defined as follows for a set of classes c and an image i if y i c indicates whether the true label of pixel i is c and i c is the probability computed by our model that pixel i is of class c thenfor style normalization we utilized a recent state of the art image to image translation model called unit introduced in
to quantify the effects of our data augmentation technique on the robustness of semantic segmentation models we ran two primary experiments 
for the first experiment we took a deeplabv3 network pretrained on the pascal voc and semantic boundaries dataset and applied transfer learning with two additional datasets the first dataset consisted of the 587 cityscapes training images and 302 additional gta5 images and the second dataset consisted of the 587 cityscapes training images and the 302 gta5 images mapped to the cityscapes style domain with unit 
for the second experiment we trained deeplabv3 from scratch with two datasets the first dataset consisted of the 587 cityscapes training images and 587 additional gta5 images and the second dataset consisted of the 587 cityscapes images and 587 additional gta5 images mapped to the cityscapes style domain with unit the standard benchmark statistic for semantic segmentation is mean intersection over union score miou 
intuitively the intersection over union quantifies how accurately a particular model estimates the location of an object relative to a ground truth image by computing the ratio of the number of pixels the model correctly identifies intersection to the total number of pixels representing either the ground truth or object or the model s prediction of the object union 
to extend this notion beyond binary classification we introduce the notion of a confusion matrix 
a confusion matrix m is defined such that m ij is the number of pixels whose ground truth label is i that the model classifies as j notice that the diagonal elements m ii represent correctly classified pixels 
suppose we have a set of class labels c we can then defineas stated above we first used a pretrained deeplabv3 model and applied transfer learning in two ways 
for both models we trained on the first combined dataset of cityscapes and gta5 587 images of each 
for the baseline model deeplabv3 was trained on this dataset to produce semantic segmentation predictions 
for the unit mapped model we first mapped the gta images in our training dataset to the cityscapes domain using the pretrained unit model 
we then trained deeplabv3 on the cityscapes images and these unit mapped gta images 
we also trained deeplab3 from scratch on the second combined dataset of cityscape and gta5 images 
our baseline and unit mapped followed the pipelines in miou scores training from scratch baseline 0 48 unit mapped 0 51 our code is available at the following link https bit ly 2pxnla9 
the downloadable zip file includes our codebase for both the unit and deeplab models 
we find similar performance between baseline and unit mapped for our models trained using transfer learning 
we hypothesize that the pretrained model may not be the best fit for the street city scenes of the cityscapes and gta datasets 
the pascal visual object classes voc dataset and the semantic boundaries dataset sbd are different tasks than the semantic segmentation of classes in street scenes we also performed qualitative analyses of our results on this experiment 
we compared the predicted semantic segmentations of our baseline model and the unit mapped model and find that the segmentations are similar with no noticeable differences 
we hypothesize that training on a larger dataset would yield higher miou scores for our unit mapped model as well as more clear visual differences 
given our constrained resources and limited compute power we were restricted to a small dataset 
our observed results in the pre trained deeplabv3 experiments reinforce the fact that more data is necessary 
the comparable performance on the task suggests that neither training regimen could shift the model s weights particularly far from the voc sbd optimum in the parameter space 
the learned features for the voc task effectively drowned out any subtleties of the street view segmentation task and the effect of our additional images 
we believe that we would find more significant improvements provided more unit mapped gta images our results from our second round of experiments where we trained deeplabv3 from scratch show some potential 
here we find that unit mapped slightly outperformed baseline 
the improved miou scores suggest that mapping synthetic data onto the real world domain could potentially improve the robustness of a real world classifier 
our results as discussed above do not yield any significant conclusions regarding our novel technique for data augmentation 
we note again that compute and time restrictions did not allow us to train deeplabv3 with sufficiently many training samples to achieve baseline results as reported in other papers 
nonetheless our results yield various promising avenues for future research 
in particular the superior performance of the deeplabv3 model with our novel technique for data augmentation when trained from scratch in comparison to the model with simply a combined dataset suggests that our technique could be successful if we used more training samples 
another area for future work is exploring the efficacy of our data augmentation approach across other tasks in computer vision 
for instance we would like to test our methodology on object detection and localization 
there were two main tasks over the course of this project data preprocessing and training deeplabv3 
evani worked on training the deeplabv3 model using transfer learning for our initial results 
andrew handled parts of the data preprocessing such as converting gta5 images to the cityscapes style domain with the unit model and contributed to training deeplabv3 for the initial results 
felix worked on training the deeplabv3 codebase from scratch and some of the data preprocessing such as making the gta5 and cityscapes labels compatible 
in this work we implemented and trained an end to end deep neural network songnet to perform real time music genre classification 
music can be represented in various forms time series decimals spectrum in frequency domain and spectrograms etc 
the spectrogram stands out as the most popular choice since it incorporates time and frequency information 
in this project we used the convolutional recurrent neural network c rnn to classify music 
the convolutional network extracts features of spectrogram before feeding them into recurrent network which then performs classification considering both transient and overall characteristics of music 
taking only raw audio as input the c rnn achieved 65 23 accuracy on fma small dataset beating the best baseline by 41 
with the enormous growth of music released online managing music library manually has become more and more challenging not only for users but also audio streaming service companies such as spotify and itunes 
fast and accurate music classification is in high demand while it is non trivial for machines to perform the task automatically at human level besides music genre classification is an essential backbone for music recommendation and unknown soundtrack recognition which will benefit music service platforms a lot 
building a robust music classifier using machine learning techniques is essential to automate tagging unlabled music and improve users experience of media players and music libraries in recent years convolutional neural networks cnns have brought revolutionary changes to computer vision community
music genre classification has been actively studied since the early days of the internet 
tzanetakis and cook in recent years using audio spectrogram has become mainstream for music genre classification 
spectrogram encodes time and frequency information of a given music as a whole 
wyse this work aims to train a c rnn model with melspectrogram as the only feature and compare this model with the traditional machine learning classifiers that need to be trained with hand crafted features and metadata 
the dataset used for this project is the free music archive fma an interactive library of high quality legal audio downloads direct by wfmu 
furthermore it provides music s associated information including precomputed features user level metadata etc 
free music archive 2 
to ensure data is balanced among different genres we only use a small subset fma small for the scope of this project 
it con the fma provided fine genre information for each track with built in genre hierarchy which is claimed by the artists themselves 
in each of the track table the ids of all the genres indicated by artists are included and the root genres are provided in genre top column the preprocessed dataset is split into 70 training 20 validation 10 test sets respectively 
a popular representation of sound is the spectrogram which captures both time and frequency information 
in this study we used mel spectrogram as the only input to train our nerual network 
a mel spectrogram is a spectrogram transformed to have frequencies in mel scale which basically is a logarithmic scale more naturally representing how human actually senses different sound frequencies 
it is simple to implement thanks to librosa aside from the music features extracted by librosa fma also provides music metadata such as release year number of listens composers durations etc 
there are 140 features in total that could be used for training 
we trained four traditional classification models on the dataset as baseline classifiers including k nearest neighbors logistic regression multilayer perception and linear support vector machine 
it was found that baseline models could achieve no higher than 50 accuracy 
method 4 1 baseline classifiers
since these models were merely used for comparison we adopted the default implementation and parameters in scikit learn library 
the input features include all 140 features provided by fma 
as shown in to start features are extracted from the spectrograms using convolutional layers 
it is important to point out that the features are translation invariant only in time domain frequencies do matter and needs to be distinguished 
songnet architecture
thus 2 d convolution seems unsuitable in this case we are interested in changes across time every convolutional layer should look at a small period of time as a whole extract the most valuable information and create a feature map that is still a sequence over time 
then one dimensional convolutions across the time axis were adopted 
each convolution is followed by relu activation and 1 d max pooling 
to regularize the model we added dropout to every convolutional layers the cnn outputs a sequence of features and it is then fed to rnn represented by a time distributed fully connected layer with softmax activation essentially giving us a sequence of 8 dimensional vectors 8 is the number of genres in fma small at each timestep 
the rnn part is designed to find both dependencies across short period of time and a long term structure of a song 
these vectors are interpreted as the networks belief of the music genre at the particular point of time i e 
probability distributions 
to reduce the time series of 8 d probability vectors into a single one genre probability distribution we simply take the mean 
it is the most intuitive way to tackle the disproportion problem of inferring music genre per timestep versus just one label for the whole song but it turns out to very effective 
the accuracies of baseline classifiers and songnet are reported in the table below 
it can be observed that our c rnn model outperforms the best baseline by 41 
the validation set was used to help us tune hyperparameters of songnet 
during training the learning rate was initially set to 0 001 and further decayed subject to reducelronplateau scheduler 
the reported numbers are accuracies with respect to the test set 
it is worth mentioning that all of our baseline models were trained and tested with rich features including music metadata year artist etc 
however in the current c rnn model setting we decided not to incorporate metadata for simpler training setup 
model accuracy
the fact that c rnn model still beats the best baseline by a significant amount even without metadata demonstrates the power of deep learning models on classification tasks 
to further interpret the results and guide future work we plotted the confusion matrix
error analysis
in computer vision convolutional layers are used to extract features from images 
low level kernels can detect edges or corners and higher level kernels can capture more sophisticated structures 
kernel clips
in our setting we also expect convolutional layers to do similar things 
songnet has 3 convolutional layers so we expect kernels to extract different levels of music genre kernels 
it would be straightforward and much clearer if kernel numbers are converted to music clips 
after listening to some of kernels we found that kernel clips in the first convolutional layer are mainly basic beats and elements of music 
the clips from the last convolutional layer however are already human listenable syn thesized music clips of certain genres 
we demonstrated the kernel clips during the poster session and uploaded them to google drive link for grading purposes 
the ultimate goal of songnet is real time genre classification as the soundtrack plays 
this is the reason why we combined recurrent network with convolutional neural network in our architecture 
real time
as discussed in the architecture section for each timestep the model outputs a probability distribution vector among 8 different genres so it enables real time classification 
it is better to show this functionality with a gui 
due to limited timeline we did not implement it yet but it could be an interesting extension in future work 
following our discussion above we conclude two possible extensions of current work 
to solve the experimental genre issue because it contributes a lot to the loss 
it is worth trying to incorporate music metadata 
to further increase the test accuracy it is essential
we expect the metadata to help increase the performance because even though this music itself shares some features with other genres such as rock and electronic additional information such as artists and album years will be able to help the model better classify this genre 2 
build a graphical user interface to allow users upload a music clip and then visualize the real time classification 
this is fun as well as beneficial for further tuning the model 
it s fun because users can have a better way of interaction with the model 
as users upload more songs we could collect more data to improve the model 
we have developed a weakly supervised method for localizing pneumonia on chest x rays 
our model includes two parts 1 a 10 layer convolutional neural network cnn that predicts the presence of pneumonia and 2 a class activation map cam that localizes the pneumonia manifestation without requiring bounding box labels 
by having our weakly supervised approach achieve slightly better performance than a supervised method r cnn we believe that this brings tremendous value in labeling diseases in images that are often unannotated in medical records 
thus our method has the potential to provide care to populations with inadequate access to imaging diagnostic specialists while automate other medical image data sets 
pneumonia is an inflammatory condition of the lung that is responsible for more than 1 million hospitalizations and 50 000 deaths every year in the united states 
globally pneumonia is responsible for over 15 of all deaths of children under the age of 5 however chest x ray images generated from hospitals do not specify the precise location of the pneumonia which is a significant challenge when training a machine learning model for this purpose 
at most doctors will keep a brief description such as aggregation of lung opacity on the patient s lower right lung 
this is because the precise pixel location of lung opacity on the x ray image is only part of the equation for diagnosing and only the final conclusion is recorded in the electrical health record ehr 
to developed a machine learning algorithm that predicts pneumonia location using traditional supervised methods requires the 0 precise x y coordinate labels that datasets lack 
hence this deficiency in labelled datasets commonly observed in the medical imaging field is the motivation behind our work in this work we tackle this challenge in a novel approach we use a weakly supervised approach to automate localizing pneumonia in chest x rays 
our model is considered weakly supervised because it only needs the binary labels pneumonia vs no pneumonia during training to estimate a bounding box around the region of the lung opacity 
at a high level our weakly supervised algorithm works as follows 1 input an x ray image in u net architecture for data augmentation 2 input augmented image and original image in a 10 layer cnn architecture to classify if given image is pneumonia positive and 3 if image is pneumonia positive apply cam to precisely localize the pneumonia aggregation 
there have been recent efforts in detecting pneumonia using x ray images 
for instance the chexnet 10 team modified chestx ray14 s 13 algorithm to increase the accuracy in detecting 14 diseases including pneumonia 
however neither effort localizes using bounding boxes and both use imagenet to pretrain their models 
despite the fact that both works achieve high accuracy neither solves the problem of clearly annotating the pneumonia manifestation using bounding boxes in the x ray images 
as such we leverage the work of four algorithms in our approach 1 r cnn 6 2 cam 14 3 vgg architecture
3 0 1 
dataset we acquired our dataset from kaggles rsna pneumonia detection competition
dataset and feature engineering
each pixel of the images was normalized by subtracting the mean and dividing by the standard deviation at that location 
we also compressed the original image from 1024 1024 pixels down to 128 128 pixels allowing us to expedite the training of our neural network 
feature engineering
a u net neural network was used to predict the confidence of each pixel belonging to the lung 
then we segmented the lung by multiplying the original image matrix with the localization matrix figure 2 
both the original and the segmented images were fed into our model as inputs to provide our model with a hypothesis of lung location 
the first part of our work is to build a cnn model that can accurately classify whether a given image is labeled as pneumonia or not 
the images that were predicted as pneumonia positive are then fed into our localization model 
since the first part of this project is a supervised classification task svm random forest and logistic regression were used to baseline our classification model 
these models could not take a matrix of pixels as an input so we flatten the images into one dimensional vectors 
our best model contains 10 convolutional layers each with zero padding to keep the size of the original image and we used relu as the activation function 
the convolution filters are matrix of weights that slides through the original image to pick up patterns for prediction 
cnn architecture
the cam requires our model to only have one fully connected fc layer and a global average pool gap layer before that 
the gap layer takes the average of the output for each of the convolution filters 
the fc layer connects the flattened averages to the two classes 
the model was trained with an adam optimizer with 0 0001 learning rate on 20 epochs 
the second part of our project is to build a weakly supervised model that can predict the localization of pneumonia on the positively classified images without the training labels of the locations 
localization
to generate region proposals we slide a small network over the convolutional feature map output by the last shared convolutional layer 
this layer comes from the classifier that is trained on predicting pneumonia 
supervised r cnn approach benchmark 
this small network takes in a small spatial window from the cnn feature map and predicts whether or not these windows contain pneumonia or not pneumonia 
a window is defined as having four coordinates x1 y1 x2 y2 
we only keep the windows that are classified as having pneumonia and by how much these spatial windows overlap with the ground truth labels 
then for each spatial window the features from the original cnn feature map is mapped back to the cnn feature map from the classifier and these windows are pooled to the same size and are feed to two networks one network to predict class background or pneumonia and another network to predict the coordinates figure 3 b 
our weakly supervised portion of the model consists of the following components figure 3c 
weakly supervised approach
a cam that takes in the output of the final cnn model and the fc layer weights for the pneumonia class neuron and sums up the weighted outputs using the following formula where x is the input image features f k give the output from the last convolution layer given x and w c k is the fully connect weight for the k th filter output to class c in our case class c is the pneumonia class 
cam
the output from cam was then scaled into a 3 channel rgb heatmap 
heatmap
to find individual clusters of predictions on the heatmap we applied a depth first search clustering algorithm algorithm 1 on a random non zero pixel on the heatmap and repeated until all non zero pixels are clustered algorithm 1 dfs cluster algorithm class index 0 while still exist non zero pixel without class label do pick a random non zero pixel without class label assign pixel to class index for each neighbor pixel do if if neighbor is also a non zero pixel without class then recursively apply dfs end end end class index 1
dfs clustering
lastly we drew a bounding box around each clusters by finding the minimum and maximum x y coordinates of the clusters and only kept boxes that are within 2 standard deviations of all predictions 
bounding box
input features running any localizing model on the original x rays yielded low iou scores 
we discovered that often time the model will localize matter denser than the lung such as muscle tissue and equipment as pneumonia positive figure 4 right 
experimentation
and since a high iou score correspond to a more accurate and tightly fitted localization we experimented with a number of approaches to increase our iou score we started by only feeding in the segmented lungs from u net as the input for our model 
though initially the classification results were promising the iou score did not improve significantly 
we soon discovered that in instances of sever pneumonia infection where the density of that part of the lung and surrounding tissue were almost identical the u net algorithm segmented out that part of the lung 
this in turn yielded inaccurate localization results where the algorithm localizes on the healthy part of the lung figure 4 left mid after testing out different combination we found the best results can be achieved when we use both the original and segmented image simultaneously by running them through two channels of the network 
we hypothesize that including the segmented healthy part of the lung provides the model with extra information on where the likely locations of the lung opacity 
we started our classification model using the vgg16 model which includes 26 layers and 138 357 544 weights 
it was clear that the large number of weights caused our model to quickly overfit with training accuracy as high as 96 but 72 for validation 
model architecture and parameter tuning
we then modified the architecture by removing layer by layer until bias of the model dropped but still were able to make above 90 accuracy on the validation set then we tested different filter sizes of the convolutional layers to improve the classification 
our highest validation accuracy was achieved starting with small 3x3 filters and gradually increase the filter size to 16x16 at the last layer 
however a big filter size at the last convolution layer gave us imprecise prediction of the the bounding boxes and caused a sharp decrease in the iou score 
therefore we decided to sacrifice some classification accuracy for an increased iou score by keeping all filters to 3x3 finally different optimizers were tested to train our model 
the standard stochastic gradient descent algorithm trains very slowly and does not converge to above 85 accuracy 
adam and adagrad converge faster but adam achieved a higher validation accuracy 
we also tested out different learning rates 0 001 0001 0 0001 for each optimizer 
learning rate of 0 001 caused the weights to blow up and achieve lower than 50 accuracy 
however a learning rate of 0 0001 with adam optimizer gave us our higher accuracy in the shortest period of time 
formula 2 the iou formula
our cnn significantly outperformed traditional classifiers without over fitting
based on our result we have shown that our weakly supervised method is able to localize pneumonia slightly better than a supervised method 
we predict that our model can perform even better if we have the computing power to train on the full images as a lot of information are lost during compression 
we also expect improvements by including more training data or transferring learned models from similar works such as chestxnet 
if improved to human level performance our weakly supervised model not only can automate pneumonia location annotation and classification tasks but also can be used to automate other medical image datasets 
mars huang came up with project idea and methodology 
build the cnn classifier and tested different architectures 
modified the classifier to fit in to class actiation mapping 
implemented cam dfs clustering algorithm 
made functions to draw bounding box calculate iou and feature engineering 
tried to implement em and kmeans to cluster heatmap regions 
attempted to reduce dimentions of the data for baseline by using factor analysis 
tested all baselines for classification portion of the project and experimentation in the classification and weakly supervised localization 
created mltoolkit for baselines 
generated all figures major contributed to the paper and poster 
set up google cloud medi monam lead in reading literature to gather knowledge in the field 
unet segmentation feature engineering 
experimented with methods to cluster heatmap islands 
experimented with implementation of vgg16 
contributed to poster and paper 
printed poster emanuel cortes built the supervised model for classification and localization 
implemented a resnet backbone classifier custom region proposal layer roi pooling and a bounding box regressor that is pretrained on the coco dataset and finetuned on kaggles rsna pneumonia detection competition dataset 
experimented with feeding other cnn based backbone classifier architectures whose out 
we compared traditional machine learning methods naive bayes linear discriminant analysis knearest neighbors random forest support vector machine and deep learning methods convolutional neural networks in ship detection of satellite images 
we found that among all traditional methods we have tried random forest gave the best performance 93 accuracy 
among deep learning approaches the simple train from scratch cnn model achieve 94 accuracy which outperforms the pre trained cnn model using transfer learning 
the fast growing shipping traffic increases the chances of infractions at sea such as environmentally devastating ship accidents piracy illegal fishing drug trafficking and illegal cargo movement 
comprehensive maritime monitoring services helps support the maritime industry to increase knowledge anticipate threats trigger alerts and improve efficiency at sea 
this challenge origins partly from the airbus ship detection challenge on kaggle 
we plan to come up with a solution to efficiently detect ship in satellite images 
this classification is challenging because boats are really small in the satellite images 
various scenes including open water wharf buildings and clouds appear in the dataset in this paper we compared traditional methods and deep learning methods in solving the classification problem 
for traditional methods we experimented with naive bayes linear discriminant analysis k nearest neighbors random forest and support vector machine model 
before training these models we did image features extraction by finding global feature descriptors of every image which includes color histogram hu moments haralick texture and histogram of oriented gradients hog 
we found that feature engineering significantly improved the performance of traditional models 
we also noticed that for some model certain com for deep learning method we used a pre trained network densenet 169 of imagenet architecture as the baseline network referred as tl cnn 
we then designed a simple cnn model consisting of 4 convolutional layers and 4 maxpooling layers referred as sim cnn without using transfer learning approach 
we observed that the simple train from scratch cnn model worked better than the pre trained cnn model 
recently machine learning and artificial intelligence have attracted increasing attention and achieved great success in different areas including computer vision when narrow down to the problem of ship detection there are no research papers studying this problem since it is a recent kaggle challenge problem 
however there are several attempts made by some kaggle users 
for example kevin mader
we obtained a public dataset provided on the airbus ship detection challenge website
data description
first of all we re sized the original image to the size of 256 256 3 then we applied different data preprocessing techniques for traditional machine learning algorithms and deep learning algorithms 
we used hand engineering features extraction methods gogul09 2017 to obtain three different global features for traditional ml algorithms 
the images were converted to grayscale for hu and ha and to hsv color space for his before extraction as shown in hu moments hu features are used to captured the general shape information 
feature extraction for traditional machine learning method
hu moment or image moment is a certain particular weighted average moment of the image pixels intensities 
simple properties of the image which are found via image moments include area or total intensity centroid and information about its orientation 
to enhance the robustness of our cnn model for all the images in the training set we implemented data augmentation method such as rotation shifting adjusting brightness shearing intensity zooming and flipping 
data augmentation can improve the models ability to generalize and correctly label images with some sort of distortion which can be regarded as adding noise to the data to reduce variance 
data augmentation for cnn model
to classify whether an image contains ships or not several standard machine learning algorithms and deep learning algorithms were implemented 
we compared different approaches to evaluate how different model performed for this specific task 
for all the algorithms the feature vector is x x 1 
the algorithm finds a linear combination of features that characterizes and separates two classes 
it assumes that the conditional probability density functions p x y 0 and p x y 0 are both normally distributed with mean and co variance parameters 0 0 and 1 1 
linear discriminant analysis lda 
lda makes simplifying assumption that the co variances are identical 0 1 and the variances have full rank 
after training on data to estimate mean and co variance bayes theorem is applied to predict the probabilities 
the algorithm classifies an object by a majority vote of its neighbors with the object being assigned to the class that is most common among its 5 nearest neighbors 
the distance metric uses standard euclidean distance as
k nearest neighbors knn 
naive bayes is a conditional probability model 
to determine whether there is a ship in the image given a feature vector x x 1 
naive bayes nb 
using 
x n 
with the naive conditional independent assumption the joint model can be expressedthe algorithm is an ensemble learning method 
bootstrap samples are selected from the training data and then the model learns classification trees using only some subset of the features at random instead of examining all possible feature splits 
after training prediction is made by taking the majority vote of the learned classification trees 
the depth of tree is limited by 5 and the number of trees is 10 
the algorithm finds the maximum margin between different classes by determining the weights and bias of the separating hyperplane 
the soft margin svm classifier minimizes the loss asthe fit time complexity is more than quadratic with the number of samples 
convolutional neural network cnn which is prevailing in the area of computer vision is proved to be extremely powerful in learning effective feature representations from a large number of data 
it is capable of extracting the underlying structure features of the data which produce better representation than hand crafted features since the learned features adapt better to the tasks at hand 
convolutional neural network cnn 
in our project we experimented two different cnn models 
the motivation of using a transfer learning technique is because cnns are very good feature extractors this means that you can extract useful attributes from an already trained cnn with its trained weights 
hence a cnn with pretrained network can provide a reasonable baseline result as mentioned in section 2 we reproduced the so called transfer learning cnn tl cnn as our baseline model 
transfer learning cnn tl cnn 
we transferred the dense169 
instead of using a pre trained cnn model we decided to construct a simple cnn model named sim cnn with a few layers and trained it from scratch 
one might hope that this could improve the performance of tl cnn since its cnn weights were trained specifically by our dataset the
simple cnn sim cnn 
for the traditional machine learning algorithms discussed in section we applied sklearn package in python to realize each algorithms for the deep learning approach the two cnn models were trained in microsoft azure cloud with gpus 
cross entropy loss was used 
we trained the model for 30 epochs using mini batch size 64 with batch normalization 
we applied adam optimizer as well as decaying learning rate to facilitate model training 
the weights of model would not be updated if the loss of development set did not improve after training an epoch 
comparing performance with and without feature engineering we found that feature engineering significantly improved performance in general 
instead of directly training on image pixels information the information extracted from feature engineering amplified the signal of whether an image containing ships especially for nb and svm approaches 
however since 0 84 of the images in test set are labeled as no ships among those traditional machine learning method only lda and rf outperformed the accuracy of blindly guesting no ships 
in addition we learned that some algorithms give much better performance when working with only certain combination of features 
it is suggested that haralick textures information of image is of great importance for nb method improving from 0 42 to 0 75 
we applied 10 fold cross validation to each machine learning algorithms and create the corresponding box plot in
test accuracy
consider our dataset being imbalanced instead of using a threshold of 0 5 we wanted to find one that would take imbalance into consideration as suggested by
threshold scanning
among all traditional methods random forest gave the best result 0 93 accuracy with feature engineering 
as for the cnn model our train from scratch sim cnn model outperforms the baseline tl cnn model based on pre trained densenet 169 network 
conclusoin future works
in the future for traditional machine learning algorithms we plan to improve feature engineering by extracting global features along with local features such as sift surf or dense which could be used along with bag of visual words bovw technique 
for deep learning algorithms to achieve better performance we will try implementing different networks e g deeper network to train the classifier 
last but not least it is more challenging but also more interesting to try applying segmentation technique to identify the locations of all ships in a image 
all three members of this team work together and contribute equally to this project in data prepossessing algorithm designing model designing model training and report writing please follow project code or https github com cs229shipdetection cs229project airbus ship detection for the project code 
team contribution project code
in the modern era an enormous amount of digital pictures from personal photos to medical images is produced and stored every day 
it is more and more common to have thousands of photos sitting in our smart phones however what comes with the convenience of recording unforgettable moments is the pain of searching for a specific picture or frame 
how nice it would be to be able to find the desired image just by typing one or few words to describe it 
in this context automated captionimage retrieval is becoming an increasingly attracting feature comparable to text search in this project we consider the task of content based image retrieval and propose effective neural network based solutions for that 
specifically the input to our algorithm is a collection of raw images in which the user would like to search and a query sentence meant to describe the desired image 
the output of the algorithm would be a list of top images that we think are relevant to the query sentence 
in particular we train a recurrent neural network to obtain a representation of the sentence that will be properly aligned with the corresponding image features in a shared highdimensional space 
the images are found based on nearest neighborhood search in that shared space the paper is organized as follows first we briefly summarize the most relevant work related to our task then we describe the dataset employed for training and the features of our problem 
subsequently we introduce our models namely a multi response linear regression model and a deep learning method inspired by
under the umbrella of multimodal machine learning caption image retrieval has received much attention in recent years 
one main class of strategies is to learn separate representations for each of the modalities and then coordinate them via some constraint 
a natural choice of constraint is similarity either in the sense of cosine distance a different class of constraint considered in more recently there is another line of work that tries to improve retrieval performance with the use of generative models 
in under the hood of most state of the art models the choice of pretrained features embeddings plays an important role 
we use vgg 19
we train our models using the microsoft coco dataset three teddy bears laying in bed under the covers a group of stuffed animals sitting next to each other in bed a white beige and brown baby bear under a beige white comforter a trio of teddy bears bundled up on a bed three stuffed animals lay in a bed cuddled together to represent images a common choice is to use a pretrained image model as a feature extractor and use the last layer of the forward pass as the representation 
in the present work we employ the f c7 features of the 19 layer vgg network
in this section we describe the methods that we use for this task 
they include a traditional supervised method based on multiple response linear regression and methods based on neural networks 
in multimodal machine learning a common approach is coordinating the representations of different modalities so that certain similarity among their respective spaces are enforced 
our task involves texts and images 
baseline method
to represent the captions we simply average the glove vectors relative to the words of the sentence though more sophisticated methods exist 
let f glove be the sentence features and f vgg be the image features coming from the vgg network 
in order to encourage similarity between these two different types of representation we would like to find a weight matrix such that this is known as multi response linear regression 
as a generalization of linear regression it has closed form solution or can be solve by stochastic gradient descent when we have a large dataset 
at test time when we are given a caption c t we compute the caption feature vector f glove c t and find the image s closest to that 
our method is inspired by where f i and f c are embedding functions for images and captions respectively 
there is a negative sign since we would like larger s to indicate more similarity 
multimodal neural network methods
for the purpose of contrasting correct and incorrect matches we introduce negative examples that comes handy in the same training batch 
the cost can thus be expressed aswhere c i is the true caption image pair c and i refer to incorrect captions and images for the selected pair 
therefore the cost function enforces positive i e 
correct examples to have zeropenalty and negative i e 
incorrect examples to have penalty greater than a margin feature extraction for both modalities is similar to the baseline 
the embedding function f i is obtained by opportunely weighting the outcome before the output layer of the vgg network and f c takes the last state of a recurrent neural network rnn with gated recurrent unit gru activation functions where w i is a n 4096 matrix of weights to be trained and n is the number of features of the embedding space 
the embedding function f c is now the outcome of a recurrent neural network rnn with gated recurrent unit gru activation functions figure 2 recurrent neural network to process captions we observe that in in addition we explore the usage of a different modules in the rnn namely long short term memory lstm and different rnn architectures such as stacked bidirectional rnns 
metric the metric we use in this project is recall k r k 
given a list of predicted rankings r i 1 i m for m images based on their corresponding input captions we definewe should notice that this metric also depends on the size of the image database 
for example searching over a one million image database is clearly harder than a one hundred database 
in this project we focus on the size of 1k images 
in addition we will also look at some conditional metrics such as the length of the caption to better understand the results hyperparameters we started with a set of hyperparameters suggested in results in we also compare the baseline method and the neural network solution through some real examples 
we see that the baseline model works well for simple queries like single word object names 
however for longer captions as in
gru cnn
in this project our emphasis is more on language models because as a first step we would like to accurately identify the semantics implied by the query 
on the image side we only represent each by its features extracted from a pretrained network 
although we see the image feature is able to capture small details in the image it can still be the bottleneck as our language model becomes more sophisticated 
in the future we would like to endow a dynamic attention mechanism so that the model will be able to choose adaptively the region s to focus on in the image 
this might be done either by including some pretrained features in the lower layers or by computing features on sub regions of the image 
there are some initial attemps in this direction such as link to the code https github com giacomolamberti90 cs229 project
activity recognition is an important task in several healthcare applications 
by continuously monitoring and analyzing user activity it is possible to provide automated recommendations to both patients and doctors
because of its many applications supervised human activity classification using sensor data is a relatively popular research area 
through our research we have found that related articles and their approaches can generally be divided into three categories naive bayes classification svm decision trees and neural networks 
we consider the use of naive bayes as a classifier for human activity as clever and interesting since it is usually used for text classification 
one such article that uses the naive bayes classifier is long yin and aarts 2009
we used the pamap2 dataset from the uci repository of machine learning datasets 
as a baseline measure we incorporated a standard logistic regression model with a loss function of the form shown in 1 
to make the model more robust against overfitting l2 regularization was incorporated with the c parameter inversely related to the strength of regularization 
a regression
in order to pick the optimal value for c the model was independently trained over a range of c values 
the c value that produced the highest accuracy on the validation set was selected and tested on the test set for each training phase 5 fold cross validation was incorporated in order to reduce the variance in a trained model 
stochastic average gradient sag descent was selected as the solver to use for training as it generally provides fast convergence for large feature sets such as ours
we wanted to also create a non linear classifier in the hopes that it can outperform the linear logistic regression model so a svm was a natural choice as it is fairly easy to implement with few parameters to tune 
we created our svm s by solving the following primal problem and the decision function is defined as through training an svm model can create multiple hyperplanes to split the training set into its labeled categories 
b support vector machine
this is done through the use of kernels that transform the input data into a higher dimension so that the data can then be linearly separated 
part of the advantage for svm s is that only a fraction represented by n in eqn 3 of the original training set has to be retrained for creating the hyperplane during predictions 
another advantage is that various kernel functions can easily be tested and selected for the one that best fits a particular application 
thus to select the optimal kernel function along with the c parameter specifying the softmargin size a grid search was performed over three standard kernel functions polynomial rbf and linear with a range of c values for each 
based on our literature review we believed that deep learning techniques should work well for this classification problem 
we therefore implemented a multilayer perceptron architecture for multi class classification 
c deep learning
a multilayer perceptron architecture is a fully connected feedforward neural network with one input layer one or more hidden layers and one output layer 
formally the mlp can be considered a function f r n r k where n is the number of input features and k is the number of classes 
each hidden layer can be formalized as f r a r b where a is the input size and b is the output size 
in matrix notation it would be where x is the input vector w c is the weight matrix associated with layer c and b c is the bias vector associated with layer c and a c is the activation function associated with layer c we use softmax as the final activation so each prediction is size kx1 where k is the number of classes 
the classifier predicts a score for each class instead of simply producing a single class label 
this can give a sense of how close we are to the correct label 
we converted the true labels to use one hot encoding that is we took an mx1 array and made it mxk where m is the number of datapoints 
we initially found that making the model deeper produced worse results but increasing the number of neurons per layer improved the accuracy 
our final architecture is layer1 relu layer2 relu layer3 softmax 
the layers have weight sizes of layer1 n 512 layer2 512 k 
in order to generalize better we apply dropout for each hidden layer this helps combat overfitting by suppressing each node with 50 probability 
we tried different gradient descent rules with the best result coming from the sgd optimizer 
to evaluate loss we use categorical cross entropy which is as follows we use this in conjunction with softmax activation for the final layer which gives us a probability or confidence for each prediction 
softmax output for the i th element is as follows this is advantageous because we have multiple classes instead of simple binary classification 
we want the loss to give a sense of the degree of error for each category instead of a simple yes or no 
for example assume we have 3 classes and the first class is the correct label for this time step 
consider two example outputs 0 98 0 01 0 01 and 0 51 0 48 0 01 
the first output is clearly superior to the second because it has a higher confidence for the correct class however both will predict the first class 
if we use something like simple error rate for our loss we are not able to capture the confidence of our predictions 
using softmax activation with cross entropy loss helps us capture this difference 
our final architecture is shown in
decision trees are a useful method for multi class classification for nonlinear feature sets 
decision trees perform greedy splits on the each feature of the data at a specific threshold 
d trees
in order to choose a split a decision tree seeks to maximize the difference between the loss of the parent node and the sum of the losses of the child nodes 
the specific loss function we used was the gini loss shown below where p mk is the proportion of examples in class k present in region r m and q m is the proportion of examples in r m from tree t with t different r m regions there are multiple methods of regularizing or preventing overfitting for decision trees including setting a minimum size of leaf terminal nodes and setting a maximum tree depth setting a maximum number of nodes 3 random forest another ensemble method for decision trees for the purpose of improving prediction accuracy is random forest 
random forest is a form of bagging bootstrap aggregation which involves sampling with replacement from the original population for the purpose of reducing variance at the expense of an increase in bias increased computational cost and decreased interpretability of the trees 
for a random forest a large number of decision trees are generated and the bias is further reduced by decorrelating the trees by only considering a subset of the total number of features at each split in the decision tree
as discussed previously we were interested in classification performance on both the full set of features and performance when using a reduced set of features 
we tried several feature combinations and discovered that we could get decent performance when using just the hand imu plus heart rate sensor 
v results and discussion
the hand imu performed better than any individual imu 
this is a positive result as we are particularly interested in applications where a user is holding a phone or wearing a smart watch 
we will refer to the hand imu plus heart rate data as the reduced or limited feature set 
our primary evaluation metric for all models was classification accuracy 
this is simply the count of correctly classified data points divided by the count of classifications attempted 
it is as follows where y is our set of predicted labels and y is the set of true labels 
we used various loss functions as described in the subsection for each technique 
the full results are shown below in
through 5 fold cross validation in
in order to tune the maximum depth hyperparameter of the decision tree we used scikit learn s validation curve function to perform 5 fold cross validation 
the training and crossvalidation curves for the limited feature set is shown in the left side of 4 below as a function of maximum tree depth and similar curves were produced for the full feature set it is typical in practice to use the one standard error rule when using cross validation to choose the simplest model whose accuracy or error is within one standard deviation of the best performing model 2 boosted decision trees for boosted decision trees we used the default learning rate and manually tuned the maximum depth of the base decision tree estimators and the number of trees 
c decision trees 1 ordinary decision trees
since the base estimator of boosting should be a weak learner and since the strong learner from the results from ordinary decision trees had a maximum depth of 15 we decided to limit our weak learners to having a maximum depth of less than or equal to 10 
for both the limited and full feature sets we created tables that presented the training and validation accuracy values for different combinations of maximum depth of the weak learners from 1 10 and number of trees 50 100 250 or 500 
using the one standard error rule from the optimal validation performance we chose 500 trees of maxdepth 10 for the limited feature set and 250 trees of maxdepth 9 
using these models the limited feature set scored an accuracy of 0 940 on the test set and the full feature set scored an accuracy of 0 985 on the test set 
a confusion matrix for the limited feature set is shown below in 5
we decided to include 100 trees in our random forest model because 100 was a well performing trade off of time and accuracy for random forests increasing the number of trees will only serve to decrease the variance and will not increase the likelihood of overfitting 
we used the default option of only considering a random subset of the square root of the total number of features for each split 
3 random forest
in order to tune the maximum depth for each decision tree in the random forest we again used scikit learn s validation curve function to perform 5 fold cross validation 
the training and cross validation curves for the limited feature set and the full feature set as a function of maximum tree depth in the random forest were created using a similar approach to the cross validation for ordinary decision trees 
using the one standard error rule for the validation accuracy the results suggest that for both the limited and full feature sets a maximum depth of 20 should be used 
using a maximum depth of 20 the limited featureset achieved a test accuracy of 0 937 and the full featureset achieved a test accuracy of 0 980 
from the confusion matrix for this model the most frequent misclassifications were between vacuum cleaning and ironing ascending and descending stairs vacuum cleaning and ascending descending stairs 
these are exactly the common misclassifications found in boosting and are expected because of the relative similarity in hand motions and heart rate between these activities 
the multilayer perceptron neural network was able to achieve high classification accuracy 
when trained on the entire dataset it consistently achieved training accuracies greater than 98 and test accuracies greater than 95 
d deep learning
the best model we trained produced a test accuracy of 98 1 
when trained on the reduced feature set consisting of only the hand imu and the heart rate sensor it achieved 81 4 test accuracy 
we noted a trend in accuracy vs hidden layer size 
increasing the size of each layer number of neurons improved performance while increasing the depth number of hidden layers degraded performance 
we did not notice a significant difference in performance when using relu vs other activation functions however we did find that our model converged faster when using softmax for the final activation function in conjunction with categorical cross entropy loss 
as expected reducing the dropout rate tended to improve training accuracy but reducing it too much caused a degradation in test accuracy 
in this project we were limited in both time and compute and we believe we can improve accuracy given more of both 
we can improve performance by training for more epochs 
loss continued to decrease at the end of our training indicating performance was still improving when training finished 
we could also further increase the number of neurons per hidden layer at the cost of a larger model with slower training time 
unsurprisingly logistic regression performed the worst 
having the advantage of extremely low memory usage and speed for predictions it can still be a viable method in lowcompute devices like microcontrollers 
svm s on the other hand provide great results in accuracy but may not be viable solutions for microcontrollers because of the large number of support vectors required to store and do operations on for predicting 
for decision tree methods as expected ensembling methods improved test performance over ordinary decision trees for both the full and limited feature sets 
boosted decision trees performed slightly better than random forest on both feature sets which is promising because it is generally less computationally intensive and thus is a good candidate for a model to actually deploy in a smart device 
we would also be interested in exploring different types of deep learning architectures 
we considered using rnn s recurrent neural networks but our feature set had a relatively large number of features per time step and the activities did not involve more than a few actions so it was not necessary to take history into account when classifying a single time step 
as such simple feed forward neural nets were sufficient for this problem 
however we would like to explore cnn s convolutional neural networks which could potentially give similar or improved performance while using substantially less memory 
in general the limited feature set performed only slightly worse than the full feature set on all of the methods which is a promising result for actual deployment in smart devices 
in the future we would like to test these models using real imu s 
in particular we would want to see if a low compute embedded device could perform classifications with neural nets or svm s in real time in addition to computationally cheaper methods such as decision trees 
all code used in this project can be found at https github com aristosathens human a ctivity c lassif ier contributions aristos zach and navjot all contributed equally to this project 
aristos focused on deep learning navjot focused on logistic regression and svm and zach focused on trees ordinary decision trees boosting and random forests 
appendices
all three members worked on data preprocessing analysis and writing this report 
in this paper i describe a real time image processing pipeline for fruit fly videos that can detect the position oriention sex and for male flies wing angles 
the machine learning algorithms used include a decision tree linear and logistic regressions and principal component analysis 
the histogram of oriented gradients 2 descriptor is used as well to generate features 
ultimately i achieved a processing throughput of 84 frames per second on 1530x1530 grayscale frames without gpu acceleration and demonstrated high accuracy across several metrics 
fruit flies are often used in neurobiology experiments because they can be genetically modified using well established tools and because it is feasible to do in vivo whole brain imaging of them in addition to other reasons 
in these experiments it is often desirable to measure the behavior of each fly by recording its position and orientation over time along with its leg and wing positions automated tools for doing this kind of video analysis exist but are usually too slow to run in real time 
this is problematic for two reasons first real time operation is a prerequisite for running closed loop behavior experiments 
second in an experiment where flies are recorded continuously over a long period e g a circadian rhythm study video processing will become the bottleneck for experimental throughput unless it runs in real time or faster to address these issues i sought to develop a tool for the realtime video analysis of fruit flies 
in this project i chose to focus on a specific experiment being developed in prof tom clandinin s lab at stanford to study the courtship interaction between one male and one female fly 
as a result the input to my algorithm is a grayscale video of the two flies in this paper i ll start off by describing some existing tools for fly video analysis section 2 and will then move on to describe the dataset i worked with section 3 
next i ll describe the algorithm i developed which consists of four distinct processing steps using machine learning section 4 
finally i ll wrap up the experimental results section 6 and conclusion section 7 
the source code and dataset for this project are available on github at https github 
com sgherbst cs229 project git 
the tool considered a gold standard of sorts for automated fruit fly video analysis is called flytracker in another project finally in the past year two different approaches to fly video analysis using deep neural networks were published deeplabcut in this project i sought to combine various aspects of these previous studies 
on one hand i wanted to develop an algorithm that could run in real time on large frames 1530x1530 without gpu acceleration and i wanted training to be fast to allow for more experimentation 
hence i needed to further simplify the machine learning models as compared to deeplabcut and leap 
but i still wanted to apply supervised learning to take advantage of labeled data departing from the hand crafted image processing rules of flytracker 
the source data for this project was a 15 minute grayscale video of the interaction between one male fly and one female fly 
the video was furnished by dr ryan york of prof clandinin s lab as an example of the kind of footage that will be produced by the experimental rig they are developing 
i did a bit of initial preprocessing to crop the video to a 1530x1530 frame that exactly contained the circular well in which the flies were placed working from the cropped video i then hand annotated 326 frames using labelme additional preprocessing and data augmentation was used throughout the image processing pipeline and these steps will be covered in the next section 
as shown in the pipeline described below was implemented in python using the packages scikit learn
if there are two contours with one fly each the next stage of the image processing pipeline determines which is the male fly and which is the female fly 
this classification is done jointly that is the classifier is a given a list of the two contours and asked whether that list is ordered male female or female male 
pipeline stage vs 
in general this is fairly straightforward since female fruit flies are larger than male fruit flies 
but in some cases such as when the flies are climbing along the walls making the distinction can be a bit trickier there are four input features for this pipeline stage namely the area and aspect ratio of both contours the latter determined via image moment analysis in this case a rescaling step is needed before training the logistic regression because contour areas and aspects ratios are of vastly different scales so the above update rule would otherwise perform quite poorly 
to further improve the quality of training data augmentation was applied by swapping the order of the two contours and their labels for each example 
in the third pipeline stage the 0 360 orientation of both flies is determined and this is done in a way that reduces the machine learning task to a binary classification 
as pre processing the image is first masked to everything expect the body of one fly after which point the orientation of the fly is determined using image
pipeline stage orientation
read frame from video 
threshold image and extract contours 
vs orientation wingangle
classify each contour as containing 0 1 or 2 flies if there are two one fly contours identify which is the male fly and which is the female fly for both flies determine the 0 360 orientation of the fly body for the male fly determine the 0 90 angle of each wing with respect to the major axis of its body 
moments where pq is central image moment of the masked image defined by the summation where x is the center of mass of the image and f x y is the intensity at pixel x y 
unfortunately the orientation angle produced using this approach has a sign ambiguity it cannot discern whether an object is facing forwards or backwards 
as a result i needed to develop a machine learning algorithm to decide if the orientation computed via image moments should be corrected by adding 180 
as shown in briefly the hog descriptor after computing the hog descriptor for a fly image the descriptor is projected onto a basis of 15 principal components or in other words the eigenvectors corresponding to the 15 largest eigenvectors of i x i x i t where x i is the hog descriptor of the ith training example as the final step of this pipeline stage the dimensionally reduced hog descriptor is fed into a logistic regression 2 
somewhat surprisingly as shown in
in the final pipeline stage the angles of the right and left wings of the male fly are determined 
similarly to the previous stage preprocessing is used to construct a region of interest roi containing just the male fly and its wings 
pipeline stage wing angle
that roi can then be orientated in an upright direction using the results of the preceding pipeline stage 
the preprocessing uses median blurring adaptive thresholding and erosion to preserve fly wings while removing fly legs in my approach the wing angles are determined separately by dividing the image of the male fly into two halves one for each wing 
as shown in the final step in this pipeline stage is a linear regression on the reduced dimensionality hog descriptors 
briefly linear regression seeks to minimize the mean squared error between predicted and given labels 
the optimal parameters can be computed directly by the equation x t x 1 x t y where x contains the features of all training examples and y is a vector of their labels 
after computing the predicted label given features x is simply t x 
for all four pipeline stages error was evaluated by retaining one third of the dataset for testing this test set was not used at any point during training 
the first three stages of the pipeline were classifiers so their test error is reported simply as misclassification error 
as seen in 2sgmxtq 
the wing angles over time are also plotted in
in this report i described a real time image processing pipeline intended for neurobiology experiments that takes as input a grayscale video of a pair of fruit flies and produces as output an estimate of the position orientation and sex of each fly in addition to the wing angles for the male fly 
the pipeline had four distinct stages employing machine learning techniques including a decision tree logistic regresion linear regression and pca 
for both classification and regression tasks i demonstrated good accuracy on 1530x1530 frames at a throughput of 84 fps without gpu acceleration in the orientation and wingangle stages hog was used in conjunction with pca to produce input features for a trained model 
in both cases i was surprised how well this worked even when just one or two principal components were used 
i think one reason this approach was successful was that i designed the preprocessing stages in a way that increased hog variance due to the variable of interest orientation or wing angle while decreasing hog variance due to other variables fly legs the other wing the other fly background roughness etc 
this in turn was a useful lesson about the role of preprocessing and feature selection in machine learning in the future there are a number of possible directions to explore 
first i could try measuring leg positions from the video preliminary experiments suggest that legs tips are selected fairly reliably with a keypoint detector such as sift
amazon fulfillment centers are bustling hubs of innovation that allow amazon to deliver millions of products to over 100 countries worldwide 
these products are randomly placed in bins which are carried by robots occasionally items are misplaced while being handled resulting in a mismatch the recorded bin inventory versus its actual content 
the paper describes methods to predict the number of items in a bin thus detecting any inventory variance 
by correcting variance upon detection amazon will better serve its customers specifically the input to our model is a raw color photo of the products in a bin 
to find the best solution to the inventory mismatch problem amazon published the bin image dataset which is detailed in section ii the output of a model is the bin s predicted quantity the number of products in the image while we started with linear methods the quest for model performance lead us to non linear algorithms and ultimately to convolutional deep learning 
section iii summarizes each algorithms applied 
they include logistic regression and classification trees summarized in section iii a and section iii b support vector machines linear kernel polynomial kernel radial kernel 
the algorithms are summarized in section iii c convolutional neural network resnet cross entropy loss function with learning rate optimizations 
the algorithm summary in section iii d section iv summarizes performance resuts 
with convolutional neural networks we were able to achieve an overall accuracy exceeding 56 
this is over 60 better than with support vector machines 
for the latter we attained accuracy of over 30 
we operated on a reduced dataset for bins that contained up to 5 products for a random baseline probability of 16 6 we are among the first to publish results on amazon s bin image dataset 
prior art by eunbyung park of the university of north carolina at chapel hill
the data set contains 535 234 images which contain 459 476 different product skews of different shapes and sizes 
each image metadata tuple corresponds to a bin with products 
a input data set
the metadata includes the actual count of objects in the bin which is used as the label to train our model we worked with the subset of 150k images each containing with up to five products 
we split this as follows 70 training 20 validation and 10 test 
before model training images were normalized 1 re sized to 224x224 pixels 2 tansformed for zero mean and unit variance 3 for convolutional models the dataset was augmented with horizontal flips of every image
b data engineering
we explored the blob features extraction 
blobs are bright on dark or dark on bright regions in an image 
c feature engineering blobs
all the bins in which items are placed are similar and if items are present in the bin the idea is to make an attempt to create features assuming items in the bin are relatively bright on the backgrounds 
we used laplacian of gaussian approach it computes the laplacian of images with successively increasing standard deviation and stacks them up in a cube 
blobs are local maximas in this cube 
note the yellow circles in
the probability that each observation is classified to each class is defined as a logistic function as followthe observation is assigned to the class in which it has highest probability 
in a classification tree each observation belongs to the most commonly occurring class of training observations in the region to which it belongs 
we use the gini index of which measures total variance across the k classes as the loss function 
b classification tree
gini index is defined by
the support vector machine svm is an extension of the support vector classifier that results from enlarging the feature space in a specific way using kernels 
feature space is enlarged in order to accommodate a non linear boundary between the classes 
the kernel approach enable an efficient computational approach for svm 
we have attempted several kernel types as follow polynomial kernel
resnet 1 classifier and loss function softmax layer and cross entropy loss cel function were used since we are solving multi class classification problem2 learning rate finder learning rate determines the step size of the update and is one of the key hyper parameters to training a network 
for some of our experiments we set the learning rate based on an approach introduced in the paper cyclical learning rates for training neural networks smith and leslie n
d convolutional neural networks
the performance of the best methods as well as the rationale leading to identifying them is outlined below given the nature of the data set we expect the decision boundary to be highly non linear 
iv experiments
several multi class classifiers were explored with raw pixel data 
a multi class classification
next with the intention of arriving at a more useful description of an image than raw pixel data a number of feature extraction algorithms were attempted 
with respect to histogram of oriented gradients hog evaluation suggests that the images in the data set do not have enough dominant gradients 
thus identifying products in a bin is difficult 
in part this may be due to amazon s usage of tape to cover products in a bin 
for many images the tape occludes the products causing a significant information loss 
first manually cleaning the data to remove such images would enhance learning 
second with the adam optimizer we have seen that performance increases with a larger dataset 
vi future work
we are intrigued by the possibility that photos be taken by from different angles 
and that metadata connect the content of a bin over time so belief state may be tracked 
third with over 450k product skews images are bound to violate our assumption that they re drawn from a single distribution 
thus forming ensemble models with different architectures and learning approaches may achieve higher accuracy 
the project s repository is https github com onenow aiinventory reconciliation vii 
contributions pablo s primary contribution was on support vector machines sravan s on convolutional neural networks and nutchapols across the board 
the authors are grateful to professor andrew ng for his masterly transmission of machine learning to us 
pablo rodriguez bertorello leads next generation data engineering at cadreon a maketing technology platform company 
previously he was cto of airfox which completed a successful initial coin offering 
authors
he is the co inventor of cloud platform company acquired by oracle 
and the original designer of the data bus for intel s itanium processor 
pablo has been issued over a dozen patents sravan sripada works at amazon 
he is interested in applying artificial intelligence techniques to solve problems in retail cloud computing and voice controlled devices nutchapol dendumrongsup is a master s student at the institute for computational and mathematical engineering and deapartment of energy resources engineering at stanford 
he is interested in the application of machine learning in the energy industry and the traditional reservoir simulation in oil and gas industry 
office hours at stanford are typically subject to significant variance in student demand 
to tackle this problem we predict student demand at any office hours on an hourly basis using data scraped from queuestatus carta and course syllabi 
we conducted experiments using regression on fully connected nns univariate and multivariate lstms and compared with an ensemble of multimodal classification models such as random forests and svms 
we compared different losses such as mse mae huber and our own sqhuber against normalized inputs and evaluate on student demand with and without smoothing 
results show that our models predict demand well on held out test quarters both in seen and unseen courses 
our model could thus be a useful reference for both new and existing courses 
among cs students at stanford the experience of queueing at office hours ohs is practically universal 
office hours is an important part of any class allowing students to get valuable one on one help 
unfortunately student demand is prone to high variance resulting in sometimes students waiting hours before receiving help or conversely teaching assistants tas waiting hours before any students arrive 
in particular periods of overcrowding are a source of stress for both students and tas and are among the most commonly cited sources of negative experience on carta 
thus improvements in oh scheduling could significantly improve overall course experience for all parties however as with all logistical decision making at universities there are significant complexities in the process 
our project addresses the arguably most variable component of the input predicting peaks of student demand 
using hourly oh data scraped from queuestatus course information from carta and major dates from class syllabi we trained a fully connected neural network model that predicts the hourly load influx for any given course and quarter 
we define the load influx as the average serve time for the day times the number of student signups 
conceptually this is the aggregate ta time needed to satisfy all student demand over some period 
note in terms of dataset and big picture goals this is a shared project between cs229 and cs221 
for cs229 we focused on a more theoretical approach in predicting load influx by designing and evaluating new loss functions catered towards data with high variance and fluctuations 
we also combine an ensemble of approaches to fine tune our prediction by using signal processing practices as well as experiment with multimodal classification using svms and random forest models 
for cs221 we focus on assigning tas to the surge timings using modified gibbs sampling and em algorithms as well as lstm prediction models approaches of a cs229 project that had a similar goal 
troccoli et 
al used custom feature extractors to predict wait times at the lair cs106 office hours 
to obtain data we set up a pipeline that scrapes hourly office hours from queuestatus 
through customized json parsing we were able to obtain a combined 17 quarters of data across 7 prominent cs classes 
after preprocessing to remove all entries with zero serves and signups we ended with 4672 hours or just under 200 straight days worth of oh data 
a summary is shown below 
we experimented with a plethora of features to augment our dataset with and decided on the following predictors based on a combination of logic and significant correlation with load influx 
on a per class basis we used number of enrolled students instructor rating and proportion of freshman graduate phd students enrolled 
on a per hour day basis we used days until next assignment due days after previous assignment due days until an exam hour of day weekdays 
for the hourly daily features validation testing found that one hot bucket encodings were more effective for predictions 
day differences were bucketed in ranges of 10 to 5 4 to 3 2 to 1 and 0 
hour of day was evenly bucketed into morning noon afternoon and evening 
each entry corresponds to one hour of oh and every entry in the same course quarter shares the same course quarter features 
as discussed later we also experimented with log transformations as our ultimate goal is to predict entire unseen quarters we separated our training validation test sets by entire quarters 
due to our limited sample size we use k fold cross validation to tune hyperparameters where k is our number of quarters 
our test set consisted of 4 total classes cs110 spring 2018 and cs107 spring 2017 as unseen quarters of classes we trained on and cs224n winter 2018 and cs231n spring 2018 as entirely unseen courses 
our training set thus consisted of the remaining classes totaling 13 quarters of data between 5 unique classes we note that after training models to predict load influx on these datasets we do not predict hourly student demand for tas as is ideal 
rather we predict hourly student demand for tas given that office hours is held 
we determined that current ta assignments are uncorrelated with time of day p 0 63 cor test in r and typically scheduled throughout active hours 
therefore we assume that the status quo scheduling of office hours is frequent and unbiased enough such that real student demand is proportional to the student demand given office hours is held 
we first implemented multimodal classification models as baselines where instead of using equidepth buckets we divided the minimum and maximum load influx into 7 logarithmic time buckets 
using svms with radial kernel and random forests with 1000 estimators we obtained an initial baseline with accuracy 0 422 and 0 359 respectively with the confusion matrix as shown below 
multimodal classification
we see that even with fine tuning of hyperparameters the classification models have decent performance but with large skew and variance in predicting high load influxes which could be possibly due to class imbalance in different buckets when on a log scale 
we thus choose to focus on regression next to predict the spikes of load influx in different hours 
we also set up baselines by training fully connected networks and lstms for regression tasks 
the fcn which approximates functions with non linearities and multiple layers that activate depending on weights mapped to higher dimension across stacked layers has input size 30 with our 30 features with 2 hidden layers of size 15 and 4 respectively followed by a single output final layer 
regression fcn huber and sqhuber loss
each hidden layer uses a relu activation function with a linear activation for the output layer 
we also experimented with 3 4 hidden layers which led to overfitting even with normalization techniques that performed worse on the validation set lstms long short term memory is a form of recurrent neural network focused in 221 report 
it addresses vanishing gradients while factoring in previous states with a recurrence formula at each time step which makes it suitable for temporal data 
we used two lstm cells in autoregressive lstm with window size of 16 and each output was fed back as part of the next window 
all input features were normalized in a range 0 1 for every experiment and all baseline models were compiled with adam optimizer with early stopping to prevent overfitting 
due to insufficient data we face high variance in training lstms with the initial baselines reported below therefore we choose to continue work on the fully connected network fcn 
however in our fcn we notice our predictions for load influx throughout the quarter suffer from a consistent offset from the mean of the distribution 
upon inspection we suspect that the large amount of outliers may have caused the bias due to their huge penalties while minimizing the l2 norm loss function 
thus we seek a new loss function that doesn t penalize outliers as heavily 
the huber loss is particularly useful for this since it scales linearly outside a specified domain we compare this traditional loss function with a novel loss function we designed for the purposes of experimentation the sqhuber loss 
the sqhuber loss is defined as the sqhuber loss is piece wise continuous and scales proportional to the square root of the residual for values above a specified domain 
thus it is even more robust to significant amounts of outliers 
the load influx is an erratic function 
large fluctuations or spikes in the load are difficult to predict without overfitting the model thus transforming the training labels actual load influx before training may be fruitful 
transforming the load influx data
we attempted two methods to transform our data for better predictions 1 
hanning window a 1 d convolution with a hanning window 
this reduces spikes and thus potential to overfit 
during validation tests we find the root mean squared error rmse to be heavily dependent on the course and quarter used 
thus to reduce variance in rmse obtained from validation tests we used leave one out cross validation for our validation studies where we stochastically choose a course quarter pair as the validation set and use the remainder of the joint training validation sets for training 
k fold cross validation
this process is repeated for k 8 iterations with the validation rmse the mean of the results 
note that for each of the k iterations the validation set was stochastically chosen and isolated from the training data with the parameters of the model reset between iterations 
the results for cross validation between models are tabulated in final evaluation on test set 
we obtained an avg 
rmse of 124 466 for our set of seen courses in an unseen quarter and 106 478 for our set of unseen courses in unseen quarters 
furthermore similar to
overall our project provides the first general use model for predicting student demand at stanford cs office hours 
using hourly queuestatus data and course information we were able to generate realistic predictions for office hours load in a wide range of cs classes 
conclusion and future works
ultimately out of several tried our best model was our fully connected neural network using mean absolute error and trained on log transformed load influx 
although a slightly different model using our custom sqhuber loss gave marginally lower rmse it failed to retain spike information due to perhaps too much outlier penalty 
our rmse indicates that the model is off by an average of 2 hours students in testing 
empirically we see this is a mostly a result of slightly misplaced and or incorrectly heighted spikes since our final log model makes predictions that are then exponentiated it often predicts the locations of spikes correctly but fails to capture exact magnitude 
thus although our system may not be able to predict exact student demand it can still serve as a valuable guideline regarding when to expect relative peaks 
furthermore we constructed a basic gui in r that given basic course information generates oh hourly load influx for the whole quarter within a minute demoed during poster session 
so far chris piech has expressed interest in using our model next spring 
given more time we would like to extend our predictions to more classes and perhaps even other universities using queuestatus 
with the rising popularity of peer to peer lending platforms in recent years investors now have easy access to this alternative investment asset class by lending money to individual borrowers through platforms such as lendingclub prosper marketplace and upstart or to small businesses through funding circle the process starts with borrowers submitting loan applications to the platform which performs credit reviews and either approves or denies each application 
the platform also uses a proprietary model to determine the interest rate of approved loans based on the credit worthiness of borrowers 
i motivation
approved loans are then listed on the platform for investor funding 
investors usually want to diversify their portfolio by only investing a small amount e g 
25 in each loan 
hence it is desirable for investors to be able to independently evaluate the credit risk of a large number of listed loans quickly and invest in those with lower perceived risks this motivates us to build machine learned classification and regression models that can quantify the credit risk with a lendingclub historical loan dataset 
specifically we build and evaluate classifiers that predict whether a given loan will be fully paid by the borrower as well as regressors that predict the annualized net return from investment in a given loan 
finally we simulate and evaluate a simple loan selection strategy by investing in loans that pass a certain regressor prediction threshold 
there have been many studies on classification models predicting lendingclub loan default 
chang et al 
tsai et al 
in addition to classification models that predict loan default gutierrez and mathieson pujun et al 
we worked with public dataset published by lending club
a dataset overview
columns with empty values for most of the rows as well as columns with the same values across all rows are dropped in order to have a cleaner dataset 
free form text columns are also dropped because we posited that these fields would have more noise and are better tackled at a later stage when we have better understanding of the problem for features with missing values they are categorized into three cases and treated differently mean set zero set and max set 
b feature preprocessing
for mean set fields we took the average of the non empty values 
one such example is debt to income ratio dti borrowers with lower dti likely have lower risks compared to those with higher dtis 
for loan applicants missing dti information it is unreasonable to reward them by assigning zero dti hence taking average is a good starting point 
in the case of max set missing values are replaced with a constant factor multiplied with the maximum value in that column 
for instance if the data for the number of months since last delinquency is missing it would be unfair to punish the applicants by assigning zero for missing data 
finally zeros are given for zero set which we believe would be a neutral replacement for the missing data categorical features such as obfuscated zipcode e g 
940xx are replaced with their one hot representations 
features with date values are converted into the number of days since epoch 
normalization is then performed at the end on all features so they have zero mean and one standard deviation after the above preprocessing we ended up with 1 097 features 
we then ran pca on the dataset with the hope to further reduce feature size 
unfortunately the 95 variance threshold corresponds to around 900 features which is close to 95 of the total number of features and therefore means that we cannot significantly reduce the feature size without sacrificing variances see
for classification model both default and charged off are assigned label 0 and fully paid is assigned label 1 
for regression model we use annualized return rate calculated from loan amount total payment made by the borrower and the time interval between loan initiation and the date of last payment 
c label definition
our classification goal is to predict which class the loan belongs to either default or fully paid 
in the following sections we will share and discuss our experiments using logistic regression neutral networks and random forest for classification problem 
iv classification problem overview
for metrics to evaluate classification performance we use confusion matrix whose columns represent predicted values and rows represent true values 
we also measure precision recall f1 score the harmonic mean of precision and recall and weighted average as defined to derive optimal parameters the model iteratively updates weights by minimizing the negative log likelihood with l2 regularizationto tackle the class imbalance problem only 19 of our dataset are negative examples we used balanced weight for class labels which is inversely proportional to class frequencies in the input data n samples total n classes label count after running logistic regression with the above setting for a maximum of 1000 iterations we arrived at the following results as we can see logistic regression is doing fairly well compared to naive models that blindly predict positive for all examples or randomly guess positive and negative with 50 chance 
thanks to l2 regularization we did not observe overfitting issues 
one thing that we noticed and would like to improve upon is the precision and recall for negative class 
although we used balanced class weights to offset data imbalance the prediction precision is only slightly better than randomly guessing 
therefore we suspect there may be non linear relationships in the dataset that is not learned by logistic regression which leads to our exploration with neural network next 
we constructed a fully connected neural network with 4 hidden layers of shape j 
the final output of the network uses cross entropy log loss as loss function to arrive at optimal parameters the model iteratively updates weights within each layer using gradient descentbased solver with a mini batch size of 200 learning rate of 0 001 and l2 regularization penalty of 0 0001 we obtained the following results training set result the model has high variance and is suffering from overfitting 
b neural network
compared with the logistic regression model this neural network model achieves a better weighted precision at the expense of weighted recall and the difference between precision and recall is less polarized compared to that of the logistic regression 
random forest classifier is one of the tree ensemble methods that make decision splits using a random subset of features and combine the output of multiple weak classifiers to derive a strong classifier of lower variance at the cost of higher bias we started off our venture into random forest with 200 trees using gini loss 1 1 j 0 p 2 j 
decision splits are based on at most 50 features to reduce variance 
c random forest
after training we reached the following result although the performance is on par with neural network and logistic regression random forest s overfitting problem is much more prominent than any other models even after restricting the maximum number of features considered for decision splits to 50 
based on our explorations with logistic regression neural network and random forest we are able to achieve weighted average of 0 89 for both precision and recall 
more specifically our classification results appear to be better than the works done by the previous project
d classification model conclusion
we strive to predict the investment return if we were to invest in a given loan 
our goal is to build regression models that predict the net annualized return nar of a given loan in a way similar to how lendingclub calculates nar for investors where x la is the loan amount x t p is total payment made by the borrower and d is the number of days between loan funding and date of last payment we evaluate regression models in terms of mean square error mse and coefficient of determination r 2 is the mean of the true labels 
vi regression problem overview
the coefficient of determination tells us how much variability of the true nars can be explained by the model 
the goal of linear regression is to find a linear hyperplane that minimizes the ordinary least squares 
specifically it finds parameters that minimizes
split mse r 2 train 0 040 0 243 test 5 014 9 494 10 22the extremely skewed mse and r 2 values on the test set clearly indicate a high variance problem of the model which overfits the training examples 
to rectify this we employ l2 regularization in our next model 
performance of linear regression 
ridge regression adds an l2 regularization term to the cost function of linear regressionbut otherwise works the same way as linear regression performance of ridge regression with 1 split mse r 2 train 0 040 0 243 test 0 040 0 238as expected l2 regularization mitigated the problem of overfitting giving similar metrics for both train and test sets 
r 2 0 24 means that 24 of the nar s variability can be explained by the ridge regression model 
b ridge regression
we next try nonlinear models to further decrease mse and increase r 2 
the fully connected neural network regression model is very similar to the classifier described earlier in section v b 
the only difference is that all neurons use the relu activation function f x max 0 x and the neural network tries to minimize the squared loss on the training set we used the adam stochastic gradient based optimizer we see that the neural network regressor performs much better than ridge regression thanks to its ability to model non linear relationships 
c neural network
a decision tree regression model infers decision rules from example features by finding a feature split for each non leaf node that maximizes the variance reduction as measured by mse 
the mean of leaf node example labels is the output of the decision tree regressor decision trees tend to overfit especially when the tree is deep and leaf nodes comprise too few examples 
d random forest
limiting the maximum depth or the minimum leaf node examples not only reduces overfitting but also speeds up training significantly as random forest model builds numerous decision trees before taking the average of their predictions specifically random forest regressor repeatedly builds decision trees on a bootstrap sample drawn from the training set and considers a random subset of features as candidates when finding an optimal split 
from these results we see that as we allow the decision trees to grow deeper bias increases while variance decreases 
the performance of random forest regressor beats both ridge regression and neural network likely due to the fact that decision trees are able to capture very nuanced and nonlinear relationships 
our best random forest regressor achieves a root mse of 0 036 0 19 on the test set which implies that the predicted nar is estimated to differ from the true nar by 0 19 
while this may appear very large at first glance the model can actually be very useful in formulating a loan selection strategy 
viii loan selection strategy
loan defaults usually happen soon after loan funding and the chance of default decreases as more payment is made 
as a result most true nars of defaulted loans are well below 0 5 so the model can still very accurately tell us that investing in loans like these likely result in losses in light of this we experimented with the strategy of investing in loans with model nar predictions higher than a reasonable threshold m 0 
intuitively the threshold m can serve as a parameter investors can tune according to their investment account size the bigger m is the more stringent the loan selection is so less amount of money can be invested but hopefully the annualized return will be higher due to investing in loans more selectively in order to determine a reasonable range of values for m we rank the training set examples by model predictions from high to low 
for a specific threshold m 0 132 on both training and test set the strategy yields an annualized return of 15 with 1 7 loans picked and invested 
comparing our models with those from related work ours have better precision recall and are more practical in terms of enabling implementable investment strategies 
in the case of classification models random forest achieved 0 89 weighted average precision and recall 
ix conclusion
but it is also important to note that the random forest and neural network models do have higher variance than desired and have space for improvement 
for the regression counterpart random forest is able to attain 0 315 coefficient of determination and to deliver predictions that lead to a profitable and actionable loan selection strategy in the sense that the return rate is higher than s p 500 s 10 annualized return for the past 90 years
we obtained a regression prediction threshold based on the training set and simulated the strategy on the test set 
both sets comprise loans initiated within the same periods we worked with a 70 training and 30 test split for simplicity in this project 
x future work
the absence of a development set didn t afford us much opportunity to tune the hyperparameters of our models such as the number of decision trees to use in random forest models and the number of hidden layers and neurons of each layer in neural network models 
having a small development set would enable us to tune some hyper parameters quickly to help improve model performance metrics there are definitely factors that contribute to default not captured by features in our dataset 
we can add external features such as macroeconomic metrics that have been historically correlated to bond default rate 
for categorical features like employment title we can join them with signals such as average income by industry similar to what chang et al 
we can also make better use of existing features in the lendingclub dataset 
one example is loan description which the borrower enters at the time of loan application 
instead of dropping such freeform features we can try applying some statistical natural language processing techniques such as tf idf as chang et al 
finally we notice that lendingclub also publishes declined loan datasets
the two of us paired up on all components of this project including dataset cleaning feature engineering model formulation evaluation and the write up of this report and the poster codebase https goo gl sxf1rm
xi contributions
mild traumatic brain injury mtbi more commonly known as concussion has become a serious health concern with recent increase in media coverage on the long term health issues of professional athletes and military personnel 
acute symptoms include dizziness confusion and personality changes which can remain for days or even years after injury according to the cdc contact sports such as football are one of the leading causes of mtbi 
in these sports mtbi is diagnosed by a sideline clinician through subjective evaluation of symptoms and neurological testing 
because of the large variance of symptoms within different individuals and the pressure of athletes to return to play mtbi can often be missed by these tests in this project our goal is to train a neural network which will automatically extract relevant features to classify between real impacts and false positives 
the input to our algorithm is mouthguard time series data 
currently there are a number of sensor systems used for measuring head impact kinematics in contact sports 
many of these systems use a simple linear acceleration threshold for differentiating impacts and non impacts however this leaves the device prone to a large number of false positives 
many companies and research groups are developing proprietary algorithms for detecting impacts but little has been published validating their accuracy to the best of our knowledge only one study has attempted to use a neural network algorithm for detecting head impacts and non impacts from kinematic sensor data this study used a simple net with a single fully connected layer and only achieved 47 specificity and 88 sensitivity on their dataset of soccer athletes
our dataset is 527 examples of which half are labeled real or true impact and the other half are labeled as false impacts 
the dataset was obtained by instrumenting stanford football athletes over the fall 2017 season with the camarillo lab instrumented mouthguard 
to obtain the ground truth labels for the dataset videos of each game and practice time synced with the mouthguards were analyzed according to the methodology outlined by kuo et al we split our dataset up into 70 for training and 15 for both evaluation and testing for when leveraging k fold cross validation and 70 for training and 30 for evaluation for when training our selected architecture 
each example has dimension 199x6 comprised of 6 time traces of length 199 200 ms 
the six time traces are the linear acceleration at the head center of gravity in the x y and z axes and angular velocity of the head in the x y and z anatomical planes 
the data was sampled with a time step of 1000 hz with 50 ms recorded pre trigger and 150 ms post trigger for 299 data points 
data was pre processed using standardization by subtracting out the mean of each sensor s values and dividing by the standard deviation 
a convolutional neural network is a class of deep neural networks comprised of convolutional layers 
in convolutional layers each sensor measurement is convolved with a weighting function w in the case of a 1d input to a 1d convolutional layer the i th product element can be found as follows where b is the bias term d is the filter width and w d a re each of the filters 
in the case where the input to the 1d convolution is a multi channeled such as our application where we are stacking six input signals the output of the convolution each channel is added to give the following where s is the number of input channels in our case six 
the output of a 1d convolutional layer is a single vector 
in 2d convolutional layers this process is repeated in two dimensions providing a two dimensional output 
convolutional neural networks commonly have pooling operations which combine outputs of neuron clusters at one layer into a single neuron in the next layer 
max pooling layers use the maximum value from a specified cluster of neurons while average pooling uses the average of a specified cluster 
further dropout layers can be added to help prevent overfitting a dropout layer will randomly ignore a certain percent of the layer interconnections during training we investigated multiple different convolutional neural network architectures using keras and tensorflow written in python specifically we developed both sequential models and recursive network models 
in investigating proper model architecture we utilized the k fold cross validation technique k 10 as we knew that 527 examples is not a very large amount and gathering more data was not feasible within the scope of this project 
in training all of our networks the number of epochs was increased indefinitely until five consecutive epochs did not result in an improved evaluation binary cross entropy loss 
following completion of training the model at the end of the epoch with the lowest evaluation loss was saved and used for analysis we developed and compared two primary architectures 
the recursivenet model has the most convolutional layers as seen in
in all testing the metric we optimized for was accuracy but we also performed tests on precision specificity and sensitivity 
the equations for the metrics are described below where tp is true positive tn is true negative fp is false positive and fn is false negative 
using our baseline hyperparameters in preliminary testing we found that the hiknet had comparable accuracy to recursivenet at a much lower computational cost 
thus we focused our hyperparameter tuning on the hiknet architecture we tuned the final hiknet using a greedy optimization scheme for number of 1d conv layers 2d conv layers and type of final layer 
because our parameters were initialized to random values and convergence is highly dependent on weight initialization we also did a parameter sweep to find the optimal filter size kernel width and dropout threshold 
the filter size was changed between the values of 15 and 200 the kernel width was between 0 and 50 and the dropout threshold was swept between 0 and 0 6 
the optimal dropout threshold was found to be 0 4 kernel width was 15 and filter number was 150 
we found that the optimal kernel width and dropout threshold for hiknet was the same for the perceptionnet the final performance metrics are summarized in
conclusion future workin conclusion a low parameter neural network performed very well and achieved better performance metrics than the existing svm classifier trained on the same mouthguard time series data set 
we created two deep convolutional neural networks one that was recursive and one that was sequential and although both performed similarly we chose the hiknet because it had fewer parameters and a higher confidence in its ability to generalize to other kinematic time series datasets than the recursivenet based on our literature search 
we used a greedy optimization scheme to build the architecture of hiknet and did a parameter sweep to find the optimal filter size kernel width and dropout percent as an immediate next step we can apply our neural networks to a larger mouthguard dataset as more data is collected in the camarillo lab to further tune parameters 
in future work we can use the same mouthguard and video impact footage to create a dataset with more specific labels i e 
where the impact was located on the head body impact or no impact 
using this data we could create a softmax classifier to predict whether an impact occurred and where it occurred on the head and body 
lastly once more concussion data is obtained we could create a neural network that could detect whether an impact occurred and predicts if the impact resulted in a concussion or not 
this would require additional data beyond just the mouthguard data such as clinical diagnoses and medical records 
the ultimate goal would be to have a device that could instantly tell if an impact resulted in concussion although it may take years to obtain the dataset needed to train this classifier the performance of our network architecture gives promise that this could be possible using a similar methodology as put forth in this work 
michael fanton developed hiknet neural network architecture in keras set up architecture optimization helped with statistical analyses provided background information nicholas gaudio lead the insight into keras and the model architecture setup created the recursivenet setup auto epoch stopping and saved the best epoch model conducted experiments to find the optimal filter width and filter number 
alissa ling preprocessed data wrote the k fold function optimized the dropout threshold lead the final poster wrote first draft of sections 9 
with the recent failure of senate bill sb 827 in california pressure is higher than ever on state politicians to better understand and respond to the increasing unaffordability of california s urban centers 
designed to issue more housing construction permits in high opportunity areas sb 827 was ironically crippled by its failure to explicitly acknowledge the possible gentrification externalities of new housing construction 
because of the astronomical and increasing cost of housing more californians live in poverty than in any other state when cost of living is accounted for one tool that academics use to design thoughtful housing policy is the gentrification early warning system i 
using california wide census data to classify emergent gentrification and to understand the leading indicators of gentrification through feature selection ii 
and modelling the state s housing market as an interconnected network to test an economic theory of how gentrification spreads specifically we use machine learning techniques primarily non parametric models such as random forests and gradient boosting to ascertain the leading indicators of gentrification at the census tract level in california 
we formulate the problem as binary classification over a five year time horizon using custom designed responses to proxy for whether gentrification was observed in a community over the prediction period 
we source data from american factfinder aff a public information tool produced by the united states census prior research describes gentrification in terms of either rising costs of living or displacement of the poor as income distributions shift towards affluence because gentrification occurs over a long time horizon we split the feature set around a pivot year of 2012 we compute the responses using the data from years 2012 2016 with the data from 2010 and 2011 used as features in the above formulation t 2012 and t 2016 
splitting the data to forecast gentrification over a long time horizon comports with previous research to model the second response we use an imputed measurement of the inter year intra tract change in the income distribution of the tract see for each tract we compute the hellinger distance between the observed income distribution and a baseline in which all residents are perfectly affluent with probability 1 
ii data responses and features
tracts with low hellinger distances tend to be high income tracts with high hellinger distances tend to be low income 
finally we compute the response by taking the differences of these hellinger distances for each tract between 2012 the pivot year and 2016 
a tract that becomes more affluent gentrifies from 2012 to 2016 has a negative difference and vice versa for a tract that becomes more low income 
we rescale the responses so that they are bounded between 0 and 100 and positive differences signal gentrification 
finally we relabel each response 1 gentrification occurred or 0 gentrification did not occur for both the monthly cost of housing and income distribution shift responses we characterize each census tract using a vector of roughly 150 features assembled from tables s2502 s2503 b25085 and dp03 in aff 
these include tracts demographic and economic characteristics such as employment by industry ethnic and racial composition level education and more additionally we engineer four features based on the theory of spatial equilibrium proposed in prior work on endogenous gentrification here y j denotes each response computed between the pre pivot years 2010 and 2011 
likewise for each response we compute the local moran s i statistic a measure of spatial clustering where z k is the deviation of the response of interest from the mean across all n tracts in the training sample computed between 2010 and 2011 the observation period 
we do not use time invariant features describing the geography of the census tracts 
these ought not add much explanatory power to a model that forecasts gentrification by time 
likewise we do not add network topological features from e g 
overall the data consist of 8 056 observations for each of california s census tracts with one dropped due to missing data 
surprisingly a priori we observed the classes to be roughly balanced for both responses suggesting that there still exist pockets of affordability in the state 
we split the data into a training set comprising 90 7 262 of the observations and validation and test sets comprising 5 397 respectively 
we applied four machine learning methods to each classification problem defining gentrification as the change in monthly cost and as the shift in income distribution 
we used a random forest classifier a gradient boosting model xgboost an 1 penalized logistic regression and an ensemble approach that classified census tracts according to a majority vote of the aforementioned three models random forests are a variant of bagged decision trees a random forest classifier grows a substantial number of independent classification trees each of which minimizes the gini impurity of its leaf nodes through recursive binary splitting gini impurity measures how often a randomly chosen observation in the node would be mislabelled if it were assigned a random label according to the distribution of responses in the node 
as classification trees grown on the same set of bootstrapped data tend to be highly correlated the random forest algorithm decorrelates the trees by constraining each split in each tree to be on a random subsample of features in the feature space gradient boosting is an ensemble technique using classification trees in which trees are grown sequentially as opposed to simultaneously in random forests 
later trees are grown to minimize the errors made by their predecessors 
each subsequent tree learns from the mistakes made earlier in training 
xgboost a popular implementation of gradient boosting which enables regularization of the trees minimizes the loss function where i are the predicted class each f k is a decision tree and is a regularization function of the number of leaves in each tree and the weights of those leaves for the random forest estimator we tuned n the number of trees and p the number of features in the random split set at every split 
for the xgboost estimator we tuned the learning rate the tree depth d on each tree and the regularization parameter our final unitary model was the only parametric estimator 1 penalized logistic regression commonly known as the lasso 
the lasso estimator is a variation on linear regression that logit transforms the responses to estimate logistic regression models p r y i 1 as logistic in the features where l is the logistic loss function 
because the lasso penalizes parameter coefficients in absolute value it implicitly performs feature selection as features with little predictive power have their parameter coefficients driven to zero 
for the lasso estimator we tuned the regularization parameter c we tuned all hyperparameters via two stage grid search 
first we drew test hyperparameters uniformly from a representative interval around the model implementations default parameters in
parameter the high value of and low value of c found by grid search on the validation set suggest that models that perform poorly may be vulnerable to overfitting especially given the high feature dimensionality 
we evaluated each classifier on each of the two responses using accuracy precision and recall 
while accuracy measures the proportion of test set class assignments that match the true labels precision and recall provide granular insight into classification errors 
recall quantifies the proportion of positive classes instantiations of gentrification that were correctly captured by the classifier precision quantifies the prediction accuracy solely among the samples that were predicted to be in the positive class 
precision is commonly used when the cost of false positives is high such as when there may be resources wasted in a misdirected policy response 
recall is commonly used when the cost of false negatives is high such as when families are being displaced 
while no one metric dominates in importance in this domain precision and recall illuminate why the performance of all classifiers on the task of classifying tracts according to their change in income distribution during the prediction period was so poor 
all four classifiers outperformed the no information classifier in predicting whether a tract would gentrify as defined by a rise in the monthly cost of housing see by contrast no model outperformed the no information classifier in predicting whether a tract would gentrify based on its income distribution 
this is not surprising given how uncorrelated these responses were with 0 06 
the high recalls and relatively low precisions reported by the random forest logit model and voting classifier suggest a plausible explanation that all three were overly trigger happy in labelling tracts as positive instantiations of the response leading to high counts of true positive labelings and few false negatives boosting recall as well as high counts of false positive labelings damping precision 
the confusion matrix for the random forest estimator the best model on this problemindicates that the estimator guessed positive 86 of the time an overwhelming majority given that the classes were balanced in the training and test sets see
in this research we develop a classifier to predict whether gentrification will occur in a california census tract with 65 accuracy 
we defined gentrification as an increase in the inflation adjusted monthly cost of housing and observed experimentally that other definitions such as ones based on localities income distributions yielded noisy results using public data 
non parametric ensemble models such as random forests and xgboost outperformed parametric models which may have overfit the training data 
furthermore engineered features describing the spatial characteristics of each census tract proved most consequential lending credence to the theory that housing markets in spatial disequilibrium precede gentrification further work might refine the spatially engineered features by e g 
weighting the network adjacency matrix so that the i jth entry denotes inverse intercentroid distance instead of adjacency 
alternatively further work might focus on better defining gentrification by quantifying displacement of families or collapsing the bins of the income distribution response to increase the signal in the data 
finally causal work could ascertain the drivers of gentrification as opposed to simply leading indicators 
accurately forecasting gentrification continues to be a pressing problem for california policymakers 
all code written for this project can be found here 
vi code
stock market predictions lend themselves well to a machine learning framework due to their quantitative nature 
a supervised learning model to predict stock movement direction can combine technical information and qualitative sentiment through news encoded into fixed length real vectors 
we attempt a large range of models both to encode qualitative sentiment information into features and to make a final up or down prediction on the direction of a particular stock given encoded news and technical features 
we find that a universal sentence encoder combined with svms achieves encouraging results on our data 
stock market predictions have been a pivotal and controversial subject in the field of finance 
some theorists believe in the efficient market hypothesis that stock prices reflect all current information and thus think that the stock market is inherently unpredictable 
others have attempted to predict the market through fundamental analysis technical analysis and more recently machine learning 
a technique such as machine learning may lend itself well to such an application because of the fundamentally quantitative nature of the stock market 
current machine learning models have focused on technical analyses or sentiment as a single feature 
but since the stock market is also heavily dependent on market sentiment and fundamental company information which cannot be captured with a simple numeric indicator we decided to create a machine learning model that takes in both stock financial data and news information which we encode into a fixed length vector 
our model tries to predict stock direction using a variety of techniques including svms and neural networks 
by creating a machine learning model that combines the approaches of technical analysis and fundamental analysis we hope our model can paint a better picture of the overall market 
sentiment analysis and machine learning for stock predictions is an active research area 
existing work to predict stock movement direction using sentiment analysis includes dictionary based correlation finding methods and sentiment mood detection algorithms 
related work and analysis
several papers such as nagar and hahsler
our dataset is composed of trading macro technical and news data related to 20 nasdaq companies from 2013 to 2017 
we used the yahoo finance api to extract trading related information on each stock ticker including price and volume on a daily basis 
data sources
we also extracted overarching macro data including quarterly gdp cpi and daily libor from the fed website 
in addition we computed technical indicators including cci rsi and evm from trading data 
finally we scraped daily news headlines and snippets for each ticker from new york times and google news 
we used a few approaches to merge and preprocess the data 
to match quarterly macro data e g gdp with other daily data we assumed the macro features to be constant throughout the same quarter 
we processed the news data using text representation and sentiment analysis models which will be discussed in detail in section 4 before merging it to the full data set 
for tickers which have multiple news for certain dates we averaged the sentiment encoded vectors for google news and used the top 1 news for new york times because new york times ranks top articles 
for tickers which don t have news articles on certain dates we replaced the missing value with the latest available news 
we choose not to normalize the data to avoid destroying correlations of the sparse matrix 
furthermore we classified the 1 day next day stock movement into a binary label y where y 1 if adj 
close price last adj 
close price and y 0 if adj 
close price 
finally we built two datasets using news from new york times and google respectively each of which contains 24k entries and 70 features 
we split all samples before 2017 into the training set and hold out the rest as test set 
we plotted label y on the first two principal components of news data 
the plot reveals the complicated nature of the features implying that high dimension classifiers are required for the dataset 
data visualization
in general the problem is a supervised learning problem i e we are predicting the next day movement of the stock by taking in the trading information about the stock and the information from the ticker specific daily news 
the task can be split into two parts namely to represent the news as a fixed length real scalar or vector and to use the news together with trading information technical indicators and macro data to make the prediction 
model overview
in order to capture the semantic information of the text and represent it in a vector space we eventually decided to use a google universal sentence encode use as the encoder section 4 2 
in terms of the stock prediction model which is trained to take in all of the technical and encoded features to make the prediction we used logistic regression random forest svm and a variety of neural networks section 4 3 
the text representation model is required to convert news sentences and snippets to real value fixed length scalar or vector representations that can be used in the stock movement prediction model 
the goal is to have the model best capture fine grained semantic and syntactic information 
text representation
one method we tried was to directly extract a sentiment signal scalar from a given sentence vector 
dictionary based approaches use statistical information of word occurence count to extract useful information 
we used a sentimentanalysis package from r with a financial dictionary similar to the one mentioned in related work to get a scalar sentiment score for each of our sentences 
we also tried using a pre trained sentiment lstm model which was trained using labeled data from cnn news 5 to extract the sentiment from the headline and snippet text 
however neither of the methods mentioned above achieved a reasonable accuracy in making the overall prediction and a plausible reason is that the high level sentiment information is not sufficient in representing the text 
thus we used sentence embeddings to produce a vector space with a more meaningful substructure to represent the news and fed the entire vector embedding into our classification model 
recent methods of finding fixed length real vector representations of words and sentences have succeeded in a wide range of tasks from sentiment analysis to question and answering 
these models can be broadly divided into word encoding methods and sentence encoding methods 
to evaluate each of these models to choose one for us to use we took several sentences and compared the results of the encoding to see if the encoders captured the similarities and differences between sentences 
word encoding strategies include word2vec elmo glove and fasttext 
these models use the bag of words technique which detect how often words appear in similar context of other words to get a vector representation of each word though the fasttext actually goes character by character 
we noticed that one problem with using one of these word encoding strategies on our sentences is that it does not consider the words of the sentence together and we are unsure about how to composite the words to the sentence 
thus we decided to choose a method that encoded entire sentences such as skip thoughts similar to word2vec but with sentences instead infersent looks at pairs of sentences or a universal sentence encoder 
the use consists of a deep average network dan structure although this structure also takes an average of words there are layers of dropout that allow important words to be highlighted 
there was also another variant of the use that used a transformer module a novel neural network architecture based on a self attention mechanism of context this method achieves the best in detecting sentence similarities however we found this technique to be too slow on our data 
eventually we decided to use the pre trained google dan use as our sentence representation because of its ability to detect features in a large range of sentence types including our news large pretrained corpus and dropout technique 
the pca is based off of all of the seen vectors in the training set and the principal components stay the same for the test set 
logistic regression is used as a baseline to map the features to stock movement random forest which constructs a multitude of decision trees at training time and outputs the class that is the majority vote of the individual leaves is widely used for classification and regression 
we tuned depth of the tree and leaf size to regularize the model 
stock movement prediction
support vector machines are mentioned in previous research fully connected neural networks are composed by interconnected neurons whose activation functions capture the non linear relationship between the variables and the binary result 
we tuned the model on different parameters and the best performance model structure consists of two hidden layers 50 2 or 10 10 with relu activation and a learning rate of 1e 3 although it did vary based on dataset convolutional neural networks have been widely used for image processing 
we thought it might be effective to do convolutions over the sentence embeddings because of their structure however we also acknowledge that because of the pca and the way the google use dan works the adjacent features may not be relevant to each other 
two 1d conv layers each followed by a pooling layer are included before the final fully connected layer 
we picked the learning rate with which the model converges most effectively 1e 3 recurrent neural networks are proven to be effective in dealing with sequential data with the output being dependent on the previous computations 
we are training separate rnns for each ticker 
the learning rate is 1e 4 
to examine the model stability we trained each model on the two datasets using ny times and google news separately with a learning rate mentioned in 4 3 respectively 
the mini batch size selected was the largest possible value to fit into cpu for rnn the mini batch is all data for each ticker 
we evaluate the model using mainly test accuracy 
meanwhile we also monitor the f 1 score to ensure balanced performance on both 0 and 1 labels 
as shown in table 1 svm with rbf kernel is the best performing model on both datasets 
neural network and cnn also achieved decent performance 
however results from logistic regression and random forest are not satisfactory 
best model svm with rbf kernel is able to project the features onto a high dimensional space and effectively separate the labels 
we tuned the cost parameter to prevent overfitting 
precision and recall rates of the best performing models on google news and ny times are shown in table 2 
although we attempted to achieve a balanced performance on 0 and 1 labels the selected model still outputs a relatively imbalanced confusion matrix 
we believe that such issue is raised by our loss function which is designed to maximize the overall accuracy but not to ensure the performance on both labels 
bias data visualization reveals that our dataset is not separable in a low dimension space which explains why random forest and logistic regression with simple structure are not working well variance random forest shows the overfitting problem even after regularization 
our dataset contains features which might be positively or negatively correlated with each other e g vectors representing news headlines 
selecting a subset of such features may not be able to reduce the variance efficiently 
stability as mentioned before for the rnn in order to capture the time series nature of each stock we split the dataset by ticker before running the model which in turn shrunk the data size 
additionally some of the tickers had relatively sparse unique news data 
furthermore it is probable that the deep structure of the model caused the gradient update to be inefficient 
we also found that the rnn is very sensitive to the initialization of the hidden state which shed some light on the inefficacy of back propagation 
to fix this we might change the structure of hidden state or use a different activation function 
these are some possible reasons the rnn outputs a high proportion of 1s or 0s on some of the subsets and cannot be used as a stable model for future predictions to gain better understanding of the model performance we plotted the true and predicted stock movement of facebook in 2017 as follows where the same color on the same day indicates correct predictions 
examining the predictions closely we found that the best performing model svm is more able to detect major up downs than smaller changes 
6 conclusion future workin conclusion we think stock specific news might help in predicting next day stock movement 
however it is hard to turn such informational edge into a profitable trading strategy given that we are merely predicting ups and downs 
in addition our model seems to be more able to detect major movements than smaller ones 
we believe the following steps can be taken to improve model performance in the future customized loss function we think achieving high accuracy and balanced performance on 1 and 0 labels are both important in stock movement prediction 
however the second goal was not built into the loss function of our models 
as the next step we can customize the loss function e g as binary cross entropy to obtain a more balanced performance enhance data quality to make the project usable in real life we built the dataset using news we scraped from the internet 
such data might include irrelevant or inaccurate news which increases noise 
in the future we think adding more cleaning techniques and including models to detect unhelpful news may help 
our team spent 50 percent of our time on collecting and preprocessing data 20 percent on text representation and 30 percent price movement modelling and debugging 
given the challenging nature of our topic three of us worked closely during the whole process 
chris contributed primarily to collecting the trading data working on sentiment signal modelling using text representations and applying the models to new york times data 
yilun contributed primarily to collecting sentiment data and testing and debugging the rnn and cnn models 
iris contributed primarily to collecting sentiment and trading data data preprocessing and applying the models to google news data 
we would like to thank the entire cs 229 teaching staff including our mentor atharva parulekar for providing invaluable feedback thorughout the course of the project 8 references bibliography
the finance industry has been revolutionized by the increased availability of data the rise in computing power and the popularization of machine learning algorithms 
according to the wall street journal 2017b quantitative hedge funds represented 27 of total trading activity in 2017 rivaling the 29 that represents all individual investors 
most of these institutions are applying a machine learning approach to investing despite this boom in data driven strategies the literature that analyzes machine learning methods in financial forecasting is very limited with most papers focusing on stock return prediction 
the objective of this paper is to produce directional fx forecasts that are able to yield profitable investment strategies 
hence we approach the problem from two perspectives 1 classification of long short signals 2 point forecasts of fx levels that translate into long short signals these frameworks allow us to exploit different machine learning methodologies to solve a single problem designing a profitable fx strategy based on ml generated forecasts 
machine learning methods have long been used in stock return prediction 
for instance variations of principal component analysis an unsupervised learning technique have been applied by nevertheless despite the large adoption of machine learning in stock return forecasting ml applications in fx prediction have been widely ignored by the literature 
few exceptions are available 
the main contribution of this paper is the assessment of the statistical and economic performance of ml generated directional forecasts iii 
datasets we make use of two different datasets to explore the forecasting power of two types of variables market and fundamentals 
we define a market variable as an indicator with daily to weekly frequency that has a close relationship with traded securities 
on the other hand we define a fundamental variable as an indicator with monthly frequency that is closely related to the macroeconomy finally we limit the scope of our project to forecasting the usdmxn which is the exchange rate between the us dollar usd and the mexican peso mxn expressed in mxn per usd 
however the exercise can be generalized to other currencies 
all data was retrieved either from bloomberg the global financial dataset or the federal reserve bank 
we obtained the weekly closing price of the usdmxn currency pair which we use as our target variable 
in addition we consider 25 features across both mexico and the united states 
a market variables dataset
a summary is shown in
the fundamental variables data uses the monthly closing price of the usdmxn currency pair as our target variable 
we use 27 features that describe the macroeconomic conditions of both the us and mexico between march 1990 and october 2018 
b fundamental variables dataset
the additional features that are considered in this dataset are detailed in
almost all data processing is identical in both datasets 
we first split the data into 60 train set 20 validation set and 20 test set 
c data processing
these subsets are taken sequentially in order to keep the time series nature of the data and to guarantee our algorithms train exclusively on past data to translate our problem into a classification problem we introduce the signal t variable which we set to 1 if the usdmxn was higher tomorrow than today 
this is we also perform data processing on the features 
in particular we standardize using the mean and standard deviation of the training set for every covariate for the fundamentals dataset covariates are lagged by an additional period 
this is done to approximate the fact that it is extremely rare to obtain real time macroeconomic data 
by lagging the features by one month we ensure we are not peeking into the future by including unpublished data 
first we perform binary classification on the signal t variable we constructed in the data processing step 
this essentially transforms what initially is a continuous variable problem into a classification task on a second exercise we use ml algorithms to construct point forecasts for our raw continuous target variable usdmxn t 
a frameworks
we then construct an estimated long short signal by computing both strategies yield a binary signal output that we can execute as a trading strategy 
the performance of different machine learning algorithms is tested for each framework 
in particular we considered 1 logistic linear regression we use logistic and linear regression as our benchmark models 2 regularized logistic linear regression we consider l 1 and l 2 regularization applied to logistic and linear regression 
b models
this allows to reduce overfitting in the validation set 
the hyperparameter which penalizes large coefficients is tuned using the validation set accuracy 3 support vector machines regression svm svr it is highly likely that fitting fx dynamics requires a non linear boundary 
svm svr with a gaussian kernel provide the flexibility to generate a non linear boundary as a result of the infinite dimensional feature vector generated by the kernel 
tree based models allow us to capture complex interactions between the variables 
unlike random forests which require bootstrapping gbc allows us to keep the time series structure of the data while considering non linearities 
4 gradient boosting classifier regression gbc gbr 
it is important to notice that gbc and gbr is just considered for the market variables dataset due to the division of work between the authors see section ix 
neural networks can model complex relationships between input features which could improve the forecasting performance 
we consider fullyconnected networks 
5 neural networks nn 
the architecture is shown in our choice for loss depends on the framework 
we select logistic loss for classification and mean squared error for the continuous target variable problem 
we choose the proper activations in the same fashion sigmoid is used for classification while relu is used for the continuous target variable 
finally we use dropout or activation regularization to avoid overfitting 
all model parameters are tuned using the validation set 
we use accuracy as our performance evaluation in the binary classification model and mean squared error in the continuous target variable model 
v hyperparameter tuning
the resulting parameters are detailed in there is however an important caveat when interpreting the results 
being a measurement of the fraction of predictions that we can correctly forecast accuracy does not differentiate between true positives and true negatives 
a successful trading strategy should exploit true positives and true negatives while minimizing false positives and false negatives to discern between these cases given the bad results of the confusion matrix for the binary classification problem we explore the results of the continuous experiments 
vii 
economic performance a model with very successful statistical performance of long short signals does not imply positive economic implications 
this is an inherent problem in directional forecasts a profitable investment strategy requires algorithms that correctly predict the direction of very large movements in the price of the asset 
in our case if an algorithm correctly predicts most small changes but misses large jumps in the exchange rate it is very likely that it will produce negative economic performance upon execution 
this issue has been previously assessed in the literature by at the end of every period the position is closed profits are cashed in and the strategy is repeated 
finally we use a longonly strategy as our benchmark for economic performance a 
binary classification the statistically best performing model corresponds to the economically most profitable specification 
however it is important to notice that this positive result is mostly driven by a single correct bet made between weeks 725 and 750 
all other strategies produce profits that are equal to or worse than the long only benchmark these results can be explained by the bad performance of the models in terms of the confusion matrix 
due to the very low true negative rate of most models all specifications are close to the long only benchmark and the departures are a consequence of few correct or incorrect short bets 
the differences with respect to the binary classification results are once again significant 
the final cumulative return in the continuous target variable framework is around 15 higher than under the binary classification framework 
furthermore all strategies outperform the long only benchmark with the best strategy being ridge regression 
in addition the economic effect of an improved true negative rate is considerable 
unlike the binary classification case the outperformance of all strategies with respect to the benchmark is not driven by few correct short positions 
b continuous variable target
moreover the reduction in the true positive rate observed for the continuous target variable framework does not significantly penalize cumulative profits 
the gains of a high specificity outweigh any losses derived from the reduction in sensitivity a natural question to address is which variables explain exchange rate forecasts the most 
finally another interesting insight is that the usdmxn reacts strongly to global and emerging market em fixed income indicators 
in theory the bilateral exchange rate should react strongly to the interest rate differential between the two countries 
we believe the observed result provides evidence of investor behavior 
as documented in recent years by
this paper makes use of machine learning methods to forecast the us dollar against mexican peso exchange rate 
we use an innovative framework to find the best possible performance 
first we consider a market variables dataset and a fundamentals dataset on which we train ml algorithms 
second we conduct binary classification experiments and continuous target experiments to produce the same output a binary long short signal on which we are able to execute a simple trading strategy our results suggest that continuous target prediction outperforms binary classification not only in terms of accuracy but also in terms of specificity and sensitivity 
the economic results are in line with this finding with all algorithms outperforming a long only benchmark 
the best results are produced by svm in the binary classification case and ridge regression in the continuous target case both in terms of accuracy and cumulative profits 
last we find that the fundamentals dataset yields poor results future work could focus in several areas 
first the recursive validation procedure proposed in
the team worked on the same problem but used different datasets 
the contribution to this work was as follows christian gonz lez rojas was in charge of data collecting data processing algorithm selection and algorithm implementation on the market variables dataset for both the continuous and the binary framework 
ix contributions
he decided to consider gbc gbr as an additional model to further test the value of nonlinear relationships 
he was also responsible for writing the cs229 poster and the cs229 final report 
his data and code can be found at this link molly herman worked on data collection data processing and algorithms for the fundamentals dataset 
she was responsible for modifying the cs229 poster to create an alternative version for the cs229a presentation and was in charge of writing her own final report for cs229a the division of work for the poster and the final report was done to provide deeper insight on the results to which each author contributed the most 
the 2sigma competition at kaggle aims at advancing our understanding of how the content of news analytics might influence the performance of stock prices 
for this purpose a large set of daily market and news data is provided for a subset of us listed financial instruments 
this data shall be used to train any kind of learning algorithm deemed useful in order to predict future stock market returns the competition comprises two stages with two different evaluation periods 
in the first stage the predictions are tested against historical data of the period 1 1 2017 to 7 31 2018 
this stage will be terminated early next year at which time the final submissions of the participating teams must be handed in 
the latter will then be evaluated against future data for about six months to identify the best performing submission which will be disclosed 7 15 2019 the objective function for this machine learning task is set the same for all participants in the competition and constructed as follows for each day t within the evaluation period the value x t is calculated aswhere for any financial asset i 1 m the term ti 1 1 stands for the predicted confidence value that it s ten day market adjusted leading return r ti r is either positive or negative 
the universe variable u ti 0 1 controls whether the asset i is included in the evaluation at the particular evaluation day t finally the score which determines the position in the competition is composed of the mean and the standard deviation of the daily value x t with score 0 for x t 0 we apply three different algorithms to this problem logistic regression neural network and gradient boosting tree 
there have been multiple attempts looking into the popular topic of forecasting stock price with techniques of machine learning 
based on the works we find the focus of these research projects vary mainly in three ways 
1 the text information used in prediction ranges from public news economy trend to exclusive information about the characteristics of the company 
2 the targeting price change can be near term high frequency less than a minute short term tomorrow to a few days later and long term months later 
3 datasets and features
all the data used in the project is provided by kaggle 
two sources of data are provided one for market data and one for news data both spanning from 2007 to the end of 2016 
description
the market data contains various financial market information for 3511 us listed instruments 
it is comprised of more than 4 million samples and 16 features the returnsopennextmktres10 column indicates the market normalized return for the next 10 days and thus serves as the ground truth value for the prediction task 
the news data contains information at both article level and asset level 
there are more 9 million samples and 35 features 
most of the news features are either numerical or type indicators except the headline feature which contains text 
the news data provided is intentionally not normalized both data sets can be joined by using either the time stamp asset code or asset name 
as shown in
processing
we chose logistic regression as a starting point for establishing a baseline score 
the logistic regression takes in all the features as is such that it does not include higher degree terms 
because of the large size of the training data small regularization is used 
the log likely hood is
we implement a fully connected neural network with two inputs 
into the first input branch we feed all numerical values of the preprocessed dataset while the second input branch encodes the categorical data asset code for each sample in a trainable embedding layer 
after batch normalisation and two fully connected layers for the numerical part and one fully connected layer for the categorical part both branches of the network are concatenated 
the concatenated data is finally fed into one more fully connected layer followed by the output layer 
all fully connected layers use relu activation except the output layer which has a sigmoid activation function 
gradient boosting is a technique that combines weak predicting models into an ensemble to produce a much stronger one 
it is typically implemented on decision trees 
gradient boosting
like other boosting algorithms gradient boosting is an iterative operation 
at each iteration the algorithm creates a new estimator that minimizes the loss with respect to the current model 
this minimization can be approximated by fitting the new estimator to the gradient of loss such that where f k is the ensemble model at kth iteration r ik is the gradient residual of the loss function with respect to f k 1 for ith data h k is the new model that fits r ik for i 1 m l is the loss function binary log loss function for this project it is similar to the normal gradient descent except that the gradient descent is performed on the output of the model instead of the parameters of each weak model 
the regularization is achieved through several ways by slowly decreasing the learning rate setting the number of minimum samples in a tree leaf limiting number of leaves or penalizing the complexity of the tree model such as l2 regularization 
lightgbm library is used to implement this algorithm in this project 
it converts continuous features into bins which reduces memory and boosts speed and grows each tree with the priority given to the leaf with maximum delta loss leading to lower overall loss 
a auc curves the auc score for the logistic regression the fully connected neural network model and the light gbm model is 0 5 0 5799 and 0 5753 respectively as shown by
out of the three attempted algorithms the neural network performs the best followed closely by the gradient boosting tree while the logistic regression behaves almost like a random guess 
as the logistic regression is fairly a simple algorithm with linear mappings to each feature dimension its incapability to capture the complex relationship is expected 
on the other hand both the neural network and gradient boosting tree are powerful non linear algorithms with a large degree of flexibility and control making them competent to model complex situations for future work deeper dive into the feature engineering is needed 
it is also worth exploring to combine neural network and gradient boosting tree in an ensemble fashion to produce a stronger model 
one of the news features is text based thus natural language processing can be implemented to extract useful information from it 
given the large parameter sets for the neural network and the gradient boosting tree achieve the optimum parameters is both difficult and time consuming 
however there is still possible room to make improvement by further tuning the parameters 
lastly choosing a more powerful baseline such as the support vector machine instead of the simple logistic regression should be considered 
barthold albrecht has additional contribution on establishment of the logistic regression model and the fully connected neural network model 
yanzhuo wang has additional contribution on establishment of the logistic regression model and the lgbm model 
xiaofang zhu has additional contribution on establishing the fully connected neural network model 
https drive google com open id 1mnf5opuzbdvotkxl6sjkglpqabcn8v9x
the real estate market in large metropolitan areas across usa and canada is characterized by high volatility 
home prices in popular technological and cultural centers such as vancouver canada have been reportedly growing over the last decade owing to a constant influx of people including immigrants attracted by a combination of career opportunities and a superb geographical setting by the ocean bay and nearby mountains 
as a result predicting home prices has become a big challenge 
real estate agents use their domain knowledge to estimate a home price aiding sellers and buyers in the transaction 
this estimate is often very subjective and facilitates bubbling the home prices especially in highly attractive areas like vancouver 
therefore our main study goal was to come up with an automated way of pricetagging a home based on its characteristics including floor area number of rooms location and others the input to our algorithm is a dataset of all condo listings under 2 5mm cad in downtown vancouver between january 2016 and october 2018 containing approximately 50 features after pre processing 
we then use linear regression neural networks and boosted tree models to predict the expected selling price of a condo 
there is a good number of articles related to real estate pricing predictions 
in general it is difficult to compare the results given the diversity of features used to model the predictions and relevant error analysis 
however there are some common themes that we tried to reproduce and improve upon in our research 
more often than not the authors attempt to use linear regression and boosted trees regression algorithms another author in another example related to bay area house pricing prediction
the original dataset we received had exhaustive information about all condos listed for sale under 2 5mm cad in downtown vancouver between january 2016 and october 2018 
the data was pulled from an official canadian real estate listings database called mls with the help of a local real estate agent and contained approximately 10 000 listings 
a raw data
each listing had up to 237 features including immanent property characteristics like square footage number of bedrooms and bathrooms maintenance fees and relational characteristics like address vicinity to schools and public transportation and views 
the features can be classified into three categories structured data e g total floor area semistructured data e g address unstructured data e g listing agent comments 
furthermore data can be categorized into various types interval scaled variables e g number of bedrooms year built etc 
temporal e g date property was sold rank e g floor number boolean e g fireplace yes no categorical e g dwelling type 
our dataset did not have the geographical location of homes originally only the physical addresses 
since location is supposedly important for home value we used the google maps api 12 to geocode condo addresses to geographical coordinates latitude longitude 
b feature engineering geocoding and bucketing
in addition since the area of interest was relatively small only about 9 km 2 we approximated it with a flat rectangle and converted geographic to cartesian coordinates mapping all condos to a c feature engineering view scoring
cleaning the data was essential to having an accurate model since it was originally input by real estate agents and was prone to mistakes 
we removed outliers for the numerical features using three sigma rule 
d data cleaning
we then imputed data for features that had occasionally missing data less than 1 with medians 
lastly we standardized the data 
the final feature set consisted of 48 features 
to improve the quality of our predictions we performed error analysis with k fold cross validation cv 
we split our dataset randomly into a training and test set 80 20 
a error analysis
the training set was used in cv setting where it was sequentially split into training and validation subsets k 5 times 
the training subset was used to fit the model to the data and validation subset was used to compute errors 
the average of k errors the cv error characterized how accurately our model performed we used two main metrics for calculating errors mean squared error and coefficient of determination r squared with y i predicted variable observations its mean x i vector of independent variables features f x the model mapping features x on y and n number of observations i 1 n 
mse characterizes the average of squared deviations of predictions from observations with mse 0 corresponding to an idealistic model exactly mimicking observations 
r 2 measures how well the model captures variability of observations given observed features with r 2 1 being an idealistic scenario 
one can show that these properties are closely related such thatgiving it the meaning of the fraction of explained variance in y we calculated mse and r 2 for both cv as explained above and training sets and also calculated the ratio of mse on cv and training sets to characterize how well our model generalized to new data model variance v ar m se cv m se train 
a good model would have this metric not much larger than 1 
a v ar 1 would mean we overfitted the data and our model would most likely perform badly on new data 
finally after tuning each model and obtaining best set of coefficients we calculated mse and r 2 for test set as the final unbiased accuracy characteristic 
we used multiple multiple predictors linear regression as our benchmark model 
as the name suggests it assumes a linear relationship between the features and the predicted target variable and treats it as a linear combination of features f x t x b where x is the feature vector is the vector of model coefficients and b is the bias 
b linear regression
to train the linear regression model one needs to find the coefficients given data x and target variable observations y as approximation of the predictions of f x 
by minimizing the least squares cost function w r t 
coefficients they are found effectively using normal equation the added benefit of lasso regularization is that it sets coefficients of unimportant features to 0 and can be used as a feature selection technique for other methods 
this was precisely the reason we used lasso in our research 
one property of neural networks that makes them a popular ml method is their ability to perform end to end learning given some input features x a network is able to determine the appropriate intermediary features and weights of those features on its own a neural network s ability to model non linear data stems from its use of activation functions in between its neuron layers 
one example of a commonly used activation function is the rectified linear unit relu function its main advantage is that it has a very simple gradient and doesn t suffer from vanishing gradients at extreme values although it can cause dead neurons when the product of the weights and inputs skews negative the leaky relu activation function addresses this shortcoming where is a small number e g 0 1 dropout layers are commonly used to address overfitting in neural networks 
this is implemented by randomly dropping a small percentage of nodes in a layer during each update iteration preventing the network from over relying on any individual neuron 
neural networks learn through the backpropagation of error gradients and the weights w l at a layer l are updated by where is the learning rate l i is the cost function and l i is a loss function least squares in our case for an i th example 
as alternate methods accounting for nonlinear relationship between features and target variable we use tree based techniques random forest and gradient boosting 
the random forest rf is a special case of ensemble ml methods when individual models are combined together to produce balanced model that have higher generalization power than separate models in the case of rf individual learners are regression trees termed as where feature space is partitioned into m regions r 1 r m and the c m ave y i x i r m 
e random forests and gradient boosting
at each tree node the binary partition is performed on one variable 
finding best binary partition in terms of split variable j and split point s by minimizing sum of squares is computationally infeasible and a greedy algorithm is used when decision is being made only one step forward 
starting with all the data a pair of half planes is defined then j and s are found by solving after finding the split the data is partitioned into two regions and the splitting process is repeated on both regions and all subsequent regions until some stopping criterion is met 
among different criteria most popular is stopping growing a tree when minimum node size is reached 
individual trees are prone to overfitting and rf method overcomes this problem by combining multiple trees grown on separate data subsets 
the default approach to forming subsets in rf is bootstrap sampling with replacement when dataset size remains the same but its composition varies among samples 
this way overfitting is decreased as each individual tree is learning from a different subset of data 
moreover a random subset k rather than the whole list of features m is considered at each split where usually m k this way if few features dominate the rest in their contribution to the target variable their contribution to the final model is decreased as now the chances for them to be selected for a split are reduced another tree based ensemble technique is called boosting when power of combining weak learners is leveraged 
we start off with the linear regression lr using all features 
the reported cv mse is 0 19 this made us try nonlinear learning methods and the first one was neural networks nn 
v experiments results
there is no formula for building the perfect nn architecture 
many design decisions are empirical and based on past experiences using them 
as a result we experimented with various layer depths neuron counts and activation functions 
we found that in general deeper networks with smaller layers performed better than shallow networks with larger layers 
this is supported by the nn initially suffered from significant overfitting cv mse was 10x of training mse 
this issue was addressed by adding dropout layers to the network and choosing the best configuration 5 
as a result the train cv mse difference dropped to below 15 gradient boosting regression gb finally provided most robust and accurate model leveraging non linear nature of interaction between condo features and target variable reporting cv set mse 0 1 and test mse 0 09 with m se cv m se train being 1 25
in addition to testing the three models we looked at the top features highly correlated with selling price of a condo 
they were in order 1 number of bedrooms 2 number of bathrooms 3 total floor area 4 number of parking spaces 5 maintenance fees we also noticed that the more expensive the condo was the more subjective the price appeared to be and the worse our models performed 
speaking about listing price that we intentionally excluded from the model we note that it is highly correlated with our target variable the selling price 
the mse between them is about 6 times smaller than our best mse 
one might think that the domain knowledge of real estate agents is very thorough in estimating home value but there is a paradox 
when people want to sell or buy a home they first look at the listing price and therefore the resulting selling price often is very close to the listing price with listing price being a major driving factor 
our goal on the other hand was to come up with the emotionfree algorithm that uses only bare facts about property itself and external factors 
comparing metrics of our model to those achieved by others we see that theres still room for improvement 
our findings show that gradient boosting had the best performance followed closely by random forest and neural networks 
this makes sense because both algorithms are useful for non linear modeling 
the small data scale in our project lends itself more favorably to trees whereas it makes it more likely for a neural network to overfit resulting in slightly lower overall performance the models described in this paper can be further improved in future work through usage of additional training datasets and additional feature engineering 
the federal interest rate has a direct effect on the supply of money and affordability of housing which can affect the selling price 
over the timespan of the training dataset used in this study the federal interest rate changed from a low of 0 5 up to a high of 1 75 which could significantly affect the selling price of a condo 
an additional dataset that would improve the model is upcoming new condo developments 
large growing cities often have new real estate being built 
additional inventory coming onto the market would affect existing condo prices negatively by increasing the available supply and alternatives for buyers lastly future work should focus on adding more temporal components to the model for example through features such as listing date number of condos sold in last n days and n day average sell price 
we d like to thank adina dragasanu from re max crest realty for providing us the data that enabled our research 
there are thousands of companies coming out worldwide each year 
over the past decades there has been a rapid growth in the formation of new companies both in the us and china 
thus it is an important and challenging task to understand what makes companies successful and to predict the success of a company 
in this project we used crunchbase data to build a predictive model through supervised learning to classify which start ups are successful and which aren t 
we explored k nearest neighbours knn model on this task and compared it with logistic regression lr and random forests rf model in previous work 
we used f1 score as the metric and found that knn model has a better performance on this task which achieves 44 45 of f1 score and 73 70 of accuracy 
thousands of companies are emerging around the world each year 
among them some are merged and acquired m a or go to public ipo while others may vanish and disappear 
what makes this difference and leads to the different endings for each company 
how to predict the success of companies 
if the investors can know how likely the company will achieve success given their current information they can make a better decision on the investments 
therefore in this project given some key features of a company we want to predict the probability of its success 
more specifically the input features are of two types text features such as industry category list and location and numerical features such as the amount of money a company already raised 
we then use logistic regression random forests and k nearest neighbours to output a predicted probability of success 
here we define the company success as the event that gives a large sum of money to the company s founders investors and early employees specifically through a process of m a merger and acquisition or an ipo initial public offering 
as machine learning becomes a more and more popular tool for researchers to utilize in the field of finance and investment we have found some related work to predict companies business success with machine learning and crunchbase bento lisin and nesterenko indeed these works propose a variety of efficient methods that we can use to predict the success of company 
however we notice that none of them implement k nearest neighbours model 
in this project we aim to apply knn model to solving this problem 
the dataset we used was extracted from crunchbase data export containing 60k companies information updated to december 2015 
there were four data files named company investments rounds and acquisition 
the company file contains most comprehensive information of the companies while other files contains more detailed information regarding the investment operations 
thus we chose the file company as the base and extracted meaningful features from other files to add into it 
the company dataset consists the following columns name company s name
dataset overview
we labeled the company that has m a with 1 otherwise 0 
we plotted the amount of the 0 or 1 labeled data as we noticed some skewness regarding the distribution of date of funding events in this dataset as shown in
cleaning and labeling
we selected the most essential features to companies business success and end up with input features as category country funding rounds funding total usd and the difference between when first funding at and last funding at the training set is composed of two parts 
the first part of data is the numerical data number of funding rounds and total funding 
the second part of data is the date in string format such as first funding at final funding at and funded at columns 
as there are too many missing data for funded at we finally chose first funding at and final funding at columns converted them from timestamp to numerical utc format and calculated a duration column with the subtracted data 
the goal of this project is to make a binary prediction on the status of start ups whether they have gone through m a or ipo 
in this project we explored logistic regression random forests and k nearest neighbors 
logistic regression is a simple algorithm that is commonly used in binary classification 
due to its efficiency it is the first model we selected to do the classification 
the hypothesis of logistic regression algorithm is as follows the algorithm optimize by maximizing the following log likelihood function 
random forests construct a multitude of decision trees at training time and outputting the mode of the classification result of individual trees 
at each split point in the decision tree only a subset of features are selected to take into consideration by the algorithm 
the candidate features are generated using bootstrap 
compared to an individual tree bootstrapping mitigates the variance by averaging the results of a large number of decision trees 
an instance is classified by a majority vote of its k nearest neighbours 
the algorithm assigns class j to x i that maximizes 
in a confusion matrix we describe the performance of a classification model 
each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class vice versa 
selected metrics
there are four basic terms in a confusion matrix here we select three metrics accuracy f1 score and auc score accuracy the proportion we have predicted right 
false positive rate fpr f p f p t nf1 score auc score area under the roc curve auc score area under roc curve total area
to utilize more data in the training we split the dataset into three parts 95 data as training set 5 as cross validation set and 5 as test set 
since the dataset is quite imbalanced we up sample the minority class label 1 in the training set to balance the data but keep the cross validation set and test set untouched see we also normalize all the numerical features such as funding rounds and funding duration and use bag of words to encode the text features such as category list and country code 
after preprocessing the data we concatenate the two types of features and feed them to logistic regression model random forest model and k nearest neighbours model 
for random forest and k nearest neighbors model we used random search to tune the hyperparameters 
a list of hyperparameters and their associated range is summarized in the table below see
5 50 k number of neighbours 10 100 we use accuracy f1 score and auc score to compare the performance of different models but the f1 score is our primary metric 
the figure below summarize the results of each model on the validation set see
hyperparameters range number of trees in rf 
in the future we should include more features of the companies and examine which features are more significant than others 
also we will try more complex models such as neural network and pre trained word embedding 
using kernel method to move the data to higher dimensional space is also a good direction 
in addition more new questions are to explore such as predicting the total funding size for a company regression problem 
welcome to check our code here https github com chenchenpan predict success of startups
thank you to the cs 229 teaching staff including prof andrew ng and the tas 
we construct using equation 1 the outcome variables the aud basis and the jpy basis 
interest rates are taken to be the fixed rate in overnight index swaps against central bank policy rates 
we further collect 32 data series for each of the three relevant currencies aud jpy and usd 
our data capture activities in the financial markets conditions of the economy the state of international trade and the stance of economic policies see we first pre process the data by computing and using for most of the features the percentage change between two observations 
this is done to both normalize and extract more meaningful information 
we next augment the data in two ways 
first while all financial markets data are reported daily other data are available only monthly or quarterly for low frequency series we impute daily observations based on the last available entry 
second we interact and pairwise interact all features these additional features are used in the polynomial specification of regularized regressions 
with the cleaned data set we construct two distinct samples that emphasize different aspects of the data 
the complete sample retains the longest possible time horizon by including series that are available between 2004 and 2017 
the post crisis finally we split each of our two samples into test vs training sets in two different ways 
in both cases we arrive at a test set of 400 observations which is about a year and half in calendar days 
the contiguous split uses as test set the last 200 observations and the middle 200 observations in the post crisis period this method emphasizes the time series nature of our outcome variable 
the random split uses as test set 400 randomly chosen observations in the post crisis period which reflects more of a cross sectional test of the basis predictions ultimately each of our ml models is applied to 8 distinct sample split combination for each of the aud and jpy bases we have either the complete or the post crisis sample and within each sample we split train vs test using either a contiguous or a random approach 
regularized linear regressions extend the ordinary least squares regression algorithm by allowing regularizations on the fitted model parameters 
regularization prevents the linear regression from overfitting especially when a large set of features are present 
regularized regression
any regularized linear regression solves the following optimization program where is a hyperparameter that determines the strength of regularization and determines the specific regularization function employed setting 0 corresponds to regularizating l 1 norm which is also known as lasso regression 
this regularization will set less relevant s to zero thereby achieving model parsimony 
an 1 corresponds to using l 2 regularization or ridge regression 
it shrinks the coefficients of variables that are highly correlated 
both of these algorithms also have a bayesian interpretation with lasso corresponding to a laplace prior and ridge to a normal prior over the regression coefficients 
finally we also consider an elastic net algorithm with 1 2 
this trades off the two previous regularization methods in addition to linear features we also consider a second degree polynomial of the features in these regularized regressions in order to capture non linearities 
all features and the y are standardized when training the algorithm in order for the regularization to work as intended 
we implement all algorithms in the statistical software r
an algorithm might need to capture higher dimensional feature interactions in order to predict well 
a regression tree allows for considering such non linear interactions among features 
regression trees
in each step the tree splits the data in one node the parent node into two subsets children nodes based on the value of one feature 
the splitting rule is chosen to minimize the purity of the children nodes 
the algorithm stops once the purity of the children nodes does not improve over the purity of the parent node 
the prediction in each node j that contains m j training examples is j and the purity of a node d j is calculated with the deviance measure we implement the regression tree algorithm using the r package tree
regression tree algorithms can exhibit high variance 
this problem can be remedied using a random forest 
the random forest uses bootstrap to grow multiple trees and returns the average prediction of those trees as its final prediction 
it can be shown that the variance of a random forest containing n trees each with variance 2 and correlation is hence the overall variance can be decreased by choosing a high number of trees n we choose to grow 2000 and decorrelating the trees to achieve a low 
the forest decorrelates its trees in two ways 
first each tree is grown out of a bootstrapped sample which is different for each tree 
moreover at each node the algorithm only considers splitting on a random sub sample of all available features 
the size of this sub sample is mtry and is a hyperparameter that we will tune 
each tree in the forest grows until it reaches a minimum number of terminal nodes leaves and that is set to five in our case 
these measures contribute to less correlated trees and a lower overall variance of the random forest 
we implement the random forest algorithm using the r package ranger
before presenting the ml results we discuss the tuning of two key hyper parameters in regularized regressions and mtry in random forest across lasso ridge and elastic net we tune the strength of our regularization penalty via the one standard error approach 
that is the glmnet package supplies 100 s we choose the optimal as the value that s one standard error away from the that minimizes the cross validation error so as to reflect estimation errors inherent in calculating mses random forest has many degrees of freedom 
results and discussions
we focus on optimizing mtry or the number of randomly chosen features a node could split on 
splitting only on a subset of features at each node reduces correlation among trees and drives down variance of the overall model 
for each of the 8 random forests one on each sample split we ran ranger with mtry 5 6 7 15 
the heuristic is to set mtry equal to the square root of feature dimension which would be 7 or 8 depending on the sample split 
we choose the optimal mtry to be the number that minimizes the out of bag mse in the training set 
our final mtry s include four 12 s four 14 s one 10 and one 15 
we set the other parameters of the forest to 2000 trees and minimum of 5 leaves per node the performance of all eight of our ml models on the four contiguous splits are summarized in the mse on the random splits are not shown due to space constraint 
the performance on the random splits are stunningly good the random forests generate mses on the test set of between 10 to 20 which is incredibly small compared to the 10 to 25 standard deviation of the outcome variables 
however we embrace this success with reservation as it is difficult to interpret randomly selected observations with imputed values focusing our analysis on the performance on the contiguous splits we highlight three takeaways 
first random forests achieve strong prediction performance 
forests not only have the lowest mse in test sets in all but one sample but do so with a substantial margin 
comparing to regularized regressions forests allow non linear effects and compare to regression trees forests lower variance by bagging and splitting on only a subset of features at each node 
the superior performance of forests suggest that these are two important considerations we plot in second regularized regressions are informative about bias vs variance in the prediction 
looking at the mses in the training vs test sets across the linear vs polynomial specifications of regularized regressions we note that the prediction error in the aud basis is likely caused by a bias problem as the mse decreases with the inclusion of the higher dimensional features in both the training and the test set 
in contrast the results in jpy basis indicates a variance problem i e 
overfitting as the polynomial improves the training error but increases the test error finally performance differ dramatically in the middle vs end test blocks 
in results not shown due to space constraint we note that all models have respectable performance on the test block taken from the middle of the post crisis period 
yet most models struggle with predictions in the last 200 observations 
one potential reason is that outcome variables in this period exhibit patterns that have hitherto not been observed low variance elevated level and are thus difficult to predict via a supervised learning algorithm 
violations of the covered interest rate parity condition are important phenomena in the global foreign exchange market and a better understanding of the cross currency basis can have profound implications on the theory of asset pricing 
in this project we take a step toward this understanding by predicting bases using machine learning techniques 
we find that random forests achieve fairly good predictions as measured by mse on test sets that encompass two separate blocks of observations in the post crisis period 
this performance likely owes to random forest s ability to flexibly introduce non linear feature effects and strike a balance between bias and variance minimization in the future we would collect more economic features and use higher order polynomial features to improve the regularized linear regressions of aud basis given the observed bias issue with the aud data 
we will expand the set of algorithms employed to improve the performance on the jpy data as most algorithms seem to suffer from a variance problem 
specifically we will apply the boosting technique and we will consider training a neural network overall we are encouraged to see that we found models that perform reasonably well 
importantly the features selected as important by our various models are intuitive and sensible 
we hope to more closely examine the contribution of these features in the future and extend this analysis to a larger set of currency bases 
all tasks were performed by amy and stefan in equal parts 
there are several studies evaluating the impact of increase in government spending for specific sectors in quality indexes for example the cited works by baldacci et al sutherland et al and gupta et al 
in the first example different variations of the least mean squared error regression model are tested as well as a covariance structure model based on latent variables 
the models are used to find the relation between public spending and quality indexes 
in all the studies there was statistical evidence of a correlation between increase in spending and increase in the indexes 
the present project aims to explore this relation to create a tool for budget planning 
the target variable as explained previously is the ideb score of each school 
this data can be found on the page http ideb inep gov br 
it contains the evolution of the in brazilian public schools for the years of 2013 2015 and 2017 
this index combines the scores of students in a national mandatory exam with data provided by each school describing rate approvals to assess the quality of basic education in public schools 
in the year of 2017 the goals for the 2019 and 2021 were established the second data source is the brazilian school census of 2013 http portal inep gov br microdados that has survey data on every public school in brazil 
this dataset contains information describing many aspects of the infrastructure of the school the qualification of teachers and the profile of students 
some examples of features include total number of students number of professors by level and area of education number of laboratories computers and offices number of students per race number of classes per subject total amount of time spent by students with extracurricular activities in total after the preparation of data the dataset includes 353 features per school 
most of them are count variables as the number of professors from each educational background number of different equipment etc 
a big part of transforming the data included counting different categories in categorical variables 
for example there is one entry in the original data for each student and teacher in the school in the final dataset there is only counts for the number of male female students mathematics biology chemistry teachers etc the last data source is the website https www fazenda sp gov br 
it has data detailing all of the disbursement made by the state government of sao paulo since 2010 
the database contains information on the targeted sector education health transportation etc 
the subarea primary secondary higher education etc 
as well as a more detailed classification of the purpose of the spending scholarship for poor students construction of new schools purchase of food or transportation for students etc 
some of the expenditure categories include transportation for students food for students and workers
the final project includes three different models that process data in different phases 
first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 
the purpose of this model is to reduce the number of features from the census considered in the next phase 
only the most important variables detected by the algorithm in this phase continue in the dataset 
the final set of variables have an accumulated feature importance of 0 99 in the model 
in this phase 3 different algorithms are tested svm gradient boosted trees gbt and ridge regression 
for the gbt two different implementations are evaluated scikit learn and lightgbm 
the evaluation metric chosen is the r defined as the model chosen is the one that presents greatest r in the test set 638 schools 
after the selection of variables 129 features continued to the next phase the clustering algorithm 
this second model uses the descriptive variables to separate schools in groups with similar needs 
since the final goal of the project is to define the budget and its optimal distribution for each school there is the need to isolate the effect of other variables not related to expenditure that are correlated to the ideb 
this is the purpose of the second stage in the data processing framework the evaluation metric for the clusters is the mean silhouette coefficient 
for one sample in the train set the silhouette is given by where a is the mean intra cluster euclidean distance to the considered point and b is the euclidean distance to the nearest point in other cluster 
two different approaches are tested both of them use k means as the main algorithm 
in one of them however the original data is first transformed with principal component analysis pca in order to reduce the dimensionality of the dataset 
the model used data from 9837 schools this phase did not consider only schools administered by the state government but also those ran by the federal and city governments the final phase is a combination of multiple classifiers one for each cluster 
each model predicts whether the school achieved its goal for the 2017 ideb 
the input variables are the expenditure data for each school there are 3152 schools and 711 features 
the assumption in this phase is that after isolating the effects of descriptive variables in the ideb it is possible to find an expenditure distribution that will minimize the total sum of investments per school while allowing it to achieve its goal 
this distribution will be equal for all schools in the same cluster the evaluation metric is the f1 score that combines both precision and recall in order to guarantee that the model do not present good performance only for the most common class 
the f1 score is given by in this phase only one algorithm was implemented derived from the first part the gbt implementation in scikit learn 
the final tool can be applied in the estimation of the budget for each school in the chosen approach first all the schools that achieved their goals and for which the model presented correct predictions are selected 
the initial budget estimate for each category is the minimum value greater than 0 if there is one found for that category in this group of schools 
if the model predicts success in goal achievement with this expenditure distribution it is considered as the final budget if the model predicts fail in goal achievement one of the categories is chosen to be increased 
the probability of selecting a specific category is equal to the normalized feature importance of the variable that represents this category according to the final model 
the budget for the selected category than assumes the value of the second lowest expenditure for this category in the selected subset of schools 
this process is repeated until the model predicts success in goal achievement 
if a specific category achieves its maximum possible value its probability of being selected in the following iterations goes to 0 
the results for the each model tested is in
regression
the 129 most important features in the previous model are then used in the clustering algorithm to separate schools in groups 
the results of the two models tested are in table 2 
the number of clusters varied from 4 to 20 and in the final model consisted of 10 clusters in a tradeoff between increase in the silhouette and guaranteeing a reasonable number of schools in each cluster 
still some clusters ended up with few schools to the minimum of one 
these consist of outliers and these clusters did not enter in the next phase 
silhouette k means 0 767
although the silhouette for the model with pca preprocessing was higher both algorithms presented a very similar result with clusters almost identical 
the output of both cases were tested in the classification phase and the clusters from the model with pca presented a better weighted average f1 score for the classifiers 
pca k means 0 805
for this reason this was the selected model 
as expected by separating schools into clusters the performance of the classifiers increase 
this means that it is easier for the model to find patterns in expenditure data when schools with similar descriptive features are grouped together and isolated from other groups 
this supports the initial hypothesis 
however it is also possible to observe that some clusters as number 0 presented a test f1 score lower than the model with all schools together 
this might be an indication that this cluster is not homogeneous in terms of characteristics that might affect the ideb 
in addition clusters as number 5 had problems with overfitting due to the small sample of schools it represents 
cluster number 3 had excellent performance which indicates that this cluster is homogeneous and that it is possible to find a common expenditure distribution for these schools that will allow them to achieve their goals 
for this cluster the method of budget estimation described previously was implemented 
the prediction for the initial budget estimation minimum values for each category was 1 therefore there was no need to iteratively search for the expenditure distribution applying the minimum estimated budget for each school there is a reduction of r 314 972 841 00 in the total spending of the government of sao paulo with the schools in cluster 3 
in addition according to the model all schools would have achieved their goals using the estimated expenditure distribution while with the current budget approximately 30 of schools in this cluster did not achieve their goals 
the usefulness of the tool developed in this study depends heavily on the quality of the clustering algorithm 
for the initial assumption about the clusters to hold all the relevant factors associated with the schools that do not relate to their spending and that affect the ideb must be represented in the clustering variables 
when this is the case the separation of schools in groups will be able to isolate the effect of this variables and all variation observed in the ideb will be explained solely by differences in the expenditure distribution in the present project the assumption was valid for some clusters mainly number 3 
for this cluster it was possible to create a good predictor of goal achievement only with expenditure features 
when this condition is present this tool can be very useful in minimizing the budget of the schools while guaranteeing they will achieve their goals however other clusters mainly number 0 are too heterogeneous to have the ideb explained only with spending data 
it means that to create a good predictor for goal achievement more variables are needed 
therefore for this clusters it is not possible to explain goal achievement only as a function of spending distribution to solve the problem described above the first step would be to incorporate new variables in the clustering phase 
for example sociodemographic variables of the region where the school is located are probably highly correlated with its ideb also 
features as the average income of residents average number of people in one house and distance from the center of the city are not present in the school census data used in the first two phases of the project the brazilian census have this type of sociodemographic data 
however in this dataset locations are described as sectors and each sector has its own code 
the problem when linking this dataset to the school census is that the last one does not have information on the code of the sector where the school is 
this needs to be solved in order to include data from the brazilian census in the clustering algorithm a second point of improvement is aggregating redundant categories of spending 
this problem was detailed previously 
because of these redundancies it might be difficult for the last model to identify the real impact of each subarea of investment on the ideb other limitation also explained previously is that the data provided by the government of sao paulo does not have detailed spending for each school 
there certainly is in the government database this type of data however it is not open to the public finally this project did not explicitly try to find a causal relation between the input features and the target variable which is a necessary step in the design of public policies 
a qualitative evaluation of the importance of the features in the first and third models as well as the impact they have on the target variable needs to be conducted 
this would be better performed with the assistance of specialists in the area this tool however is a good starting point for the government to explore quantitative tools in the design of public policies 
in a real implementation there would be an evaluation period when the tool would suggest the expenditure distribution and after its implementation the results would be reevaluated and incorporated in the model 
pricing a rental property on airbnb is a challenging task for the owner as it determines the number of customers for the place 
on the other hand customers have to evaluate an offered price with minimal knowledge of an optimal value for the property 
final report 1 introduction
this project aims to develop a price prediction model using a range of methods from linear regression to tree based models support vector regression svr k means clustering kmc and neural networks nns to tackle this challenge 
features of the rentals owner characteristics and the customer reviews will be used to predict the price of the listing 
existing literature shows that some studies focus on non shared property purchase or rental price predictions 
in a cs229 project yu and wu this project has tried to further the experimented methods from the literature by focusing on a variety of feature selection techniques implementing neural networks and leveraging the customer reviews through sentiment analysis 
the authors were unable to find the last two mentioned undertakings in the existing literature 
the main data source for this study is the public airbnb dataset for new york city 1 
the dataset includes 50 221 entries each with 96 features 
the reviews for each listing were analyzed using textblob 2 sentiment analysis module 
this method assigns a score between 1 and 1 to each review and the scores are averaged across each listing 
sentiment analysis on the reviews
the final scores for each listing was included as a new feature in the model 
after data preprocessing the feature vector contained 764 elements which was deemed excessive and when fed to models resulted in a high variance of error 
consequently several feature selection techniques were used to find the features with the most predictive values to both reduce the model variances and reduce the computation time 
based on prior experience with housing price estimation the first effort was manual selection of features to create a baseline for the selection process the second method was tuning the coefficient of linear regression model with lasso regularization trained on the train split from which the model with the best performance over validation split was selected 
second set of features consisted of 78 features with non zero values based on this method finally lowest p values of regular linear regression model trained on train split were used to choose the third set of features 
selection was bound by the total number of features to remain less than 100 
the final set of features were those for which linear regression model performed best on validation split the performance of manually selected features as well as p value and lasso feature selection schemes were compared using the r 2 score of the linear regression models trained on the validation set 
all models outperformed the baseline model which used the whole feature set and the second method lasso regularization yielded the highest r 2 score 
linear regression was set as a baseline model on the dataset using all of the features as model inputs 
after selecting a set of features using lasso feature selection several machine learning models were considered in order to find the optimal one 
all of the models except neural networks were implemented using scikit learn library
linear regression with l 2 regularization adds a penalizing term to the squared error cost function in order to help the algorithm converge for linearly separable data and reduce overfitting 
therefore ridge regression minimizes j y x 2 2 2 2 with respect to where x is a design matrix and is a hyperparameter 
since the baseline models were observed to have high variance ridge regression seemed to be an appropriate choice to solve the issue 
in order to capture the non linearity of the data the training examples were split into different clusters using k means clustering on the features and the ridge regression was run on each of the individual clusters 
the data clusters were identified using the following algorithm 
k means clustering with ridge regression
initialize cluster centroids i k randomly repeat assgin each point to a cluster for each centroid calculate the loss function for the assignments and check for convergence until convergence
algorithm 1 k means clustering
in order to model the non linear relationship between the covariates the team employed support vector regression with rbf kernel to identify a linear boundary in a high dimensional feature space 
using the implementation based on libsvm paper m where c 0 0 are given parameters 
this problem can be converted into a dual problem that does not involve x but involves k x z x z instead 
since we are using rbf kernel k x z exp x z 2 2 2 
neural network was used to build a model that combined the input features into high level predictors 
the architecture of the optimized network had 3 fully connected layers 20 neurons in the first hidden layer with relu activation function 5 neurons in the second hidden layer with relu activation function and 1 output neuron with a linear activation function 
since the relationship between the feature vector and price is non linear regression tree seemed like a proper model for this problem 
regression trees split the data points into regions according to the following formula where j is the feature the dataset is split on t is the threshold of the split r p is the parent region and r 1 and r 2 are the child regions 
gradient boost tree ensemble
squared error is used as the loss function since standalone regression trees have low predictive accuracies individually gradient boost tree ensemble was used to increase the models performance 
the idea behind a gradient boost is to improve on a previous iteration of the model by correcting its predictions using another model based on the negative gradient of the loss 
the algorithm for the gradient boosting is the following
initialize f 0 to be a constant model for m 1 number of iterations do for all training examples 
algorithm 2 gradient boosting
mean absolute error mae mean squared error mse and r 2 score were used to evaluate the trained models 
training 39 980 examples and validation 4 998 examples splits were used to choose the best performing models within each category 
experiments and discussion
the test set containing 4 998 examples was used to provide an unbiased estimate of error with the final models trained on both train and validation splits 
results for the final models 3 are provided below 
the outlined models had relatively similar r 2 scores which implicates that lasso feature importance analysis had made the most impact on improving the performance of the models by reducing the variance 
even after the feature selection the resulting input vector was relatively large leaving room for model overfitting 
this explains why gradient boost a tree based model prone to high varianceperformed worse than the rest of the models despite it not performing the worst on the training set 
despite expanding the number of features in the feature vector svr with rbf kernel turned out to be the best performing model with the least mae and mse and the highest r 2 score on both train and test sets figure 2 
rbf feature mapping was able to better model the prices of the apartments which have a non linear relationship with the apartment features 
since regularization is taken into account in the svr optimization problem parameter tuning ensured that the model was not overfitting 
ridge regression neural network k means ridge regression models had similar r 2 scores even though the last two models are more complex than ridge regression 
the architecture complexity of neural network was limited by the insufficient number of training examples for having too many unknown weights 
k means clustering model faced a similar issue since the frequency of some prices was greatly exceeding the frequency of others some clusters received too few training examples and drove down the overall model performance 
this project attempts to come up with the best model for predicting the airbnb prices based on a set of features including property specifications owner information and customer reviews on the listings 
machine learning techniques including linear regression tree based models svr and neural networks along with feature importance analyses are employed to achieve the best results in terms of mean squared error mean absolute error and r 2 score 
the initial experimentation with the baseline model proved that the abundance of features leads to high variance and weak performance of the model on the validation set compared to the training set 
lasso cross validation feature importance analysis reduced the variance and using advanced models such as svr and neural networks resulted in higher r 2 score for both the validation and test sets 
among the models tested support vector regression svr performed the best and produced an r 2 score of 69 and a mse of 0 147 defined on ln price on the test set 
this level of accuracy is a promising outcome given the heterogeneity of the dataset and the involved hidden factors including the personal characteristics of the owners which were impossible to consider the future works on this project can include i studying other feature selection schemes such as random forest feature importance ii further experimentation with neural net architectures and iii getting more training examples from other hospitality services such as vrbo to boost the performance of k means clustering with ridge regression model in particular 
liubov nikolenko data cleaning splitting categorical features implementing sentiment analysis of the reviews initial neural network implementation svr implementation and tuning k means ridge tuning hoormazd rezaei implementation of linear regression and tree ensembles datapreprocessing implementation of the evaluation metrics feature selection methods implementation tuning of the neural network pouya rezazadeh data cleaning and auxiliary visualization splitting categorical features result visualizations tree ensembles tuning k means ridge implementation 
doodle recognition has important consequences in computer vision and pattern recognition especially in relation to the handling of noisy datasets 
in this paper we build a multi class classifier to assign hand drawn doodles from google s online game quick draw 
into 345 unique categories 
to do so we implement and compare multiple variations of k nearest neighbors and a convolutional neural network which achieve 35 accuracy and 60 accuracy respectively 
by evaluating the models performance and learned features we can identify distinct characteristics of the dataset that will prove important for future work 
in november 2016 google released an online game titled quick draw 
that challenges players to draw a given object in under 20 seconds 
however this is no ordinary game while the user is drawing an advanced neural network attempts to guess the category of the object and its predictions evolve as the user adds more and more detail beyond just the scope of quick draw the ability to recognize and classify hand drawn doodles has important implications for the development of artificial intelligence at large 
for example research in computer vision and pattern recognition especially in subfields such as optical character recognition ocr would benefit greatly from the advent of a robust classifier on high noise datasets for the purposes of this project we choose to focus on classification of the finished doodles in their entirety 
while a simpler premise than that of the original game s this task remains difficult due to the large number of categories 345 wide variation of doodles within even a single category and confusing similarity between doodles across multiple categories thus we create a multi class classifier whose input is a quick draw 
doodle and whose output is the predicted category for the depicted object 
similar to our task google engineers ha and eck used the quick draw 
online dataset to train their recurrent neural network rnn to learn sketch abstractions 
kim and saverese experimented with svm and knn performance on image classification specifically on airplanes cars faces and motorbikes 
lu and tran architected a convolutional neural network cnn to tackle sketch classification 
the state of the art as of 2017 comes from a cnn developed by seddati et al 
with their deepsketch 3 model for sketch classification 
google publicly released a quick draw 
dataset containing over 50 million images across 345 categories 
there are multiple different representations for the images 
one dataset represents each drawing as a series of line vectors and another contains each image in a 28x28 grayscale matrix 
because we focus on classification of the entire doodle in this project we use the latter version of the dataset 
we treat each 28x28 pixel image as a 784 dimensional vector 
to test our models we split the data into three different folds 70 for training 15 for validation and 15 for testing 
to reduce computation time and storage of the data we decided to create a smaller subset of the original dataset by randomly sampling 1 of the drawings from each category as a result we obtain approximately 350 000 examples for the training set and 75 000 examples each for the validation and testing set 
furthermore the number of drawings in each category is balanced so this leaves approximately 1000 examples per category in the training dataset 
for our baseline we intuitively assume that all the images in a particular category should look relatively similar 
based on this assumption one way we could determine which category a given drawing belongs to is by looking at which training examples are the most nearby to the doodle under test this intuition corresponds with the k nearest neighbors knn algorithm 
1 closest centroid 1 cc 
the vanilla knn algorithm computes the k training examples that are closest in l 1 or l 2 distance to our current drawing 
then it predicts the category that occurs the greatest number of times among those k neighbors 
however because we have 350k training examples and 75k validation examples this algorithm requires at least 3 5 10 5 7 5 10 4 784 2 10 13 operations to evaluate the entire validation set which is too slow consequently we propose a less computationally expensive variant of knn which we call 1 closest centroid 1 cc 
at a high level 1 cc equivalent to supervised kmeans clustering in which we compute a centroid for each category c using the training dataset and classify test examples according to the closest categories in more detail for each category c we calculate a centroid vector v c by taking the average of all of the vectors belonging to category c then to classify a given vector u we compute arg min c u v c 2 which seeks to minimize the squared difference in pixel values between the two images 
effectively we are choosing the category whose mean representation vector is closest in euclidean distance to our given vector u 
this reduces the number of points we look at for each u to only 345 one per category 
1 cc makes the simplifying assumption that all doodles in a category will be similar to each other 
however in reality there are many different ways to draw a given object 
knn with k means 
for example bear can be drawn with multiple representations as seen in
we noticed that voting in knn often ended up with ties 
to mitigate ties we further extend knn to not only utilize multiple clusters per category but also to use a weighted voting schema when tallying for final predictions 
knn with k means and weighted voting
we name this method knn weighted for short intuitively we wish to count votes from closer centroids more than votes from more distance centroids 
thus we experiment with two different weighting schemas distance weighting 
with distance weighting each cen troid s c s weighted vote w i is equal towhere x i is the vector representation of the test example 
rank weighting 
with rank weighting we first sort all centroids by increasing distance to the test example 
within this sorted order the centroid c i at rank i has a weighted vote equal to
as a comparison against the above knn methods we implement a convolutional neural network cnn a stateof the art model known for being able to recognize and quickly learn local features within an image to achieve the best results we perform data preprocessing 
first we calculate the mean across all training examples as well as the standard deviation 
we then for each example training validation and test subtract and divide by 
to account for division by zero errors when dividing by we add an offset of 10 to beforehand the model architecture is shown in
while raw accuracy is a good measure of a model s performance it penalizes harshly for an incorrect prediction wrong predictions receive 0 points and right predictions receive 1 point 
since we have so many categories including some that are extremely similar such as cake and birthday cake we evaluate our methods not only with raw accuracy but also with a scoring metric that is more lenient of incorrect predictions thus predictions are evaluated using mean average precision 3 map 3 where u is the number of drawings in the test set p k is the precision at cutoff k and n is the number of predictions per drawing 
put more intuitively the equation considers the top 3 predictions p 1 p 2 p 3 that the model makes for a given drawing 
it then assigns a score of 1 i if p i is the correct label for the image and a score of 0 if the correct label is not in the top 3 guesses 
note that map 1 is equivalent to singleprediction accuracy 
as seen in 1 cc performed best on the categories stairs circle and door 
knn performed best on the categories stairs the eiffel tower and bowtie 
1 cc and knn analysis
for these categories the centroids are either simplistic circle door or are distinct in shape stairs the eiffel tower bowtie which causes the doodles to have less variance 
thus the centroids are generally contain a clear outline of the object on the other hand 1 cc performed worst on the categories flip flops garden hose and wrist watch and knn performed worst on dog string bean and peas 
the centroids for these bottom 3 categories are much more vague 
for example dog was often confused with other four legged animals such as horse and cow furthermore some categories produced nearly identical centroids such as circle and octagon in
knn with weighted votes by rank produced the highest map 3 and map 1 scores out of all the knn models 
knn weighted analysis
to achieve the best performance for the cnn model we tuned various hyperparameters including the number of units in each dense layer dropout rate and learning rate 
overall we found that the model producing the best map 3 score on the validation set had three dense layers with 700 500 and 400 units with each layer having a dropout rate of 0 2 
cnn analysis
furthermore we trained our model with learning rate of 1 10 3 and batch size of 32 across 20 epochs 
the end architecture fits the data well as we see from the loss plot in inspecting the accuracy distribution across individual category we note from
we found that our cnn outperformed our extended knn algorithm with map 3 values of 62 1 and 34 4 respectively although both algorithms perform much better than random guessing of 0 5 but lower than human guessing of 73 0 
although knn was able to identify multiple representations of the same category which increased accuracy compared to 1 nn knn still came short compared to our cnn due to its inability to recognize features and distinguish between apples and blueberries due to the presence of a stem for future work we would like to experiment with advanced cnn architectures such as vgg net and resnet which have already reached state of the art levels of image classification performance although not for sketches in particular 
additionally we have only used approximately 1 of the total quick draw 
dataset and we believe training our models on the complete dataset would improve accuracy as well incorporating stroke order information and extract features such as velocity and acceleration 
finally we believe that ensembling techniques are interesting particularly for lighterweight methods such as knn 
