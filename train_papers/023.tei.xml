<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/lopez/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5-SNAPSHOT" ident="GROBID" when="2019-03-19T09:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Machine Learning for Autonomous Jellyfish Orientation Determination and Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Wiktor</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunet</forename><surname>Id</surname></persName>
						</author>
						<title level="a" type="main">Machine Learning for Autonomous Jellyfish Orientation Determination and Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Automated jellyfish classification and orientation determination can be used to improve the robustness of an autonomous jelly tracking system. A set of training images were segmented and augmented with orientation data, and a feature vector was generated for each image. An SVM was then trained to estimate a jelly's orientation. In simulations on unseen test images, the approach was able to correctly determine the orientation of a jellyfish to within ±15 • in 68% of cases. This is improved to within ±15 • in 96% of cases on a training set. The algorithm was successful at categorizing the 3D angle of the jelly out of the image plane, and in certain cases was also able to distinguish between a jelly and a foreign object in the image frame.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Research performed at the Monterey Bay Aquarium Research Institute (MBARI) on gelatinous marine animals (commonly known as Jellyfish, abbreviated Jellies here) frequently requires that observations be taken over a long period of time. This is typically done using a Remotely Operated Vehicle (ROV) driven by a human pilot, but the concentration required to continually deliver precise thruster control can be tiring for the pilot <ref type="bibr" target="#b2">[3]</ref>. A semi-autonomous pilot aid for jelly tracking has been demonstrated to greatly reduce fatigue, allowing observation runs to last as long as 90 minutes <ref type="bibr" target="#b1">[2]</ref>. Farther in the future, a more robust tracking system could eventually allow a fully automated Autonomous Underwater Vehicle (AUV) to follow jellies over many hours or days, collecting valuable data in all different environmental conditions.</p><p>The pilot aid system previously tested at MBARI relies on stereo vision and blob-tracking techniques. To differentiate between jellies and the surrounding ocean, a grayscale image is first thresholded to detect all light-colored blobs <ref type="bibr" target="#b0">[1]</ref>. After filtering the blobs by pixel area to eliminate marine snow, each jelly is tracked by the coordinates of the blob centroid. Basic target recognition is performed by monitoring the apparent size of the blob and its distance from the camera, enabling the system to continue tracking the target in a few cases where other objects entered the frame <ref type="bibr" target="#b0">[1]</ref>. A control law uses this information along with a dynamic model of the vehicle to attempt to maintain the relative position between the ROV and the jelly. However, no attempt has been made to date to account for the dynamics and motion of the jelly itself.</p><p>Past experiments demonstrated that many of the tracking failures occurred either due to the target moving out of the camera frame, or else from a loss of target recognition when other animals pass near the jelly <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b0">[1]</ref>. Machine learning techniques offer the possibility to improve the robustness of the system in both areas. In the first case, machine learning algorithms can estimate the orientation of the jelly in the image, allowing the control law to account for its potential future motion. Jellyfish are able to accelerate, swim, and change orientations, but in general at any given time will move only in an axial direction. Therefore, if a machine learning algorithm is able to supply an orientation estimate, the ROV will be able to follow the path of a jelly even if it leaves the camera frame. The angle of the jelly from the vertical direction is critical to know the direction that the cameras should pan if the target is lost. However, a 3D orientation estimate is also desired because the cameras can lose track of the jelly if it gets too close. Therefore, an angle into or out of the image plane would help predict if the ROV will need to advance or back up. Similarly, machine learning also offers the possibility of improving the target tracking algorithms to reject other objects that enter the camera frame. By matching the features of the target jelly against a library of similar animals (or the same jelly at earlier times), the system would be able to continue following the target even after interferences that ended previous experiments <ref type="bibr" target="#b0">[1]</ref>.</p><p>This project presents several new machine learning techniques to improve the robustness of the jelly tracking system. First, machine learning can be used to improve the segmentation processes used to isolate the target in an image frame. An algorithm to autonomously estimate the 3D orientation of a jelly is also outlined. To demonstrate the technique, a single genus -paraphyllina -was selected due to its clear axial features <ref type="figure" target="#fig_0">(Figure 1</ref>). MBARI supplied a set of several hundred images from past ROV dives showing paraphyllina in various orientations. This database is split into a set of training images and 100 test images, and cross validation is used to demonstrate the applicability of the method to new, unseen jelly images. The algorithm uses several techniques to generate a feature vector for each image, which can then be used in a standard multi-class classification scheme to categorize the 3D orientation of the jelly. Finally, the same feature vectors can be used to distinguish between jelly and non-jelly objects in a frame to isolate the traget and reliably track it in a wide range of environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Image Processing</head><p>MBARI provided a set of several thousand images of jellies taken from previous research dives. After sorting the images and removing any with a target too small or out of focus, 140 usable images of paraphyllina in various orientations remained. This training set was augmented by rotating each image by 90 • , 180 • , and 270 • to provide additional orientations on which to train the algorithm, for a total database of m = 560 images. Orientation data was manually added to each image by drawing an estimate of the jelly's axis on the frame, and an estimated depth angle was entered as either -90 • (jelly pointed directly away from camera), -45 • , 0 • , 45 • , or 90 • (jelly pointed directly towards the camera) ( <ref type="figure" target="#fig_1">Figure 2</ref>). The angle data for each image was then bucketed into 14 categories β (30 • each, plus out-of-frame and into-frame, see <ref type="figure" target="#fig_2">Figure 3</ref>), and stored as a matrix showing the true orientation for the image:</p><formula xml:id="formula_0">Y =         −y (1) T − . . . −y (i) T − . . . −y (m) T         ; y (i) =        1{β i = 1} . . . 1{β i = k} . . . 1{β i = 14}       </formula><p>Two separate image segmentation methods were tested for comparison. First, a threshold-based blob detection algorithm similar to <ref type="bibr" target="#b0">[1]</ref> was implemented in MATLAB. The image was converted to grayscale, and all pixels below an individuallytuned threshold were considered background and discarded. The thresholded image was then passed to a blob detection routine that finds the centroid and area of each light region in the image. All blobs with an area below a preset minimum are rejected to eliminate marine snow, and the largest remaining blob is stored as the jelly.</p><p>This technique works well for a large number of images, such as <ref type="figure" target="#fig_1">Figure 2</ref>. However, by converting the image to grayscale at the start, this method effectively discards twothirds of the usable information in the frame. In certain situations, the brightness of the jelly pixels can be very close to the intensity of the surrounding ocean, and color is actually a much better indicator. For this reason, an alternative segmentation technique based on machine-learning was also tested. In this method, an Expectation-Maximization (EM) algorithm is used on the full-color image to model the pixel colors as being drawn from two separate probability distributions (one corresponding to the jelly, and the other for the ocean). Every pixel color is treated as a point in 3D space, and the algorithm finds a mean and covariance for the two distributions. Finally, each pixel is assigned a label based on the likelihood of it being drawn from either distribution. <ref type="figure" target="#fig_3">Figure 4</ref> shows the red and blue pixel values, along with the two probability distributions found by the EM algorithm. There is still an ambiguity in determining which of the two distributions corresponds to the jelly. This ambiguity can be overcome using prior knowledge, such as the fact that the ocean pixels have more blue content, or the fact that the jelly almost always occupies fewer than half of the pixels in a frame. Finally, once the pixels have been labeled and the background pixels discarded, the same blob tracking method can be used to remove marine snow and isolate the jelly. <ref type="figure" target="#fig_4">Figure 5</ref> shows a comparison of the thresholded and EM-based segmentation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Orientation Classification</head><p>To train a classifier to recognize the orientation of a jelly, each image i must be represented as a feature vector φ i that describes the elements of the image. This feature vector can be generated in several ways. In the first method, the images are first sorted by their orientation category. Several thousand features are then generated for each image using the Red and blue content of all pixels in a sample image. Blue points were categorized by the EM algorithm as "background", and red points were categorized as "jelly". Contour lines for the two probability distributions are also shown.</p><p>Speeded Up Robust Features (SURF) algorithm. However, not all features generated from SURF will be useful for orientation prediction. Therefore, the K-means algorithm is used to reduce the number of features across all categories and find the features most highly associated with the jellies' orientation. The K-means approach reduces the feature set to a 500-element vector for each image.</p><p>Once the feature vectors φ i have been generated by the K-means algorithm, an SVM is trained on the data. Cross validation is used to ensure that the model is not overfitting the training data. The set of 560 images is divided with 80% in a training set and 20% in the test set. Using a one-vsall approach, a hypothesis function h θ k (x (i) ) = θ T k φ(x (i) ) is trained on this training set. The SVM produces a vector θ k for each of the 14 orientation categories. To make a prediction for an image x (i) , the margin for each category k is calculated. The algorithm ultimately selects the category with the highest probability as the most likely orientation:</p><formula xml:id="formula_1">k prediction = argmax k θ T k φ(x (i) ).</formula><p>An alternative approach to image classification relies on neural networks. For comparison with the procedure outlined above, a neural network was trained using MATLAB's neural network toolbox on the thresholded images (with no orientation data). This neural network generated a 50-element feature vector φ i describing each image. A softmax function then uses this set of features Φ along with the orientation data Y to make a classification prediction. To improve the features generated for each image, the neural network can then finetuned with a second pass using the prediction errors from the first softmax function. Similar to the approach above, the most likely orientation category is returned.</p><p>Either method outlined above can also be extended to other aspects of the jelly tracking system. For example, when a second object enters the image frame, the algorithm can estimate which of the two is a jelly by attempting to predict an orientation for both. The algorithm will have higher confidence for the object with features most similar to the training images. Therefore, this single classifier can serve as a rudimentary approach to differentiating between multiple objects in a frame simply by comparing the orientation probabilities for each. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Image Processing</head><p>The EM-based image segmentation technique was highly successful at differentiating between the jelly and the surrounding ocean. <ref type="figure" target="#fig_4">Figure 5</ref> shows an example where thresholding removed a significant portion of the jelly features, and still failed to completely remove the background. The EM approach, on the other hand, removed all of the background without significantly affecting the jelly. The downside is that the EM algorithm required significantly more time to run since it had to perform up to 20 iterations on thousands of pixels. However, this would not necessarily be a significant problem in practice since the color distribution does not change significantly from one frame to the next. The EM algorithm could therefore be run only once every few seconds to generate new estimates for the mean and covariance, and simply use the previous values for each frame in between.</p><p>Other image processing techniques also helped increase the classification accuracy. As <ref type="figure">Figure 6</ref> shows, the strongest features generated by the SURF algorithm are on the tentacles of the jelly and not on its body. However, these tend to be poor predictors of orientation since the tentacles can point in arbitrary directions. Removing the tentacles from the image using MATLAB's imerode() and imdilate() commands on the segmentation mask increased the accuracy of the classifier by roughly 10%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Orientation Classification</head><p>The K-means approach was successful at categorizing the jelly orientation for most images. When applied to the training data, the algorithm categorized the orientation correctly for 96% of the images. The classifier also performed well on the test set, correctly classifying 68% of the images. More importantly still, even in situations when the classifier made an error, it tended not to predict orientations too far from the correct class. In many cases, determining the exact orientation of the jelly is less important than knowing the quadrant in which it is likely to be found if it leaves the frame. Therefore, a jelly that the classifier labels as being in an adjacent category may also be considered successful. For example, if the true jelly orientation is in the −15 • to 15 • category, but the classifier labels it as 16 • to 45 • , the camera still knows the approximate direction in which it should pan, so the label may be considered a partial success. By this measure, the classifier was successful on 99.7% of the training images and 88% of the test images.</p><p>The 3D category prediction was also relatively successful, although it had significantly fewer images on which to train since it wasn't possible to increase the data set by rotating these images. The out-of-plane angle is difficult to measure even for a human observer, so the 3D classifier focused simply on categorizing jellies pointed almost directly into or out of the plane (such as <ref type="figure" target="#fig_4">Figure 5</ref>) rather than trying to estimate an exact angle. This is still useful information for the control system, though, since the ROV may need to move to the side in situations where the jelly is pointed directly towards it to avoid making contact and disturbing the jelly. Using this measure, the classifier was successful at categorizing the 3D angle for 94% of images. Note that in this case the data set was too small to split into a complete training and test set, so the accuracy is only reported for the training set.</p><p>Extending the K-means/SVM classifier to other problems was somewhat successful. In test cases where the orientation SVM was used to differentiate between a jelly and a nonjelly object (for example, a piece of seaweed), the prediction confidence was significantly higher for the jelly, so the method  would be able to reject the other object. However, when comparing an image of paraphyllina with some other type of jelly, the classifier's confidence in the orientation prediction was a poor indicator of which jelly to follow. Many other factors, such as lighting, view angle, and distance, tended to have more of an effect on margin than the type of jelly. Therefore, while simply comparing the confidence in an orientation prediction might have some value for rejecting foreign objects, a second classifier trained to distinguish between the target jelly and alternate marine animals would be more effective</p><p>The algorithm was also able to classify the orientation using the alternative neural network approach. In general, though, this did not perform as well as the previous method. A neural network trained with a softmax function and finetuned with back propagation determined the correct labeling for 80% of the training set, but only 40% of the test set. Using the within-one-category criteria, the neural network was successful at labelling 89% of the training set and 71% of the test set. Besides being less accurate than the K-means approach, the neural network also has the disadvantage that it does not generate a useful feature vector and instead operates as a "black box" that simply inputs an image and returns a predicted class. Attempts to train a classifier using a feature vector output by the neural network were not successful. An SVM trained on these feature vectors only achieved 20% accuracy. This is better than random chance (8%), but it is not accurate enough for practical classification. This limits the usefulness of the approach since it can no longer be extended to other aspects of the jelly tracking problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. FUTURE WORK</head><p>The main task remaining is to fully implement the algorithm to classify jelly orientations from video frames in real time. This is necessary before the approach can be tested on an ROV for use in a control architecture. The image encoder can be stored, so it does not need to re-run the full k-means analysis for each new image. The trained SVM can also be stored, which should speed up the process enough to take place in near real time. While further improvements to the test image classifier accuracy are desirable, the current architecture offers enough accuracy to deal with the relatively rare failure scenarios in the existing control system.</p><p>There are several avenues that could also be explored to improve the current classification scheme. For example, the classifier could be extended to take previous orientations into account. Since jellies usually do not move very quickly, cases with low prediction confidence could be improved by considering a previous estimate with higher margin and using that data to guide the prediction. Another approach would be to attempt to directly model the orientation of a jelly from the feature set without reducing it into discrete buckets. This has the advantage of allowing for a loss function that heavily penalizes orientation estimates farther from the true orientation. Following this approach could potentially improve the number of images correctly classified in the "within-onecategory" region.</p><p>Finally, a second classifier could be implemented to help distinguish between a target jelly and other objects that may enter the frame. This will require modifying the blob tracking algorithm to follow multiple objects, but otherwise the approach is identical to the orientation classification technique demonstrated here. Once a specific type of jelly has been targeted, a classifier trained to recognize that genus could be used to reject all other animals that enter the frame. Using this approach, a single routine will ultimately be able to take in a new image, classify it as target/other, estimate the 3D orientation of the jelly, and then pass all this information to the controller to enable a highly robust tracking system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>Using a training set of images, two methods for segmenting the image to isolate the jelly have been implemented. A grayscale threshold-based approach was functional for a majority of the images, but an approach that uses the EM algorithm to estimate the mean and covariance of the color distributions was demonstrated to be more effective. This method had a slower run time, but had the advantage of being able to distinguish between the target and the background for darker images where thresholding failed.</p><p>Machine learning techniques were also effective for estimating the orientation of a jelly from an image. By training an SVM on features generated using SURF and K-means algorithms, the approach was able to correctly classify the orientation for 96% of training images and 68% of test images. The 3D angle out of the plane was also included in the data, and the classifier successfully distinguished between jellies pointed towards, away, and normal to the camera. Finally, the classifier was able to roughly distinguish between jellies and other objects using the confidence in its orientation predictions. A neural network approach was also implemented, but it was significantly less accurate than this method and therefore will not be pursued further.</p><p>Once implemented on an ROV, these improvements will make a significant difference to the robustness of the jelly tracking system. The new image segmentation technique could allow an existing control architecture to get a more accurate estimate of the position and shape of the jelly, allowing for better tracking as outlined in <ref type="bibr" target="#b1">[2]</ref>. An orientation estimate generated using this algorithm will then allow for a more advanced control law that can take the jelly's current and projected motion into account. Finally, a method to distinguish between the target and other objects would also fix the failure modes that ended several of the previous tracking trials. All together, these improvements could dramatically increase the observation time of the jelly tracking system and provide unprecedented data for marine scientists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ACKNOWLEDGMENTS</head><p>This work would not have been possible without the assistance and training images from the Monterey Bay Aquarium Research Institute. Prof. Stephen Rock (Stanford University Department of Aeronautics and Astronautics) and the members of the Aerospace Robotics Lab also contributed invaluable advice and feedback.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Sample paraphyllina image (MBARI).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Jelly orientation. A manual estimate of the jelly's orientation is shown in red. The estimated orientation for this example is 44 • (category 2) with a depth of 0 • .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>The orientation is bucketed into one of 14 categories, 1-12 for the in-plane angle from vertical, and 13-14 for the 3D out-of-plane angle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Red and blue content of all pixels in a sample image. Blue points were categorized by the EM algorithm as "background", and red points were categorized as "jelly". Contour lines for the two probability distributions are also shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Image segmentation methods. (a) Raw image. (b) Jelly segmented using a grayscale threshold. (c) Jelly segmented using EM-based technique.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>strongest features found by the SURF algorithm. (a) Jelly image with tentacles. (b) Jelly image with tentacles removed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Training images confusion matrix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Test images confusion matrix.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segmentation methods for visual tracking of deep-ocean jellyfish using a conventional camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Rife</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Rock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE JOURNAL OF OCEANIC ENGINEERING</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Design and validation of a robotic control law for observation of deep-ocean jellyfish</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Rife</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Rock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A pilot-aid for rov based tracking of gelatinous animals in the midwater</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Rife</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Rocke</surname></persName>
		</author>
		<idno type="doi">DOI10.1109/OCEANS.2001.968274</idno>
	</analytic>
	<monogr>
		<title level="m">OCEANS, 2001. MTS/IEEE Conference and Exhibition</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1137" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
