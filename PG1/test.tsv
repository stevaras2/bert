Label	id1	id2	features	sentence
0	109501	9501	we see that the baseline model works well for simple queries like single word object names .first sentence.however for longer captions as in.gru cnn	we see that the baseline model works well for simple queries like single word object names 
0	109502	9502	however for longer captions as in.we see that the baseline model works well for simple queries like single word object names .last sentence.gru cnn	however for longer captions as in
0	109503	9503	in this project our emphasis is more on language models because as a first step we would like to accurately identify the semantics implied by the query .first sentence.on the image side we only represent each by its features extracted from a pretrained network .future work	in this project our emphasis is more on language models because as a first step we would like to accurately identify the semantics implied by the query 
0	109504	9504	on the image side we only represent each by its features extracted from a pretrained network .in this project our emphasis is more on language models because as a first step we would like to accurately identify the semantics implied by the query .although we see the image feature is able to capture small details in the image it can still be the bottleneck as our language model becomes more sophisticated .future work	on the image side we only represent each by its features extracted from a pretrained network 
0	109505	9505	although we see the image feature is able to capture small details in the image it can still be the bottleneck as our language model becomes more sophisticated .on the image side we only represent each by its features extracted from a pretrained network .in the future we would like to endow a dynamic attention mechanism so that the model will be able to choose adaptively the region s to focus on in the image .future work	although we see the image feature is able to capture small details in the image it can still be the bottleneck as our language model becomes more sophisticated 
0	109506	9506	in the future we would like to endow a dynamic attention mechanism so that the model will be able to choose adaptively the region s to focus on in the image .although we see the image feature is able to capture small details in the image it can still be the bottleneck as our language model becomes more sophisticated .this might be done either by including some pretrained features in the lower layers or by computing features on sub regions of the image .future work	in the future we would like to endow a dynamic attention mechanism so that the model will be able to choose adaptively the region s to focus on in the image 
0	109507	9507	this might be done either by including some pretrained features in the lower layers or by computing features on sub regions of the image .in the future we would like to endow a dynamic attention mechanism so that the model will be able to choose adaptively the region s to focus on in the image .there are some initial attemps in this direction such as link to the code https github com giacomolamberti90 cs229 project.future work	this might be done either by including some pretrained features in the lower layers or by computing features on sub regions of the image 
0	109508	9508	there are some initial attemps in this direction such as link to the code https github com giacomolamberti90 cs229 project.this might be done either by including some pretrained features in the lower layers or by computing features on sub regions of the image .last sentence.future work	there are some initial attemps in this direction such as link to the code https github com giacomolamberti90 cs229 project
1	109509	9509	activity recognition is an important task in several healthcare applications .first sentence.by continuously monitoring and analyzing user activity it is possible to provide automated recommendations to both patients and doctors.i introduction	activity recognition is an important task in several healthcare applications 
0	109510	9510	by continuously monitoring and analyzing user activity it is possible to provide automated recommendations to both patients and doctors.activity recognition is an important task in several healthcare applications .last sentence.i introduction	by continuously monitoring and analyzing user activity it is possible to provide automated recommendations to both patients and doctors
1	109511	9511	because of its many applications supervised human activity classification using sensor data is a relatively popular research area .first sentence.through our research we have found that related articles and their approaches can generally be divided into three categories naive bayes classification svm decision trees and neural networks .ii related work	because of its many applications supervised human activity classification using sensor data is a relatively popular research area 
1	109512	9512	through our research we have found that related articles and their approaches can generally be divided into three categories naive bayes classification svm decision trees and neural networks .because of its many applications supervised human activity classification using sensor data is a relatively popular research area .we consider the use of naive bayes as a classifier for human activity as clever and interesting since it is usually used for text classification .ii related work	through our research we have found that related articles and their approaches can generally be divided into three categories naive bayes classification svm decision trees and neural networks 
1	109513	9513	we consider the use of naive bayes as a classifier for human activity as clever and interesting since it is usually used for text classification .through our research we have found that related articles and their approaches can generally be divided into three categories naive bayes classification svm decision trees and neural networks .one such article that uses the naive bayes classifier is long yin and aarts 2009.ii related work	we consider the use of naive bayes as a classifier for human activity as clever and interesting since it is usually used for text classification 
0	109514	9514	one such article that uses the naive bayes classifier is long yin and aarts 2009.we consider the use of naive bayes as a classifier for human activity as clever and interesting since it is usually used for text classification .last sentence.ii related work	one such article that uses the naive bayes classifier is long yin and aarts 2009
0	109515	9515	we used the pamap2 dataset from the uci repository of machine learning datasets .first sentence.last sentence.iii dataset and features	we used the pamap2 dataset from the uci repository of machine learning datasets 
1	109516	9516	as a baseline measure we incorporated a standard logistic regression model with a loss function of the form shown in 1 .first sentence.to make the model more robust against overfitting l2 regularization was incorporated with the c parameter inversely related to the strength of regularization .a regression	as a baseline measure we incorporated a standard logistic regression model with a loss function of the form shown in 1 
1	109517	9517	to make the model more robust against overfitting l2 regularization was incorporated with the c parameter inversely related to the strength of regularization .as a baseline measure we incorporated a standard logistic regression model with a loss function of the form shown in 1 .in order to pick the optimal value for c the model was independently trained over a range of c values .a regression	to make the model more robust against overfitting l2 regularization was incorporated with the c parameter inversely related to the strength of regularization 
1	109518	9518	in order to pick the optimal value for c the model was independently trained over a range of c values .to make the model more robust against overfitting l2 regularization was incorporated with the c parameter inversely related to the strength of regularization .the c value that produced the highest accuracy on the validation set was selected and tested on the test set for each training phase 5 fold cross validation was incorporated in order to reduce the variance in a trained model .a regression	in order to pick the optimal value for c the model was independently trained over a range of c values 
1	109519	9519	the c value that produced the highest accuracy on the validation set was selected and tested on the test set for each training phase 5 fold cross validation was incorporated in order to reduce the variance in a trained model .in order to pick the optimal value for c the model was independently trained over a range of c values .stochastic average gradient sag descent was selected as the solver to use for training as it generally provides fast convergence for large feature sets such as ours.a regression	the c value that produced the highest accuracy on the validation set was selected and tested on the test set for each training phase 5 fold cross validation was incorporated in order to reduce the variance in a trained model 
1	109520	9520	stochastic average gradient sag descent was selected as the solver to use for training as it generally provides fast convergence for large feature sets such as ours.the c value that produced the highest accuracy on the validation set was selected and tested on the test set for each training phase 5 fold cross validation was incorporated in order to reduce the variance in a trained model .last sentence.a regression	stochastic average gradient sag descent was selected as the solver to use for training as it generally provides fast convergence for large feature sets such as ours
1	109521	9521	we wanted to also create a non linear classifier in the hopes that it can outperform the linear logistic regression model so a svm was a natural choice as it is fairly easy to implement with few parameters to tune .first sentence.we created our svm s by solving the following primal problem and the decision function is defined as through training an svm model can create multiple hyperplanes to split the training set into its labeled categories .b support vector machine	we wanted to also create a non linear classifier in the hopes that it can outperform the linear logistic regression model so a svm was a natural choice as it is fairly easy to implement with few parameters to tune 
1	109522	9522	we created our svm s by solving the following primal problem and the decision function is defined as through training an svm model can create multiple hyperplanes to split the training set into its labeled categories .we wanted to also create a non linear classifier in the hopes that it can outperform the linear logistic regression model so a svm was a natural choice as it is fairly easy to implement with few parameters to tune .this is done through the use of kernels that transform the input data into a higher dimension so that the data can then be linearly separated .b support vector machine	we created our svm s by solving the following primal problem and the decision function is defined as through training an svm model can create multiple hyperplanes to split the training set into its labeled categories 
1	109523	9523	this is done through the use of kernels that transform the input data into a higher dimension so that the data can then be linearly separated .we created our svm s by solving the following primal problem and the decision function is defined as through training an svm model can create multiple hyperplanes to split the training set into its labeled categories .part of the advantage for svm s is that only a fraction represented by n in eqn 3 of the original training set has to be retrained for creating the hyperplane during predictions .b support vector machine	this is done through the use of kernels that transform the input data into a higher dimension so that the data can then be linearly separated 
1	109524	9524	part of the advantage for svm s is that only a fraction represented by n in eqn 3 of the original training set has to be retrained for creating the hyperplane during predictions .this is done through the use of kernels that transform the input data into a higher dimension so that the data can then be linearly separated .another advantage is that various kernel functions can easily be tested and selected for the one that best fits a particular application .b support vector machine	part of the advantage for svm s is that only a fraction represented by n in eqn 3 of the original training set has to be retrained for creating the hyperplane during predictions 
1	109525	9525	another advantage is that various kernel functions can easily be tested and selected for the one that best fits a particular application .part of the advantage for svm s is that only a fraction represented by n in eqn 3 of the original training set has to be retrained for creating the hyperplane during predictions .thus to select the optimal kernel function along with the c parameter specifying the softmargin size a grid search was performed over three standard kernel functions polynomial rbf and linear with a range of c values for each .b support vector machine	another advantage is that various kernel functions can easily be tested and selected for the one that best fits a particular application 
1	109526	9526	thus to select the optimal kernel function along with the c parameter specifying the softmargin size a grid search was performed over three standard kernel functions polynomial rbf and linear with a range of c values for each .another advantage is that various kernel functions can easily be tested and selected for the one that best fits a particular application .last sentence.b support vector machine	thus to select the optimal kernel function along with the c parameter specifying the softmargin size a grid search was performed over three standard kernel functions polynomial rbf and linear with a range of c values for each 
1	109527	9527	based on our literature review we believed that deep learning techniques should work well for this classification problem .first sentence.we therefore implemented a multilayer perceptron architecture for multi class classification .c deep learning	based on our literature review we believed that deep learning techniques should work well for this classification problem 
1	109528	9528	we therefore implemented a multilayer perceptron architecture for multi class classification .based on our literature review we believed that deep learning techniques should work well for this classification problem .a multilayer perceptron architecture is a fully connected feedforward neural network with one input layer one or more hidden layers and one output layer .c deep learning	we therefore implemented a multilayer perceptron architecture for multi class classification 
1	109529	9529	a multilayer perceptron architecture is a fully connected feedforward neural network with one input layer one or more hidden layers and one output layer .we therefore implemented a multilayer perceptron architecture for multi class classification .formally the mlp can be considered a function f r n r k where n is the number of input features and k is the number of classes .c deep learning	a multilayer perceptron architecture is a fully connected feedforward neural network with one input layer one or more hidden layers and one output layer 
0	109530	9530	formally the mlp can be considered a function f r n r k where n is the number of input features and k is the number of classes .a multilayer perceptron architecture is a fully connected feedforward neural network with one input layer one or more hidden layers and one output layer .each hidden layer can be formalized as f r a r b where a is the input size and b is the output size .c deep learning	formally the mlp can be considered a function f r n r k where n is the number of input features and k is the number of classes 
0	109531	9531	each hidden layer can be formalized as f r a r b where a is the input size and b is the output size .formally the mlp can be considered a function f r n r k where n is the number of input features and k is the number of classes .in matrix notation it would be where x is the input vector w c is the weight matrix associated with layer c and b c is the bias vector associated with layer c and a c is the activation function associated with layer c we use softmax as the final activation so each prediction is size kx1 where k is the number of classes .c deep learning	each hidden layer can be formalized as f r a r b where a is the input size and b is the output size 
1	109532	9532	in matrix notation it would be where x is the input vector w c is the weight matrix associated with layer c and b c is the bias vector associated with layer c and a c is the activation function associated with layer c we use softmax as the final activation so each prediction is size kx1 where k is the number of classes .each hidden layer can be formalized as f r a r b where a is the input size and b is the output size .the classifier predicts a score for each class instead of simply producing a single class label .c deep learning	in matrix notation it would be where x is the input vector w c is the weight matrix associated with layer c and b c is the bias vector associated with layer c and a c is the activation function associated with layer c we use softmax as the final activation so each prediction is size kx1 where k is the number of classes 
0	109533	9533	the classifier predicts a score for each class instead of simply producing a single class label .in matrix notation it would be where x is the input vector w c is the weight matrix associated with layer c and b c is the bias vector associated with layer c and a c is the activation function associated with layer c we use softmax as the final activation so each prediction is size kx1 where k is the number of classes .this can give a sense of how close we are to the correct label .c deep learning	the classifier predicts a score for each class instead of simply producing a single class label 
0	109534	9534	this can give a sense of how close we are to the correct label .the classifier predicts a score for each class instead of simply producing a single class label .we converted the true labels to use one hot encoding that is we took an mx1 array and made it mxk where m is the number of datapoints .c deep learning	this can give a sense of how close we are to the correct label 
1	109535	9535	we converted the true labels to use one hot encoding that is we took an mx1 array and made it mxk where m is the number of datapoints .this can give a sense of how close we are to the correct label .we initially found that making the model deeper produced worse results but increasing the number of neurons per layer improved the accuracy .c deep learning	we converted the true labels to use one hot encoding that is we took an mx1 array and made it mxk where m is the number of datapoints 
1	109536	9536	we initially found that making the model deeper produced worse results but increasing the number of neurons per layer improved the accuracy .we converted the true labels to use one hot encoding that is we took an mx1 array and made it mxk where m is the number of datapoints .our final architecture is layer1 relu layer2 relu layer3 softmax .c deep learning	we initially found that making the model deeper produced worse results but increasing the number of neurons per layer improved the accuracy 
0	109537	9537	our final architecture is layer1 relu layer2 relu layer3 softmax .we initially found that making the model deeper produced worse results but increasing the number of neurons per layer improved the accuracy .the layers have weight sizes of layer1 n 512 layer2 512 k .c deep learning	our final architecture is layer1 relu layer2 relu layer3 softmax 
1	109538	9538	the layers have weight sizes of layer1 n 512 layer2 512 k .our final architecture is layer1 relu layer2 relu layer3 softmax .in order to generalize better we apply dropout for each hidden layer this helps combat overfitting by suppressing each node with 50 probability .c deep learning	the layers have weight sizes of layer1 n 512 layer2 512 k 
1	109539	9539	in order to generalize better we apply dropout for each hidden layer this helps combat overfitting by suppressing each node with 50 probability .the layers have weight sizes of layer1 n 512 layer2 512 k .we tried different gradient descent rules with the best result coming from the sgd optimizer .c deep learning	in order to generalize better we apply dropout for each hidden layer this helps combat overfitting by suppressing each node with 50 probability 
0	109540	9540	we tried different gradient descent rules with the best result coming from the sgd optimizer .in order to generalize better we apply dropout for each hidden layer this helps combat overfitting by suppressing each node with 50 probability .to evaluate loss we use categorical cross entropy which is as follows we use this in conjunction with softmax activation for the final layer which gives us a probability or confidence for each prediction .c deep learning	we tried different gradient descent rules with the best result coming from the sgd optimizer 
1	109541	9541	to evaluate loss we use categorical cross entropy which is as follows we use this in conjunction with softmax activation for the final layer which gives us a probability or confidence for each prediction .we tried different gradient descent rules with the best result coming from the sgd optimizer .softmax output for the i th element is as follows this is advantageous because we have multiple classes instead of simple binary classification .c deep learning	to evaluate loss we use categorical cross entropy which is as follows we use this in conjunction with softmax activation for the final layer which gives us a probability or confidence for each prediction 
1	109542	9542	softmax output for the i th element is as follows this is advantageous because we have multiple classes instead of simple binary classification .to evaluate loss we use categorical cross entropy which is as follows we use this in conjunction with softmax activation for the final layer which gives us a probability or confidence for each prediction .we want the loss to give a sense of the degree of error for each category instead of a simple yes or no .c deep learning	softmax output for the i th element is as follows this is advantageous because we have multiple classes instead of simple binary classification 
0	109543	9543	we want the loss to give a sense of the degree of error for each category instead of a simple yes or no .softmax output for the i th element is as follows this is advantageous because we have multiple classes instead of simple binary classification .for example assume we have 3 classes and the first class is the correct label for this time step .c deep learning	we want the loss to give a sense of the degree of error for each category instead of a simple yes or no 
0	109544	9544	for example assume we have 3 classes and the first class is the correct label for this time step .we want the loss to give a sense of the degree of error for each category instead of a simple yes or no .consider two example outputs 0 98 0 01 0 01 and 0 51 0 48 0 01 .c deep learning	for example assume we have 3 classes and the first class is the correct label for this time step 
0	109545	9545	consider two example outputs 0 98 0 01 0 01 and 0 51 0 48 0 01 .for example assume we have 3 classes and the first class is the correct label for this time step .the first output is clearly superior to the second because it has a higher confidence for the correct class however both will predict the first class .c deep learning	consider two example outputs 0 98 0 01 0 01 and 0 51 0 48 0 01 
1	109546	9546	the first output is clearly superior to the second because it has a higher confidence for the correct class however both will predict the first class .consider two example outputs 0 98 0 01 0 01 and 0 51 0 48 0 01 .if we use something like simple error rate for our loss we are not able to capture the confidence of our predictions .c deep learning	the first output is clearly superior to the second because it has a higher confidence for the correct class however both will predict the first class 
1	109547	9547	if we use something like simple error rate for our loss we are not able to capture the confidence of our predictions .the first output is clearly superior to the second because it has a higher confidence for the correct class however both will predict the first class .using softmax activation with cross entropy loss helps us capture this difference .c deep learning	if we use something like simple error rate for our loss we are not able to capture the confidence of our predictions 
1	109548	9548	using softmax activation with cross entropy loss helps us capture this difference .if we use something like simple error rate for our loss we are not able to capture the confidence of our predictions .our final architecture is shown in.c deep learning	using softmax activation with cross entropy loss helps us capture this difference 
0	109549	9549	our final architecture is shown in.using softmax activation with cross entropy loss helps us capture this difference .last sentence.c deep learning	our final architecture is shown in
0	109550	9550	decision trees are a useful method for multi class classification for nonlinear feature sets .first sentence.decision trees perform greedy splits on the each feature of the data at a specific threshold .d trees	decision trees are a useful method for multi class classification for nonlinear feature sets 
0	109551	9551	decision trees perform greedy splits on the each feature of the data at a specific threshold .decision trees are a useful method for multi class classification for nonlinear feature sets .in order to choose a split a decision tree seeks to maximize the difference between the loss of the parent node and the sum of the losses of the child nodes .d trees	decision trees perform greedy splits on the each feature of the data at a specific threshold 
1	109552	9552	in order to choose a split a decision tree seeks to maximize the difference between the loss of the parent node and the sum of the losses of the child nodes .decision trees perform greedy splits on the each feature of the data at a specific threshold .the specific loss function we used was the gini loss shown below where p mk is the proportion of examples in class k present in region r m and q m is the proportion of examples in r m from tree t with t different r m regions there are multiple methods of regularizing or preventing overfitting for decision trees including setting a minimum size of leaf terminal nodes and setting a maximum tree depth setting a maximum number of nodes 3 random forest another ensemble method for decision trees for the purpose of improving prediction accuracy is random forest .d trees	in order to choose a split a decision tree seeks to maximize the difference between the loss of the parent node and the sum of the losses of the child nodes 
1	109553	9553	the specific loss function we used was the gini loss shown below where p mk is the proportion of examples in class k present in region r m and q m is the proportion of examples in r m from tree t with t different r m regions there are multiple methods of regularizing or preventing overfitting for decision trees including setting a minimum size of leaf terminal nodes and setting a maximum tree depth setting a maximum number of nodes 3 random forest another ensemble method for decision trees for the purpose of improving prediction accuracy is random forest .in order to choose a split a decision tree seeks to maximize the difference between the loss of the parent node and the sum of the losses of the child nodes .random forest is a form of bagging bootstrap aggregation which involves sampling with replacement from the original population for the purpose of reducing variance at the expense of an increase in bias increased computational cost and decreased interpretability of the trees .d trees	the specific loss function we used was the gini loss shown below where p mk is the proportion of examples in class k present in region r m and q m is the proportion of examples in r m from tree t with t different r m regions there are multiple methods of regularizing or preventing overfitting for decision trees including setting a minimum size of leaf terminal nodes and setting a maximum tree depth setting a maximum number of nodes 3 random forest another ensemble method for decision trees for the purpose of improving prediction accuracy is random forest 
1	109554	9554	random forest is a form of bagging bootstrap aggregation which involves sampling with replacement from the original population for the purpose of reducing variance at the expense of an increase in bias increased computational cost and decreased interpretability of the trees .the specific loss function we used was the gini loss shown below where p mk is the proportion of examples in class k present in region r m and q m is the proportion of examples in r m from tree t with t different r m regions there are multiple methods of regularizing or preventing overfitting for decision trees including setting a minimum size of leaf terminal nodes and setting a maximum tree depth setting a maximum number of nodes 3 random forest another ensemble method for decision trees for the purpose of improving prediction accuracy is random forest .for a random forest a large number of decision trees are generated and the bias is further reduced by decorrelating the trees by only considering a subset of the total number of features at each split in the decision tree.d trees	random forest is a form of bagging bootstrap aggregation which involves sampling with replacement from the original population for the purpose of reducing variance at the expense of an increase in bias increased computational cost and decreased interpretability of the trees 
1	109555	9555	for a random forest a large number of decision trees are generated and the bias is further reduced by decorrelating the trees by only considering a subset of the total number of features at each split in the decision tree.random forest is a form of bagging bootstrap aggregation which involves sampling with replacement from the original population for the purpose of reducing variance at the expense of an increase in bias increased computational cost and decreased interpretability of the trees .last sentence.d trees	for a random forest a large number of decision trees are generated and the bias is further reduced by decorrelating the trees by only considering a subset of the total number of features at each split in the decision tree
1	109556	9556	as discussed previously we were interested in classification performance on both the full set of features and performance when using a reduced set of features .first sentence.we tried several feature combinations and discovered that we could get decent performance when using just the hand imu plus heart rate sensor .v results and discussion	as discussed previously we were interested in classification performance on both the full set of features and performance when using a reduced set of features 
1	109557	9557	we tried several feature combinations and discovered that we could get decent performance when using just the hand imu plus heart rate sensor .as discussed previously we were interested in classification performance on both the full set of features and performance when using a reduced set of features .the hand imu performed better than any individual imu .v results and discussion	we tried several feature combinations and discovered that we could get decent performance when using just the hand imu plus heart rate sensor 
0	109558	9558	the hand imu performed better than any individual imu .we tried several feature combinations and discovered that we could get decent performance when using just the hand imu plus heart rate sensor .this is a positive result as we are particularly interested in applications where a user is holding a phone or wearing a smart watch .v results and discussion	the hand imu performed better than any individual imu 
0	109559	9559	this is a positive result as we are particularly interested in applications where a user is holding a phone or wearing a smart watch .the hand imu performed better than any individual imu .we will refer to the hand imu plus heart rate data as the reduced or limited feature set .v results and discussion	this is a positive result as we are particularly interested in applications where a user is holding a phone or wearing a smart watch 
1	109560	9560	we will refer to the hand imu plus heart rate data as the reduced or limited feature set .this is a positive result as we are particularly interested in applications where a user is holding a phone or wearing a smart watch .our primary evaluation metric for all models was classification accuracy .v results and discussion	we will refer to the hand imu plus heart rate data as the reduced or limited feature set 
0	109561	9561	our primary evaluation metric for all models was classification accuracy .we will refer to the hand imu plus heart rate data as the reduced or limited feature set .this is simply the count of correctly classified data points divided by the count of classifications attempted .v results and discussion	our primary evaluation metric for all models was classification accuracy 
0	109562	9562	this is simply the count of correctly classified data points divided by the count of classifications attempted .our primary evaluation metric for all models was classification accuracy .it is as follows where y is our set of predicted labels and y is the set of true labels .v results and discussion	this is simply the count of correctly classified data points divided by the count of classifications attempted 
0	109563	9563	it is as follows where y is our set of predicted labels and y is the set of true labels .this is simply the count of correctly classified data points divided by the count of classifications attempted .we used various loss functions as described in the subsection for each technique .v results and discussion	it is as follows where y is our set of predicted labels and y is the set of true labels 
0	109564	9564	we used various loss functions as described in the subsection for each technique .it is as follows where y is our set of predicted labels and y is the set of true labels .the full results are shown below in.v results and discussion	we used various loss functions as described in the subsection for each technique 
0	109565	9565	the full results are shown below in.we used various loss functions as described in the subsection for each technique .last sentence.v results and discussion	the full results are shown below in
0	109566	9566	through 5 fold cross validation in.first sentence.last sentence.a logistic regression	through 5 fold cross validation in
1	109567	9567	in order to tune the maximum depth hyperparameter of the decision tree we used scikit learn s validation curve function to perform 5 fold cross validation .first sentence.the training and crossvalidation curves for the limited feature set is shown in the left side of 4 below as a function of maximum tree depth and similar curves were produced for the full feature set it is typical in practice to use the one standard error rule when using cross validation to choose the simplest model whose accuracy or error is within one standard deviation of the best performing model 2 boosted decision trees for boosted decision trees we used the default learning rate and manually tuned the maximum depth of the base decision tree estimators and the number of trees .c decision trees 1 ordinary decision trees	in order to tune the maximum depth hyperparameter of the decision tree we used scikit learn s validation curve function to perform 5 fold cross validation 
1	109568	9568	the training and crossvalidation curves for the limited feature set is shown in the left side of 4 below as a function of maximum tree depth and similar curves were produced for the full feature set it is typical in practice to use the one standard error rule when using cross validation to choose the simplest model whose accuracy or error is within one standard deviation of the best performing model 2 boosted decision trees for boosted decision trees we used the default learning rate and manually tuned the maximum depth of the base decision tree estimators and the number of trees .in order to tune the maximum depth hyperparameter of the decision tree we used scikit learn s validation curve function to perform 5 fold cross validation .since the base estimator of boosting should be a weak learner and since the strong learner from the results from ordinary decision trees had a maximum depth of 15 we decided to limit our weak learners to having a maximum depth of less than or equal to 10 .c decision trees 1 ordinary decision trees	the training and crossvalidation curves for the limited feature set is shown in the left side of 4 below as a function of maximum tree depth and similar curves were produced for the full feature set it is typical in practice to use the one standard error rule when using cross validation to choose the simplest model whose accuracy or error is within one standard deviation of the best performing model 2 boosted decision trees for boosted decision trees we used the default learning rate and manually tuned the maximum depth of the base decision tree estimators and the number of trees 
1	109569	9569	since the base estimator of boosting should be a weak learner and since the strong learner from the results from ordinary decision trees had a maximum depth of 15 we decided to limit our weak learners to having a maximum depth of less than or equal to 10 .the training and crossvalidation curves for the limited feature set is shown in the left side of 4 below as a function of maximum tree depth and similar curves were produced for the full feature set it is typical in practice to use the one standard error rule when using cross validation to choose the simplest model whose accuracy or error is within one standard deviation of the best performing model 2 boosted decision trees for boosted decision trees we used the default learning rate and manually tuned the maximum depth of the base decision tree estimators and the number of trees .for both the limited and full feature sets we created tables that presented the training and validation accuracy values for different combinations of maximum depth of the weak learners from 1 10 and number of trees 50 100 250 or 500 .c decision trees 1 ordinary decision trees	since the base estimator of boosting should be a weak learner and since the strong learner from the results from ordinary decision trees had a maximum depth of 15 we decided to limit our weak learners to having a maximum depth of less than or equal to 10 
1	109570	9570	for both the limited and full feature sets we created tables that presented the training and validation accuracy values for different combinations of maximum depth of the weak learners from 1 10 and number of trees 50 100 250 or 500 .since the base estimator of boosting should be a weak learner and since the strong learner from the results from ordinary decision trees had a maximum depth of 15 we decided to limit our weak learners to having a maximum depth of less than or equal to 10 .using the one standard error rule from the optimal validation performance we chose 500 trees of maxdepth 10 for the limited feature set and 250 trees of maxdepth 9 .c decision trees 1 ordinary decision trees	for both the limited and full feature sets we created tables that presented the training and validation accuracy values for different combinations of maximum depth of the weak learners from 1 10 and number of trees 50 100 250 or 500 
1	109571	9571	using the one standard error rule from the optimal validation performance we chose 500 trees of maxdepth 10 for the limited feature set and 250 trees of maxdepth 9 .for both the limited and full feature sets we created tables that presented the training and validation accuracy values for different combinations of maximum depth of the weak learners from 1 10 and number of trees 50 100 250 or 500 .using these models the limited feature set scored an accuracy of 0 940 on the test set and the full feature set scored an accuracy of 0 985 on the test set .c decision trees 1 ordinary decision trees	using the one standard error rule from the optimal validation performance we chose 500 trees of maxdepth 10 for the limited feature set and 250 trees of maxdepth 9 
1	109572	9572	using these models the limited feature set scored an accuracy of 0 940 on the test set and the full feature set scored an accuracy of 0 985 on the test set .using the one standard error rule from the optimal validation performance we chose 500 trees of maxdepth 10 for the limited feature set and 250 trees of maxdepth 9 .a confusion matrix for the limited feature set is shown below in 5.c decision trees 1 ordinary decision trees	using these models the limited feature set scored an accuracy of 0 940 on the test set and the full feature set scored an accuracy of 0 985 on the test set 
0	109573	9573	a confusion matrix for the limited feature set is shown below in 5.using these models the limited feature set scored an accuracy of 0 940 on the test set and the full feature set scored an accuracy of 0 985 on the test set .last sentence.c decision trees 1 ordinary decision trees	a confusion matrix for the limited feature set is shown below in 5
1	109574	9574	we decided to include 100 trees in our random forest model because 100 was a well performing trade off of time and accuracy for random forests increasing the number of trees will only serve to decrease the variance and will not increase the likelihood of overfitting .first sentence.we used the default option of only considering a random subset of the square root of the total number of features for each split .3 random forest	we decided to include 100 trees in our random forest model because 100 was a well performing trade off of time and accuracy for random forests increasing the number of trees will only serve to decrease the variance and will not increase the likelihood of overfitting 
1	109575	9575	we used the default option of only considering a random subset of the square root of the total number of features for each split .we decided to include 100 trees in our random forest model because 100 was a well performing trade off of time and accuracy for random forests increasing the number of trees will only serve to decrease the variance and will not increase the likelihood of overfitting .in order to tune the maximum depth for each decision tree in the random forest we again used scikit learn s validation curve function to perform 5 fold cross validation .3 random forest	we used the default option of only considering a random subset of the square root of the total number of features for each split 
1	109576	9576	in order to tune the maximum depth for each decision tree in the random forest we again used scikit learn s validation curve function to perform 5 fold cross validation .we used the default option of only considering a random subset of the square root of the total number of features for each split .the training and cross validation curves for the limited feature set and the full feature set as a function of maximum tree depth in the random forest were created using a similar approach to the cross validation for ordinary decision trees .3 random forest	in order to tune the maximum depth for each decision tree in the random forest we again used scikit learn s validation curve function to perform 5 fold cross validation 
1	109577	9577	the training and cross validation curves for the limited feature set and the full feature set as a function of maximum tree depth in the random forest were created using a similar approach to the cross validation for ordinary decision trees .in order to tune the maximum depth for each decision tree in the random forest we again used scikit learn s validation curve function to perform 5 fold cross validation .using the one standard error rule for the validation accuracy the results suggest that for both the limited and full feature sets a maximum depth of 20 should be used .3 random forest	the training and cross validation curves for the limited feature set and the full feature set as a function of maximum tree depth in the random forest were created using a similar approach to the cross validation for ordinary decision trees 
1	109578	9578	using the one standard error rule for the validation accuracy the results suggest that for both the limited and full feature sets a maximum depth of 20 should be used .the training and cross validation curves for the limited feature set and the full feature set as a function of maximum tree depth in the random forest were created using a similar approach to the cross validation for ordinary decision trees .using a maximum depth of 20 the limited featureset achieved a test accuracy of 0 937 and the full featureset achieved a test accuracy of 0 980 .3 random forest	using the one standard error rule for the validation accuracy the results suggest that for both the limited and full feature sets a maximum depth of 20 should be used 
0	109579	9579	using a maximum depth of 20 the limited featureset achieved a test accuracy of 0 937 and the full featureset achieved a test accuracy of 0 980 .using the one standard error rule for the validation accuracy the results suggest that for both the limited and full feature sets a maximum depth of 20 should be used .from the confusion matrix for this model the most frequent misclassifications were between vacuum cleaning and ironing ascending and descending stairs vacuum cleaning and ascending descending stairs .3 random forest	using a maximum depth of 20 the limited featureset achieved a test accuracy of 0 937 and the full featureset achieved a test accuracy of 0 980 
0	109580	9580	from the confusion matrix for this model the most frequent misclassifications were between vacuum cleaning and ironing ascending and descending stairs vacuum cleaning and ascending descending stairs .using a maximum depth of 20 the limited featureset achieved a test accuracy of 0 937 and the full featureset achieved a test accuracy of 0 980 .these are exactly the common misclassifications found in boosting and are expected because of the relative similarity in hand motions and heart rate between these activities .3 random forest	from the confusion matrix for this model the most frequent misclassifications were between vacuum cleaning and ironing ascending and descending stairs vacuum cleaning and ascending descending stairs 
1	109581	9581	these are exactly the common misclassifications found in boosting and are expected because of the relative similarity in hand motions and heart rate between these activities .from the confusion matrix for this model the most frequent misclassifications were between vacuum cleaning and ironing ascending and descending stairs vacuum cleaning and ascending descending stairs .last sentence.3 random forest	these are exactly the common misclassifications found in boosting and are expected because of the relative similarity in hand motions and heart rate between these activities 
1	109582	9582	the multilayer perceptron neural network was able to achieve high classification accuracy .first sentence.when trained on the entire dataset it consistently achieved training accuracies greater than 98 and test accuracies greater than 95 .d deep learning	the multilayer perceptron neural network was able to achieve high classification accuracy 
1	109583	9583	when trained on the entire dataset it consistently achieved training accuracies greater than 98 and test accuracies greater than 95 .the multilayer perceptron neural network was able to achieve high classification accuracy .the best model we trained produced a test accuracy of 98 1 .d deep learning	when trained on the entire dataset it consistently achieved training accuracies greater than 98 and test accuracies greater than 95 
0	109584	9584	the best model we trained produced a test accuracy of 98 1 .when trained on the entire dataset it consistently achieved training accuracies greater than 98 and test accuracies greater than 95 .when trained on the reduced feature set consisting of only the hand imu and the heart rate sensor it achieved 81 4 test accuracy .d deep learning	the best model we trained produced a test accuracy of 98 1 
1	109585	9585	when trained on the reduced feature set consisting of only the hand imu and the heart rate sensor it achieved 81 4 test accuracy .the best model we trained produced a test accuracy of 98 1 .we noted a trend in accuracy vs hidden layer size .d deep learning	when trained on the reduced feature set consisting of only the hand imu and the heart rate sensor it achieved 81 4 test accuracy 
0	109586	9586	we noted a trend in accuracy vs hidden layer size .when trained on the reduced feature set consisting of only the hand imu and the heart rate sensor it achieved 81 4 test accuracy .increasing the size of each layer number of neurons improved performance while increasing the depth number of hidden layers degraded performance .d deep learning	we noted a trend in accuracy vs hidden layer size 
1	109587	9587	increasing the size of each layer number of neurons improved performance while increasing the depth number of hidden layers degraded performance .we noted a trend in accuracy vs hidden layer size .we did not notice a significant difference in performance when using relu vs other activation functions however we did find that our model converged faster when using softmax for the final activation function in conjunction with categorical cross entropy loss .d deep learning	increasing the size of each layer number of neurons improved performance while increasing the depth number of hidden layers degraded performance 
1	109588	9588	we did not notice a significant difference in performance when using relu vs other activation functions however we did find that our model converged faster when using softmax for the final activation function in conjunction with categorical cross entropy loss .increasing the size of each layer number of neurons improved performance while increasing the depth number of hidden layers degraded performance .as expected reducing the dropout rate tended to improve training accuracy but reducing it too much caused a degradation in test accuracy .d deep learning	we did not notice a significant difference in performance when using relu vs other activation functions however we did find that our model converged faster when using softmax for the final activation function in conjunction with categorical cross entropy loss 
0	109589	9589	as expected reducing the dropout rate tended to improve training accuracy but reducing it too much caused a degradation in test accuracy .we did not notice a significant difference in performance when using relu vs other activation functions however we did find that our model converged faster when using softmax for the final activation function in conjunction with categorical cross entropy loss .in this project we were limited in both time and compute and we believe we can improve accuracy given more of both .d deep learning	as expected reducing the dropout rate tended to improve training accuracy but reducing it too much caused a degradation in test accuracy 
1	109590	9590	in this project we were limited in both time and compute and we believe we can improve accuracy given more of both .as expected reducing the dropout rate tended to improve training accuracy but reducing it too much caused a degradation in test accuracy .we can improve performance by training for more epochs .d deep learning	in this project we were limited in both time and compute and we believe we can improve accuracy given more of both 
0	109591	9591	we can improve performance by training for more epochs .in this project we were limited in both time and compute and we believe we can improve accuracy given more of both .loss continued to decrease at the end of our training indicating performance was still improving when training finished .d deep learning	we can improve performance by training for more epochs 
0	109592	9592	loss continued to decrease at the end of our training indicating performance was still improving when training finished .we can improve performance by training for more epochs .we could also further increase the number of neurons per hidden layer at the cost of a larger model with slower training time .d deep learning	loss continued to decrease at the end of our training indicating performance was still improving when training finished 
1	109593	9593	we could also further increase the number of neurons per hidden layer at the cost of a larger model with slower training time .loss continued to decrease at the end of our training indicating performance was still improving when training finished .last sentence.d deep learning	we could also further increase the number of neurons per hidden layer at the cost of a larger model with slower training time 
0	109594	9594	unsurprisingly logistic regression performed the worst .first sentence.having the advantage of extremely low memory usage and speed for predictions it can still be a viable method in lowcompute devices like microcontrollers .vi conclusion and future work	unsurprisingly logistic regression performed the worst 
1	109595	9595	having the advantage of extremely low memory usage and speed for predictions it can still be a viable method in lowcompute devices like microcontrollers .unsurprisingly logistic regression performed the worst .svm s on the other hand provide great results in accuracy but may not be viable solutions for microcontrollers because of the large number of support vectors required to store and do operations on for predicting .vi conclusion and future work	having the advantage of extremely low memory usage and speed for predictions it can still be a viable method in lowcompute devices like microcontrollers 
1	109596	9596	svm s on the other hand provide great results in accuracy but may not be viable solutions for microcontrollers because of the large number of support vectors required to store and do operations on for predicting .having the advantage of extremely low memory usage and speed for predictions it can still be a viable method in lowcompute devices like microcontrollers .for decision tree methods as expected ensembling methods improved test performance over ordinary decision trees for both the full and limited feature sets .vi conclusion and future work	svm s on the other hand provide great results in accuracy but may not be viable solutions for microcontrollers because of the large number of support vectors required to store and do operations on for predicting 
1	109597	9597	for decision tree methods as expected ensembling methods improved test performance over ordinary decision trees for both the full and limited feature sets .svm s on the other hand provide great results in accuracy but may not be viable solutions for microcontrollers because of the large number of support vectors required to store and do operations on for predicting .boosted decision trees performed slightly better than random forest on both feature sets which is promising because it is generally less computationally intensive and thus is a good candidate for a model to actually deploy in a smart device .vi conclusion and future work	for decision tree methods as expected ensembling methods improved test performance over ordinary decision trees for both the full and limited feature sets 
1	109598	9598	boosted decision trees performed slightly better than random forest on both feature sets which is promising because it is generally less computationally intensive and thus is a good candidate for a model to actually deploy in a smart device .for decision tree methods as expected ensembling methods improved test performance over ordinary decision trees for both the full and limited feature sets .we would also be interested in exploring different types of deep learning architectures .vi conclusion and future work	boosted decision trees performed slightly better than random forest on both feature sets which is promising because it is generally less computationally intensive and thus is a good candidate for a model to actually deploy in a smart device 
0	109599	9599	we would also be interested in exploring different types of deep learning architectures .boosted decision trees performed slightly better than random forest on both feature sets which is promising because it is generally less computationally intensive and thus is a good candidate for a model to actually deploy in a smart device .we considered using rnn s recurrent neural networks but our feature set had a relatively large number of features per time step and the activities did not involve more than a few actions so it was not necessary to take history into account when classifying a single time step .vi conclusion and future work	we would also be interested in exploring different types of deep learning architectures 
1	109600	9600	we considered using rnn s recurrent neural networks but our feature set had a relatively large number of features per time step and the activities did not involve more than a few actions so it was not necessary to take history into account when classifying a single time step .we would also be interested in exploring different types of deep learning architectures .as such simple feed forward neural nets were sufficient for this problem .vi conclusion and future work	we considered using rnn s recurrent neural networks but our feature set had a relatively large number of features per time step and the activities did not involve more than a few actions so it was not necessary to take history into account when classifying a single time step 
1	109601	9601	as such simple feed forward neural nets were sufficient for this problem .we considered using rnn s recurrent neural networks but our feature set had a relatively large number of features per time step and the activities did not involve more than a few actions so it was not necessary to take history into account when classifying a single time step .however we would like to explore cnn s convolutional neural networks which could potentially give similar or improved performance while using substantially less memory .vi conclusion and future work	as such simple feed forward neural nets were sufficient for this problem 
1	109602	9602	however we would like to explore cnn s convolutional neural networks which could potentially give similar or improved performance while using substantially less memory .as such simple feed forward neural nets were sufficient for this problem .in general the limited feature set performed only slightly worse than the full feature set on all of the methods which is a promising result for actual deployment in smart devices .vi conclusion and future work	however we would like to explore cnn s convolutional neural networks which could potentially give similar or improved performance while using substantially less memory 
1	109603	9603	in general the limited feature set performed only slightly worse than the full feature set on all of the methods which is a promising result for actual deployment in smart devices .however we would like to explore cnn s convolutional neural networks which could potentially give similar or improved performance while using substantially less memory .in the future we would like to test these models using real imu s .vi conclusion and future work	in general the limited feature set performed only slightly worse than the full feature set on all of the methods which is a promising result for actual deployment in smart devices 
1	109604	9604	in the future we would like to test these models using real imu s .in general the limited feature set performed only slightly worse than the full feature set on all of the methods which is a promising result for actual deployment in smart devices .in particular we would want to see if a low compute embedded device could perform classifications with neural nets or svm s in real time in addition to computationally cheaper methods such as decision trees .vi conclusion and future work	in the future we would like to test these models using real imu s 
1	109605	9605	in particular we would want to see if a low compute embedded device could perform classifications with neural nets or svm s in real time in addition to computationally cheaper methods such as decision trees .in the future we would like to test these models using real imu s .last sentence.vi conclusion and future work	in particular we would want to see if a low compute embedded device could perform classifications with neural nets or svm s in real time in addition to computationally cheaper methods such as decision trees 
0	109606	9606	all code used in this project can be found at https github com aristosathens human a ctivity c lassif ier contributions aristos zach and navjot all contributed equally to this project .first sentence.aristos focused on deep learning navjot focused on logistic regression and svm and zach focused on trees ordinary decision trees boosting and random forests .appendices	all code used in this project can be found at https github com aristosathens human a ctivity c lassif ier contributions aristos zach and navjot all contributed equally to this project 
1	109607	9607	aristos focused on deep learning navjot focused on logistic regression and svm and zach focused on trees ordinary decision trees boosting and random forests .all code used in this project can be found at https github com aristosathens human a ctivity c lassif ier contributions aristos zach and navjot all contributed equally to this project .all three members worked on data preprocessing analysis and writing this report .appendices	aristos focused on deep learning navjot focused on logistic regression and svm and zach focused on trees ordinary decision trees boosting and random forests 
0	109608	9608	all three members worked on data preprocessing analysis and writing this report .aristos focused on deep learning navjot focused on logistic regression and svm and zach focused on trees ordinary decision trees boosting and random forests .last sentence.appendices	all three members worked on data preprocessing analysis and writing this report 
1	109609	9609	in this paper i describe a real time image processing pipeline for fruit fly videos that can detect the position oriention sex and for male flies wing angles .first sentence.the machine learning algorithms used include a decision tree linear and logistic regressions and principal component analysis .abstract	in this paper i describe a real time image processing pipeline for fruit fly videos that can detect the position oriention sex and for male flies wing angles 
0	109610	9610	the machine learning algorithms used include a decision tree linear and logistic regressions and principal component analysis .in this paper i describe a real time image processing pipeline for fruit fly videos that can detect the position oriention sex and for male flies wing angles .the histogram of oriented gradients 2 descriptor is used as well to generate features .abstract	the machine learning algorithms used include a decision tree linear and logistic regressions and principal component analysis 
0	109611	9611	the histogram of oriented gradients 2 descriptor is used as well to generate features .the machine learning algorithms used include a decision tree linear and logistic regressions and principal component analysis .ultimately i achieved a processing throughput of 84 frames per second on 1530x1530 grayscale frames without gpu acceleration and demonstrated high accuracy across several metrics .abstract	the histogram of oriented gradients 2 descriptor is used as well to generate features 
0	109612	9612	ultimately i achieved a processing throughput of 84 frames per second on 1530x1530 grayscale frames without gpu acceleration and demonstrated high accuracy across several metrics .the histogram of oriented gradients 2 descriptor is used as well to generate features .last sentence.abstract	ultimately i achieved a processing throughput of 84 frames per second on 1530x1530 grayscale frames without gpu acceleration and demonstrated high accuracy across several metrics 
0	109613	9613	fruit flies are often used in neurobiology experiments because they can be genetically modified using well established tools and because it is feasible to do in vivo whole brain imaging of them in addition to other reasons .first sentence.in these experiments it is often desirable to measure the behavior of each fly by recording its position and orientation over time along with its leg and wing positions automated tools for doing this kind of video analysis exist but are usually too slow to run in real time .introduction	fruit flies are often used in neurobiology experiments because they can be genetically modified using well established tools and because it is feasible to do in vivo whole brain imaging of them in addition to other reasons 
1	109614	9614	in these experiments it is often desirable to measure the behavior of each fly by recording its position and orientation over time along with its leg and wing positions automated tools for doing this kind of video analysis exist but are usually too slow to run in real time .fruit flies are often used in neurobiology experiments because they can be genetically modified using well established tools and because it is feasible to do in vivo whole brain imaging of them in addition to other reasons .this is problematic for two reasons first real time operation is a prerequisite for running closed loop behavior experiments .introduction	in these experiments it is often desirable to measure the behavior of each fly by recording its position and orientation over time along with its leg and wing positions automated tools for doing this kind of video analysis exist but are usually too slow to run in real time 
0	109615	9615	this is problematic for two reasons first real time operation is a prerequisite for running closed loop behavior experiments .in these experiments it is often desirable to measure the behavior of each fly by recording its position and orientation over time along with its leg and wing positions automated tools for doing this kind of video analysis exist but are usually too slow to run in real time .second in an experiment where flies are recorded continuously over a long period e g a circadian rhythm study video processing will become the bottleneck for experimental throughput unless it runs in real time or faster to address these issues i sought to develop a tool for the realtime video analysis of fruit flies .introduction	this is problematic for two reasons first real time operation is a prerequisite for running closed loop behavior experiments 
1	109616	9616	second in an experiment where flies are recorded continuously over a long period e g a circadian rhythm study video processing will become the bottleneck for experimental throughput unless it runs in real time or faster to address these issues i sought to develop a tool for the realtime video analysis of fruit flies .this is problematic for two reasons first real time operation is a prerequisite for running closed loop behavior experiments .in this project i chose to focus on a specific experiment being developed in prof tom clandinin s lab at stanford to study the courtship interaction between one male and one female fly .introduction	second in an experiment where flies are recorded continuously over a long period e g a circadian rhythm study video processing will become the bottleneck for experimental throughput unless it runs in real time or faster to address these issues i sought to develop a tool for the realtime video analysis of fruit flies 
0	109617	9617	in this project i chose to focus on a specific experiment being developed in prof tom clandinin s lab at stanford to study the courtship interaction between one male and one female fly .second in an experiment where flies are recorded continuously over a long period e g a circadian rhythm study video processing will become the bottleneck for experimental throughput unless it runs in real time or faster to address these issues i sought to develop a tool for the realtime video analysis of fruit flies .as a result the input to my algorithm is a grayscale video of the two flies in this paper i ll start off by describing some existing tools for fly video analysis section 2 and will then move on to describe the dataset i worked with section 3 .introduction	in this project i chose to focus on a specific experiment being developed in prof tom clandinin s lab at stanford to study the courtship interaction between one male and one female fly 
0	109618	9618	as a result the input to my algorithm is a grayscale video of the two flies in this paper i ll start off by describing some existing tools for fly video analysis section 2 and will then move on to describe the dataset i worked with section 3 .in this project i chose to focus on a specific experiment being developed in prof tom clandinin s lab at stanford to study the courtship interaction between one male and one female fly .next i ll describe the algorithm i developed which consists of four distinct processing steps using machine learning section 4 .introduction	as a result the input to my algorithm is a grayscale video of the two flies in this paper i ll start off by describing some existing tools for fly video analysis section 2 and will then move on to describe the dataset i worked with section 3 
0	109619	9619	next i ll describe the algorithm i developed which consists of four distinct processing steps using machine learning section 4 .as a result the input to my algorithm is a grayscale video of the two flies in this paper i ll start off by describing some existing tools for fly video analysis section 2 and will then move on to describe the dataset i worked with section 3 .finally i ll wrap up the experimental results section 6 and conclusion section 7 .introduction	next i ll describe the algorithm i developed which consists of four distinct processing steps using machine learning section 4 
0	109620	9620	finally i ll wrap up the experimental results section 6 and conclusion section 7 .next i ll describe the algorithm i developed which consists of four distinct processing steps using machine learning section 4 .the source code and dataset for this project are available on github at https github .introduction	finally i ll wrap up the experimental results section 6 and conclusion section 7 
0	109621	9621	the source code and dataset for this project are available on github at https github .finally i ll wrap up the experimental results section 6 and conclusion section 7 .com sgherbst cs229 project git .introduction	the source code and dataset for this project are available on github at https github 
0	109622	9622	com sgherbst cs229 project git .the source code and dataset for this project are available on github at https github .last sentence.introduction	com sgherbst cs229 project git 
1	109623	9623	the tool considered a gold standard of sorts for automated fruit fly video analysis is called flytracker in another project finally in the past year two different approaches to fly video analysis using deep neural networks were published deeplabcut in this project i sought to combine various aspects of these previous studies .first sentence.on one hand i wanted to develop an algorithm that could run in real time on large frames 1530x1530 without gpu acceleration and i wanted training to be fast to allow for more experimentation .related work	the tool considered a gold standard of sorts for automated fruit fly video analysis is called flytracker in another project finally in the past year two different approaches to fly video analysis using deep neural networks were published deeplabcut in this project i sought to combine various aspects of these previous studies 
0	109624	9624	on one hand i wanted to develop an algorithm that could run in real time on large frames 1530x1530 without gpu acceleration and i wanted training to be fast to allow for more experimentation .the tool considered a gold standard of sorts for automated fruit fly video analysis is called flytracker in another project finally in the past year two different approaches to fly video analysis using deep neural networks were published deeplabcut in this project i sought to combine various aspects of these previous studies .hence i needed to further simplify the machine learning models as compared to deeplabcut and leap .related work	on one hand i wanted to develop an algorithm that could run in real time on large frames 1530x1530 without gpu acceleration and i wanted training to be fast to allow for more experimentation 
0	109625	9625	hence i needed to further simplify the machine learning models as compared to deeplabcut and leap .on one hand i wanted to develop an algorithm that could run in real time on large frames 1530x1530 without gpu acceleration and i wanted training to be fast to allow for more experimentation .but i still wanted to apply supervised learning to take advantage of labeled data departing from the hand crafted image processing rules of flytracker .related work	hence i needed to further simplify the machine learning models as compared to deeplabcut and leap 
0	109626	9626	but i still wanted to apply supervised learning to take advantage of labeled data departing from the hand crafted image processing rules of flytracker .hence i needed to further simplify the machine learning models as compared to deeplabcut and leap .last sentence.related work	but i still wanted to apply supervised learning to take advantage of labeled data departing from the hand crafted image processing rules of flytracker 
0	109627	9627	the source data for this project was a 15 minute grayscale video of the interaction between one male fly and one female fly .first sentence.the video was furnished by dr ryan york of prof clandinin s lab as an example of the kind of footage that will be produced by the experimental rig they are developing .dataset	the source data for this project was a 15 minute grayscale video of the interaction between one male fly and one female fly 
0	109628	9628	the video was furnished by dr ryan york of prof clandinin s lab as an example of the kind of footage that will be produced by the experimental rig they are developing .the source data for this project was a 15 minute grayscale video of the interaction between one male fly and one female fly .i did a bit of initial preprocessing to crop the video to a 1530x1530 frame that exactly contained the circular well in which the flies were placed working from the cropped video i then hand annotated 326 frames using labelme additional preprocessing and data augmentation was used throughout the image processing pipeline and these steps will be covered in the next section .dataset	the video was furnished by dr ryan york of prof clandinin s lab as an example of the kind of footage that will be produced by the experimental rig they are developing 
1	109629	9629	i did a bit of initial preprocessing to crop the video to a 1530x1530 frame that exactly contained the circular well in which the flies were placed working from the cropped video i then hand annotated 326 frames using labelme additional preprocessing and data augmentation was used throughout the image processing pipeline and these steps will be covered in the next section .the video was furnished by dr ryan york of prof clandinin s lab as an example of the kind of footage that will be produced by the experimental rig they are developing .last sentence.dataset	i did a bit of initial preprocessing to crop the video to a 1530x1530 frame that exactly contained the circular well in which the flies were placed working from the cropped video i then hand annotated 326 frames using labelme additional preprocessing and data augmentation was used throughout the image processing pipeline and these steps will be covered in the next section 
0	109630	9630	as shown in the pipeline described below was implemented in python using the packages scikit learn.first sentence.last sentence.methods	as shown in the pipeline described below was implemented in python using the packages scikit learn
0	109631	9631	if there are two contours with one fly each the next stage of the image processing pipeline determines which is the male fly and which is the female fly .first sentence.this classification is done jointly that is the classifier is a given a list of the two contours and asked whether that list is ordered male female or female male .pipeline stage vs 	if there are two contours with one fly each the next stage of the image processing pipeline determines which is the male fly and which is the female fly 
0	109632	9632	this classification is done jointly that is the classifier is a given a list of the two contours and asked whether that list is ordered male female or female male .if there are two contours with one fly each the next stage of the image processing pipeline determines which is the male fly and which is the female fly .in general this is fairly straightforward since female fruit flies are larger than male fruit flies .pipeline stage vs 	this classification is done jointly that is the classifier is a given a list of the two contours and asked whether that list is ordered male female or female male 
0	109633	9633	in general this is fairly straightforward since female fruit flies are larger than male fruit flies .this classification is done jointly that is the classifier is a given a list of the two contours and asked whether that list is ordered male female or female male .but in some cases such as when the flies are climbing along the walls making the distinction can be a bit trickier there are four input features for this pipeline stage namely the area and aspect ratio of both contours the latter determined via image moment analysis in this case a rescaling step is needed before training the logistic regression because contour areas and aspects ratios are of vastly different scales so the above update rule would otherwise perform quite poorly .pipeline stage vs 	in general this is fairly straightforward since female fruit flies are larger than male fruit flies 
1	109634	9634	but in some cases such as when the flies are climbing along the walls making the distinction can be a bit trickier there are four input features for this pipeline stage namely the area and aspect ratio of both contours the latter determined via image moment analysis in this case a rescaling step is needed before training the logistic regression because contour areas and aspects ratios are of vastly different scales so the above update rule would otherwise perform quite poorly .in general this is fairly straightforward since female fruit flies are larger than male fruit flies .to further improve the quality of training data augmentation was applied by swapping the order of the two contours and their labels for each example .pipeline stage vs 	but in some cases such as when the flies are climbing along the walls making the distinction can be a bit trickier there are four input features for this pipeline stage namely the area and aspect ratio of both contours the latter determined via image moment analysis in this case a rescaling step is needed before training the logistic regression because contour areas and aspects ratios are of vastly different scales so the above update rule would otherwise perform quite poorly 
0	109635	9635	to further improve the quality of training data augmentation was applied by swapping the order of the two contours and their labels for each example .but in some cases such as when the flies are climbing along the walls making the distinction can be a bit trickier there are four input features for this pipeline stage namely the area and aspect ratio of both contours the latter determined via image moment analysis in this case a rescaling step is needed before training the logistic regression because contour areas and aspects ratios are of vastly different scales so the above update rule would otherwise perform quite poorly .last sentence.pipeline stage vs 	to further improve the quality of training data augmentation was applied by swapping the order of the two contours and their labels for each example 
1	109636	9636	in the third pipeline stage the 0 360 orientation of both flies is determined and this is done in a way that reduces the machine learning task to a binary classification .first sentence.as pre processing the image is first masked to everything expect the body of one fly after which point the orientation of the fly is determined using image.pipeline stage orientation	in the third pipeline stage the 0 360 orientation of both flies is determined and this is done in a way that reduces the machine learning task to a binary classification 
0	109637	9637	as pre processing the image is first masked to everything expect the body of one fly after which point the orientation of the fly is determined using image.in the third pipeline stage the 0 360 orientation of both flies is determined and this is done in a way that reduces the machine learning task to a binary classification .last sentence.pipeline stage orientation	as pre processing the image is first masked to everything expect the body of one fly after which point the orientation of the fly is determined using image
0	109638	9638	read frame from video .first sentence.threshold image and extract contours .vs orientation wingangle	read frame from video 
0	109639	9639	threshold image and extract contours .read frame from video .classify each contour as containing 0 1 or 2 flies if there are two one fly contours identify which is the male fly and which is the female fly for both flies determine the 0 360 orientation of the fly body for the male fly determine the 0 90 angle of each wing with respect to the major axis of its body .vs orientation wingangle	threshold image and extract contours 
1	109640	9640	classify each contour as containing 0 1 or 2 flies if there are two one fly contours identify which is the male fly and which is the female fly for both flies determine the 0 360 orientation of the fly body for the male fly determine the 0 90 angle of each wing with respect to the major axis of its body .threshold image and extract contours .moments where pq is central image moment of the masked image defined by the summation where x is the center of mass of the image and f x y is the intensity at pixel x y .vs orientation wingangle	classify each contour as containing 0 1 or 2 flies if there are two one fly contours identify which is the male fly and which is the female fly for both flies determine the 0 360 orientation of the fly body for the male fly determine the 0 90 angle of each wing with respect to the major axis of its body 
0	109641	9641	moments where pq is central image moment of the masked image defined by the summation where x is the center of mass of the image and f x y is the intensity at pixel x y .classify each contour as containing 0 1 or 2 flies if there are two one fly contours identify which is the male fly and which is the female fly for both flies determine the 0 360 orientation of the fly body for the male fly determine the 0 90 angle of each wing with respect to the major axis of its body .unfortunately the orientation angle produced using this approach has a sign ambiguity it cannot discern whether an object is facing forwards or backwards .vs orientation wingangle	moments where pq is central image moment of the masked image defined by the summation where x is the center of mass of the image and f x y is the intensity at pixel x y 
0	109642	9642	unfortunately the orientation angle produced using this approach has a sign ambiguity it cannot discern whether an object is facing forwards or backwards .moments where pq is central image moment of the masked image defined by the summation where x is the center of mass of the image and f x y is the intensity at pixel x y .as a result i needed to develop a machine learning algorithm to decide if the orientation computed via image moments should be corrected by adding 180 .vs orientation wingangle	unfortunately the orientation angle produced using this approach has a sign ambiguity it cannot discern whether an object is facing forwards or backwards 
1	109643	9643	as a result i needed to develop a machine learning algorithm to decide if the orientation computed via image moments should be corrected by adding 180 .unfortunately the orientation angle produced using this approach has a sign ambiguity it cannot discern whether an object is facing forwards or backwards .as shown in briefly the hog descriptor after computing the hog descriptor for a fly image the descriptor is projected onto a basis of 15 principal components or in other words the eigenvectors corresponding to the 15 largest eigenvectors of i x i x i t where x i is the hog descriptor of the ith training example as the final step of this pipeline stage the dimensionally reduced hog descriptor is fed into a logistic regression 2 .vs orientation wingangle	as a result i needed to develop a machine learning algorithm to decide if the orientation computed via image moments should be corrected by adding 180 
1	109644	9644	as shown in briefly the hog descriptor after computing the hog descriptor for a fly image the descriptor is projected onto a basis of 15 principal components or in other words the eigenvectors corresponding to the 15 largest eigenvectors of i x i x i t where x i is the hog descriptor of the ith training example as the final step of this pipeline stage the dimensionally reduced hog descriptor is fed into a logistic regression 2 .as a result i needed to develop a machine learning algorithm to decide if the orientation computed via image moments should be corrected by adding 180 .somewhat surprisingly as shown in.vs orientation wingangle	as shown in briefly the hog descriptor after computing the hog descriptor for a fly image the descriptor is projected onto a basis of 15 principal components or in other words the eigenvectors corresponding to the 15 largest eigenvectors of i x i x i t where x i is the hog descriptor of the ith training example as the final step of this pipeline stage the dimensionally reduced hog descriptor is fed into a logistic regression 2 
0	109645	9645	somewhat surprisingly as shown in.as shown in briefly the hog descriptor after computing the hog descriptor for a fly image the descriptor is projected onto a basis of 15 principal components or in other words the eigenvectors corresponding to the 15 largest eigenvectors of i x i x i t where x i is the hog descriptor of the ith training example as the final step of this pipeline stage the dimensionally reduced hog descriptor is fed into a logistic regression 2 .last sentence.vs orientation wingangle	somewhat surprisingly as shown in
0	109646	9646	in the final pipeline stage the angles of the right and left wings of the male fly are determined .first sentence.similarly to the previous stage preprocessing is used to construct a region of interest roi containing just the male fly and its wings .pipeline stage wing angle	in the final pipeline stage the angles of the right and left wings of the male fly are determined 
1	109647	9647	similarly to the previous stage preprocessing is used to construct a region of interest roi containing just the male fly and its wings .in the final pipeline stage the angles of the right and left wings of the male fly are determined .that roi can then be orientated in an upright direction using the results of the preceding pipeline stage .pipeline stage wing angle	similarly to the previous stage preprocessing is used to construct a region of interest roi containing just the male fly and its wings 
0	109648	9648	that roi can then be orientated in an upright direction using the results of the preceding pipeline stage .similarly to the previous stage preprocessing is used to construct a region of interest roi containing just the male fly and its wings .the preprocessing uses median blurring adaptive thresholding and erosion to preserve fly wings while removing fly legs in my approach the wing angles are determined separately by dividing the image of the male fly into two halves one for each wing .pipeline stage wing angle	that roi can then be orientated in an upright direction using the results of the preceding pipeline stage 
1	109649	9649	the preprocessing uses median blurring adaptive thresholding and erosion to preserve fly wings while removing fly legs in my approach the wing angles are determined separately by dividing the image of the male fly into two halves one for each wing .that roi can then be orientated in an upright direction using the results of the preceding pipeline stage .as shown in the final step in this pipeline stage is a linear regression on the reduced dimensionality hog descriptors .pipeline stage wing angle	the preprocessing uses median blurring adaptive thresholding and erosion to preserve fly wings while removing fly legs in my approach the wing angles are determined separately by dividing the image of the male fly into two halves one for each wing 
0	109650	9650	as shown in the final step in this pipeline stage is a linear regression on the reduced dimensionality hog descriptors .the preprocessing uses median blurring adaptive thresholding and erosion to preserve fly wings while removing fly legs in my approach the wing angles are determined separately by dividing the image of the male fly into two halves one for each wing .briefly linear regression seeks to minimize the mean squared error between predicted and given labels .pipeline stage wing angle	as shown in the final step in this pipeline stage is a linear regression on the reduced dimensionality hog descriptors 
0	109651	9651	briefly linear regression seeks to minimize the mean squared error between predicted and given labels .as shown in the final step in this pipeline stage is a linear regression on the reduced dimensionality hog descriptors .the optimal parameters can be computed directly by the equation x t x 1 x t y where x contains the features of all training examples and y is a vector of their labels .pipeline stage wing angle	briefly linear regression seeks to minimize the mean squared error between predicted and given labels 
0	109652	9652	the optimal parameters can be computed directly by the equation x t x 1 x t y where x contains the features of all training examples and y is a vector of their labels .briefly linear regression seeks to minimize the mean squared error between predicted and given labels .after computing the predicted label given features x is simply t x .pipeline stage wing angle	the optimal parameters can be computed directly by the equation x t x 1 x t y where x contains the features of all training examples and y is a vector of their labels 
0	109653	9653	after computing the predicted label given features x is simply t x .the optimal parameters can be computed directly by the equation x t x 1 x t y where x contains the features of all training examples and y is a vector of their labels .last sentence.pipeline stage wing angle	after computing the predicted label given features x is simply t x 
0	109654	9654	for all four pipeline stages error was evaluated by retaining one third of the dataset for testing this test set was not used at any point during training .first sentence.the first three stages of the pipeline were classifiers so their test error is reported simply as misclassification error .results	for all four pipeline stages error was evaluated by retaining one third of the dataset for testing this test set was not used at any point during training 
0	109655	9655	the first three stages of the pipeline were classifiers so their test error is reported simply as misclassification error .for all four pipeline stages error was evaluated by retaining one third of the dataset for testing this test set was not used at any point during training .as seen in 2sgmxtq .results	the first three stages of the pipeline were classifiers so their test error is reported simply as misclassification error 
0	109656	9656	as seen in 2sgmxtq .the first three stages of the pipeline were classifiers so their test error is reported simply as misclassification error .the wing angles over time are also plotted in.results	as seen in 2sgmxtq 
0	109657	9657	the wing angles over time are also plotted in.as seen in 2sgmxtq .last sentence.results	the wing angles over time are also plotted in
1	109658	9658	in this report i described a real time image processing pipeline intended for neurobiology experiments that takes as input a grayscale video of a pair of fruit flies and produces as output an estimate of the position orientation and sex of each fly in addition to the wing angles for the male fly .first sentence.the pipeline had four distinct stages employing machine learning techniques including a decision tree logistic regresion linear regression and pca .conclusion	in this report i described a real time image processing pipeline intended for neurobiology experiments that takes as input a grayscale video of a pair of fruit flies and produces as output an estimate of the position orientation and sex of each fly in addition to the wing angles for the male fly 
0	109659	9659	the pipeline had four distinct stages employing machine learning techniques including a decision tree logistic regresion linear regression and pca .in this report i described a real time image processing pipeline intended for neurobiology experiments that takes as input a grayscale video of a pair of fruit flies and produces as output an estimate of the position orientation and sex of each fly in addition to the wing angles for the male fly .for both classification and regression tasks i demonstrated good accuracy on 1530x1530 frames at a throughput of 84 fps without gpu acceleration in the orientation and wingangle stages hog was used in conjunction with pca to produce input features for a trained model .conclusion	the pipeline had four distinct stages employing machine learning techniques including a decision tree logistic regresion linear regression and pca 
1	109660	9660	for both classification and regression tasks i demonstrated good accuracy on 1530x1530 frames at a throughput of 84 fps without gpu acceleration in the orientation and wingangle stages hog was used in conjunction with pca to produce input features for a trained model .the pipeline had four distinct stages employing machine learning techniques including a decision tree logistic regresion linear regression and pca .in both cases i was surprised how well this worked even when just one or two principal components were used .conclusion	for both classification and regression tasks i demonstrated good accuracy on 1530x1530 frames at a throughput of 84 fps without gpu acceleration in the orientation and wingangle stages hog was used in conjunction with pca to produce input features for a trained model 
0	109661	9661	in both cases i was surprised how well this worked even when just one or two principal components were used .for both classification and regression tasks i demonstrated good accuracy on 1530x1530 frames at a throughput of 84 fps without gpu acceleration in the orientation and wingangle stages hog was used in conjunction with pca to produce input features for a trained model .i think one reason this approach was successful was that i designed the preprocessing stages in a way that increased hog variance due to the variable of interest orientation or wing angle while decreasing hog variance due to other variables fly legs the other wing the other fly background roughness etc .conclusion	in both cases i was surprised how well this worked even when just one or two principal components were used 
1	109662	9662	i think one reason this approach was successful was that i designed the preprocessing stages in a way that increased hog variance due to the variable of interest orientation or wing angle while decreasing hog variance due to other variables fly legs the other wing the other fly background roughness etc .in both cases i was surprised how well this worked even when just one or two principal components were used .this in turn was a useful lesson about the role of preprocessing and feature selection in machine learning in the future there are a number of possible directions to explore .conclusion	i think one reason this approach was successful was that i designed the preprocessing stages in a way that increased hog variance due to the variable of interest orientation or wing angle while decreasing hog variance due to other variables fly legs the other wing the other fly background roughness etc 
0	109663	9663	this in turn was a useful lesson about the role of preprocessing and feature selection in machine learning in the future there are a number of possible directions to explore .i think one reason this approach was successful was that i designed the preprocessing stages in a way that increased hog variance due to the variable of interest orientation or wing angle while decreasing hog variance due to other variables fly legs the other wing the other fly background roughness etc .first i could try measuring leg positions from the video preliminary experiments suggest that legs tips are selected fairly reliably with a keypoint detector such as sift.conclusion	this in turn was a useful lesson about the role of preprocessing and feature selection in machine learning in the future there are a number of possible directions to explore 
1	109664	9664	first i could try measuring leg positions from the video preliminary experiments suggest that legs tips are selected fairly reliably with a keypoint detector such as sift.this in turn was a useful lesson about the role of preprocessing and feature selection in machine learning in the future there are a number of possible directions to explore .last sentence.conclusion	first i could try measuring leg positions from the video preliminary experiments suggest that legs tips are selected fairly reliably with a keypoint detector such as sift
0	109665	9665	amazon fulfillment centers are bustling hubs of innovation that allow amazon to deliver millions of products to over 100 countries worldwide .first sentence.these products are randomly placed in bins which are carried by robots occasionally items are misplaced while being handled resulting in a mismatch the recorded bin inventory versus its actual content .i introduction	amazon fulfillment centers are bustling hubs of innovation that allow amazon to deliver millions of products to over 100 countries worldwide 
1	109666	9666	these products are randomly placed in bins which are carried by robots occasionally items are misplaced while being handled resulting in a mismatch the recorded bin inventory versus its actual content .amazon fulfillment centers are bustling hubs of innovation that allow amazon to deliver millions of products to over 100 countries worldwide .the paper describes methods to predict the number of items in a bin thus detecting any inventory variance .i introduction	these products are randomly placed in bins which are carried by robots occasionally items are misplaced while being handled resulting in a mismatch the recorded bin inventory versus its actual content 
1	109667	9667	the paper describes methods to predict the number of items in a bin thus detecting any inventory variance .these products are randomly placed in bins which are carried by robots occasionally items are misplaced while being handled resulting in a mismatch the recorded bin inventory versus its actual content .by correcting variance upon detection amazon will better serve its customers specifically the input to our model is a raw color photo of the products in a bin .i introduction	the paper describes methods to predict the number of items in a bin thus detecting any inventory variance 
1	109668	9668	by correcting variance upon detection amazon will better serve its customers specifically the input to our model is a raw color photo of the products in a bin .the paper describes methods to predict the number of items in a bin thus detecting any inventory variance .to find the best solution to the inventory mismatch problem amazon published the bin image dataset which is detailed in section ii the output of a model is the bin s predicted quantity the number of products in the image while we started with linear methods the quest for model performance lead us to non linear algorithms and ultimately to convolutional deep learning .i introduction	by correcting variance upon detection amazon will better serve its customers specifically the input to our model is a raw color photo of the products in a bin 
1	109669	9669	to find the best solution to the inventory mismatch problem amazon published the bin image dataset which is detailed in section ii the output of a model is the bin s predicted quantity the number of products in the image while we started with linear methods the quest for model performance lead us to non linear algorithms and ultimately to convolutional deep learning .by correcting variance upon detection amazon will better serve its customers specifically the input to our model is a raw color photo of the products in a bin .section iii summarizes each algorithms applied .i introduction	to find the best solution to the inventory mismatch problem amazon published the bin image dataset which is detailed in section ii the output of a model is the bin s predicted quantity the number of products in the image while we started with linear methods the quest for model performance lead us to non linear algorithms and ultimately to convolutional deep learning 
0	109670	9670	section iii summarizes each algorithms applied .to find the best solution to the inventory mismatch problem amazon published the bin image dataset which is detailed in section ii the output of a model is the bin s predicted quantity the number of products in the image while we started with linear methods the quest for model performance lead us to non linear algorithms and ultimately to convolutional deep learning .they include logistic regression and classification trees summarized in section iii a and section iii b support vector machines linear kernel polynomial kernel radial kernel .i introduction	section iii summarizes each algorithms applied 
0	109671	9671	they include logistic regression and classification trees summarized in section iii a and section iii b support vector machines linear kernel polynomial kernel radial kernel .section iii summarizes each algorithms applied .the algorithms are summarized in section iii c convolutional neural network resnet cross entropy loss function with learning rate optimizations .i introduction	they include logistic regression and classification trees summarized in section iii a and section iii b support vector machines linear kernel polynomial kernel radial kernel 
0	109672	9672	the algorithms are summarized in section iii c convolutional neural network resnet cross entropy loss function with learning rate optimizations .they include logistic regression and classification trees summarized in section iii a and section iii b support vector machines linear kernel polynomial kernel radial kernel .the algorithm summary in section iii d section iv summarizes performance resuts .i introduction	the algorithms are summarized in section iii c convolutional neural network resnet cross entropy loss function with learning rate optimizations 
0	109673	9673	the algorithm summary in section iii d section iv summarizes performance resuts .the algorithms are summarized in section iii c convolutional neural network resnet cross entropy loss function with learning rate optimizations .with convolutional neural networks we were able to achieve an overall accuracy exceeding 56 .i introduction	the algorithm summary in section iii d section iv summarizes performance resuts 
0	109674	9674	with convolutional neural networks we were able to achieve an overall accuracy exceeding 56 .the algorithm summary in section iii d section iv summarizes performance resuts .this is over 60 better than with support vector machines .i introduction	with convolutional neural networks we were able to achieve an overall accuracy exceeding 56 
0	109675	9675	this is over 60 better than with support vector machines .with convolutional neural networks we were able to achieve an overall accuracy exceeding 56 .for the latter we attained accuracy of over 30 .i introduction	this is over 60 better than with support vector machines 
0	109676	9676	for the latter we attained accuracy of over 30 .this is over 60 better than with support vector machines .we operated on a reduced dataset for bins that contained up to 5 products for a random baseline probability of 16 6 we are among the first to publish results on amazon s bin image dataset .i introduction	for the latter we attained accuracy of over 30 
1	109677	9677	we operated on a reduced dataset for bins that contained up to 5 products for a random baseline probability of 16 6 we are among the first to publish results on amazon s bin image dataset .for the latter we attained accuracy of over 30 .prior art by eunbyung park of the university of north carolina at chapel hill.i introduction	we operated on a reduced dataset for bins that contained up to 5 products for a random baseline probability of 16 6 we are among the first to publish results on amazon s bin image dataset 
0	109678	9678	prior art by eunbyung park of the university of north carolina at chapel hill.we operated on a reduced dataset for bins that contained up to 5 products for a random baseline probability of 16 6 we are among the first to publish results on amazon s bin image dataset .last sentence.i introduction	prior art by eunbyung park of the university of north carolina at chapel hill
0	109679	9679	the data set contains 535 234 images which contain 459 476 different product skews of different shapes and sizes .first sentence.each image metadata tuple corresponds to a bin with products .a input data set	the data set contains 535 234 images which contain 459 476 different product skews of different shapes and sizes 
0	109680	9680	each image metadata tuple corresponds to a bin with products .the data set contains 535 234 images which contain 459 476 different product skews of different shapes and sizes .the metadata includes the actual count of objects in the bin which is used as the label to train our model we worked with the subset of 150k images each containing with up to five products .a input data set	each image metadata tuple corresponds to a bin with products 
1	109681	9681	the metadata includes the actual count of objects in the bin which is used as the label to train our model we worked with the subset of 150k images each containing with up to five products .each image metadata tuple corresponds to a bin with products .we split this as follows 70 training 20 validation and 10 test .a input data set	the metadata includes the actual count of objects in the bin which is used as the label to train our model we worked with the subset of 150k images each containing with up to five products 
0	109682	9682	we split this as follows 70 training 20 validation and 10 test .the metadata includes the actual count of objects in the bin which is used as the label to train our model we worked with the subset of 150k images each containing with up to five products .last sentence.a input data set	we split this as follows 70 training 20 validation and 10 test 
1	109683	9683	before model training images were normalized 1 re sized to 224x224 pixels 2 tansformed for zero mean and unit variance 3 for convolutional models the dataset was augmented with horizontal flips of every image.first sentence.last sentence.b data engineering	before model training images were normalized 1 re sized to 224x224 pixels 2 tansformed for zero mean and unit variance 3 for convolutional models the dataset was augmented with horizontal flips of every image
0	109684	9684	we explored the blob features extraction .first sentence.blobs are bright on dark or dark on bright regions in an image .c feature engineering blobs	we explored the blob features extraction 
0	109685	9685	blobs are bright on dark or dark on bright regions in an image .we explored the blob features extraction .all the bins in which items are placed are similar and if items are present in the bin the idea is to make an attempt to create features assuming items in the bin are relatively bright on the backgrounds .c feature engineering blobs	blobs are bright on dark or dark on bright regions in an image 
1	109686	9686	all the bins in which items are placed are similar and if items are present in the bin the idea is to make an attempt to create features assuming items in the bin are relatively bright on the backgrounds .blobs are bright on dark or dark on bright regions in an image .we used laplacian of gaussian approach it computes the laplacian of images with successively increasing standard deviation and stacks them up in a cube .c feature engineering blobs	all the bins in which items are placed are similar and if items are present in the bin the idea is to make an attempt to create features assuming items in the bin are relatively bright on the backgrounds 
0	109687	9687	we used laplacian of gaussian approach it computes the laplacian of images with successively increasing standard deviation and stacks them up in a cube .all the bins in which items are placed are similar and if items are present in the bin the idea is to make an attempt to create features assuming items in the bin are relatively bright on the backgrounds .blobs are local maximas in this cube .c feature engineering blobs	we used laplacian of gaussian approach it computes the laplacian of images with successively increasing standard deviation and stacks them up in a cube 
0	109688	9688	blobs are local maximas in this cube .we used laplacian of gaussian approach it computes the laplacian of images with successively increasing standard deviation and stacks them up in a cube .note the yellow circles in.c feature engineering blobs	blobs are local maximas in this cube 
0	109689	9689	note the yellow circles in.blobs are local maximas in this cube .last sentence.c feature engineering blobs	note the yellow circles in
0	109690	9690	the probability that each observation is classified to each class is defined as a logistic function as followthe observation is assigned to the class in which it has highest probability .first sentence.last sentence.a logistic regression	the probability that each observation is classified to each class is defined as a logistic function as followthe observation is assigned to the class in which it has highest probability 
0	109691	9691	in a classification tree each observation belongs to the most commonly occurring class of training observations in the region to which it belongs .first sentence.we use the gini index of which measures total variance across the k classes as the loss function .b classification tree	in a classification tree each observation belongs to the most commonly occurring class of training observations in the region to which it belongs 
0	109692	9692	we use the gini index of which measures total variance across the k classes as the loss function .in a classification tree each observation belongs to the most commonly occurring class of training observations in the region to which it belongs .gini index is defined by.b classification tree	we use the gini index of which measures total variance across the k classes as the loss function 
0	109693	9693	gini index is defined by.we use the gini index of which measures total variance across the k classes as the loss function .last sentence.b classification tree	gini index is defined by
0	109694	9694	the support vector machine svm is an extension of the support vector classifier that results from enlarging the feature space in a specific way using kernels .first sentence.feature space is enlarged in order to accommodate a non linear boundary between the classes .c support vector machines	the support vector machine svm is an extension of the support vector classifier that results from enlarging the feature space in a specific way using kernels 
0	109695	9695	feature space is enlarged in order to accommodate a non linear boundary between the classes .the support vector machine svm is an extension of the support vector classifier that results from enlarging the feature space in a specific way using kernels .the kernel approach enable an efficient computational approach for svm .c support vector machines	feature space is enlarged in order to accommodate a non linear boundary between the classes 
0	109696	9696	the kernel approach enable an efficient computational approach for svm .feature space is enlarged in order to accommodate a non linear boundary between the classes .we have attempted several kernel types as follow polynomial kernel.c support vector machines	the kernel approach enable an efficient computational approach for svm 
0	109697	9697	we have attempted several kernel types as follow polynomial kernel.the kernel approach enable an efficient computational approach for svm .last sentence.c support vector machines	we have attempted several kernel types as follow polynomial kernel
1	109698	9698	resnet 1 classifier and loss function softmax layer and cross entropy loss cel function were used since we are solving multi class classification problem2 learning rate finder learning rate determines the step size of the update and is one of the key hyper parameters to training a network .first sentence.for some of our experiments we set the learning rate based on an approach introduced in the paper cyclical learning rates for training neural networks smith and leslie n.d convolutional neural networks	resnet 1 classifier and loss function softmax layer and cross entropy loss cel function were used since we are solving multi class classification problem2 learning rate finder learning rate determines the step size of the update and is one of the key hyper parameters to training a network 
0	109699	9699	for some of our experiments we set the learning rate based on an approach introduced in the paper cyclical learning rates for training neural networks smith and leslie n.resnet 1 classifier and loss function softmax layer and cross entropy loss cel function were used since we are solving multi class classification problem2 learning rate finder learning rate determines the step size of the update and is one of the key hyper parameters to training a network .last sentence.d convolutional neural networks	for some of our experiments we set the learning rate based on an approach introduced in the paper cyclical learning rates for training neural networks smith and leslie n
0	109700	9700	the performance of the best methods as well as the rationale leading to identifying them is outlined below given the nature of the data set we expect the decision boundary to be highly non linear .first sentence.last sentence.iv experiments	the performance of the best methods as well as the rationale leading to identifying them is outlined below given the nature of the data set we expect the decision boundary to be highly non linear 
0	109701	9701	several multi class classifiers were explored with raw pixel data .first sentence.last sentence.a multi class classification	several multi class classifiers were explored with raw pixel data 
0	109702	9702	next with the intention of arriving at a more useful description of an image than raw pixel data a number of feature extraction algorithms were attempted .first sentence.with respect to histogram of oriented gradients hog evaluation suggests that the images in the data set do not have enough dominant gradients .b feature selection	next with the intention of arriving at a more useful description of an image than raw pixel data a number of feature extraction algorithms were attempted 
1	109703	9703	with respect to histogram of oriented gradients hog evaluation suggests that the images in the data set do not have enough dominant gradients .next with the intention of arriving at a more useful description of an image than raw pixel data a number of feature extraction algorithms were attempted .thus identifying products in a bin is difficult .b feature selection	with respect to histogram of oriented gradients hog evaluation suggests that the images in the data set do not have enough dominant gradients 
0	109704	9704	thus identifying products in a bin is difficult .with respect to histogram of oriented gradients hog evaluation suggests that the images in the data set do not have enough dominant gradients .in part this may be due to amazon s usage of tape to cover products in a bin .b feature selection	thus identifying products in a bin is difficult 
0	109705	9705	in part this may be due to amazon s usage of tape to cover products in a bin .thus identifying products in a bin is difficult .for many images the tape occludes the products causing a significant information loss .b feature selection	in part this may be due to amazon s usage of tape to cover products in a bin 
0	109706	9706	for many images the tape occludes the products causing a significant information loss .in part this may be due to amazon s usage of tape to cover products in a bin .last sentence.b feature selection	for many images the tape occludes the products causing a significant information loss 
0	109707	9707	first manually cleaning the data to remove such images would enhance learning .first sentence.second with the adam optimizer we have seen that performance increases with a larger dataset .vi future work	first manually cleaning the data to remove such images would enhance learning 
0	109708	9708	second with the adam optimizer we have seen that performance increases with a larger dataset .first manually cleaning the data to remove such images would enhance learning .we are intrigued by the possibility that photos be taken by from different angles .vi future work	second with the adam optimizer we have seen that performance increases with a larger dataset 
0	109709	9709	we are intrigued by the possibility that photos be taken by from different angles .second with the adam optimizer we have seen that performance increases with a larger dataset .and that metadata connect the content of a bin over time so belief state may be tracked .vi future work	we are intrigued by the possibility that photos be taken by from different angles 
0	109710	9710	and that metadata connect the content of a bin over time so belief state may be tracked .we are intrigued by the possibility that photos be taken by from different angles .third with over 450k product skews images are bound to violate our assumption that they re drawn from a single distribution .vi future work	and that metadata connect the content of a bin over time so belief state may be tracked 
0	109711	9711	third with over 450k product skews images are bound to violate our assumption that they re drawn from a single distribution .and that metadata connect the content of a bin over time so belief state may be tracked .thus forming ensemble models with different architectures and learning approaches may achieve higher accuracy .vi future work	third with over 450k product skews images are bound to violate our assumption that they re drawn from a single distribution 
0	109712	9712	thus forming ensemble models with different architectures and learning approaches may achieve higher accuracy .third with over 450k product skews images are bound to violate our assumption that they re drawn from a single distribution .the project s repository is https github com onenow aiinventory reconciliation vii .vi future work	thus forming ensemble models with different architectures and learning approaches may achieve higher accuracy 
0	109713	9713	the project s repository is https github com onenow aiinventory reconciliation vii .thus forming ensemble models with different architectures and learning approaches may achieve higher accuracy .contributions pablo s primary contribution was on support vector machines sravan s on convolutional neural networks and nutchapols across the board .vi future work	the project s repository is https github com onenow aiinventory reconciliation vii 
0	109714	9714	contributions pablo s primary contribution was on support vector machines sravan s on convolutional neural networks and nutchapols across the board .the project s repository is https github com onenow aiinventory reconciliation vii .last sentence.vi future work	contributions pablo s primary contribution was on support vector machines sravan s on convolutional neural networks and nutchapols across the board 
0	109715	9715	the authors are grateful to professor andrew ng for his masterly transmission of machine learning to us .first sentence.last sentence.acknowledgment	the authors are grateful to professor andrew ng for his masterly transmission of machine learning to us 
0	109716	9716	pablo rodriguez bertorello leads next generation data engineering at cadreon a maketing technology platform company .first sentence.previously he was cto of airfox which completed a successful initial coin offering .authors	pablo rodriguez bertorello leads next generation data engineering at cadreon a maketing technology platform company 
0	109717	9717	previously he was cto of airfox which completed a successful initial coin offering .pablo rodriguez bertorello leads next generation data engineering at cadreon a maketing technology platform company .he is the co inventor of cloud platform company acquired by oracle .authors	previously he was cto of airfox which completed a successful initial coin offering 
0	109718	9718	he is the co inventor of cloud platform company acquired by oracle .previously he was cto of airfox which completed a successful initial coin offering .and the original designer of the data bus for intel s itanium processor .authors	he is the co inventor of cloud platform company acquired by oracle 
0	109719	9719	and the original designer of the data bus for intel s itanium processor .he is the co inventor of cloud platform company acquired by oracle .pablo has been issued over a dozen patents sravan sripada works at amazon .authors	and the original designer of the data bus for intel s itanium processor 
0	109720	9720	pablo has been issued over a dozen patents sravan sripada works at amazon .and the original designer of the data bus for intel s itanium processor .he is interested in applying artificial intelligence techniques to solve problems in retail cloud computing and voice controlled devices nutchapol dendumrongsup is a master s student at the institute for computational and mathematical engineering and deapartment of energy resources engineering at stanford .authors	pablo has been issued over a dozen patents sravan sripada works at amazon 
0	109721	9721	he is interested in applying artificial intelligence techniques to solve problems in retail cloud computing and voice controlled devices nutchapol dendumrongsup is a master s student at the institute for computational and mathematical engineering and deapartment of energy resources engineering at stanford .pablo has been issued over a dozen patents sravan sripada works at amazon .he is interested in the application of machine learning in the energy industry and the traditional reservoir simulation in oil and gas industry .authors	he is interested in applying artificial intelligence techniques to solve problems in retail cloud computing and voice controlled devices nutchapol dendumrongsup is a master s student at the institute for computational and mathematical engineering and deapartment of energy resources engineering at stanford 
0	109722	9722	he is interested in the application of machine learning in the energy industry and the traditional reservoir simulation in oil and gas industry .he is interested in applying artificial intelligence techniques to solve problems in retail cloud computing and voice controlled devices nutchapol dendumrongsup is a master s student at the institute for computational and mathematical engineering and deapartment of energy resources engineering at stanford .last sentence.authors	he is interested in the application of machine learning in the energy industry and the traditional reservoir simulation in oil and gas industry 
0	109723	9723	office hours at stanford are typically subject to significant variance in student demand .first sentence.to tackle this problem we predict student demand at any office hours on an hourly basis using data scraped from queuestatus carta and course syllabi .abstract	office hours at stanford are typically subject to significant variance in student demand 
1	109724	9724	to tackle this problem we predict student demand at any office hours on an hourly basis using data scraped from queuestatus carta and course syllabi .office hours at stanford are typically subject to significant variance in student demand .we conducted experiments using regression on fully connected nns univariate and multivariate lstms and compared with an ensemble of multimodal classification models such as random forests and svms .abstract	to tackle this problem we predict student demand at any office hours on an hourly basis using data scraped from queuestatus carta and course syllabi 
1	109725	9725	we conducted experiments using regression on fully connected nns univariate and multivariate lstms and compared with an ensemble of multimodal classification models such as random forests and svms .to tackle this problem we predict student demand at any office hours on an hourly basis using data scraped from queuestatus carta and course syllabi .we compared different losses such as mse mae huber and our own sqhuber against normalized inputs and evaluate on student demand with and without smoothing .abstract	we conducted experiments using regression on fully connected nns univariate and multivariate lstms and compared with an ensemble of multimodal classification models such as random forests and svms 
1	109726	9726	we compared different losses such as mse mae huber and our own sqhuber against normalized inputs and evaluate on student demand with and without smoothing .we conducted experiments using regression on fully connected nns univariate and multivariate lstms and compared with an ensemble of multimodal classification models such as random forests and svms .results show that our models predict demand well on held out test quarters both in seen and unseen courses .abstract	we compared different losses such as mse mae huber and our own sqhuber against normalized inputs and evaluate on student demand with and without smoothing 
0	109727	9727	results show that our models predict demand well on held out test quarters both in seen and unseen courses .we compared different losses such as mse mae huber and our own sqhuber against normalized inputs and evaluate on student demand with and without smoothing .our model could thus be a useful reference for both new and existing courses .abstract	results show that our models predict demand well on held out test quarters both in seen and unseen courses 
0	109728	9728	our model could thus be a useful reference for both new and existing courses .results show that our models predict demand well on held out test quarters both in seen and unseen courses .last sentence.abstract	our model could thus be a useful reference for both new and existing courses 
0	109729	9729	among cs students at stanford the experience of queueing at office hours ohs is practically universal .first sentence.office hours is an important part of any class allowing students to get valuable one on one help .introduction	among cs students at stanford the experience of queueing at office hours ohs is practically universal 
0	109730	9730	office hours is an important part of any class allowing students to get valuable one on one help .among cs students at stanford the experience of queueing at office hours ohs is practically universal .unfortunately student demand is prone to high variance resulting in sometimes students waiting hours before receiving help or conversely teaching assistants tas waiting hours before any students arrive .introduction	office hours is an important part of any class allowing students to get valuable one on one help 
1	109731	9731	unfortunately student demand is prone to high variance resulting in sometimes students waiting hours before receiving help or conversely teaching assistants tas waiting hours before any students arrive .office hours is an important part of any class allowing students to get valuable one on one help .in particular periods of overcrowding are a source of stress for both students and tas and are among the most commonly cited sources of negative experience on carta .introduction	unfortunately student demand is prone to high variance resulting in sometimes students waiting hours before receiving help or conversely teaching assistants tas waiting hours before any students arrive 
0	109732	9732	in particular periods of overcrowding are a source of stress for both students and tas and are among the most commonly cited sources of negative experience on carta .unfortunately student demand is prone to high variance resulting in sometimes students waiting hours before receiving help or conversely teaching assistants tas waiting hours before any students arrive .thus improvements in oh scheduling could significantly improve overall course experience for all parties however as with all logistical decision making at universities there are significant complexities in the process .introduction	in particular periods of overcrowding are a source of stress for both students and tas and are among the most commonly cited sources of negative experience on carta 
1	109733	9733	thus improvements in oh scheduling could significantly improve overall course experience for all parties however as with all logistical decision making at universities there are significant complexities in the process .in particular periods of overcrowding are a source of stress for both students and tas and are among the most commonly cited sources of negative experience on carta .our project addresses the arguably most variable component of the input predicting peaks of student demand .introduction	thus improvements in oh scheduling could significantly improve overall course experience for all parties however as with all logistical decision making at universities there are significant complexities in the process 
0	109734	9734	our project addresses the arguably most variable component of the input predicting peaks of student demand .thus improvements in oh scheduling could significantly improve overall course experience for all parties however as with all logistical decision making at universities there are significant complexities in the process .using hourly oh data scraped from queuestatus course information from carta and major dates from class syllabi we trained a fully connected neural network model that predicts the hourly load influx for any given course and quarter .introduction	our project addresses the arguably most variable component of the input predicting peaks of student demand 
1	109735	9735	using hourly oh data scraped from queuestatus course information from carta and major dates from class syllabi we trained a fully connected neural network model that predicts the hourly load influx for any given course and quarter .our project addresses the arguably most variable component of the input predicting peaks of student demand .we define the load influx as the average serve time for the day times the number of student signups .introduction	using hourly oh data scraped from queuestatus course information from carta and major dates from class syllabi we trained a fully connected neural network model that predicts the hourly load influx for any given course and quarter 
0	109736	9736	we define the load influx as the average serve time for the day times the number of student signups .using hourly oh data scraped from queuestatus course information from carta and major dates from class syllabi we trained a fully connected neural network model that predicts the hourly load influx for any given course and quarter .conceptually this is the aggregate ta time needed to satisfy all student demand over some period .introduction	we define the load influx as the average serve time for the day times the number of student signups 
0	109737	9737	conceptually this is the aggregate ta time needed to satisfy all student demand over some period .we define the load influx as the average serve time for the day times the number of student signups .note in terms of dataset and big picture goals this is a shared project between cs229 and cs221 .introduction	conceptually this is the aggregate ta time needed to satisfy all student demand over some period 
0	109738	9738	note in terms of dataset and big picture goals this is a shared project between cs229 and cs221 .conceptually this is the aggregate ta time needed to satisfy all student demand over some period .for cs229 we focused on a more theoretical approach in predicting load influx by designing and evaluating new loss functions catered towards data with high variance and fluctuations .introduction	note in terms of dataset and big picture goals this is a shared project between cs229 and cs221 
1	109739	9739	for cs229 we focused on a more theoretical approach in predicting load influx by designing and evaluating new loss functions catered towards data with high variance and fluctuations .note in terms of dataset and big picture goals this is a shared project between cs229 and cs221 .we also combine an ensemble of approaches to fine tune our prediction by using signal processing practices as well as experiment with multimodal classification using svms and random forest models .introduction	for cs229 we focused on a more theoretical approach in predicting load influx by designing and evaluating new loss functions catered towards data with high variance and fluctuations 
0	109740	9740	we also combine an ensemble of approaches to fine tune our prediction by using signal processing practices as well as experiment with multimodal classification using svms and random forest models .for cs229 we focused on a more theoretical approach in predicting load influx by designing and evaluating new loss functions catered towards data with high variance and fluctuations .for cs221 we focus on assigning tas to the surge timings using modified gibbs sampling and em algorithms as well as lstm prediction models approaches of a cs229 project that had a similar goal .introduction	we also combine an ensemble of approaches to fine tune our prediction by using signal processing practices as well as experiment with multimodal classification using svms and random forest models 
1	109741	9741	for cs221 we focus on assigning tas to the surge timings using modified gibbs sampling and em algorithms as well as lstm prediction models approaches of a cs229 project that had a similar goal .we also combine an ensemble of approaches to fine tune our prediction by using signal processing practices as well as experiment with multimodal classification using svms and random forest models .troccoli et .introduction	for cs221 we focus on assigning tas to the surge timings using modified gibbs sampling and em algorithms as well as lstm prediction models approaches of a cs229 project that had a similar goal 
0	109742	9742	troccoli et .for cs221 we focus on assigning tas to the surge timings using modified gibbs sampling and em algorithms as well as lstm prediction models approaches of a cs229 project that had a similar goal .al used custom feature extractors to predict wait times at the lair cs106 office hours .introduction	troccoli et 
0	109743	9743	al used custom feature extractors to predict wait times at the lair cs106 office hours .troccoli et .last sentence.introduction	al used custom feature extractors to predict wait times at the lair cs106 office hours 
0	109744	9744	to obtain data we set up a pipeline that scrapes hourly office hours from queuestatus .first sentence.through customized json parsing we were able to obtain a combined 17 quarters of data across 7 prominent cs classes .datasets and features	to obtain data we set up a pipeline that scrapes hourly office hours from queuestatus 
0	109745	9745	through customized json parsing we were able to obtain a combined 17 quarters of data across 7 prominent cs classes .to obtain data we set up a pipeline that scrapes hourly office hours from queuestatus .after preprocessing to remove all entries with zero serves and signups we ended with 4672 hours or just under 200 straight days worth of oh data .datasets and features	through customized json parsing we were able to obtain a combined 17 quarters of data across 7 prominent cs classes 
0	109746	9746	after preprocessing to remove all entries with zero serves and signups we ended with 4672 hours or just under 200 straight days worth of oh data .through customized json parsing we were able to obtain a combined 17 quarters of data across 7 prominent cs classes .a summary is shown below .datasets and features	after preprocessing to remove all entries with zero serves and signups we ended with 4672 hours or just under 200 straight days worth of oh data 
0	109747	9747	a summary is shown below .after preprocessing to remove all entries with zero serves and signups we ended with 4672 hours or just under 200 straight days worth of oh data .we experimented with a plethora of features to augment our dataset with and decided on the following predictors based on a combination of logic and significant correlation with load influx .datasets and features	a summary is shown below 
1	109748	9748	we experimented with a plethora of features to augment our dataset with and decided on the following predictors based on a combination of logic and significant correlation with load influx .a summary is shown below .on a per class basis we used number of enrolled students instructor rating and proportion of freshman graduate phd students enrolled .datasets and features	we experimented with a plethora of features to augment our dataset with and decided on the following predictors based on a combination of logic and significant correlation with load influx 
0	109749	9749	on a per class basis we used number of enrolled students instructor rating and proportion of freshman graduate phd students enrolled .we experimented with a plethora of features to augment our dataset with and decided on the following predictors based on a combination of logic and significant correlation with load influx .on a per hour day basis we used days until next assignment due days after previous assignment due days until an exam hour of day weekdays .datasets and features	on a per class basis we used number of enrolled students instructor rating and proportion of freshman graduate phd students enrolled 
0	109750	9750	on a per hour day basis we used days until next assignment due days after previous assignment due days until an exam hour of day weekdays .on a per class basis we used number of enrolled students instructor rating and proportion of freshman graduate phd students enrolled .for the hourly daily features validation testing found that one hot bucket encodings were more effective for predictions .datasets and features	on a per hour day basis we used days until next assignment due days after previous assignment due days until an exam hour of day weekdays 
1	109751	9751	for the hourly daily features validation testing found that one hot bucket encodings were more effective for predictions .on a per hour day basis we used days until next assignment due days after previous assignment due days until an exam hour of day weekdays .day differences were bucketed in ranges of 10 to 5 4 to 3 2 to 1 and 0 .datasets and features	for the hourly daily features validation testing found that one hot bucket encodings were more effective for predictions 
0	109752	9752	day differences were bucketed in ranges of 10 to 5 4 to 3 2 to 1 and 0 .for the hourly daily features validation testing found that one hot bucket encodings were more effective for predictions .hour of day was evenly bucketed into morning noon afternoon and evening .datasets and features	day differences were bucketed in ranges of 10 to 5 4 to 3 2 to 1 and 0 
0	109753	9753	hour of day was evenly bucketed into morning noon afternoon and evening .day differences were bucketed in ranges of 10 to 5 4 to 3 2 to 1 and 0 .each entry corresponds to one hour of oh and every entry in the same course quarter shares the same course quarter features .datasets and features	hour of day was evenly bucketed into morning noon afternoon and evening 
0	109754	9754	each entry corresponds to one hour of oh and every entry in the same course quarter shares the same course quarter features .hour of day was evenly bucketed into morning noon afternoon and evening .as discussed later we also experimented with log transformations as our ultimate goal is to predict entire unseen quarters we separated our training validation test sets by entire quarters .datasets and features	each entry corresponds to one hour of oh and every entry in the same course quarter shares the same course quarter features 
1	109755	9755	as discussed later we also experimented with log transformations as our ultimate goal is to predict entire unseen quarters we separated our training validation test sets by entire quarters .each entry corresponds to one hour of oh and every entry in the same course quarter shares the same course quarter features .due to our limited sample size we use k fold cross validation to tune hyperparameters where k is our number of quarters .datasets and features	as discussed later we also experimented with log transformations as our ultimate goal is to predict entire unseen quarters we separated our training validation test sets by entire quarters 
0	109756	9756	due to our limited sample size we use k fold cross validation to tune hyperparameters where k is our number of quarters .as discussed later we also experimented with log transformations as our ultimate goal is to predict entire unseen quarters we separated our training validation test sets by entire quarters .our test set consisted of 4 total classes cs110 spring 2018 and cs107 spring 2017 as unseen quarters of classes we trained on and cs224n winter 2018 and cs231n spring 2018 as entirely unseen courses .datasets and features	due to our limited sample size we use k fold cross validation to tune hyperparameters where k is our number of quarters 
0	109757	9757	our test set consisted of 4 total classes cs110 spring 2018 and cs107 spring 2017 as unseen quarters of classes we trained on and cs224n winter 2018 and cs231n spring 2018 as entirely unseen courses .due to our limited sample size we use k fold cross validation to tune hyperparameters where k is our number of quarters .our training set thus consisted of the remaining classes totaling 13 quarters of data between 5 unique classes we note that after training models to predict load influx on these datasets we do not predict hourly student demand for tas as is ideal .datasets and features	our test set consisted of 4 total classes cs110 spring 2018 and cs107 spring 2017 as unseen quarters of classes we trained on and cs224n winter 2018 and cs231n spring 2018 as entirely unseen courses 
1	109758	9758	our training set thus consisted of the remaining classes totaling 13 quarters of data between 5 unique classes we note that after training models to predict load influx on these datasets we do not predict hourly student demand for tas as is ideal .our test set consisted of 4 total classes cs110 spring 2018 and cs107 spring 2017 as unseen quarters of classes we trained on and cs224n winter 2018 and cs231n spring 2018 as entirely unseen courses .rather we predict hourly student demand for tas given that office hours is held .datasets and features	our training set thus consisted of the remaining classes totaling 13 quarters of data between 5 unique classes we note that after training models to predict load influx on these datasets we do not predict hourly student demand for tas as is ideal 
0	109759	9759	rather we predict hourly student demand for tas given that office hours is held .our training set thus consisted of the remaining classes totaling 13 quarters of data between 5 unique classes we note that after training models to predict load influx on these datasets we do not predict hourly student demand for tas as is ideal .we determined that current ta assignments are uncorrelated with time of day p 0 63 cor test in r and typically scheduled throughout active hours .datasets and features	rather we predict hourly student demand for tas given that office hours is held 
0	109760	9760	we determined that current ta assignments are uncorrelated with time of day p 0 63 cor test in r and typically scheduled throughout active hours .rather we predict hourly student demand for tas given that office hours is held .therefore we assume that the status quo scheduling of office hours is frequent and unbiased enough such that real student demand is proportional to the student demand given office hours is held .datasets and features	we determined that current ta assignments are uncorrelated with time of day p 0 63 cor test in r and typically scheduled throughout active hours 
0	109761	9761	therefore we assume that the status quo scheduling of office hours is frequent and unbiased enough such that real student demand is proportional to the student demand given office hours is held .we determined that current ta assignments are uncorrelated with time of day p 0 63 cor test in r and typically scheduled throughout active hours .last sentence.datasets and features	therefore we assume that the status quo scheduling of office hours is frequent and unbiased enough such that real student demand is proportional to the student demand given office hours is held 
1	109762	9762	we first implemented multimodal classification models as baselines where instead of using equidepth buckets we divided the minimum and maximum load influx into 7 logarithmic time buckets .first sentence.using svms with radial kernel and random forests with 1000 estimators we obtained an initial baseline with accuracy 0 422 and 0 359 respectively with the confusion matrix as shown below .multimodal classification	we first implemented multimodal classification models as baselines where instead of using equidepth buckets we divided the minimum and maximum load influx into 7 logarithmic time buckets 
1	109763	9763	using svms with radial kernel and random forests with 1000 estimators we obtained an initial baseline with accuracy 0 422 and 0 359 respectively with the confusion matrix as shown below .we first implemented multimodal classification models as baselines where instead of using equidepth buckets we divided the minimum and maximum load influx into 7 logarithmic time buckets .we see that even with fine tuning of hyperparameters the classification models have decent performance but with large skew and variance in predicting high load influxes which could be possibly due to class imbalance in different buckets when on a log scale .multimodal classification	using svms with radial kernel and random forests with 1000 estimators we obtained an initial baseline with accuracy 0 422 and 0 359 respectively with the confusion matrix as shown below 
1	109764	9764	we see that even with fine tuning of hyperparameters the classification models have decent performance but with large skew and variance in predicting high load influxes which could be possibly due to class imbalance in different buckets when on a log scale .using svms with radial kernel and random forests with 1000 estimators we obtained an initial baseline with accuracy 0 422 and 0 359 respectively with the confusion matrix as shown below .we thus choose to focus on regression next to predict the spikes of load influx in different hours .multimodal classification	we see that even with fine tuning of hyperparameters the classification models have decent performance but with large skew and variance in predicting high load influxes which could be possibly due to class imbalance in different buckets when on a log scale 
1	109765	9765	we thus choose to focus on regression next to predict the spikes of load influx in different hours .we see that even with fine tuning of hyperparameters the classification models have decent performance but with large skew and variance in predicting high load influxes which could be possibly due to class imbalance in different buckets when on a log scale .last sentence.multimodal classification	we thus choose to focus on regression next to predict the spikes of load influx in different hours 
1	109766	9766	we also set up baselines by training fully connected networks and lstms for regression tasks .first sentence.the fcn which approximates functions with non linearities and multiple layers that activate depending on weights mapped to higher dimension across stacked layers has input size 30 with our 30 features with 2 hidden layers of size 15 and 4 respectively followed by a single output final layer .regression fcn huber and sqhuber loss	we also set up baselines by training fully connected networks and lstms for regression tasks 
1	109767	9767	the fcn which approximates functions with non linearities and multiple layers that activate depending on weights mapped to higher dimension across stacked layers has input size 30 with our 30 features with 2 hidden layers of size 15 and 4 respectively followed by a single output final layer .we also set up baselines by training fully connected networks and lstms for regression tasks .each hidden layer uses a relu activation function with a linear activation for the output layer .regression fcn huber and sqhuber loss	the fcn which approximates functions with non linearities and multiple layers that activate depending on weights mapped to higher dimension across stacked layers has input size 30 with our 30 features with 2 hidden layers of size 15 and 4 respectively followed by a single output final layer 
0	109768	9768	each hidden layer uses a relu activation function with a linear activation for the output layer .the fcn which approximates functions with non linearities and multiple layers that activate depending on weights mapped to higher dimension across stacked layers has input size 30 with our 30 features with 2 hidden layers of size 15 and 4 respectively followed by a single output final layer .we also experimented with 3 4 hidden layers which led to overfitting even with normalization techniques that performed worse on the validation set lstms long short term memory is a form of recurrent neural network focused in 221 report .regression fcn huber and sqhuber loss	each hidden layer uses a relu activation function with a linear activation for the output layer 
1	109769	9769	we also experimented with 3 4 hidden layers which led to overfitting even with normalization techniques that performed worse on the validation set lstms long short term memory is a form of recurrent neural network focused in 221 report .each hidden layer uses a relu activation function with a linear activation for the output layer .it addresses vanishing gradients while factoring in previous states with a recurrence formula at each time step which makes it suitable for temporal data .regression fcn huber and sqhuber loss	we also experimented with 3 4 hidden layers which led to overfitting even with normalization techniques that performed worse on the validation set lstms long short term memory is a form of recurrent neural network focused in 221 report 
0	109770	9770	it addresses vanishing gradients while factoring in previous states with a recurrence formula at each time step which makes it suitable for temporal data .we also experimented with 3 4 hidden layers which led to overfitting even with normalization techniques that performed worse on the validation set lstms long short term memory is a form of recurrent neural network focused in 221 report .we used two lstm cells in autoregressive lstm with window size of 16 and each output was fed back as part of the next window .regression fcn huber and sqhuber loss	it addresses vanishing gradients while factoring in previous states with a recurrence formula at each time step which makes it suitable for temporal data 
0	109771	9771	we used two lstm cells in autoregressive lstm with window size of 16 and each output was fed back as part of the next window .it addresses vanishing gradients while factoring in previous states with a recurrence formula at each time step which makes it suitable for temporal data .all input features were normalized in a range 0 1 for every experiment and all baseline models were compiled with adam optimizer with early stopping to prevent overfitting .regression fcn huber and sqhuber loss	we used two lstm cells in autoregressive lstm with window size of 16 and each output was fed back as part of the next window 
1	109772	9772	all input features were normalized in a range 0 1 for every experiment and all baseline models were compiled with adam optimizer with early stopping to prevent overfitting .we used two lstm cells in autoregressive lstm with window size of 16 and each output was fed back as part of the next window .due to insufficient data we face high variance in training lstms with the initial baselines reported below therefore we choose to continue work on the fully connected network fcn .regression fcn huber and sqhuber loss	all input features were normalized in a range 0 1 for every experiment and all baseline models were compiled with adam optimizer with early stopping to prevent overfitting 
1	109773	9773	due to insufficient data we face high variance in training lstms with the initial baselines reported below therefore we choose to continue work on the fully connected network fcn .all input features were normalized in a range 0 1 for every experiment and all baseline models were compiled with adam optimizer with early stopping to prevent overfitting .however in our fcn we notice our predictions for load influx throughout the quarter suffer from a consistent offset from the mean of the distribution .regression fcn huber and sqhuber loss	due to insufficient data we face high variance in training lstms with the initial baselines reported below therefore we choose to continue work on the fully connected network fcn 
1	109774	9774	however in our fcn we notice our predictions for load influx throughout the quarter suffer from a consistent offset from the mean of the distribution .due to insufficient data we face high variance in training lstms with the initial baselines reported below therefore we choose to continue work on the fully connected network fcn .upon inspection we suspect that the large amount of outliers may have caused the bias due to their huge penalties while minimizing the l2 norm loss function .regression fcn huber and sqhuber loss	however in our fcn we notice our predictions for load influx throughout the quarter suffer from a consistent offset from the mean of the distribution 
1	109775	9775	upon inspection we suspect that the large amount of outliers may have caused the bias due to their huge penalties while minimizing the l2 norm loss function .however in our fcn we notice our predictions for load influx throughout the quarter suffer from a consistent offset from the mean of the distribution .thus we seek a new loss function that doesn t penalize outliers as heavily .regression fcn huber and sqhuber loss	upon inspection we suspect that the large amount of outliers may have caused the bias due to their huge penalties while minimizing the l2 norm loss function 
0	109776	9776	thus we seek a new loss function that doesn t penalize outliers as heavily .upon inspection we suspect that the large amount of outliers may have caused the bias due to their huge penalties while minimizing the l2 norm loss function .the huber loss is particularly useful for this since it scales linearly outside a specified domain we compare this traditional loss function with a novel loss function we designed for the purposes of experimentation the sqhuber loss .regression fcn huber and sqhuber loss	thus we seek a new loss function that doesn t penalize outliers as heavily 
0	109777	9777	the huber loss is particularly useful for this since it scales linearly outside a specified domain we compare this traditional loss function with a novel loss function we designed for the purposes of experimentation the sqhuber loss .thus we seek a new loss function that doesn t penalize outliers as heavily .the sqhuber loss is defined as the sqhuber loss is piece wise continuous and scales proportional to the square root of the residual for values above a specified domain .regression fcn huber and sqhuber loss	the huber loss is particularly useful for this since it scales linearly outside a specified domain we compare this traditional loss function with a novel loss function we designed for the purposes of experimentation the sqhuber loss 
0	109778	9778	the sqhuber loss is defined as the sqhuber loss is piece wise continuous and scales proportional to the square root of the residual for values above a specified domain .the huber loss is particularly useful for this since it scales linearly outside a specified domain we compare this traditional loss function with a novel loss function we designed for the purposes of experimentation the sqhuber loss .thus it is even more robust to significant amounts of outliers .regression fcn huber and sqhuber loss	the sqhuber loss is defined as the sqhuber loss is piece wise continuous and scales proportional to the square root of the residual for values above a specified domain 
0	109779	9779	thus it is even more robust to significant amounts of outliers .the sqhuber loss is defined as the sqhuber loss is piece wise continuous and scales proportional to the square root of the residual for values above a specified domain .last sentence.regression fcn huber and sqhuber loss	thus it is even more robust to significant amounts of outliers 
0	109780	9780	the load influx is an erratic function .first sentence.large fluctuations or spikes in the load are difficult to predict without overfitting the model thus transforming the training labels actual load influx before training may be fruitful .transforming the load influx data	the load influx is an erratic function 
1	109781	9781	large fluctuations or spikes in the load are difficult to predict without overfitting the model thus transforming the training labels actual load influx before training may be fruitful .the load influx is an erratic function .we attempted two methods to transform our data for better predictions 1 .transforming the load influx data	large fluctuations or spikes in the load are difficult to predict without overfitting the model thus transforming the training labels actual load influx before training may be fruitful 
0	109782	9782	we attempted two methods to transform our data for better predictions 1 .large fluctuations or spikes in the load are difficult to predict without overfitting the model thus transforming the training labels actual load influx before training may be fruitful .hanning window a 1 d convolution with a hanning window .transforming the load influx data	we attempted two methods to transform our data for better predictions 1 
0	109783	9783	hanning window a 1 d convolution with a hanning window .we attempted two methods to transform our data for better predictions 1 .this reduces spikes and thus potential to overfit .transforming the load influx data	hanning window a 1 d convolution with a hanning window 
0	109784	9784	this reduces spikes and thus potential to overfit .hanning window a 1 d convolution with a hanning window .last sentence.transforming the load influx data	this reduces spikes and thus potential to overfit 
1	109785	9785	during validation tests we find the root mean squared error rmse to be heavily dependent on the course and quarter used .first sentence.thus to reduce variance in rmse obtained from validation tests we used leave one out cross validation for our validation studies where we stochastically choose a course quarter pair as the validation set and use the remainder of the joint training validation sets for training .k fold cross validation	during validation tests we find the root mean squared error rmse to be heavily dependent on the course and quarter used 
1	109786	9786	thus to reduce variance in rmse obtained from validation tests we used leave one out cross validation for our validation studies where we stochastically choose a course quarter pair as the validation set and use the remainder of the joint training validation sets for training .during validation tests we find the root mean squared error rmse to be heavily dependent on the course and quarter used .this process is repeated for k 8 iterations with the validation rmse the mean of the results .k fold cross validation	thus to reduce variance in rmse obtained from validation tests we used leave one out cross validation for our validation studies where we stochastically choose a course quarter pair as the validation set and use the remainder of the joint training validation sets for training 
0	109787	9787	this process is repeated for k 8 iterations with the validation rmse the mean of the results .thus to reduce variance in rmse obtained from validation tests we used leave one out cross validation for our validation studies where we stochastically choose a course quarter pair as the validation set and use the remainder of the joint training validation sets for training .note that for each of the k iterations the validation set was stochastically chosen and isolated from the training data with the parameters of the model reset between iterations .k fold cross validation	this process is repeated for k 8 iterations with the validation rmse the mean of the results 
1	109788	9788	note that for each of the k iterations the validation set was stochastically chosen and isolated from the training data with the parameters of the model reset between iterations .this process is repeated for k 8 iterations with the validation rmse the mean of the results .the results for cross validation between models are tabulated in final evaluation on test set .k fold cross validation	note that for each of the k iterations the validation set was stochastically chosen and isolated from the training data with the parameters of the model reset between iterations 
0	109789	9789	the results for cross validation between models are tabulated in final evaluation on test set .note that for each of the k iterations the validation set was stochastically chosen and isolated from the training data with the parameters of the model reset between iterations .we obtained an avg .k fold cross validation	the results for cross validation between models are tabulated in final evaluation on test set 
0	109790	9790	we obtained an avg .the results for cross validation between models are tabulated in final evaluation on test set .rmse of 124 466 for our set of seen courses in an unseen quarter and 106 478 for our set of unseen courses in unseen quarters .k fold cross validation	we obtained an avg 
0	109791	9791	rmse of 124 466 for our set of seen courses in an unseen quarter and 106 478 for our set of unseen courses in unseen quarters .we obtained an avg .furthermore similar to.k fold cross validation	rmse of 124 466 for our set of seen courses in an unseen quarter and 106 478 for our set of unseen courses in unseen quarters 
0	109792	9792	furthermore similar to.rmse of 124 466 for our set of seen courses in an unseen quarter and 106 478 for our set of unseen courses in unseen quarters .last sentence.k fold cross validation	furthermore similar to
0	109793	9793	overall our project provides the first general use model for predicting student demand at stanford cs office hours .first sentence.using hourly queuestatus data and course information we were able to generate realistic predictions for office hours load in a wide range of cs classes .conclusion and future works	overall our project provides the first general use model for predicting student demand at stanford cs office hours 
1	109794	9794	using hourly queuestatus data and course information we were able to generate realistic predictions for office hours load in a wide range of cs classes .overall our project provides the first general use model for predicting student demand at stanford cs office hours .ultimately out of several tried our best model was our fully connected neural network using mean absolute error and trained on log transformed load influx .conclusion and future works	using hourly queuestatus data and course information we were able to generate realistic predictions for office hours load in a wide range of cs classes 
0	109795	9795	ultimately out of several tried our best model was our fully connected neural network using mean absolute error and trained on log transformed load influx .using hourly queuestatus data and course information we were able to generate realistic predictions for office hours load in a wide range of cs classes .although a slightly different model using our custom sqhuber loss gave marginally lower rmse it failed to retain spike information due to perhaps too much outlier penalty .conclusion and future works	ultimately out of several tried our best model was our fully connected neural network using mean absolute error and trained on log transformed load influx 
1	109796	9796	although a slightly different model using our custom sqhuber loss gave marginally lower rmse it failed to retain spike information due to perhaps too much outlier penalty .ultimately out of several tried our best model was our fully connected neural network using mean absolute error and trained on log transformed load influx .our rmse indicates that the model is off by an average of 2 hours students in testing .conclusion and future works	although a slightly different model using our custom sqhuber loss gave marginally lower rmse it failed to retain spike information due to perhaps too much outlier penalty 
0	109797	9797	our rmse indicates that the model is off by an average of 2 hours students in testing .although a slightly different model using our custom sqhuber loss gave marginally lower rmse it failed to retain spike information due to perhaps too much outlier penalty .empirically we see this is a mostly a result of slightly misplaced and or incorrectly heighted spikes since our final log model makes predictions that are then exponentiated it often predicts the locations of spikes correctly but fails to capture exact magnitude .conclusion and future works	our rmse indicates that the model is off by an average of 2 hours students in testing 
1	109798	9798	empirically we see this is a mostly a result of slightly misplaced and or incorrectly heighted spikes since our final log model makes predictions that are then exponentiated it often predicts the locations of spikes correctly but fails to capture exact magnitude .our rmse indicates that the model is off by an average of 2 hours students in testing .thus although our system may not be able to predict exact student demand it can still serve as a valuable guideline regarding when to expect relative peaks .conclusion and future works	empirically we see this is a mostly a result of slightly misplaced and or incorrectly heighted spikes since our final log model makes predictions that are then exponentiated it often predicts the locations of spikes correctly but fails to capture exact magnitude 
0	109799	9799	thus although our system may not be able to predict exact student demand it can still serve as a valuable guideline regarding when to expect relative peaks .empirically we see this is a mostly a result of slightly misplaced and or incorrectly heighted spikes since our final log model makes predictions that are then exponentiated it often predicts the locations of spikes correctly but fails to capture exact magnitude .furthermore we constructed a basic gui in r that given basic course information generates oh hourly load influx for the whole quarter within a minute demoed during poster session .conclusion and future works	thus although our system may not be able to predict exact student demand it can still serve as a valuable guideline regarding when to expect relative peaks 
1	109800	9800	furthermore we constructed a basic gui in r that given basic course information generates oh hourly load influx for the whole quarter within a minute demoed during poster session .thus although our system may not be able to predict exact student demand it can still serve as a valuable guideline regarding when to expect relative peaks .so far chris piech has expressed interest in using our model next spring .conclusion and future works	furthermore we constructed a basic gui in r that given basic course information generates oh hourly load influx for the whole quarter within a minute demoed during poster session 
0	109801	9801	so far chris piech has expressed interest in using our model next spring .furthermore we constructed a basic gui in r that given basic course information generates oh hourly load influx for the whole quarter within a minute demoed during poster session .given more time we would like to extend our predictions to more classes and perhaps even other universities using queuestatus .conclusion and future works	so far chris piech has expressed interest in using our model next spring 
0	109802	9802	given more time we would like to extend our predictions to more classes and perhaps even other universities using queuestatus .so far chris piech has expressed interest in using our model next spring .last sentence.conclusion and future works	given more time we would like to extend our predictions to more classes and perhaps even other universities using queuestatus 
1	109803	9803	with the rising popularity of peer to peer lending platforms in recent years investors now have easy access to this alternative investment asset class by lending money to individual borrowers through platforms such as lendingclub prosper marketplace and upstart or to small businesses through funding circle the process starts with borrowers submitting loan applications to the platform which performs credit reviews and either approves or denies each application .first sentence.the platform also uses a proprietary model to determine the interest rate of approved loans based on the credit worthiness of borrowers .i motivation	with the rising popularity of peer to peer lending platforms in recent years investors now have easy access to this alternative investment asset class by lending money to individual borrowers through platforms such as lendingclub prosper marketplace and upstart or to small businesses through funding circle the process starts with borrowers submitting loan applications to the platform which performs credit reviews and either approves or denies each application 
1	109804	9804	the platform also uses a proprietary model to determine the interest rate of approved loans based on the credit worthiness of borrowers .with the rising popularity of peer to peer lending platforms in recent years investors now have easy access to this alternative investment asset class by lending money to individual borrowers through platforms such as lendingclub prosper marketplace and upstart or to small businesses through funding circle the process starts with borrowers submitting loan applications to the platform which performs credit reviews and either approves or denies each application .approved loans are then listed on the platform for investor funding .i motivation	the platform also uses a proprietary model to determine the interest rate of approved loans based on the credit worthiness of borrowers 
0	109805	9805	approved loans are then listed on the platform for investor funding .the platform also uses a proprietary model to determine the interest rate of approved loans based on the credit worthiness of borrowers .investors usually want to diversify their portfolio by only investing a small amount e g .i motivation	approved loans are then listed on the platform for investor funding 
0	109806	9806	investors usually want to diversify their portfolio by only investing a small amount e g .approved loans are then listed on the platform for investor funding .25 in each loan .i motivation	investors usually want to diversify their portfolio by only investing a small amount e g 
0	109807	9807	25 in each loan .investors usually want to diversify their portfolio by only investing a small amount e g .hence it is desirable for investors to be able to independently evaluate the credit risk of a large number of listed loans quickly and invest in those with lower perceived risks this motivates us to build machine learned classification and regression models that can quantify the credit risk with a lendingclub historical loan dataset .i motivation	25 in each loan 
1	109808	9808	hence it is desirable for investors to be able to independently evaluate the credit risk of a large number of listed loans quickly and invest in those with lower perceived risks this motivates us to build machine learned classification and regression models that can quantify the credit risk with a lendingclub historical loan dataset .25 in each loan .specifically we build and evaluate classifiers that predict whether a given loan will be fully paid by the borrower as well as regressors that predict the annualized net return from investment in a given loan .i motivation	hence it is desirable for investors to be able to independently evaluate the credit risk of a large number of listed loans quickly and invest in those with lower perceived risks this motivates us to build machine learned classification and regression models that can quantify the credit risk with a lendingclub historical loan dataset 
1	109809	9809	specifically we build and evaluate classifiers that predict whether a given loan will be fully paid by the borrower as well as regressors that predict the annualized net return from investment in a given loan .hence it is desirable for investors to be able to independently evaluate the credit risk of a large number of listed loans quickly and invest in those with lower perceived risks this motivates us to build machine learned classification and regression models that can quantify the credit risk with a lendingclub historical loan dataset .finally we simulate and evaluate a simple loan selection strategy by investing in loans that pass a certain regressor prediction threshold .i motivation	specifically we build and evaluate classifiers that predict whether a given loan will be fully paid by the borrower as well as regressors that predict the annualized net return from investment in a given loan 
1	109810	9810	finally we simulate and evaluate a simple loan selection strategy by investing in loans that pass a certain regressor prediction threshold .specifically we build and evaluate classifiers that predict whether a given loan will be fully paid by the borrower as well as regressors that predict the annualized net return from investment in a given loan .last sentence.i motivation	finally we simulate and evaluate a simple loan selection strategy by investing in loans that pass a certain regressor prediction threshold 
0	109811	9811	there have been many studies on classification models predicting lendingclub loan default .first sentence.chang et al .ii related work	there have been many studies on classification models predicting lendingclub loan default 
0	109812	9812	chang et al .there have been many studies on classification models predicting lendingclub loan default .tsai et al .ii related work	chang et al 
0	109813	9813	tsai et al .chang et al .in addition to classification models that predict loan default gutierrez and mathieson pujun et al .ii related work	tsai et al 
1	109814	9814	in addition to classification models that predict loan default gutierrez and mathieson pujun et al .tsai et al .last sentence.ii related work	in addition to classification models that predict loan default gutierrez and mathieson pujun et al 
1	109815	9815	we worked with public dataset published by lending club.first sentence.last sentence.a dataset overview	we worked with public dataset published by lending club
1	109816	9816	columns with empty values for most of the rows as well as columns with the same values across all rows are dropped in order to have a cleaner dataset .first sentence.free form text columns are also dropped because we posited that these fields would have more noise and are better tackled at a later stage when we have better understanding of the problem for features with missing values they are categorized into three cases and treated differently mean set zero set and max set .b feature preprocessing	columns with empty values for most of the rows as well as columns with the same values across all rows are dropped in order to have a cleaner dataset 
1	109817	9817	free form text columns are also dropped because we posited that these fields would have more noise and are better tackled at a later stage when we have better understanding of the problem for features with missing values they are categorized into three cases and treated differently mean set zero set and max set .columns with empty values for most of the rows as well as columns with the same values across all rows are dropped in order to have a cleaner dataset .for mean set fields we took the average of the non empty values .b feature preprocessing	free form text columns are also dropped because we posited that these fields would have more noise and are better tackled at a later stage when we have better understanding of the problem for features with missing values they are categorized into three cases and treated differently mean set zero set and max set 
0	109818	9818	for mean set fields we took the average of the non empty values .free form text columns are also dropped because we posited that these fields would have more noise and are better tackled at a later stage when we have better understanding of the problem for features with missing values they are categorized into three cases and treated differently mean set zero set and max set .one such example is debt to income ratio dti borrowers with lower dti likely have lower risks compared to those with higher dtis .b feature preprocessing	for mean set fields we took the average of the non empty values 
1	109819	9819	one such example is debt to income ratio dti borrowers with lower dti likely have lower risks compared to those with higher dtis .for mean set fields we took the average of the non empty values .for loan applicants missing dti information it is unreasonable to reward them by assigning zero dti hence taking average is a good starting point .b feature preprocessing	one such example is debt to income ratio dti borrowers with lower dti likely have lower risks compared to those with higher dtis 
1	109820	9820	for loan applicants missing dti information it is unreasonable to reward them by assigning zero dti hence taking average is a good starting point .one such example is debt to income ratio dti borrowers with lower dti likely have lower risks compared to those with higher dtis .in the case of max set missing values are replaced with a constant factor multiplied with the maximum value in that column .b feature preprocessing	for loan applicants missing dti information it is unreasonable to reward them by assigning zero dti hence taking average is a good starting point 
1	109821	9821	in the case of max set missing values are replaced with a constant factor multiplied with the maximum value in that column .for loan applicants missing dti information it is unreasonable to reward them by assigning zero dti hence taking average is a good starting point .for instance if the data for the number of months since last delinquency is missing it would be unfair to punish the applicants by assigning zero for missing data .b feature preprocessing	in the case of max set missing values are replaced with a constant factor multiplied with the maximum value in that column 
0	109822	9822	for instance if the data for the number of months since last delinquency is missing it would be unfair to punish the applicants by assigning zero for missing data .in the case of max set missing values are replaced with a constant factor multiplied with the maximum value in that column .finally zeros are given for zero set which we believe would be a neutral replacement for the missing data categorical features such as obfuscated zipcode e g .b feature preprocessing	for instance if the data for the number of months since last delinquency is missing it would be unfair to punish the applicants by assigning zero for missing data 
1	109823	9823	finally zeros are given for zero set which we believe would be a neutral replacement for the missing data categorical features such as obfuscated zipcode e g .for instance if the data for the number of months since last delinquency is missing it would be unfair to punish the applicants by assigning zero for missing data .940xx are replaced with their one hot representations .b feature preprocessing	finally zeros are given for zero set which we believe would be a neutral replacement for the missing data categorical features such as obfuscated zipcode e g 
0	109824	9824	940xx are replaced with their one hot representations .finally zeros are given for zero set which we believe would be a neutral replacement for the missing data categorical features such as obfuscated zipcode e g .features with date values are converted into the number of days since epoch .b feature preprocessing	940xx are replaced with their one hot representations 
0	109825	9825	features with date values are converted into the number of days since epoch .940xx are replaced with their one hot representations .normalization is then performed at the end on all features so they have zero mean and one standard deviation after the above preprocessing we ended up with 1 097 features .b feature preprocessing	features with date values are converted into the number of days since epoch 
1	109826	9826	normalization is then performed at the end on all features so they have zero mean and one standard deviation after the above preprocessing we ended up with 1 097 features .features with date values are converted into the number of days since epoch .we then ran pca on the dataset with the hope to further reduce feature size .b feature preprocessing	normalization is then performed at the end on all features so they have zero mean and one standard deviation after the above preprocessing we ended up with 1 097 features 
1	109827	9827	we then ran pca on the dataset with the hope to further reduce feature size .normalization is then performed at the end on all features so they have zero mean and one standard deviation after the above preprocessing we ended up with 1 097 features .unfortunately the 95 variance threshold corresponds to around 900 features which is close to 95 of the total number of features and therefore means that we cannot significantly reduce the feature size without sacrificing variances see.b feature preprocessing	we then ran pca on the dataset with the hope to further reduce feature size 
1	109828	9828	unfortunately the 95 variance threshold corresponds to around 900 features which is close to 95 of the total number of features and therefore means that we cannot significantly reduce the feature size without sacrificing variances see.we then ran pca on the dataset with the hope to further reduce feature size .last sentence.b feature preprocessing	unfortunately the 95 variance threshold corresponds to around 900 features which is close to 95 of the total number of features and therefore means that we cannot significantly reduce the feature size without sacrificing variances see
1	109829	9829	for classification model both default and charged off are assigned label 0 and fully paid is assigned label 1 .first sentence.for regression model we use annualized return rate calculated from loan amount total payment made by the borrower and the time interval between loan initiation and the date of last payment .c label definition	for classification model both default and charged off are assigned label 0 and fully paid is assigned label 1 
1	109830	9830	for regression model we use annualized return rate calculated from loan amount total payment made by the borrower and the time interval between loan initiation and the date of last payment .for classification model both default and charged off are assigned label 0 and fully paid is assigned label 1 .last sentence.c label definition	for regression model we use annualized return rate calculated from loan amount total payment made by the borrower and the time interval between loan initiation and the date of last payment 
1	109831	9831	our classification goal is to predict which class the loan belongs to either default or fully paid .first sentence.in the following sections we will share and discuss our experiments using logistic regression neutral networks and random forest for classification problem .iv classification problem overview	our classification goal is to predict which class the loan belongs to either default or fully paid 
1	109832	9832	in the following sections we will share and discuss our experiments using logistic regression neutral networks and random forest for classification problem .our classification goal is to predict which class the loan belongs to either default or fully paid .for metrics to evaluate classification performance we use confusion matrix whose columns represent predicted values and rows represent true values .iv classification problem overview	in the following sections we will share and discuss our experiments using logistic regression neutral networks and random forest for classification problem 
1	109833	9833	for metrics to evaluate classification performance we use confusion matrix whose columns represent predicted values and rows represent true values .in the following sections we will share and discuss our experiments using logistic regression neutral networks and random forest for classification problem .we also measure precision recall f1 score the harmonic mean of precision and recall and weighted average as defined to derive optimal parameters the model iteratively updates weights by minimizing the negative log likelihood with l2 regularizationto tackle the class imbalance problem only 19 of our dataset are negative examples we used balanced weight for class labels which is inversely proportional to class frequencies in the input data n samples total n classes label count after running logistic regression with the above setting for a maximum of 1000 iterations we arrived at the following results as we can see logistic regression is doing fairly well compared to naive models that blindly predict positive for all examples or randomly guess positive and negative with 50 chance .iv classification problem overview	for metrics to evaluate classification performance we use confusion matrix whose columns represent predicted values and rows represent true values 
1	109834	9834	we also measure precision recall f1 score the harmonic mean of precision and recall and weighted average as defined to derive optimal parameters the model iteratively updates weights by minimizing the negative log likelihood with l2 regularizationto tackle the class imbalance problem only 19 of our dataset are negative examples we used balanced weight for class labels which is inversely proportional to class frequencies in the input data n samples total n classes label count after running logistic regression with the above setting for a maximum of 1000 iterations we arrived at the following results as we can see logistic regression is doing fairly well compared to naive models that blindly predict positive for all examples or randomly guess positive and negative with 50 chance .for metrics to evaluate classification performance we use confusion matrix whose columns represent predicted values and rows represent true values .thanks to l2 regularization we did not observe overfitting issues .iv classification problem overview	we also measure precision recall f1 score the harmonic mean of precision and recall and weighted average as defined to derive optimal parameters the model iteratively updates weights by minimizing the negative log likelihood with l2 regularizationto tackle the class imbalance problem only 19 of our dataset are negative examples we used balanced weight for class labels which is inversely proportional to class frequencies in the input data n samples total n classes label count after running logistic regression with the above setting for a maximum of 1000 iterations we arrived at the following results as we can see logistic regression is doing fairly well compared to naive models that blindly predict positive for all examples or randomly guess positive and negative with 50 chance 
0	109835	9835	thanks to l2 regularization we did not observe overfitting issues .we also measure precision recall f1 score the harmonic mean of precision and recall and weighted average as defined to derive optimal parameters the model iteratively updates weights by minimizing the negative log likelihood with l2 regularizationto tackle the class imbalance problem only 19 of our dataset are negative examples we used balanced weight for class labels which is inversely proportional to class frequencies in the input data n samples total n classes label count after running logistic regression with the above setting for a maximum of 1000 iterations we arrived at the following results as we can see logistic regression is doing fairly well compared to naive models that blindly predict positive for all examples or randomly guess positive and negative with 50 chance .one thing that we noticed and would like to improve upon is the precision and recall for negative class .iv classification problem overview	thanks to l2 regularization we did not observe overfitting issues 
0	109836	9836	one thing that we noticed and would like to improve upon is the precision and recall for negative class .thanks to l2 regularization we did not observe overfitting issues .although we used balanced class weights to offset data imbalance the prediction precision is only slightly better than randomly guessing .iv classification problem overview	one thing that we noticed and would like to improve upon is the precision and recall for negative class 
1	109837	9837	although we used balanced class weights to offset data imbalance the prediction precision is only slightly better than randomly guessing .one thing that we noticed and would like to improve upon is the precision and recall for negative class .therefore we suspect there may be non linear relationships in the dataset that is not learned by logistic regression which leads to our exploration with neural network next .iv classification problem overview	although we used balanced class weights to offset data imbalance the prediction precision is only slightly better than randomly guessing 
1	109838	9838	therefore we suspect there may be non linear relationships in the dataset that is not learned by logistic regression which leads to our exploration with neural network next .although we used balanced class weights to offset data imbalance the prediction precision is only slightly better than randomly guessing .last sentence.iv classification problem overview	therefore we suspect there may be non linear relationships in the dataset that is not learned by logistic regression which leads to our exploration with neural network next 
0	109839	9839	we constructed a fully connected neural network with 4 hidden layers of shape j .first sentence.the final output of the network uses cross entropy log loss as loss function to arrive at optimal parameters the model iteratively updates weights within each layer using gradient descentbased solver with a mini batch size of 200 learning rate of 0 001 and l2 regularization penalty of 0 0001 we obtained the following results training set result the model has high variance and is suffering from overfitting .b neural network	we constructed a fully connected neural network with 4 hidden layers of shape j 
1	109840	9840	the final output of the network uses cross entropy log loss as loss function to arrive at optimal parameters the model iteratively updates weights within each layer using gradient descentbased solver with a mini batch size of 200 learning rate of 0 001 and l2 regularization penalty of 0 0001 we obtained the following results training set result the model has high variance and is suffering from overfitting .we constructed a fully connected neural network with 4 hidden layers of shape j .compared with the logistic regression model this neural network model achieves a better weighted precision at the expense of weighted recall and the difference between precision and recall is less polarized compared to that of the logistic regression .b neural network	the final output of the network uses cross entropy log loss as loss function to arrive at optimal parameters the model iteratively updates weights within each layer using gradient descentbased solver with a mini batch size of 200 learning rate of 0 001 and l2 regularization penalty of 0 0001 we obtained the following results training set result the model has high variance and is suffering from overfitting 
1	109841	9841	compared with the logistic regression model this neural network model achieves a better weighted precision at the expense of weighted recall and the difference between precision and recall is less polarized compared to that of the logistic regression .the final output of the network uses cross entropy log loss as loss function to arrive at optimal parameters the model iteratively updates weights within each layer using gradient descentbased solver with a mini batch size of 200 learning rate of 0 001 and l2 regularization penalty of 0 0001 we obtained the following results training set result the model has high variance and is suffering from overfitting .last sentence.b neural network	compared with the logistic regression model this neural network model achieves a better weighted precision at the expense of weighted recall and the difference between precision and recall is less polarized compared to that of the logistic regression 
1	109842	9842	random forest classifier is one of the tree ensemble methods that make decision splits using a random subset of features and combine the output of multiple weak classifiers to derive a strong classifier of lower variance at the cost of higher bias we started off our venture into random forest with 200 trees using gini loss 1 1 j 0 p 2 j .first sentence.decision splits are based on at most 50 features to reduce variance .c random forest	random forest classifier is one of the tree ensemble methods that make decision splits using a random subset of features and combine the output of multiple weak classifiers to derive a strong classifier of lower variance at the cost of higher bias we started off our venture into random forest with 200 trees using gini loss 1 1 j 0 p 2 j 
0	109843	9843	decision splits are based on at most 50 features to reduce variance .random forest classifier is one of the tree ensemble methods that make decision splits using a random subset of features and combine the output of multiple weak classifiers to derive a strong classifier of lower variance at the cost of higher bias we started off our venture into random forest with 200 trees using gini loss 1 1 j 0 p 2 j .after training we reached the following result although the performance is on par with neural network and logistic regression random forest s overfitting problem is much more prominent than any other models even after restricting the maximum number of features considered for decision splits to 50 .c random forest	decision splits are based on at most 50 features to reduce variance 
1	109844	9844	after training we reached the following result although the performance is on par with neural network and logistic regression random forest s overfitting problem is much more prominent than any other models even after restricting the maximum number of features considered for decision splits to 50 .decision splits are based on at most 50 features to reduce variance .last sentence.c random forest	after training we reached the following result although the performance is on par with neural network and logistic regression random forest s overfitting problem is much more prominent than any other models even after restricting the maximum number of features considered for decision splits to 50 
1	109845	9845	based on our explorations with logistic regression neural network and random forest we are able to achieve weighted average of 0 89 for both precision and recall .first sentence.more specifically our classification results appear to be better than the works done by the previous project.d classification model conclusion	based on our explorations with logistic regression neural network and random forest we are able to achieve weighted average of 0 89 for both precision and recall 
0	109846	9846	more specifically our classification results appear to be better than the works done by the previous project.based on our explorations with logistic regression neural network and random forest we are able to achieve weighted average of 0 89 for both precision and recall .last sentence.d classification model conclusion	more specifically our classification results appear to be better than the works done by the previous project
1	109847	9847	we strive to predict the investment return if we were to invest in a given loan .first sentence.our goal is to build regression models that predict the net annualized return nar of a given loan in a way similar to how lendingclub calculates nar for investors where x la is the loan amount x t p is total payment made by the borrower and d is the number of days between loan funding and date of last payment we evaluate regression models in terms of mean square error mse and coefficient of determination r 2 is the mean of the true labels .vi regression problem overview	we strive to predict the investment return if we were to invest in a given loan 
1	109848	9848	our goal is to build regression models that predict the net annualized return nar of a given loan in a way similar to how lendingclub calculates nar for investors where x la is the loan amount x t p is total payment made by the borrower and d is the number of days between loan funding and date of last payment we evaluate regression models in terms of mean square error mse and coefficient of determination r 2 is the mean of the true labels .we strive to predict the investment return if we were to invest in a given loan .the coefficient of determination tells us how much variability of the true nars can be explained by the model .vi regression problem overview	our goal is to build regression models that predict the net annualized return nar of a given loan in a way similar to how lendingclub calculates nar for investors where x la is the loan amount x t p is total payment made by the borrower and d is the number of days between loan funding and date of last payment we evaluate regression models in terms of mean square error mse and coefficient of determination r 2 is the mean of the true labels 
1	109849	9849	the coefficient of determination tells us how much variability of the true nars can be explained by the model .our goal is to build regression models that predict the net annualized return nar of a given loan in a way similar to how lendingclub calculates nar for investors where x la is the loan amount x t p is total payment made by the borrower and d is the number of days between loan funding and date of last payment we evaluate regression models in terms of mean square error mse and coefficient of determination r 2 is the mean of the true labels .last sentence.vi regression problem overview	the coefficient of determination tells us how much variability of the true nars can be explained by the model 
1	109850	9850	the goal of linear regression is to find a linear hyperplane that minimizes the ordinary least squares .first sentence.specifically it finds parameters that minimizes.a linear regression	the goal of linear regression is to find a linear hyperplane that minimizes the ordinary least squares 
0	109851	9851	specifically it finds parameters that minimizes.the goal of linear regression is to find a linear hyperplane that minimizes the ordinary least squares .last sentence.a linear regression	specifically it finds parameters that minimizes
1	109852	9852	split mse r 2 train 0 040 0 243 test 5 014 9 494 10 22the extremely skewed mse and r 2 values on the test set clearly indicate a high variance problem of the model which overfits the training examples .first sentence.to rectify this we employ l2 regularization in our next model .performance of linear regression 	split mse r 2 train 0 040 0 243 test 5 014 9 494 10 22the extremely skewed mse and r 2 values on the test set clearly indicate a high variance problem of the model which overfits the training examples 
1	109853	9853	to rectify this we employ l2 regularization in our next model .split mse r 2 train 0 040 0 243 test 5 014 9 494 10 22the extremely skewed mse and r 2 values on the test set clearly indicate a high variance problem of the model which overfits the training examples .last sentence.performance of linear regression 	to rectify this we employ l2 regularization in our next model 
1	109854	9854	ridge regression adds an l2 regularization term to the cost function of linear regressionbut otherwise works the same way as linear regression performance of ridge regression with 1 split mse r 2 train 0 040 0 243 test 0 040 0 238as expected l2 regularization mitigated the problem of overfitting giving similar metrics for both train and test sets .first sentence.r 2 0 24 means that 24 of the nar s variability can be explained by the ridge regression model .b ridge regression	ridge regression adds an l2 regularization term to the cost function of linear regressionbut otherwise works the same way as linear regression performance of ridge regression with 1 split mse r 2 train 0 040 0 243 test 0 040 0 238as expected l2 regularization mitigated the problem of overfitting giving similar metrics for both train and test sets 
1	109855	9855	r 2 0 24 means that 24 of the nar s variability can be explained by the ridge regression model .ridge regression adds an l2 regularization term to the cost function of linear regressionbut otherwise works the same way as linear regression performance of ridge regression with 1 split mse r 2 train 0 040 0 243 test 0 040 0 238as expected l2 regularization mitigated the problem of overfitting giving similar metrics for both train and test sets .we next try nonlinear models to further decrease mse and increase r 2 .b ridge regression	r 2 0 24 means that 24 of the nar s variability can be explained by the ridge regression model 
1	109856	9856	we next try nonlinear models to further decrease mse and increase r 2 .r 2 0 24 means that 24 of the nar s variability can be explained by the ridge regression model .last sentence.b ridge regression	we next try nonlinear models to further decrease mse and increase r 2 
1	109857	9857	the fully connected neural network regression model is very similar to the classifier described earlier in section v b .first sentence.the only difference is that all neurons use the relu activation function f x max 0 x and the neural network tries to minimize the squared loss on the training set we used the adam stochastic gradient based optimizer we see that the neural network regressor performs much better than ridge regression thanks to its ability to model non linear relationships .c neural network	the fully connected neural network regression model is very similar to the classifier described earlier in section v b 
1	109858	9858	the only difference is that all neurons use the relu activation function f x max 0 x and the neural network tries to minimize the squared loss on the training set we used the adam stochastic gradient based optimizer we see that the neural network regressor performs much better than ridge regression thanks to its ability to model non linear relationships .the fully connected neural network regression model is very similar to the classifier described earlier in section v b .last sentence.c neural network	the only difference is that all neurons use the relu activation function f x max 0 x and the neural network tries to minimize the squared loss on the training set we used the adam stochastic gradient based optimizer we see that the neural network regressor performs much better than ridge regression thanks to its ability to model non linear relationships 
1	109859	9859	a decision tree regression model infers decision rules from example features by finding a feature split for each non leaf node that maximizes the variance reduction as measured by mse .first sentence.the mean of leaf node example labels is the output of the decision tree regressor decision trees tend to overfit especially when the tree is deep and leaf nodes comprise too few examples .d random forest	a decision tree regression model infers decision rules from example features by finding a feature split for each non leaf node that maximizes the variance reduction as measured by mse 
1	109860	9860	the mean of leaf node example labels is the output of the decision tree regressor decision trees tend to overfit especially when the tree is deep and leaf nodes comprise too few examples .a decision tree regression model infers decision rules from example features by finding a feature split for each non leaf node that maximizes the variance reduction as measured by mse .limiting the maximum depth or the minimum leaf node examples not only reduces overfitting but also speeds up training significantly as random forest model builds numerous decision trees before taking the average of their predictions specifically random forest regressor repeatedly builds decision trees on a bootstrap sample drawn from the training set and considers a random subset of features as candidates when finding an optimal split .d random forest	the mean of leaf node example labels is the output of the decision tree regressor decision trees tend to overfit especially when the tree is deep and leaf nodes comprise too few examples 
1	109861	9861	limiting the maximum depth or the minimum leaf node examples not only reduces overfitting but also speeds up training significantly as random forest model builds numerous decision trees before taking the average of their predictions specifically random forest regressor repeatedly builds decision trees on a bootstrap sample drawn from the training set and considers a random subset of features as candidates when finding an optimal split .the mean of leaf node example labels is the output of the decision tree regressor decision trees tend to overfit especially when the tree is deep and leaf nodes comprise too few examples .from these results we see that as we allow the decision trees to grow deeper bias increases while variance decreases .d random forest	limiting the maximum depth or the minimum leaf node examples not only reduces overfitting but also speeds up training significantly as random forest model builds numerous decision trees before taking the average of their predictions specifically random forest regressor repeatedly builds decision trees on a bootstrap sample drawn from the training set and considers a random subset of features as candidates when finding an optimal split 
1	109862	9862	from these results we see that as we allow the decision trees to grow deeper bias increases while variance decreases .limiting the maximum depth or the minimum leaf node examples not only reduces overfitting but also speeds up training significantly as random forest model builds numerous decision trees before taking the average of their predictions specifically random forest regressor repeatedly builds decision trees on a bootstrap sample drawn from the training set and considers a random subset of features as candidates when finding an optimal split .the performance of random forest regressor beats both ridge regression and neural network likely due to the fact that decision trees are able to capture very nuanced and nonlinear relationships .d random forest	from these results we see that as we allow the decision trees to grow deeper bias increases while variance decreases 
1	109863	9863	the performance of random forest regressor beats both ridge regression and neural network likely due to the fact that decision trees are able to capture very nuanced and nonlinear relationships .from these results we see that as we allow the decision trees to grow deeper bias increases while variance decreases .last sentence.d random forest	the performance of random forest regressor beats both ridge regression and neural network likely due to the fact that decision trees are able to capture very nuanced and nonlinear relationships 
1	109864	9864	our best random forest regressor achieves a root mse of 0 036 0 19 on the test set which implies that the predicted nar is estimated to differ from the true nar by 0 19 .first sentence.while this may appear very large at first glance the model can actually be very useful in formulating a loan selection strategy .viii loan selection strategy	our best random forest regressor achieves a root mse of 0 036 0 19 on the test set which implies that the predicted nar is estimated to differ from the true nar by 0 19 
1	109865	9865	while this may appear very large at first glance the model can actually be very useful in formulating a loan selection strategy .our best random forest regressor achieves a root mse of 0 036 0 19 on the test set which implies that the predicted nar is estimated to differ from the true nar by 0 19 .loan defaults usually happen soon after loan funding and the chance of default decreases as more payment is made .viii loan selection strategy	while this may appear very large at first glance the model can actually be very useful in formulating a loan selection strategy 
1	109866	9866	loan defaults usually happen soon after loan funding and the chance of default decreases as more payment is made .while this may appear very large at first glance the model can actually be very useful in formulating a loan selection strategy .as a result most true nars of defaulted loans are well below 0 5 so the model can still very accurately tell us that investing in loans like these likely result in losses in light of this we experimented with the strategy of investing in loans with model nar predictions higher than a reasonable threshold m 0 .viii loan selection strategy	loan defaults usually happen soon after loan funding and the chance of default decreases as more payment is made 
1	109867	9867	as a result most true nars of defaulted loans are well below 0 5 so the model can still very accurately tell us that investing in loans like these likely result in losses in light of this we experimented with the strategy of investing in loans with model nar predictions higher than a reasonable threshold m 0 .loan defaults usually happen soon after loan funding and the chance of default decreases as more payment is made .intuitively the threshold m can serve as a parameter investors can tune according to their investment account size the bigger m is the more stringent the loan selection is so less amount of money can be invested but hopefully the annualized return will be higher due to investing in loans more selectively in order to determine a reasonable range of values for m we rank the training set examples by model predictions from high to low .viii loan selection strategy	as a result most true nars of defaulted loans are well below 0 5 so the model can still very accurately tell us that investing in loans like these likely result in losses in light of this we experimented with the strategy of investing in loans with model nar predictions higher than a reasonable threshold m 0 
1	109868	9868	intuitively the threshold m can serve as a parameter investors can tune according to their investment account size the bigger m is the more stringent the loan selection is so less amount of money can be invested but hopefully the annualized return will be higher due to investing in loans more selectively in order to determine a reasonable range of values for m we rank the training set examples by model predictions from high to low .as a result most true nars of defaulted loans are well below 0 5 so the model can still very accurately tell us that investing in loans like these likely result in losses in light of this we experimented with the strategy of investing in loans with model nar predictions higher than a reasonable threshold m 0 .for a specific threshold m 0 132 on both training and test set the strategy yields an annualized return of 15 with 1 7 loans picked and invested .viii loan selection strategy	intuitively the threshold m can serve as a parameter investors can tune according to their investment account size the bigger m is the more stringent the loan selection is so less amount of money can be invested but hopefully the annualized return will be higher due to investing in loans more selectively in order to determine a reasonable range of values for m we rank the training set examples by model predictions from high to low 
1	109869	9869	for a specific threshold m 0 132 on both training and test set the strategy yields an annualized return of 15 with 1 7 loans picked and invested .intuitively the threshold m can serve as a parameter investors can tune according to their investment account size the bigger m is the more stringent the loan selection is so less amount of money can be invested but hopefully the annualized return will be higher due to investing in loans more selectively in order to determine a reasonable range of values for m we rank the training set examples by model predictions from high to low .last sentence.viii loan selection strategy	for a specific threshold m 0 132 on both training and test set the strategy yields an annualized return of 15 with 1 7 loans picked and invested 
1	109870	9870	comparing our models with those from related work ours have better precision recall and are more practical in terms of enabling implementable investment strategies .first sentence.in the case of classification models random forest achieved 0 89 weighted average precision and recall .ix conclusion	comparing our models with those from related work ours have better precision recall and are more practical in terms of enabling implementable investment strategies 
1	109871	9871	in the case of classification models random forest achieved 0 89 weighted average precision and recall .comparing our models with those from related work ours have better precision recall and are more practical in terms of enabling implementable investment strategies .but it is also important to note that the random forest and neural network models do have higher variance than desired and have space for improvement .ix conclusion	in the case of classification models random forest achieved 0 89 weighted average precision and recall 
1	109872	9872	but it is also important to note that the random forest and neural network models do have higher variance than desired and have space for improvement .in the case of classification models random forest achieved 0 89 weighted average precision and recall .for the regression counterpart random forest is able to attain 0 315 coefficient of determination and to deliver predictions that lead to a profitable and actionable loan selection strategy in the sense that the return rate is higher than s p 500 s 10 annualized return for the past 90 years.ix conclusion	but it is also important to note that the random forest and neural network models do have higher variance than desired and have space for improvement 
1	109873	9873	for the regression counterpart random forest is able to attain 0 315 coefficient of determination and to deliver predictions that lead to a profitable and actionable loan selection strategy in the sense that the return rate is higher than s p 500 s 10 annualized return for the past 90 years.but it is also important to note that the random forest and neural network models do have higher variance than desired and have space for improvement .last sentence.ix conclusion	for the regression counterpart random forest is able to attain 0 315 coefficient of determination and to deliver predictions that lead to a profitable and actionable loan selection strategy in the sense that the return rate is higher than s p 500 s 10 annualized return for the past 90 years
1	109874	9874	we obtained a regression prediction threshold based on the training set and simulated the strategy on the test set .first sentence.both sets comprise loans initiated within the same periods we worked with a 70 training and 30 test split for simplicity in this project .x future work	we obtained a regression prediction threshold based on the training set and simulated the strategy on the test set 
1	109875	9875	both sets comprise loans initiated within the same periods we worked with a 70 training and 30 test split for simplicity in this project .we obtained a regression prediction threshold based on the training set and simulated the strategy on the test set .the absence of a development set didn t afford us much opportunity to tune the hyperparameters of our models such as the number of decision trees to use in random forest models and the number of hidden layers and neurons of each layer in neural network models .x future work	both sets comprise loans initiated within the same periods we worked with a 70 training and 30 test split for simplicity in this project 
1	109876	9876	the absence of a development set didn t afford us much opportunity to tune the hyperparameters of our models such as the number of decision trees to use in random forest models and the number of hidden layers and neurons of each layer in neural network models .both sets comprise loans initiated within the same periods we worked with a 70 training and 30 test split for simplicity in this project .having a small development set would enable us to tune some hyper parameters quickly to help improve model performance metrics there are definitely factors that contribute to default not captured by features in our dataset .x future work	the absence of a development set didn t afford us much opportunity to tune the hyperparameters of our models such as the number of decision trees to use in random forest models and the number of hidden layers and neurons of each layer in neural network models 
1	109877	9877	having a small development set would enable us to tune some hyper parameters quickly to help improve model performance metrics there are definitely factors that contribute to default not captured by features in our dataset .the absence of a development set didn t afford us much opportunity to tune the hyperparameters of our models such as the number of decision trees to use in random forest models and the number of hidden layers and neurons of each layer in neural network models .we can add external features such as macroeconomic metrics that have been historically correlated to bond default rate .x future work	having a small development set would enable us to tune some hyper parameters quickly to help improve model performance metrics there are definitely factors that contribute to default not captured by features in our dataset 
1	109878	9878	we can add external features such as macroeconomic metrics that have been historically correlated to bond default rate .having a small development set would enable us to tune some hyper parameters quickly to help improve model performance metrics there are definitely factors that contribute to default not captured by features in our dataset .for categorical features like employment title we can join them with signals such as average income by industry similar to what chang et al .x future work	we can add external features such as macroeconomic metrics that have been historically correlated to bond default rate 
1	109879	9879	for categorical features like employment title we can join them with signals such as average income by industry similar to what chang et al .we can add external features such as macroeconomic metrics that have been historically correlated to bond default rate .we can also make better use of existing features in the lendingclub dataset .x future work	for categorical features like employment title we can join them with signals such as average income by industry similar to what chang et al 
0	109880	9880	we can also make better use of existing features in the lendingclub dataset .for categorical features like employment title we can join them with signals such as average income by industry similar to what chang et al .one example is loan description which the borrower enters at the time of loan application .x future work	we can also make better use of existing features in the lendingclub dataset 
0	109881	9881	one example is loan description which the borrower enters at the time of loan application .we can also make better use of existing features in the lendingclub dataset .instead of dropping such freeform features we can try applying some statistical natural language processing techniques such as tf idf as chang et al .x future work	one example is loan description which the borrower enters at the time of loan application 
1	109882	9882	instead of dropping such freeform features we can try applying some statistical natural language processing techniques such as tf idf as chang et al .one example is loan description which the borrower enters at the time of loan application .finally we notice that lendingclub also publishes declined loan datasets.x future work	instead of dropping such freeform features we can try applying some statistical natural language processing techniques such as tf idf as chang et al 
0	109883	9883	finally we notice that lendingclub also publishes declined loan datasets.instead of dropping such freeform features we can try applying some statistical natural language processing techniques such as tf idf as chang et al .last sentence.x future work	finally we notice that lendingclub also publishes declined loan datasets
0	109884	9884	the two of us paired up on all components of this project including dataset cleaning feature engineering model formulation evaluation and the write up of this report and the poster codebase https goo gl sxf1rm.first sentence.last sentence.xi contributions	the two of us paired up on all components of this project including dataset cleaning feature engineering model formulation evaluation and the write up of this report and the poster codebase https goo gl sxf1rm
1	109885	9885	mild traumatic brain injury mtbi more commonly known as concussion has become a serious health concern with recent increase in media coverage on the long term health issues of professional athletes and military personnel .first sentence.acute symptoms include dizziness confusion and personality changes which can remain for days or even years after injury according to the cdc contact sports such as football are one of the leading causes of mtbi .introduction	mild traumatic brain injury mtbi more commonly known as concussion has become a serious health concern with recent increase in media coverage on the long term health issues of professional athletes and military personnel 
1	109886	9886	acute symptoms include dizziness confusion and personality changes which can remain for days or even years after injury according to the cdc contact sports such as football are one of the leading causes of mtbi .mild traumatic brain injury mtbi more commonly known as concussion has become a serious health concern with recent increase in media coverage on the long term health issues of professional athletes and military personnel .in these sports mtbi is diagnosed by a sideline clinician through subjective evaluation of symptoms and neurological testing .introduction	acute symptoms include dizziness confusion and personality changes which can remain for days or even years after injury according to the cdc contact sports such as football are one of the leading causes of mtbi 
1	109887	9887	in these sports mtbi is diagnosed by a sideline clinician through subjective evaluation of symptoms and neurological testing .acute symptoms include dizziness confusion and personality changes which can remain for days or even years after injury according to the cdc contact sports such as football are one of the leading causes of mtbi .because of the large variance of symptoms within different individuals and the pressure of athletes to return to play mtbi can often be missed by these tests in this project our goal is to train a neural network which will automatically extract relevant features to classify between real impacts and false positives .introduction	in these sports mtbi is diagnosed by a sideline clinician through subjective evaluation of symptoms and neurological testing 
1	109888	9888	because of the large variance of symptoms within different individuals and the pressure of athletes to return to play mtbi can often be missed by these tests in this project our goal is to train a neural network which will automatically extract relevant features to classify between real impacts and false positives .in these sports mtbi is diagnosed by a sideline clinician through subjective evaluation of symptoms and neurological testing .the input to our algorithm is mouthguard time series data .introduction	because of the large variance of symptoms within different individuals and the pressure of athletes to return to play mtbi can often be missed by these tests in this project our goal is to train a neural network which will automatically extract relevant features to classify between real impacts and false positives 
0	109889	9889	the input to our algorithm is mouthguard time series data .because of the large variance of symptoms within different individuals and the pressure of athletes to return to play mtbi can often be missed by these tests in this project our goal is to train a neural network which will automatically extract relevant features to classify between real impacts and false positives .last sentence.introduction	the input to our algorithm is mouthguard time series data 
1	109890	9890	currently there are a number of sensor systems used for measuring head impact kinematics in contact sports .first sentence.many of these systems use a simple linear acceleration threshold for differentiating impacts and non impacts however this leaves the device prone to a large number of false positives .related work	currently there are a number of sensor systems used for measuring head impact kinematics in contact sports 
1	109891	9891	many of these systems use a simple linear acceleration threshold for differentiating impacts and non impacts however this leaves the device prone to a large number of false positives .currently there are a number of sensor systems used for measuring head impact kinematics in contact sports .many companies and research groups are developing proprietary algorithms for detecting impacts but little has been published validating their accuracy to the best of our knowledge only one study has attempted to use a neural network algorithm for detecting head impacts and non impacts from kinematic sensor data this study used a simple net with a single fully connected layer and only achieved 47 specificity and 88 sensitivity on their dataset of soccer athletes.related work	many of these systems use a simple linear acceleration threshold for differentiating impacts and non impacts however this leaves the device prone to a large number of false positives 
1	109892	9892	many companies and research groups are developing proprietary algorithms for detecting impacts but little has been published validating their accuracy to the best of our knowledge only one study has attempted to use a neural network algorithm for detecting head impacts and non impacts from kinematic sensor data this study used a simple net with a single fully connected layer and only achieved 47 specificity and 88 sensitivity on their dataset of soccer athletes.many of these systems use a simple linear acceleration threshold for differentiating impacts and non impacts however this leaves the device prone to a large number of false positives .last sentence.related work	many companies and research groups are developing proprietary algorithms for detecting impacts but little has been published validating their accuracy to the best of our knowledge only one study has attempted to use a neural network algorithm for detecting head impacts and non impacts from kinematic sensor data this study used a simple net with a single fully connected layer and only achieved 47 specificity and 88 sensitivity on their dataset of soccer athletes
0	109893	9893	our dataset is 527 examples of which half are labeled real or true impact and the other half are labeled as false impacts .first sentence.the dataset was obtained by instrumenting stanford football athletes over the fall 2017 season with the camarillo lab instrumented mouthguard .dataset and features	our dataset is 527 examples of which half are labeled real or true impact and the other half are labeled as false impacts 
0	109894	9894	the dataset was obtained by instrumenting stanford football athletes over the fall 2017 season with the camarillo lab instrumented mouthguard .our dataset is 527 examples of which half are labeled real or true impact and the other half are labeled as false impacts .to obtain the ground truth labels for the dataset videos of each game and practice time synced with the mouthguards were analyzed according to the methodology outlined by kuo et al we split our dataset up into 70 for training and 15 for both evaluation and testing for when leveraging k fold cross validation and 70 for training and 30 for evaluation for when training our selected architecture .dataset and features	the dataset was obtained by instrumenting stanford football athletes over the fall 2017 season with the camarillo lab instrumented mouthguard 
1	109895	9895	to obtain the ground truth labels for the dataset videos of each game and practice time synced with the mouthguards were analyzed according to the methodology outlined by kuo et al we split our dataset up into 70 for training and 15 for both evaluation and testing for when leveraging k fold cross validation and 70 for training and 30 for evaluation for when training our selected architecture .the dataset was obtained by instrumenting stanford football athletes over the fall 2017 season with the camarillo lab instrumented mouthguard .each example has dimension 199x6 comprised of 6 time traces of length 199 200 ms .dataset and features	to obtain the ground truth labels for the dataset videos of each game and practice time synced with the mouthguards were analyzed according to the methodology outlined by kuo et al we split our dataset up into 70 for training and 15 for both evaluation and testing for when leveraging k fold cross validation and 70 for training and 30 for evaluation for when training our selected architecture 
0	109896	9896	each example has dimension 199x6 comprised of 6 time traces of length 199 200 ms .to obtain the ground truth labels for the dataset videos of each game and practice time synced with the mouthguards were analyzed according to the methodology outlined by kuo et al we split our dataset up into 70 for training and 15 for both evaluation and testing for when leveraging k fold cross validation and 70 for training and 30 for evaluation for when training our selected architecture .the six time traces are the linear acceleration at the head center of gravity in the x y and z axes and angular velocity of the head in the x y and z anatomical planes .dataset and features	each example has dimension 199x6 comprised of 6 time traces of length 199 200 ms 
0	109897	9897	the six time traces are the linear acceleration at the head center of gravity in the x y and z axes and angular velocity of the head in the x y and z anatomical planes .each example has dimension 199x6 comprised of 6 time traces of length 199 200 ms .the data was sampled with a time step of 1000 hz with 50 ms recorded pre trigger and 150 ms post trigger for 299 data points .dataset and features	the six time traces are the linear acceleration at the head center of gravity in the x y and z axes and angular velocity of the head in the x y and z anatomical planes 
0	109898	9898	the data was sampled with a time step of 1000 hz with 50 ms recorded pre trigger and 150 ms post trigger for 299 data points .the six time traces are the linear acceleration at the head center of gravity in the x y and z axes and angular velocity of the head in the x y and z anatomical planes .data was pre processed using standardization by subtracting out the mean of each sensor s values and dividing by the standard deviation .dataset and features	the data was sampled with a time step of 1000 hz with 50 ms recorded pre trigger and 150 ms post trigger for 299 data points 
0	109899	9899	data was pre processed using standardization by subtracting out the mean of each sensor s values and dividing by the standard deviation .the data was sampled with a time step of 1000 hz with 50 ms recorded pre trigger and 150 ms post trigger for 299 data points .last sentence.dataset and features	data was pre processed using standardization by subtracting out the mean of each sensor s values and dividing by the standard deviation 
0	109900	9900	a convolutional neural network is a class of deep neural networks comprised of convolutional layers .first sentence.in convolutional layers each sensor measurement is convolved with a weighting function w in the case of a 1d input to a 1d convolutional layer the i th product element can be found as follows where b is the bias term d is the filter width and w d a re each of the filters .methods	a convolutional neural network is a class of deep neural networks comprised of convolutional layers 
0	109901	9901	in convolutional layers each sensor measurement is convolved with a weighting function w in the case of a 1d input to a 1d convolutional layer the i th product element can be found as follows where b is the bias term d is the filter width and w d a re each of the filters .a convolutional neural network is a class of deep neural networks comprised of convolutional layers .in the case where the input to the 1d convolution is a multi channeled such as our application where we are stacking six input signals the output of the convolution each channel is added to give the following where s is the number of input channels in our case six .methods	in convolutional layers each sensor measurement is convolved with a weighting function w in the case of a 1d input to a 1d convolutional layer the i th product element can be found as follows where b is the bias term d is the filter width and w d a re each of the filters 
0	109902	9902	in the case where the input to the 1d convolution is a multi channeled such as our application where we are stacking six input signals the output of the convolution each channel is added to give the following where s is the number of input channels in our case six .in convolutional layers each sensor measurement is convolved with a weighting function w in the case of a 1d input to a 1d convolutional layer the i th product element can be found as follows where b is the bias term d is the filter width and w d a re each of the filters .the output of a 1d convolutional layer is a single vector .methods	in the case where the input to the 1d convolution is a multi channeled such as our application where we are stacking six input signals the output of the convolution each channel is added to give the following where s is the number of input channels in our case six 
0	109903	9903	the output of a 1d convolutional layer is a single vector .in the case where the input to the 1d convolution is a multi channeled such as our application where we are stacking six input signals the output of the convolution each channel is added to give the following where s is the number of input channels in our case six .in 2d convolutional layers this process is repeated in two dimensions providing a two dimensional output .methods	the output of a 1d convolutional layer is a single vector 
0	109904	9904	in 2d convolutional layers this process is repeated in two dimensions providing a two dimensional output .the output of a 1d convolutional layer is a single vector .convolutional neural networks commonly have pooling operations which combine outputs of neuron clusters at one layer into a single neuron in the next layer .methods	in 2d convolutional layers this process is repeated in two dimensions providing a two dimensional output 
0	109905	9905	convolutional neural networks commonly have pooling operations which combine outputs of neuron clusters at one layer into a single neuron in the next layer .in 2d convolutional layers this process is repeated in two dimensions providing a two dimensional output .max pooling layers use the maximum value from a specified cluster of neurons while average pooling uses the average of a specified cluster .methods	convolutional neural networks commonly have pooling operations which combine outputs of neuron clusters at one layer into a single neuron in the next layer 
0	109906	9906	max pooling layers use the maximum value from a specified cluster of neurons while average pooling uses the average of a specified cluster .convolutional neural networks commonly have pooling operations which combine outputs of neuron clusters at one layer into a single neuron in the next layer .further dropout layers can be added to help prevent overfitting a dropout layer will randomly ignore a certain percent of the layer interconnections during training we investigated multiple different convolutional neural network architectures using keras and tensorflow written in python specifically we developed both sequential models and recursive network models .methods	max pooling layers use the maximum value from a specified cluster of neurons while average pooling uses the average of a specified cluster 
1	109907	9907	further dropout layers can be added to help prevent overfitting a dropout layer will randomly ignore a certain percent of the layer interconnections during training we investigated multiple different convolutional neural network architectures using keras and tensorflow written in python specifically we developed both sequential models and recursive network models .max pooling layers use the maximum value from a specified cluster of neurons while average pooling uses the average of a specified cluster .in investigating proper model architecture we utilized the k fold cross validation technique k 10 as we knew that 527 examples is not a very large amount and gathering more data was not feasible within the scope of this project .methods	further dropout layers can be added to help prevent overfitting a dropout layer will randomly ignore a certain percent of the layer interconnections during training we investigated multiple different convolutional neural network architectures using keras and tensorflow written in python specifically we developed both sequential models and recursive network models 
1	109908	9908	in investigating proper model architecture we utilized the k fold cross validation technique k 10 as we knew that 527 examples is not a very large amount and gathering more data was not feasible within the scope of this project .further dropout layers can be added to help prevent overfitting a dropout layer will randomly ignore a certain percent of the layer interconnections during training we investigated multiple different convolutional neural network architectures using keras and tensorflow written in python specifically we developed both sequential models and recursive network models .in training all of our networks the number of epochs was increased indefinitely until five consecutive epochs did not result in an improved evaluation binary cross entropy loss .methods	in investigating proper model architecture we utilized the k fold cross validation technique k 10 as we knew that 527 examples is not a very large amount and gathering more data was not feasible within the scope of this project 
1	109909	9909	in training all of our networks the number of epochs was increased indefinitely until five consecutive epochs did not result in an improved evaluation binary cross entropy loss .in investigating proper model architecture we utilized the k fold cross validation technique k 10 as we knew that 527 examples is not a very large amount and gathering more data was not feasible within the scope of this project .following completion of training the model at the end of the epoch with the lowest evaluation loss was saved and used for analysis we developed and compared two primary architectures .methods	in training all of our networks the number of epochs was increased indefinitely until five consecutive epochs did not result in an improved evaluation binary cross entropy loss 
1	109910	9910	following completion of training the model at the end of the epoch with the lowest evaluation loss was saved and used for analysis we developed and compared two primary architectures .in training all of our networks the number of epochs was increased indefinitely until five consecutive epochs did not result in an improved evaluation binary cross entropy loss .the recursivenet model has the most convolutional layers as seen in.methods	following completion of training the model at the end of the epoch with the lowest evaluation loss was saved and used for analysis we developed and compared two primary architectures 
0	109911	9911	the recursivenet model has the most convolutional layers as seen in.following completion of training the model at the end of the epoch with the lowest evaluation loss was saved and used for analysis we developed and compared two primary architectures .last sentence.methods	the recursivenet model has the most convolutional layers as seen in
0	109912	9912	in all testing the metric we optimized for was accuracy but we also performed tests on precision specificity and sensitivity .first sentence.the equations for the metrics are described below where tp is true positive tn is true negative fp is false positive and fn is false negative .experiments results discussion	in all testing the metric we optimized for was accuracy but we also performed tests on precision specificity and sensitivity 
0	109913	9913	the equations for the metrics are described below where tp is true positive tn is true negative fp is false positive and fn is false negative .in all testing the metric we optimized for was accuracy but we also performed tests on precision specificity and sensitivity .using our baseline hyperparameters in preliminary testing we found that the hiknet had comparable accuracy to recursivenet at a much lower computational cost .experiments results discussion	the equations for the metrics are described below where tp is true positive tn is true negative fp is false positive and fn is false negative 
1	109914	9914	using our baseline hyperparameters in preliminary testing we found that the hiknet had comparable accuracy to recursivenet at a much lower computational cost .the equations for the metrics are described below where tp is true positive tn is true negative fp is false positive and fn is false negative .thus we focused our hyperparameter tuning on the hiknet architecture we tuned the final hiknet using a greedy optimization scheme for number of 1d conv layers 2d conv layers and type of final layer .experiments results discussion	using our baseline hyperparameters in preliminary testing we found that the hiknet had comparable accuracy to recursivenet at a much lower computational cost 
1	109915	9915	thus we focused our hyperparameter tuning on the hiknet architecture we tuned the final hiknet using a greedy optimization scheme for number of 1d conv layers 2d conv layers and type of final layer .using our baseline hyperparameters in preliminary testing we found that the hiknet had comparable accuracy to recursivenet at a much lower computational cost .because our parameters were initialized to random values and convergence is highly dependent on weight initialization we also did a parameter sweep to find the optimal filter size kernel width and dropout threshold .experiments results discussion	thus we focused our hyperparameter tuning on the hiknet architecture we tuned the final hiknet using a greedy optimization scheme for number of 1d conv layers 2d conv layers and type of final layer 
1	109916	9916	because our parameters were initialized to random values and convergence is highly dependent on weight initialization we also did a parameter sweep to find the optimal filter size kernel width and dropout threshold .thus we focused our hyperparameter tuning on the hiknet architecture we tuned the final hiknet using a greedy optimization scheme for number of 1d conv layers 2d conv layers and type of final layer .the filter size was changed between the values of 15 and 200 the kernel width was between 0 and 50 and the dropout threshold was swept between 0 and 0 6 .experiments results discussion	because our parameters were initialized to random values and convergence is highly dependent on weight initialization we also did a parameter sweep to find the optimal filter size kernel width and dropout threshold 
0	109917	9917	the filter size was changed between the values of 15 and 200 the kernel width was between 0 and 50 and the dropout threshold was swept between 0 and 0 6 .because our parameters were initialized to random values and convergence is highly dependent on weight initialization we also did a parameter sweep to find the optimal filter size kernel width and dropout threshold .the optimal dropout threshold was found to be 0 4 kernel width was 15 and filter number was 150 .experiments results discussion	the filter size was changed between the values of 15 and 200 the kernel width was between 0 and 50 and the dropout threshold was swept between 0 and 0 6 
1	109918	9918	the optimal dropout threshold was found to be 0 4 kernel width was 15 and filter number was 150 .the filter size was changed between the values of 15 and 200 the kernel width was between 0 and 50 and the dropout threshold was swept between 0 and 0 6 .we found that the optimal kernel width and dropout threshold for hiknet was the same for the perceptionnet the final performance metrics are summarized in.experiments results discussion	the optimal dropout threshold was found to be 0 4 kernel width was 15 and filter number was 150 
1	109919	9919	we found that the optimal kernel width and dropout threshold for hiknet was the same for the perceptionnet the final performance metrics are summarized in.the optimal dropout threshold was found to be 0 4 kernel width was 15 and filter number was 150 .last sentence.experiments results discussion	we found that the optimal kernel width and dropout threshold for hiknet was the same for the perceptionnet the final performance metrics are summarized in
1	109920	9920	conclusion future workin conclusion a low parameter neural network performed very well and achieved better performance metrics than the existing svm classifier trained on the same mouthguard time series data set .first sentence.we created two deep convolutional neural networks one that was recursive and one that was sequential and although both performed similarly we chose the hiknet because it had fewer parameters and a higher confidence in its ability to generalize to other kinematic time series datasets than the recursivenet based on our literature search .7 	conclusion future workin conclusion a low parameter neural network performed very well and achieved better performance metrics than the existing svm classifier trained on the same mouthguard time series data set 
1	109921	9921	we created two deep convolutional neural networks one that was recursive and one that was sequential and although both performed similarly we chose the hiknet because it had fewer parameters and a higher confidence in its ability to generalize to other kinematic time series datasets than the recursivenet based on our literature search .conclusion future workin conclusion a low parameter neural network performed very well and achieved better performance metrics than the existing svm classifier trained on the same mouthguard time series data set .we used a greedy optimization scheme to build the architecture of hiknet and did a parameter sweep to find the optimal filter size kernel width and dropout percent as an immediate next step we can apply our neural networks to a larger mouthguard dataset as more data is collected in the camarillo lab to further tune parameters .7 	we created two deep convolutional neural networks one that was recursive and one that was sequential and although both performed similarly we chose the hiknet because it had fewer parameters and a higher confidence in its ability to generalize to other kinematic time series datasets than the recursivenet based on our literature search 
1	109922	9922	we used a greedy optimization scheme to build the architecture of hiknet and did a parameter sweep to find the optimal filter size kernel width and dropout percent as an immediate next step we can apply our neural networks to a larger mouthguard dataset as more data is collected in the camarillo lab to further tune parameters .we created two deep convolutional neural networks one that was recursive and one that was sequential and although both performed similarly we chose the hiknet because it had fewer parameters and a higher confidence in its ability to generalize to other kinematic time series datasets than the recursivenet based on our literature search .in future work we can use the same mouthguard and video impact footage to create a dataset with more specific labels i e .7 	we used a greedy optimization scheme to build the architecture of hiknet and did a parameter sweep to find the optimal filter size kernel width and dropout percent as an immediate next step we can apply our neural networks to a larger mouthguard dataset as more data is collected in the camarillo lab to further tune parameters 
1	109923	9923	in future work we can use the same mouthguard and video impact footage to create a dataset with more specific labels i e .we used a greedy optimization scheme to build the architecture of hiknet and did a parameter sweep to find the optimal filter size kernel width and dropout percent as an immediate next step we can apply our neural networks to a larger mouthguard dataset as more data is collected in the camarillo lab to further tune parameters .where the impact was located on the head body impact or no impact .7 	in future work we can use the same mouthguard and video impact footage to create a dataset with more specific labels i e 
0	109924	9924	where the impact was located on the head body impact or no impact .in future work we can use the same mouthguard and video impact footage to create a dataset with more specific labels i e .using this data we could create a softmax classifier to predict whether an impact occurred and where it occurred on the head and body .7 	where the impact was located on the head body impact or no impact 
1	109925	9925	using this data we could create a softmax classifier to predict whether an impact occurred and where it occurred on the head and body .where the impact was located on the head body impact or no impact .lastly once more concussion data is obtained we could create a neural network that could detect whether an impact occurred and predicts if the impact resulted in a concussion or not .7 	using this data we could create a softmax classifier to predict whether an impact occurred and where it occurred on the head and body 
1	109926	9926	lastly once more concussion data is obtained we could create a neural network that could detect whether an impact occurred and predicts if the impact resulted in a concussion or not .using this data we could create a softmax classifier to predict whether an impact occurred and where it occurred on the head and body .this would require additional data beyond just the mouthguard data such as clinical diagnoses and medical records .7 	lastly once more concussion data is obtained we could create a neural network that could detect whether an impact occurred and predicts if the impact resulted in a concussion or not 
0	109927	9927	this would require additional data beyond just the mouthguard data such as clinical diagnoses and medical records .lastly once more concussion data is obtained we could create a neural network that could detect whether an impact occurred and predicts if the impact resulted in a concussion or not .the ultimate goal would be to have a device that could instantly tell if an impact resulted in concussion although it may take years to obtain the dataset needed to train this classifier the performance of our network architecture gives promise that this could be possible using a similar methodology as put forth in this work .7 	this would require additional data beyond just the mouthguard data such as clinical diagnoses and medical records 
1	109928	9928	the ultimate goal would be to have a device that could instantly tell if an impact resulted in concussion although it may take years to obtain the dataset needed to train this classifier the performance of our network architecture gives promise that this could be possible using a similar methodology as put forth in this work .this would require additional data beyond just the mouthguard data such as clinical diagnoses and medical records .last sentence.7 	the ultimate goal would be to have a device that could instantly tell if an impact resulted in concussion although it may take years to obtain the dataset needed to train this classifier the performance of our network architecture gives promise that this could be possible using a similar methodology as put forth in this work 
1	109929	9929	michael fanton developed hiknet neural network architecture in keras set up architecture optimization helped with statistical analyses provided background information nicholas gaudio lead the insight into keras and the model architecture setup created the recursivenet setup auto epoch stopping and saved the best epoch model conducted experiments to find the optimal filter width and filter number .first sentence.alissa ling preprocessed data wrote the k fold function optimized the dropout threshold lead the final poster wrote first draft of sections 9 .contributions	michael fanton developed hiknet neural network architecture in keras set up architecture optimization helped with statistical analyses provided background information nicholas gaudio lead the insight into keras and the model architecture setup created the recursivenet setup auto epoch stopping and saved the best epoch model conducted experiments to find the optimal filter width and filter number 
1	109930	9930	alissa ling preprocessed data wrote the k fold function optimized the dropout threshold lead the final poster wrote first draft of sections 9 .michael fanton developed hiknet neural network architecture in keras set up architecture optimization helped with statistical analyses provided background information nicholas gaudio lead the insight into keras and the model architecture setup created the recursivenet setup auto epoch stopping and saved the best epoch model conducted experiments to find the optimal filter width and filter number .last sentence.contributions	alissa ling preprocessed data wrote the k fold function optimized the dropout threshold lead the final poster wrote first draft of sections 9 
0	109931	9931	with the recent failure of senate bill sb 827 in california pressure is higher than ever on state politicians to better understand and respond to the increasing unaffordability of california s urban centers .first sentence.designed to issue more housing construction permits in high opportunity areas sb 827 was ironically crippled by its failure to explicitly acknowledge the possible gentrification externalities of new housing construction .i introduction	with the recent failure of senate bill sb 827 in california pressure is higher than ever on state politicians to better understand and respond to the increasing unaffordability of california s urban centers 
0	109932	9932	designed to issue more housing construction permits in high opportunity areas sb 827 was ironically crippled by its failure to explicitly acknowledge the possible gentrification externalities of new housing construction .with the recent failure of senate bill sb 827 in california pressure is higher than ever on state politicians to better understand and respond to the increasing unaffordability of california s urban centers .because of the astronomical and increasing cost of housing more californians live in poverty than in any other state when cost of living is accounted for one tool that academics use to design thoughtful housing policy is the gentrification early warning system i .i introduction	designed to issue more housing construction permits in high opportunity areas sb 827 was ironically crippled by its failure to explicitly acknowledge the possible gentrification externalities of new housing construction 
1	109933	9933	because of the astronomical and increasing cost of housing more californians live in poverty than in any other state when cost of living is accounted for one tool that academics use to design thoughtful housing policy is the gentrification early warning system i .designed to issue more housing construction permits in high opportunity areas sb 827 was ironically crippled by its failure to explicitly acknowledge the possible gentrification externalities of new housing construction .using california wide census data to classify emergent gentrification and to understand the leading indicators of gentrification through feature selection ii .i introduction	because of the astronomical and increasing cost of housing more californians live in poverty than in any other state when cost of living is accounted for one tool that academics use to design thoughtful housing policy is the gentrification early warning system i 
1	109934	9934	using california wide census data to classify emergent gentrification and to understand the leading indicators of gentrification through feature selection ii .because of the astronomical and increasing cost of housing more californians live in poverty than in any other state when cost of living is accounted for one tool that academics use to design thoughtful housing policy is the gentrification early warning system i .and modelling the state s housing market as an interconnected network to test an economic theory of how gentrification spreads specifically we use machine learning techniques primarily non parametric models such as random forests and gradient boosting to ascertain the leading indicators of gentrification at the census tract level in california .i introduction	using california wide census data to classify emergent gentrification and to understand the leading indicators of gentrification through feature selection ii 
1	109935	9935	and modelling the state s housing market as an interconnected network to test an economic theory of how gentrification spreads specifically we use machine learning techniques primarily non parametric models such as random forests and gradient boosting to ascertain the leading indicators of gentrification at the census tract level in california .using california wide census data to classify emergent gentrification and to understand the leading indicators of gentrification through feature selection ii .we formulate the problem as binary classification over a five year time horizon using custom designed responses to proxy for whether gentrification was observed in a community over the prediction period .i introduction	and modelling the state s housing market as an interconnected network to test an economic theory of how gentrification spreads specifically we use machine learning techniques primarily non parametric models such as random forests and gradient boosting to ascertain the leading indicators of gentrification at the census tract level in california 
1	109936	9936	we formulate the problem as binary classification over a five year time horizon using custom designed responses to proxy for whether gentrification was observed in a community over the prediction period .and modelling the state s housing market as an interconnected network to test an economic theory of how gentrification spreads specifically we use machine learning techniques primarily non parametric models such as random forests and gradient boosting to ascertain the leading indicators of gentrification at the census tract level in california .last sentence.i introduction	we formulate the problem as binary classification over a five year time horizon using custom designed responses to proxy for whether gentrification was observed in a community over the prediction period 
1	109937	9937	we source data from american factfinder aff a public information tool produced by the united states census prior research describes gentrification in terms of either rising costs of living or displacement of the poor as income distributions shift towards affluence because gentrification occurs over a long time horizon we split the feature set around a pivot year of 2012 we compute the responses using the data from years 2012 2016 with the data from 2010 and 2011 used as features in the above formulation t 2012 and t 2016 .first sentence.splitting the data to forecast gentrification over a long time horizon comports with previous research to model the second response we use an imputed measurement of the inter year intra tract change in the income distribution of the tract see for each tract we compute the hellinger distance between the observed income distribution and a baseline in which all residents are perfectly affluent with probability 1 .ii data responses and features	we source data from american factfinder aff a public information tool produced by the united states census prior research describes gentrification in terms of either rising costs of living or displacement of the poor as income distributions shift towards affluence because gentrification occurs over a long time horizon we split the feature set around a pivot year of 2012 we compute the responses using the data from years 2012 2016 with the data from 2010 and 2011 used as features in the above formulation t 2012 and t 2016 
1	109938	9938	splitting the data to forecast gentrification over a long time horizon comports with previous research to model the second response we use an imputed measurement of the inter year intra tract change in the income distribution of the tract see for each tract we compute the hellinger distance between the observed income distribution and a baseline in which all residents are perfectly affluent with probability 1 .we source data from american factfinder aff a public information tool produced by the united states census prior research describes gentrification in terms of either rising costs of living or displacement of the poor as income distributions shift towards affluence because gentrification occurs over a long time horizon we split the feature set around a pivot year of 2012 we compute the responses using the data from years 2012 2016 with the data from 2010 and 2011 used as features in the above formulation t 2012 and t 2016 .tracts with low hellinger distances tend to be high income tracts with high hellinger distances tend to be low income .ii data responses and features	splitting the data to forecast gentrification over a long time horizon comports with previous research to model the second response we use an imputed measurement of the inter year intra tract change in the income distribution of the tract see for each tract we compute the hellinger distance between the observed income distribution and a baseline in which all residents are perfectly affluent with probability 1 
0	109939	9939	tracts with low hellinger distances tend to be high income tracts with high hellinger distances tend to be low income .splitting the data to forecast gentrification over a long time horizon comports with previous research to model the second response we use an imputed measurement of the inter year intra tract change in the income distribution of the tract see for each tract we compute the hellinger distance between the observed income distribution and a baseline in which all residents are perfectly affluent with probability 1 .finally we compute the response by taking the differences of these hellinger distances for each tract between 2012 the pivot year and 2016 .ii data responses and features	tracts with low hellinger distances tend to be high income tracts with high hellinger distances tend to be low income 
0	109940	9940	finally we compute the response by taking the differences of these hellinger distances for each tract between 2012 the pivot year and 2016 .tracts with low hellinger distances tend to be high income tracts with high hellinger distances tend to be low income .a tract that becomes more affluent gentrifies from 2012 to 2016 has a negative difference and vice versa for a tract that becomes more low income .ii data responses and features	finally we compute the response by taking the differences of these hellinger distances for each tract between 2012 the pivot year and 2016 
0	109941	9941	a tract that becomes more affluent gentrifies from 2012 to 2016 has a negative difference and vice versa for a tract that becomes more low income .finally we compute the response by taking the differences of these hellinger distances for each tract between 2012 the pivot year and 2016 .we rescale the responses so that they are bounded between 0 and 100 and positive differences signal gentrification .ii data responses and features	a tract that becomes more affluent gentrifies from 2012 to 2016 has a negative difference and vice versa for a tract that becomes more low income 
0	109942	9942	we rescale the responses so that they are bounded between 0 and 100 and positive differences signal gentrification .a tract that becomes more affluent gentrifies from 2012 to 2016 has a negative difference and vice versa for a tract that becomes more low income .finally we relabel each response 1 gentrification occurred or 0 gentrification did not occur for both the monthly cost of housing and income distribution shift responses we characterize each census tract using a vector of roughly 150 features assembled from tables s2502 s2503 b25085 and dp03 in aff .ii data responses and features	we rescale the responses so that they are bounded between 0 and 100 and positive differences signal gentrification 
1	109943	9943	finally we relabel each response 1 gentrification occurred or 0 gentrification did not occur for both the monthly cost of housing and income distribution shift responses we characterize each census tract using a vector of roughly 150 features assembled from tables s2502 s2503 b25085 and dp03 in aff .we rescale the responses so that they are bounded between 0 and 100 and positive differences signal gentrification .these include tracts demographic and economic characteristics such as employment by industry ethnic and racial composition level education and more additionally we engineer four features based on the theory of spatial equilibrium proposed in prior work on endogenous gentrification here y j denotes each response computed between the pre pivot years 2010 and 2011 .ii data responses and features	finally we relabel each response 1 gentrification occurred or 0 gentrification did not occur for both the monthly cost of housing and income distribution shift responses we characterize each census tract using a vector of roughly 150 features assembled from tables s2502 s2503 b25085 and dp03 in aff 
1	109944	9944	these include tracts demographic and economic characteristics such as employment by industry ethnic and racial composition level education and more additionally we engineer four features based on the theory of spatial equilibrium proposed in prior work on endogenous gentrification here y j denotes each response computed between the pre pivot years 2010 and 2011 .finally we relabel each response 1 gentrification occurred or 0 gentrification did not occur for both the monthly cost of housing and income distribution shift responses we characterize each census tract using a vector of roughly 150 features assembled from tables s2502 s2503 b25085 and dp03 in aff .likewise for each response we compute the local moran s i statistic a measure of spatial clustering where z k is the deviation of the response of interest from the mean across all n tracts in the training sample computed between 2010 and 2011 the observation period .ii data responses and features	these include tracts demographic and economic characteristics such as employment by industry ethnic and racial composition level education and more additionally we engineer four features based on the theory of spatial equilibrium proposed in prior work on endogenous gentrification here y j denotes each response computed between the pre pivot years 2010 and 2011 
1	109945	9945	likewise for each response we compute the local moran s i statistic a measure of spatial clustering where z k is the deviation of the response of interest from the mean across all n tracts in the training sample computed between 2010 and 2011 the observation period .these include tracts demographic and economic characteristics such as employment by industry ethnic and racial composition level education and more additionally we engineer four features based on the theory of spatial equilibrium proposed in prior work on endogenous gentrification here y j denotes each response computed between the pre pivot years 2010 and 2011 .we do not use time invariant features describing the geography of the census tracts .ii data responses and features	likewise for each response we compute the local moran s i statistic a measure of spatial clustering where z k is the deviation of the response of interest from the mean across all n tracts in the training sample computed between 2010 and 2011 the observation period 
0	109946	9946	we do not use time invariant features describing the geography of the census tracts .likewise for each response we compute the local moran s i statistic a measure of spatial clustering where z k is the deviation of the response of interest from the mean across all n tracts in the training sample computed between 2010 and 2011 the observation period .these ought not add much explanatory power to a model that forecasts gentrification by time .ii data responses and features	we do not use time invariant features describing the geography of the census tracts 
0	109947	9947	these ought not add much explanatory power to a model that forecasts gentrification by time .we do not use time invariant features describing the geography of the census tracts .likewise we do not add network topological features from e g .ii data responses and features	these ought not add much explanatory power to a model that forecasts gentrification by time 
0	109948	9948	likewise we do not add network topological features from e g .these ought not add much explanatory power to a model that forecasts gentrification by time .overall the data consist of 8 056 observations for each of california s census tracts with one dropped due to missing data .ii data responses and features	likewise we do not add network topological features from e g 
0	109949	9949	overall the data consist of 8 056 observations for each of california s census tracts with one dropped due to missing data .likewise we do not add network topological features from e g .surprisingly a priori we observed the classes to be roughly balanced for both responses suggesting that there still exist pockets of affordability in the state .ii data responses and features	overall the data consist of 8 056 observations for each of california s census tracts with one dropped due to missing data 
0	109950	9950	surprisingly a priori we observed the classes to be roughly balanced for both responses suggesting that there still exist pockets of affordability in the state .overall the data consist of 8 056 observations for each of california s census tracts with one dropped due to missing data .we split the data into a training set comprising 90 7 262 of the observations and validation and test sets comprising 5 397 respectively .ii data responses and features	surprisingly a priori we observed the classes to be roughly balanced for both responses suggesting that there still exist pockets of affordability in the state 
0	109951	9951	we split the data into a training set comprising 90 7 262 of the observations and validation and test sets comprising 5 397 respectively .surprisingly a priori we observed the classes to be roughly balanced for both responses suggesting that there still exist pockets of affordability in the state .last sentence.ii data responses and features	we split the data into a training set comprising 90 7 262 of the observations and validation and test sets comprising 5 397 respectively 
1	109952	9952	we applied four machine learning methods to each classification problem defining gentrification as the change in monthly cost and as the shift in income distribution .first sentence.we used a random forest classifier a gradient boosting model xgboost an 1 penalized logistic regression and an ensemble approach that classified census tracts according to a majority vote of the aforementioned three models random forests are a variant of bagged decision trees a random forest classifier grows a substantial number of independent classification trees each of which minimizes the gini impurity of its leaf nodes through recursive binary splitting gini impurity measures how often a randomly chosen observation in the node would be mislabelled if it were assigned a random label according to the distribution of responses in the node .iii methods	we applied four machine learning methods to each classification problem defining gentrification as the change in monthly cost and as the shift in income distribution 
1	109953	9953	we used a random forest classifier a gradient boosting model xgboost an 1 penalized logistic regression and an ensemble approach that classified census tracts according to a majority vote of the aforementioned three models random forests are a variant of bagged decision trees a random forest classifier grows a substantial number of independent classification trees each of which minimizes the gini impurity of its leaf nodes through recursive binary splitting gini impurity measures how often a randomly chosen observation in the node would be mislabelled if it were assigned a random label according to the distribution of responses in the node .we applied four machine learning methods to each classification problem defining gentrification as the change in monthly cost and as the shift in income distribution .as classification trees grown on the same set of bootstrapped data tend to be highly correlated the random forest algorithm decorrelates the trees by constraining each split in each tree to be on a random subsample of features in the feature space gradient boosting is an ensemble technique using classification trees in which trees are grown sequentially as opposed to simultaneously in random forests .iii methods	we used a random forest classifier a gradient boosting model xgboost an 1 penalized logistic regression and an ensemble approach that classified census tracts according to a majority vote of the aforementioned three models random forests are a variant of bagged decision trees a random forest classifier grows a substantial number of independent classification trees each of which minimizes the gini impurity of its leaf nodes through recursive binary splitting gini impurity measures how often a randomly chosen observation in the node would be mislabelled if it were assigned a random label according to the distribution of responses in the node 
1	109954	9954	as classification trees grown on the same set of bootstrapped data tend to be highly correlated the random forest algorithm decorrelates the trees by constraining each split in each tree to be on a random subsample of features in the feature space gradient boosting is an ensemble technique using classification trees in which trees are grown sequentially as opposed to simultaneously in random forests .we used a random forest classifier a gradient boosting model xgboost an 1 penalized logistic regression and an ensemble approach that classified census tracts according to a majority vote of the aforementioned three models random forests are a variant of bagged decision trees a random forest classifier grows a substantial number of independent classification trees each of which minimizes the gini impurity of its leaf nodes through recursive binary splitting gini impurity measures how often a randomly chosen observation in the node would be mislabelled if it were assigned a random label according to the distribution of responses in the node .later trees are grown to minimize the errors made by their predecessors .iii methods	as classification trees grown on the same set of bootstrapped data tend to be highly correlated the random forest algorithm decorrelates the trees by constraining each split in each tree to be on a random subsample of features in the feature space gradient boosting is an ensemble technique using classification trees in which trees are grown sequentially as opposed to simultaneously in random forests 
0	109955	9955	later trees are grown to minimize the errors made by their predecessors .as classification trees grown on the same set of bootstrapped data tend to be highly correlated the random forest algorithm decorrelates the trees by constraining each split in each tree to be on a random subsample of features in the feature space gradient boosting is an ensemble technique using classification trees in which trees are grown sequentially as opposed to simultaneously in random forests .each subsequent tree learns from the mistakes made earlier in training .iii methods	later trees are grown to minimize the errors made by their predecessors 
0	109956	9956	each subsequent tree learns from the mistakes made earlier in training .later trees are grown to minimize the errors made by their predecessors .xgboost a popular implementation of gradient boosting which enables regularization of the trees minimizes the loss function where i are the predicted class each f k is a decision tree and is a regularization function of the number of leaves in each tree and the weights of those leaves for the random forest estimator we tuned n the number of trees and p the number of features in the random split set at every split .iii methods	each subsequent tree learns from the mistakes made earlier in training 
0	109957	9957	xgboost a popular implementation of gradient boosting which enables regularization of the trees minimizes the loss function where i are the predicted class each f k is a decision tree and is a regularization function of the number of leaves in each tree and the weights of those leaves for the random forest estimator we tuned n the number of trees and p the number of features in the random split set at every split .each subsequent tree learns from the mistakes made earlier in training .for the xgboost estimator we tuned the learning rate the tree depth d on each tree and the regularization parameter our final unitary model was the only parametric estimator 1 penalized logistic regression commonly known as the lasso .iii methods	xgboost a popular implementation of gradient boosting which enables regularization of the trees minimizes the loss function where i are the predicted class each f k is a decision tree and is a regularization function of the number of leaves in each tree and the weights of those leaves for the random forest estimator we tuned n the number of trees and p the number of features in the random split set at every split 
1	109958	9958	for the xgboost estimator we tuned the learning rate the tree depth d on each tree and the regularization parameter our final unitary model was the only parametric estimator 1 penalized logistic regression commonly known as the lasso .xgboost a popular implementation of gradient boosting which enables regularization of the trees minimizes the loss function where i are the predicted class each f k is a decision tree and is a regularization function of the number of leaves in each tree and the weights of those leaves for the random forest estimator we tuned n the number of trees and p the number of features in the random split set at every split .the lasso estimator is a variation on linear regression that logit transforms the responses to estimate logistic regression models p r y i 1 as logistic in the features where l is the logistic loss function .iii methods	for the xgboost estimator we tuned the learning rate the tree depth d on each tree and the regularization parameter our final unitary model was the only parametric estimator 1 penalized logistic regression commonly known as the lasso 
0	109959	9959	the lasso estimator is a variation on linear regression that logit transforms the responses to estimate logistic regression models p r y i 1 as logistic in the features where l is the logistic loss function .for the xgboost estimator we tuned the learning rate the tree depth d on each tree and the regularization parameter our final unitary model was the only parametric estimator 1 penalized logistic regression commonly known as the lasso .because the lasso penalizes parameter coefficients in absolute value it implicitly performs feature selection as features with little predictive power have their parameter coefficients driven to zero .iii methods	the lasso estimator is a variation on linear regression that logit transforms the responses to estimate logistic regression models p r y i 1 as logistic in the features where l is the logistic loss function 
0	109960	9960	because the lasso penalizes parameter coefficients in absolute value it implicitly performs feature selection as features with little predictive power have their parameter coefficients driven to zero .the lasso estimator is a variation on linear regression that logit transforms the responses to estimate logistic regression models p r y i 1 as logistic in the features where l is the logistic loss function .for the lasso estimator we tuned the regularization parameter c we tuned all hyperparameters via two stage grid search .iii methods	because the lasso penalizes parameter coefficients in absolute value it implicitly performs feature selection as features with little predictive power have their parameter coefficients driven to zero 
0	109961	9961	for the lasso estimator we tuned the regularization parameter c we tuned all hyperparameters via two stage grid search .because the lasso penalizes parameter coefficients in absolute value it implicitly performs feature selection as features with little predictive power have their parameter coefficients driven to zero .first we drew test hyperparameters uniformly from a representative interval around the model implementations default parameters in.iii methods	for the lasso estimator we tuned the regularization parameter c we tuned all hyperparameters via two stage grid search 
0	109962	9962	first we drew test hyperparameters uniformly from a representative interval around the model implementations default parameters in.for the lasso estimator we tuned the regularization parameter c we tuned all hyperparameters via two stage grid search .last sentence.iii methods	first we drew test hyperparameters uniformly from a representative interval around the model implementations default parameters in
1	109963	9963	parameter the high value of and low value of c found by grid search on the validation set suggest that models that perform poorly may be vulnerable to overfitting especially given the high feature dimensionality .first sentence.last sentence.model	parameter the high value of and low value of c found by grid search on the validation set suggest that models that perform poorly may be vulnerable to overfitting especially given the high feature dimensionality 
0	109964	9964	we evaluated each classifier on each of the two responses using accuracy precision and recall .first sentence.while accuracy measures the proportion of test set class assignments that match the true labels precision and recall provide granular insight into classification errors .iv discussion	we evaluated each classifier on each of the two responses using accuracy precision and recall 
0	109965	9965	while accuracy measures the proportion of test set class assignments that match the true labels precision and recall provide granular insight into classification errors .we evaluated each classifier on each of the two responses using accuracy precision and recall .recall quantifies the proportion of positive classes instantiations of gentrification that were correctly captured by the classifier precision quantifies the prediction accuracy solely among the samples that were predicted to be in the positive class .iv discussion	while accuracy measures the proportion of test set class assignments that match the true labels precision and recall provide granular insight into classification errors 
1	109966	9966	recall quantifies the proportion of positive classes instantiations of gentrification that were correctly captured by the classifier precision quantifies the prediction accuracy solely among the samples that were predicted to be in the positive class .while accuracy measures the proportion of test set class assignments that match the true labels precision and recall provide granular insight into classification errors .precision is commonly used when the cost of false positives is high such as when there may be resources wasted in a misdirected policy response .iv discussion	recall quantifies the proportion of positive classes instantiations of gentrification that were correctly captured by the classifier precision quantifies the prediction accuracy solely among the samples that were predicted to be in the positive class 
0	109967	9967	precision is commonly used when the cost of false positives is high such as when there may be resources wasted in a misdirected policy response .recall quantifies the proportion of positive classes instantiations of gentrification that were correctly captured by the classifier precision quantifies the prediction accuracy solely among the samples that were predicted to be in the positive class .recall is commonly used when the cost of false negatives is high such as when families are being displaced .iv discussion	precision is commonly used when the cost of false positives is high such as when there may be resources wasted in a misdirected policy response 
0	109968	9968	recall is commonly used when the cost of false negatives is high such as when families are being displaced .precision is commonly used when the cost of false positives is high such as when there may be resources wasted in a misdirected policy response .while no one metric dominates in importance in this domain precision and recall illuminate why the performance of all classifiers on the task of classifying tracts according to their change in income distribution during the prediction period was so poor .iv discussion	recall is commonly used when the cost of false negatives is high such as when families are being displaced 
1	109969	9969	while no one metric dominates in importance in this domain precision and recall illuminate why the performance of all classifiers on the task of classifying tracts according to their change in income distribution during the prediction period was so poor .recall is commonly used when the cost of false negatives is high such as when families are being displaced .all four classifiers outperformed the no information classifier in predicting whether a tract would gentrify as defined by a rise in the monthly cost of housing see by contrast no model outperformed the no information classifier in predicting whether a tract would gentrify based on its income distribution .iv discussion	while no one metric dominates in importance in this domain precision and recall illuminate why the performance of all classifiers on the task of classifying tracts according to their change in income distribution during the prediction period was so poor 
1	109970	9970	all four classifiers outperformed the no information classifier in predicting whether a tract would gentrify as defined by a rise in the monthly cost of housing see by contrast no model outperformed the no information classifier in predicting whether a tract would gentrify based on its income distribution .while no one metric dominates in importance in this domain precision and recall illuminate why the performance of all classifiers on the task of classifying tracts according to their change in income distribution during the prediction period was so poor .this is not surprising given how uncorrelated these responses were with 0 06 .iv discussion	all four classifiers outperformed the no information classifier in predicting whether a tract would gentrify as defined by a rise in the monthly cost of housing see by contrast no model outperformed the no information classifier in predicting whether a tract would gentrify based on its income distribution 
0	109971	9971	this is not surprising given how uncorrelated these responses were with 0 06 .all four classifiers outperformed the no information classifier in predicting whether a tract would gentrify as defined by a rise in the monthly cost of housing see by contrast no model outperformed the no information classifier in predicting whether a tract would gentrify based on its income distribution .the high recalls and relatively low precisions reported by the random forest logit model and voting classifier suggest a plausible explanation that all three were overly trigger happy in labelling tracts as positive instantiations of the response leading to high counts of true positive labelings and few false negatives boosting recall as well as high counts of false positive labelings damping precision .iv discussion	this is not surprising given how uncorrelated these responses were with 0 06 
1	109972	9972	the high recalls and relatively low precisions reported by the random forest logit model and voting classifier suggest a plausible explanation that all three were overly trigger happy in labelling tracts as positive instantiations of the response leading to high counts of true positive labelings and few false negatives boosting recall as well as high counts of false positive labelings damping precision .this is not surprising given how uncorrelated these responses were with 0 06 .the confusion matrix for the random forest estimator the best model on this problemindicates that the estimator guessed positive 86 of the time an overwhelming majority given that the classes were balanced in the training and test sets see.iv discussion	the high recalls and relatively low precisions reported by the random forest logit model and voting classifier suggest a plausible explanation that all three were overly trigger happy in labelling tracts as positive instantiations of the response leading to high counts of true positive labelings and few false negatives boosting recall as well as high counts of false positive labelings damping precision 
1	109973	9973	the confusion matrix for the random forest estimator the best model on this problemindicates that the estimator guessed positive 86 of the time an overwhelming majority given that the classes were balanced in the training and test sets see.the high recalls and relatively low precisions reported by the random forest logit model and voting classifier suggest a plausible explanation that all three were overly trigger happy in labelling tracts as positive instantiations of the response leading to high counts of true positive labelings and few false negatives boosting recall as well as high counts of false positive labelings damping precision .last sentence.iv discussion	the confusion matrix for the random forest estimator the best model on this problemindicates that the estimator guessed positive 86 of the time an overwhelming majority given that the classes were balanced in the training and test sets see
1	109974	9974	in this research we develop a classifier to predict whether gentrification will occur in a california census tract with 65 accuracy .first sentence.we defined gentrification as an increase in the inflation adjusted monthly cost of housing and observed experimentally that other definitions such as ones based on localities income distributions yielded noisy results using public data .v conclusion	in this research we develop a classifier to predict whether gentrification will occur in a california census tract with 65 accuracy 
1	109975	9975	we defined gentrification as an increase in the inflation adjusted monthly cost of housing and observed experimentally that other definitions such as ones based on localities income distributions yielded noisy results using public data .in this research we develop a classifier to predict whether gentrification will occur in a california census tract with 65 accuracy .non parametric ensemble models such as random forests and xgboost outperformed parametric models which may have overfit the training data .v conclusion	we defined gentrification as an increase in the inflation adjusted monthly cost of housing and observed experimentally that other definitions such as ones based on localities income distributions yielded noisy results using public data 
0	109976	9976	non parametric ensemble models such as random forests and xgboost outperformed parametric models which may have overfit the training data .we defined gentrification as an increase in the inflation adjusted monthly cost of housing and observed experimentally that other definitions such as ones based on localities income distributions yielded noisy results using public data .furthermore engineered features describing the spatial characteristics of each census tract proved most consequential lending credence to the theory that housing markets in spatial disequilibrium precede gentrification further work might refine the spatially engineered features by e g .v conclusion	non parametric ensemble models such as random forests and xgboost outperformed parametric models which may have overfit the training data 
1	109977	9977	furthermore engineered features describing the spatial characteristics of each census tract proved most consequential lending credence to the theory that housing markets in spatial disequilibrium precede gentrification further work might refine the spatially engineered features by e g .non parametric ensemble models such as random forests and xgboost outperformed parametric models which may have overfit the training data .weighting the network adjacency matrix so that the i jth entry denotes inverse intercentroid distance instead of adjacency .v conclusion	furthermore engineered features describing the spatial characteristics of each census tract proved most consequential lending credence to the theory that housing markets in spatial disequilibrium precede gentrification further work might refine the spatially engineered features by e g 
0	109978	9978	weighting the network adjacency matrix so that the i jth entry denotes inverse intercentroid distance instead of adjacency .furthermore engineered features describing the spatial characteristics of each census tract proved most consequential lending credence to the theory that housing markets in spatial disequilibrium precede gentrification further work might refine the spatially engineered features by e g .alternatively further work might focus on better defining gentrification by quantifying displacement of families or collapsing the bins of the income distribution response to increase the signal in the data .v conclusion	weighting the network adjacency matrix so that the i jth entry denotes inverse intercentroid distance instead of adjacency 
1	109979	9979	alternatively further work might focus on better defining gentrification by quantifying displacement of families or collapsing the bins of the income distribution response to increase the signal in the data .weighting the network adjacency matrix so that the i jth entry denotes inverse intercentroid distance instead of adjacency .finally causal work could ascertain the drivers of gentrification as opposed to simply leading indicators .v conclusion	alternatively further work might focus on better defining gentrification by quantifying displacement of families or collapsing the bins of the income distribution response to increase the signal in the data 
0	109980	9980	finally causal work could ascertain the drivers of gentrification as opposed to simply leading indicators .alternatively further work might focus on better defining gentrification by quantifying displacement of families or collapsing the bins of the income distribution response to increase the signal in the data .accurately forecasting gentrification continues to be a pressing problem for california policymakers .v conclusion	finally causal work could ascertain the drivers of gentrification as opposed to simply leading indicators 
0	109981	9981	accurately forecasting gentrification continues to be a pressing problem for california policymakers .finally causal work could ascertain the drivers of gentrification as opposed to simply leading indicators .last sentence.v conclusion	accurately forecasting gentrification continues to be a pressing problem for california policymakers 
0	109982	9982	all code written for this project can be found here .first sentence.last sentence.vi code	all code written for this project can be found here 
0	109983	9983	stock market predictions lend themselves well to a machine learning framework due to their quantitative nature .first sentence.a supervised learning model to predict stock movement direction can combine technical information and qualitative sentiment through news encoded into fixed length real vectors .abstract	stock market predictions lend themselves well to a machine learning framework due to their quantitative nature 
1	109984	9984	a supervised learning model to predict stock movement direction can combine technical information and qualitative sentiment through news encoded into fixed length real vectors .stock market predictions lend themselves well to a machine learning framework due to their quantitative nature .we attempt a large range of models both to encode qualitative sentiment information into features and to make a final up or down prediction on the direction of a particular stock given encoded news and technical features .abstract	a supervised learning model to predict stock movement direction can combine technical information and qualitative sentiment through news encoded into fixed length real vectors 
1	109985	9985	we attempt a large range of models both to encode qualitative sentiment information into features and to make a final up or down prediction on the direction of a particular stock given encoded news and technical features .a supervised learning model to predict stock movement direction can combine technical information and qualitative sentiment through news encoded into fixed length real vectors .we find that a universal sentence encoder combined with svms achieves encouraging results on our data .abstract	we attempt a large range of models both to encode qualitative sentiment information into features and to make a final up or down prediction on the direction of a particular stock given encoded news and technical features 
1	109986	9986	we find that a universal sentence encoder combined with svms achieves encouraging results on our data .we attempt a large range of models both to encode qualitative sentiment information into features and to make a final up or down prediction on the direction of a particular stock given encoded news and technical features .last sentence.abstract	we find that a universal sentence encoder combined with svms achieves encouraging results on our data 
0	109987	9987	stock market predictions have been a pivotal and controversial subject in the field of finance .first sentence.some theorists believe in the efficient market hypothesis that stock prices reflect all current information and thus think that the stock market is inherently unpredictable .introduction	stock market predictions have been a pivotal and controversial subject in the field of finance 
0	109988	9988	some theorists believe in the efficient market hypothesis that stock prices reflect all current information and thus think that the stock market is inherently unpredictable .stock market predictions have been a pivotal and controversial subject in the field of finance .others have attempted to predict the market through fundamental analysis technical analysis and more recently machine learning .introduction	some theorists believe in the efficient market hypothesis that stock prices reflect all current information and thus think that the stock market is inherently unpredictable 
0	109989	9989	others have attempted to predict the market through fundamental analysis technical analysis and more recently machine learning .some theorists believe in the efficient market hypothesis that stock prices reflect all current information and thus think that the stock market is inherently unpredictable .a technique such as machine learning may lend itself well to such an application because of the fundamentally quantitative nature of the stock market .introduction	others have attempted to predict the market through fundamental analysis technical analysis and more recently machine learning 
0	109990	9990	a technique such as machine learning may lend itself well to such an application because of the fundamentally quantitative nature of the stock market .others have attempted to predict the market through fundamental analysis technical analysis and more recently machine learning .current machine learning models have focused on technical analyses or sentiment as a single feature .introduction	a technique such as machine learning may lend itself well to such an application because of the fundamentally quantitative nature of the stock market 
0	109991	9991	current machine learning models have focused on technical analyses or sentiment as a single feature .a technique such as machine learning may lend itself well to such an application because of the fundamentally quantitative nature of the stock market .but since the stock market is also heavily dependent on market sentiment and fundamental company information which cannot be captured with a simple numeric indicator we decided to create a machine learning model that takes in both stock financial data and news information which we encode into a fixed length vector .introduction	current machine learning models have focused on technical analyses or sentiment as a single feature 
1	109992	9992	but since the stock market is also heavily dependent on market sentiment and fundamental company information which cannot be captured with a simple numeric indicator we decided to create a machine learning model that takes in both stock financial data and news information which we encode into a fixed length vector .current machine learning models have focused on technical analyses or sentiment as a single feature .our model tries to predict stock direction using a variety of techniques including svms and neural networks .introduction	but since the stock market is also heavily dependent on market sentiment and fundamental company information which cannot be captured with a simple numeric indicator we decided to create a machine learning model that takes in both stock financial data and news information which we encode into a fixed length vector 
0	109993	9993	our model tries to predict stock direction using a variety of techniques including svms and neural networks .but since the stock market is also heavily dependent on market sentiment and fundamental company information which cannot be captured with a simple numeric indicator we decided to create a machine learning model that takes in both stock financial data and news information which we encode into a fixed length vector .by creating a machine learning model that combines the approaches of technical analysis and fundamental analysis we hope our model can paint a better picture of the overall market .introduction	our model tries to predict stock direction using a variety of techniques including svms and neural networks 
1	109994	9994	by creating a machine learning model that combines the approaches of technical analysis and fundamental analysis we hope our model can paint a better picture of the overall market .our model tries to predict stock direction using a variety of techniques including svms and neural networks .last sentence.introduction	by creating a machine learning model that combines the approaches of technical analysis and fundamental analysis we hope our model can paint a better picture of the overall market 
0	109995	9995	sentiment analysis and machine learning for stock predictions is an active research area .first sentence.existing work to predict stock movement direction using sentiment analysis includes dictionary based correlation finding methods and sentiment mood detection algorithms .related work and analysis	sentiment analysis and machine learning for stock predictions is an active research area 
0	109996	9996	existing work to predict stock movement direction using sentiment analysis includes dictionary based correlation finding methods and sentiment mood detection algorithms .sentiment analysis and machine learning for stock predictions is an active research area .several papers such as nagar and hahsler.related work and analysis	existing work to predict stock movement direction using sentiment analysis includes dictionary based correlation finding methods and sentiment mood detection algorithms 
0	109997	9997	several papers such as nagar and hahsler.existing work to predict stock movement direction using sentiment analysis includes dictionary based correlation finding methods and sentiment mood detection algorithms .last sentence.related work and analysis	several papers such as nagar and hahsler
0	109998	9998	our dataset is composed of trading macro technical and news data related to 20 nasdaq companies from 2013 to 2017 .first sentence.we used the yahoo finance api to extract trading related information on each stock ticker including price and volume on a daily basis .data sources	our dataset is composed of trading macro technical and news data related to 20 nasdaq companies from 2013 to 2017 
0	109999	9999	we used the yahoo finance api to extract trading related information on each stock ticker including price and volume on a daily basis .our dataset is composed of trading macro technical and news data related to 20 nasdaq companies from 2013 to 2017 .we also extracted overarching macro data including quarterly gdp cpi and daily libor from the fed website .data sources	we used the yahoo finance api to extract trading related information on each stock ticker including price and volume on a daily basis 
0	110000	10000	we also extracted overarching macro data including quarterly gdp cpi and daily libor from the fed website .we used the yahoo finance api to extract trading related information on each stock ticker including price and volume on a daily basis .in addition we computed technical indicators including cci rsi and evm from trading data .data sources	we also extracted overarching macro data including quarterly gdp cpi and daily libor from the fed website 
1	110001	10001	in addition we computed technical indicators including cci rsi and evm from trading data .we also extracted overarching macro data including quarterly gdp cpi and daily libor from the fed website .finally we scraped daily news headlines and snippets for each ticker from new york times and google news .data sources	in addition we computed technical indicators including cci rsi and evm from trading data 
0	110002	10002	finally we scraped daily news headlines and snippets for each ticker from new york times and google news .in addition we computed technical indicators including cci rsi and evm from trading data .last sentence.data sources	finally we scraped daily news headlines and snippets for each ticker from new york times and google news 
0	110003	10003	we used a few approaches to merge and preprocess the data .first sentence.to match quarterly macro data e g gdp with other daily data we assumed the macro features to be constant throughout the same quarter .data preprocessing	we used a few approaches to merge and preprocess the data 
0	110004	10004	to match quarterly macro data e g gdp with other daily data we assumed the macro features to be constant throughout the same quarter .we used a few approaches to merge and preprocess the data .we processed the news data using text representation and sentiment analysis models which will be discussed in detail in section 4 before merging it to the full data set .data preprocessing	to match quarterly macro data e g gdp with other daily data we assumed the macro features to be constant throughout the same quarter 
1	110005	10005	we processed the news data using text representation and sentiment analysis models which will be discussed in detail in section 4 before merging it to the full data set .to match quarterly macro data e g gdp with other daily data we assumed the macro features to be constant throughout the same quarter .for tickers which have multiple news for certain dates we averaged the sentiment encoded vectors for google news and used the top 1 news for new york times because new york times ranks top articles .data preprocessing	we processed the news data using text representation and sentiment analysis models which will be discussed in detail in section 4 before merging it to the full data set 
1	110006	10006	for tickers which have multiple news for certain dates we averaged the sentiment encoded vectors for google news and used the top 1 news for new york times because new york times ranks top articles .we processed the news data using text representation and sentiment analysis models which will be discussed in detail in section 4 before merging it to the full data set .for tickers which don t have news articles on certain dates we replaced the missing value with the latest available news .data preprocessing	for tickers which have multiple news for certain dates we averaged the sentiment encoded vectors for google news and used the top 1 news for new york times because new york times ranks top articles 
0	110007	10007	for tickers which don t have news articles on certain dates we replaced the missing value with the latest available news .for tickers which have multiple news for certain dates we averaged the sentiment encoded vectors for google news and used the top 1 news for new york times because new york times ranks top articles .we choose not to normalize the data to avoid destroying correlations of the sparse matrix .data preprocessing	for tickers which don t have news articles on certain dates we replaced the missing value with the latest available news 
0	110008	10008	we choose not to normalize the data to avoid destroying correlations of the sparse matrix .for tickers which don t have news articles on certain dates we replaced the missing value with the latest available news .furthermore we classified the 1 day next day stock movement into a binary label y where y 1 if adj .data preprocessing	we choose not to normalize the data to avoid destroying correlations of the sparse matrix 
0	110009	10009	furthermore we classified the 1 day next day stock movement into a binary label y where y 1 if adj .we choose not to normalize the data to avoid destroying correlations of the sparse matrix .close price last adj .data preprocessing	furthermore we classified the 1 day next day stock movement into a binary label y where y 1 if adj 
0	110010	10010	close price last adj .furthermore we classified the 1 day next day stock movement into a binary label y where y 1 if adj .close price and y 0 if adj .data preprocessing	close price last adj 
0	110011	10011	close price and y 0 if adj .close price last adj .close price last adj .data preprocessing	close price and y 0 if adj 
0	110012	10012	close price last adj .close price and y 0 if adj .close price .data preprocessing	close price last adj 
0	110013	10013	close price .close price last adj .finally we built two datasets using news from new york times and google respectively each of which contains 24k entries and 70 features .data preprocessing	close price 
1	110014	10014	finally we built two datasets using news from new york times and google respectively each of which contains 24k entries and 70 features .close price .we split all samples before 2017 into the training set and hold out the rest as test set .data preprocessing	finally we built two datasets using news from new york times and google respectively each of which contains 24k entries and 70 features 
0	110015	10015	we split all samples before 2017 into the training set and hold out the rest as test set .finally we built two datasets using news from new york times and google respectively each of which contains 24k entries and 70 features .last sentence.data preprocessing	we split all samples before 2017 into the training set and hold out the rest as test set 
0	110016	10016	we plotted label y on the first two principal components of news data .first sentence.the plot reveals the complicated nature of the features implying that high dimension classifiers are required for the dataset .data visualization	we plotted label y on the first two principal components of news data 
0	110017	10017	the plot reveals the complicated nature of the features implying that high dimension classifiers are required for the dataset .we plotted label y on the first two principal components of news data .last sentence.data visualization	the plot reveals the complicated nature of the features implying that high dimension classifiers are required for the dataset 
0	110018	10018	in general the problem is a supervised learning problem i e we are predicting the next day movement of the stock by taking in the trading information about the stock and the information from the ticker specific daily news .first sentence.the task can be split into two parts namely to represent the news as a fixed length real scalar or vector and to use the news together with trading information technical indicators and macro data to make the prediction .model overview	in general the problem is a supervised learning problem i e we are predicting the next day movement of the stock by taking in the trading information about the stock and the information from the ticker specific daily news 
1	110019	10019	the task can be split into two parts namely to represent the news as a fixed length real scalar or vector and to use the news together with trading information technical indicators and macro data to make the prediction .in general the problem is a supervised learning problem i e we are predicting the next day movement of the stock by taking in the trading information about the stock and the information from the ticker specific daily news .in order to capture the semantic information of the text and represent it in a vector space we eventually decided to use a google universal sentence encode use as the encoder section 4 2 .model overview	the task can be split into two parts namely to represent the news as a fixed length real scalar or vector and to use the news together with trading information technical indicators and macro data to make the prediction 
1	110020	10020	in order to capture the semantic information of the text and represent it in a vector space we eventually decided to use a google universal sentence encode use as the encoder section 4 2 .the task can be split into two parts namely to represent the news as a fixed length real scalar or vector and to use the news together with trading information technical indicators and macro data to make the prediction .in terms of the stock prediction model which is trained to take in all of the technical and encoded features to make the prediction we used logistic regression random forest svm and a variety of neural networks section 4 3 .model overview	in order to capture the semantic information of the text and represent it in a vector space we eventually decided to use a google universal sentence encode use as the encoder section 4 2 
1	110021	10021	in terms of the stock prediction model which is trained to take in all of the technical and encoded features to make the prediction we used logistic regression random forest svm and a variety of neural networks section 4 3 .in order to capture the semantic information of the text and represent it in a vector space we eventually decided to use a google universal sentence encode use as the encoder section 4 2 .last sentence.model overview	in terms of the stock prediction model which is trained to take in all of the technical and encoded features to make the prediction we used logistic regression random forest svm and a variety of neural networks section 4 3 
1	110022	10022	the text representation model is required to convert news sentences and snippets to real value fixed length scalar or vector representations that can be used in the stock movement prediction model .first sentence.the goal is to have the model best capture fine grained semantic and syntactic information .text representation	the text representation model is required to convert news sentences and snippets to real value fixed length scalar or vector representations that can be used in the stock movement prediction model 
0	110023	10023	the goal is to have the model best capture fine grained semantic and syntactic information .the text representation model is required to convert news sentences and snippets to real value fixed length scalar or vector representations that can be used in the stock movement prediction model .one method we tried was to directly extract a sentiment signal scalar from a given sentence vector .text representation	the goal is to have the model best capture fine grained semantic and syntactic information 
0	110024	10024	one method we tried was to directly extract a sentiment signal scalar from a given sentence vector .the goal is to have the model best capture fine grained semantic and syntactic information .dictionary based approaches use statistical information of word occurence count to extract useful information .text representation	one method we tried was to directly extract a sentiment signal scalar from a given sentence vector 
1	110025	10025	dictionary based approaches use statistical information of word occurence count to extract useful information .one method we tried was to directly extract a sentiment signal scalar from a given sentence vector .we used a sentimentanalysis package from r with a financial dictionary similar to the one mentioned in related work to get a scalar sentiment score for each of our sentences .text representation	dictionary based approaches use statistical information of word occurence count to extract useful information 
1	110026	10026	we used a sentimentanalysis package from r with a financial dictionary similar to the one mentioned in related work to get a scalar sentiment score for each of our sentences .dictionary based approaches use statistical information of word occurence count to extract useful information .we also tried using a pre trained sentiment lstm model which was trained using labeled data from cnn news 5 to extract the sentiment from the headline and snippet text .text representation	we used a sentimentanalysis package from r with a financial dictionary similar to the one mentioned in related work to get a scalar sentiment score for each of our sentences 
1	110027	10027	we also tried using a pre trained sentiment lstm model which was trained using labeled data from cnn news 5 to extract the sentiment from the headline and snippet text .we used a sentimentanalysis package from r with a financial dictionary similar to the one mentioned in related work to get a scalar sentiment score for each of our sentences .however neither of the methods mentioned above achieved a reasonable accuracy in making the overall prediction and a plausible reason is that the high level sentiment information is not sufficient in representing the text .text representation	we also tried using a pre trained sentiment lstm model which was trained using labeled data from cnn news 5 to extract the sentiment from the headline and snippet text 
1	110028	10028	however neither of the methods mentioned above achieved a reasonable accuracy in making the overall prediction and a plausible reason is that the high level sentiment information is not sufficient in representing the text .we also tried using a pre trained sentiment lstm model which was trained using labeled data from cnn news 5 to extract the sentiment from the headline and snippet text .thus we used sentence embeddings to produce a vector space with a more meaningful substructure to represent the news and fed the entire vector embedding into our classification model .text representation	however neither of the methods mentioned above achieved a reasonable accuracy in making the overall prediction and a plausible reason is that the high level sentiment information is not sufficient in representing the text 
0	110029	10029	thus we used sentence embeddings to produce a vector space with a more meaningful substructure to represent the news and fed the entire vector embedding into our classification model .however neither of the methods mentioned above achieved a reasonable accuracy in making the overall prediction and a plausible reason is that the high level sentiment information is not sufficient in representing the text .recent methods of finding fixed length real vector representations of words and sentences have succeeded in a wide range of tasks from sentiment analysis to question and answering .text representation	thus we used sentence embeddings to produce a vector space with a more meaningful substructure to represent the news and fed the entire vector embedding into our classification model 
1	110030	10030	recent methods of finding fixed length real vector representations of words and sentences have succeeded in a wide range of tasks from sentiment analysis to question and answering .thus we used sentence embeddings to produce a vector space with a more meaningful substructure to represent the news and fed the entire vector embedding into our classification model .these models can be broadly divided into word encoding methods and sentence encoding methods .text representation	recent methods of finding fixed length real vector representations of words and sentences have succeeded in a wide range of tasks from sentiment analysis to question and answering 
0	110031	10031	these models can be broadly divided into word encoding methods and sentence encoding methods .recent methods of finding fixed length real vector representations of words and sentences have succeeded in a wide range of tasks from sentiment analysis to question and answering .to evaluate each of these models to choose one for us to use we took several sentences and compared the results of the encoding to see if the encoders captured the similarities and differences between sentences .text representation	these models can be broadly divided into word encoding methods and sentence encoding methods 
1	110032	10032	to evaluate each of these models to choose one for us to use we took several sentences and compared the results of the encoding to see if the encoders captured the similarities and differences between sentences .these models can be broadly divided into word encoding methods and sentence encoding methods .word encoding strategies include word2vec elmo glove and fasttext .text representation	to evaluate each of these models to choose one for us to use we took several sentences and compared the results of the encoding to see if the encoders captured the similarities and differences between sentences 
0	110033	10033	word encoding strategies include word2vec elmo glove and fasttext .to evaluate each of these models to choose one for us to use we took several sentences and compared the results of the encoding to see if the encoders captured the similarities and differences between sentences .these models use the bag of words technique which detect how often words appear in similar context of other words to get a vector representation of each word though the fasttext actually goes character by character .text representation	word encoding strategies include word2vec elmo glove and fasttext 
1	110034	10034	these models use the bag of words technique which detect how often words appear in similar context of other words to get a vector representation of each word though the fasttext actually goes character by character .word encoding strategies include word2vec elmo glove and fasttext .we noticed that one problem with using one of these word encoding strategies on our sentences is that it does not consider the words of the sentence together and we are unsure about how to composite the words to the sentence .text representation	these models use the bag of words technique which detect how often words appear in similar context of other words to get a vector representation of each word though the fasttext actually goes character by character 
1	110035	10035	we noticed that one problem with using one of these word encoding strategies on our sentences is that it does not consider the words of the sentence together and we are unsure about how to composite the words to the sentence .these models use the bag of words technique which detect how often words appear in similar context of other words to get a vector representation of each word though the fasttext actually goes character by character .thus we decided to choose a method that encoded entire sentences such as skip thoughts similar to word2vec but with sentences instead infersent looks at pairs of sentences or a universal sentence encoder .text representation	we noticed that one problem with using one of these word encoding strategies on our sentences is that it does not consider the words of the sentence together and we are unsure about how to composite the words to the sentence 
1	110036	10036	thus we decided to choose a method that encoded entire sentences such as skip thoughts similar to word2vec but with sentences instead infersent looks at pairs of sentences or a universal sentence encoder .we noticed that one problem with using one of these word encoding strategies on our sentences is that it does not consider the words of the sentence together and we are unsure about how to composite the words to the sentence .the use consists of a deep average network dan structure although this structure also takes an average of words there are layers of dropout that allow important words to be highlighted .text representation	thus we decided to choose a method that encoded entire sentences such as skip thoughts similar to word2vec but with sentences instead infersent looks at pairs of sentences or a universal sentence encoder 
1	110037	10037	the use consists of a deep average network dan structure although this structure also takes an average of words there are layers of dropout that allow important words to be highlighted .thus we decided to choose a method that encoded entire sentences such as skip thoughts similar to word2vec but with sentences instead infersent looks at pairs of sentences or a universal sentence encoder .there was also another variant of the use that used a transformer module a novel neural network architecture based on a self attention mechanism of context this method achieves the best in detecting sentence similarities however we found this technique to be too slow on our data .text representation	the use consists of a deep average network dan structure although this structure also takes an average of words there are layers of dropout that allow important words to be highlighted 
1	110038	10038	there was also another variant of the use that used a transformer module a novel neural network architecture based on a self attention mechanism of context this method achieves the best in detecting sentence similarities however we found this technique to be too slow on our data .the use consists of a deep average network dan structure although this structure also takes an average of words there are layers of dropout that allow important words to be highlighted .eventually we decided to use the pre trained google dan use as our sentence representation because of its ability to detect features in a large range of sentence types including our news large pretrained corpus and dropout technique .text representation	there was also another variant of the use that used a transformer module a novel neural network architecture based on a self attention mechanism of context this method achieves the best in detecting sentence similarities however we found this technique to be too slow on our data 
1	110039	10039	eventually we decided to use the pre trained google dan use as our sentence representation because of its ability to detect features in a large range of sentence types including our news large pretrained corpus and dropout technique .there was also another variant of the use that used a transformer module a novel neural network architecture based on a self attention mechanism of context this method achieves the best in detecting sentence similarities however we found this technique to be too slow on our data .the pca is based off of all of the seen vectors in the training set and the principal components stay the same for the test set .text representation	eventually we decided to use the pre trained google dan use as our sentence representation because of its ability to detect features in a large range of sentence types including our news large pretrained corpus and dropout technique 
0	110040	10040	the pca is based off of all of the seen vectors in the training set and the principal components stay the same for the test set .eventually we decided to use the pre trained google dan use as our sentence representation because of its ability to detect features in a large range of sentence types including our news large pretrained corpus and dropout technique .last sentence.text representation	the pca is based off of all of the seen vectors in the training set and the principal components stay the same for the test set 
1	110041	10041	logistic regression is used as a baseline to map the features to stock movement random forest which constructs a multitude of decision trees at training time and outputs the class that is the majority vote of the individual leaves is widely used for classification and regression .first sentence.we tuned depth of the tree and leaf size to regularize the model .stock movement prediction	logistic regression is used as a baseline to map the features to stock movement random forest which constructs a multitude of decision trees at training time and outputs the class that is the majority vote of the individual leaves is widely used for classification and regression 
0	110042	10042	we tuned depth of the tree and leaf size to regularize the model .logistic regression is used as a baseline to map the features to stock movement random forest which constructs a multitude of decision trees at training time and outputs the class that is the majority vote of the individual leaves is widely used for classification and regression .support vector machines are mentioned in previous research fully connected neural networks are composed by interconnected neurons whose activation functions capture the non linear relationship between the variables and the binary result .stock movement prediction	we tuned depth of the tree and leaf size to regularize the model 
0	110043	10043	support vector machines are mentioned in previous research fully connected neural networks are composed by interconnected neurons whose activation functions capture the non linear relationship between the variables and the binary result .we tuned depth of the tree and leaf size to regularize the model .we tuned the model on different parameters and the best performance model structure consists of two hidden layers 50 2 or 10 10 with relu activation and a learning rate of 1e 3 although it did vary based on dataset convolutional neural networks have been widely used for image processing .stock movement prediction	support vector machines are mentioned in previous research fully connected neural networks are composed by interconnected neurons whose activation functions capture the non linear relationship between the variables and the binary result 
1	110044	10044	we tuned the model on different parameters and the best performance model structure consists of two hidden layers 50 2 or 10 10 with relu activation and a learning rate of 1e 3 although it did vary based on dataset convolutional neural networks have been widely used for image processing .support vector machines are mentioned in previous research fully connected neural networks are composed by interconnected neurons whose activation functions capture the non linear relationship between the variables and the binary result .we thought it might be effective to do convolutions over the sentence embeddings because of their structure however we also acknowledge that because of the pca and the way the google use dan works the adjacent features may not be relevant to each other .stock movement prediction	we tuned the model on different parameters and the best performance model structure consists of two hidden layers 50 2 or 10 10 with relu activation and a learning rate of 1e 3 although it did vary based on dataset convolutional neural networks have been widely used for image processing 
1	110045	10045	we thought it might be effective to do convolutions over the sentence embeddings because of their structure however we also acknowledge that because of the pca and the way the google use dan works the adjacent features may not be relevant to each other .we tuned the model on different parameters and the best performance model structure consists of two hidden layers 50 2 or 10 10 with relu activation and a learning rate of 1e 3 although it did vary based on dataset convolutional neural networks have been widely used for image processing .two 1d conv layers each followed by a pooling layer are included before the final fully connected layer .stock movement prediction	we thought it might be effective to do convolutions over the sentence embeddings because of their structure however we also acknowledge that because of the pca and the way the google use dan works the adjacent features may not be relevant to each other 
0	110046	10046	two 1d conv layers each followed by a pooling layer are included before the final fully connected layer .we thought it might be effective to do convolutions over the sentence embeddings because of their structure however we also acknowledge that because of the pca and the way the google use dan works the adjacent features may not be relevant to each other .we picked the learning rate with which the model converges most effectively 1e 3 recurrent neural networks are proven to be effective in dealing with sequential data with the output being dependent on the previous computations .stock movement prediction	two 1d conv layers each followed by a pooling layer are included before the final fully connected layer 
1	110047	10047	we picked the learning rate with which the model converges most effectively 1e 3 recurrent neural networks are proven to be effective in dealing with sequential data with the output being dependent on the previous computations .two 1d conv layers each followed by a pooling layer are included before the final fully connected layer .we are training separate rnns for each ticker .stock movement prediction	we picked the learning rate with which the model converges most effectively 1e 3 recurrent neural networks are proven to be effective in dealing with sequential data with the output being dependent on the previous computations 
0	110048	10048	we are training separate rnns for each ticker .we picked the learning rate with which the model converges most effectively 1e 3 recurrent neural networks are proven to be effective in dealing with sequential data with the output being dependent on the previous computations .the learning rate is 1e 4 .stock movement prediction	we are training separate rnns for each ticker 
0	110049	10049	the learning rate is 1e 4 .we are training separate rnns for each ticker .last sentence.stock movement prediction	the learning rate is 1e 4 
1	110050	10050	to examine the model stability we trained each model on the two datasets using ny times and google news separately with a learning rate mentioned in 4 3 respectively .first sentence.the mini batch size selected was the largest possible value to fit into cpu for rnn the mini batch is all data for each ticker .experiments and results	to examine the model stability we trained each model on the two datasets using ny times and google news separately with a learning rate mentioned in 4 3 respectively 
1	110051	10051	the mini batch size selected was the largest possible value to fit into cpu for rnn the mini batch is all data for each ticker .to examine the model stability we trained each model on the two datasets using ny times and google news separately with a learning rate mentioned in 4 3 respectively .we evaluate the model using mainly test accuracy .experiments and results	the mini batch size selected was the largest possible value to fit into cpu for rnn the mini batch is all data for each ticker 
0	110052	10052	we evaluate the model using mainly test accuracy .the mini batch size selected was the largest possible value to fit into cpu for rnn the mini batch is all data for each ticker .meanwhile we also monitor the f 1 score to ensure balanced performance on both 0 and 1 labels .experiments and results	we evaluate the model using mainly test accuracy 
1	110053	10053	meanwhile we also monitor the f 1 score to ensure balanced performance on both 0 and 1 labels .we evaluate the model using mainly test accuracy .as shown in table 1 svm with rbf kernel is the best performing model on both datasets .experiments and results	meanwhile we also monitor the f 1 score to ensure balanced performance on both 0 and 1 labels 
1	110054	10054	as shown in table 1 svm with rbf kernel is the best performing model on both datasets .meanwhile we also monitor the f 1 score to ensure balanced performance on both 0 and 1 labels .neural network and cnn also achieved decent performance .experiments and results	as shown in table 1 svm with rbf kernel is the best performing model on both datasets 
0	110055	10055	neural network and cnn also achieved decent performance .as shown in table 1 svm with rbf kernel is the best performing model on both datasets .however results from logistic regression and random forest are not satisfactory .experiments and results	neural network and cnn also achieved decent performance 
0	110056	10056	however results from logistic regression and random forest are not satisfactory .neural network and cnn also achieved decent performance .last sentence.experiments and results	however results from logistic regression and random forest are not satisfactory 
1	110057	10057	best model svm with rbf kernel is able to project the features onto a high dimensional space and effectively separate the labels .first sentence.we tuned the cost parameter to prevent overfitting .discussion	best model svm with rbf kernel is able to project the features onto a high dimensional space and effectively separate the labels 
0	110058	10058	we tuned the cost parameter to prevent overfitting .best model svm with rbf kernel is able to project the features onto a high dimensional space and effectively separate the labels .precision and recall rates of the best performing models on google news and ny times are shown in table 2 .discussion	we tuned the cost parameter to prevent overfitting 
0	110059	10059	precision and recall rates of the best performing models on google news and ny times are shown in table 2 .we tuned the cost parameter to prevent overfitting .although we attempted to achieve a balanced performance on 0 and 1 labels the selected model still outputs a relatively imbalanced confusion matrix .discussion	precision and recall rates of the best performing models on google news and ny times are shown in table 2 
1	110060	10060	although we attempted to achieve a balanced performance on 0 and 1 labels the selected model still outputs a relatively imbalanced confusion matrix .precision and recall rates of the best performing models on google news and ny times are shown in table 2 .we believe that such issue is raised by our loss function which is designed to maximize the overall accuracy but not to ensure the performance on both labels .discussion	although we attempted to achieve a balanced performance on 0 and 1 labels the selected model still outputs a relatively imbalanced confusion matrix 
1	110061	10061	we believe that such issue is raised by our loss function which is designed to maximize the overall accuracy but not to ensure the performance on both labels .although we attempted to achieve a balanced performance on 0 and 1 labels the selected model still outputs a relatively imbalanced confusion matrix .bias data visualization reveals that our dataset is not separable in a low dimension space which explains why random forest and logistic regression with simple structure are not working well variance random forest shows the overfitting problem even after regularization .discussion	we believe that such issue is raised by our loss function which is designed to maximize the overall accuracy but not to ensure the performance on both labels 
1	110062	10062	bias data visualization reveals that our dataset is not separable in a low dimension space which explains why random forest and logistic regression with simple structure are not working well variance random forest shows the overfitting problem even after regularization .we believe that such issue is raised by our loss function which is designed to maximize the overall accuracy but not to ensure the performance on both labels .our dataset contains features which might be positively or negatively correlated with each other e g vectors representing news headlines .discussion	bias data visualization reveals that our dataset is not separable in a low dimension space which explains why random forest and logistic regression with simple structure are not working well variance random forest shows the overfitting problem even after regularization 
1	110063	10063	our dataset contains features which might be positively or negatively correlated with each other e g vectors representing news headlines .bias data visualization reveals that our dataset is not separable in a low dimension space which explains why random forest and logistic regression with simple structure are not working well variance random forest shows the overfitting problem even after regularization .selecting a subset of such features may not be able to reduce the variance efficiently .discussion	our dataset contains features which might be positively or negatively correlated with each other e g vectors representing news headlines 
0	110064	10064	selecting a subset of such features may not be able to reduce the variance efficiently .our dataset contains features which might be positively or negatively correlated with each other e g vectors representing news headlines .stability as mentioned before for the rnn in order to capture the time series nature of each stock we split the dataset by ticker before running the model which in turn shrunk the data size .discussion	selecting a subset of such features may not be able to reduce the variance efficiently 
1	110065	10065	stability as mentioned before for the rnn in order to capture the time series nature of each stock we split the dataset by ticker before running the model which in turn shrunk the data size .selecting a subset of such features may not be able to reduce the variance efficiently .additionally some of the tickers had relatively sparse unique news data .discussion	stability as mentioned before for the rnn in order to capture the time series nature of each stock we split the dataset by ticker before running the model which in turn shrunk the data size 
0	110066	10066	additionally some of the tickers had relatively sparse unique news data .stability as mentioned before for the rnn in order to capture the time series nature of each stock we split the dataset by ticker before running the model which in turn shrunk the data size .furthermore it is probable that the deep structure of the model caused the gradient update to be inefficient .discussion	additionally some of the tickers had relatively sparse unique news data 
0	110067	10067	furthermore it is probable that the deep structure of the model caused the gradient update to be inefficient .additionally some of the tickers had relatively sparse unique news data .we also found that the rnn is very sensitive to the initialization of the hidden state which shed some light on the inefficacy of back propagation .discussion	furthermore it is probable that the deep structure of the model caused the gradient update to be inefficient 
0	110068	10068	we also found that the rnn is very sensitive to the initialization of the hidden state which shed some light on the inefficacy of back propagation .furthermore it is probable that the deep structure of the model caused the gradient update to be inefficient .to fix this we might change the structure of hidden state or use a different activation function .discussion	we also found that the rnn is very sensitive to the initialization of the hidden state which shed some light on the inefficacy of back propagation 
0	110069	10069	to fix this we might change the structure of hidden state or use a different activation function .we also found that the rnn is very sensitive to the initialization of the hidden state which shed some light on the inefficacy of back propagation .these are some possible reasons the rnn outputs a high proportion of 1s or 0s on some of the subsets and cannot be used as a stable model for future predictions to gain better understanding of the model performance we plotted the true and predicted stock movement of facebook in 2017 as follows where the same color on the same day indicates correct predictions .discussion	to fix this we might change the structure of hidden state or use a different activation function 
1	110070	10070	these are some possible reasons the rnn outputs a high proportion of 1s or 0s on some of the subsets and cannot be used as a stable model for future predictions to gain better understanding of the model performance we plotted the true and predicted stock movement of facebook in 2017 as follows where the same color on the same day indicates correct predictions .to fix this we might change the structure of hidden state or use a different activation function .examining the predictions closely we found that the best performing model svm is more able to detect major up downs than smaller changes .discussion	these are some possible reasons the rnn outputs a high proportion of 1s or 0s on some of the subsets and cannot be used as a stable model for future predictions to gain better understanding of the model performance we plotted the true and predicted stock movement of facebook in 2017 as follows where the same color on the same day indicates correct predictions 
1	110071	10071	examining the predictions closely we found that the best performing model svm is more able to detect major up downs than smaller changes .these are some possible reasons the rnn outputs a high proportion of 1s or 0s on some of the subsets and cannot be used as a stable model for future predictions to gain better understanding of the model performance we plotted the true and predicted stock movement of facebook in 2017 as follows where the same color on the same day indicates correct predictions .6 conclusion future workin conclusion we think stock specific news might help in predicting next day stock movement .discussion	examining the predictions closely we found that the best performing model svm is more able to detect major up downs than smaller changes 
1	110072	10072	6 conclusion future workin conclusion we think stock specific news might help in predicting next day stock movement .examining the predictions closely we found that the best performing model svm is more able to detect major up downs than smaller changes .however it is hard to turn such informational edge into a profitable trading strategy given that we are merely predicting ups and downs .discussion	6 conclusion future workin conclusion we think stock specific news might help in predicting next day stock movement 
0	110073	10073	however it is hard to turn such informational edge into a profitable trading strategy given that we are merely predicting ups and downs .6 conclusion future workin conclusion we think stock specific news might help in predicting next day stock movement .in addition our model seems to be more able to detect major movements than smaller ones .discussion	however it is hard to turn such informational edge into a profitable trading strategy given that we are merely predicting ups and downs 
0	110074	10074	in addition our model seems to be more able to detect major movements than smaller ones .however it is hard to turn such informational edge into a profitable trading strategy given that we are merely predicting ups and downs .we believe the following steps can be taken to improve model performance in the future customized loss function we think achieving high accuracy and balanced performance on 1 and 0 labels are both important in stock movement prediction .discussion	in addition our model seems to be more able to detect major movements than smaller ones 
1	110075	10075	we believe the following steps can be taken to improve model performance in the future customized loss function we think achieving high accuracy and balanced performance on 1 and 0 labels are both important in stock movement prediction .in addition our model seems to be more able to detect major movements than smaller ones .however the second goal was not built into the loss function of our models .discussion	we believe the following steps can be taken to improve model performance in the future customized loss function we think achieving high accuracy and balanced performance on 1 and 0 labels are both important in stock movement prediction 
0	110076	10076	however the second goal was not built into the loss function of our models .we believe the following steps can be taken to improve model performance in the future customized loss function we think achieving high accuracy and balanced performance on 1 and 0 labels are both important in stock movement prediction .as the next step we can customize the loss function e g as binary cross entropy to obtain a more balanced performance enhance data quality to make the project usable in real life we built the dataset using news we scraped from the internet .discussion	however the second goal was not built into the loss function of our models 
1	110077	10077	as the next step we can customize the loss function e g as binary cross entropy to obtain a more balanced performance enhance data quality to make the project usable in real life we built the dataset using news we scraped from the internet .however the second goal was not built into the loss function of our models .such data might include irrelevant or inaccurate news which increases noise .discussion	as the next step we can customize the loss function e g as binary cross entropy to obtain a more balanced performance enhance data quality to make the project usable in real life we built the dataset using news we scraped from the internet 
0	110078	10078	such data might include irrelevant or inaccurate news which increases noise .as the next step we can customize the loss function e g as binary cross entropy to obtain a more balanced performance enhance data quality to make the project usable in real life we built the dataset using news we scraped from the internet .in the future we think adding more cleaning techniques and including models to detect unhelpful news may help .discussion	such data might include irrelevant or inaccurate news which increases noise 
0	110079	10079	in the future we think adding more cleaning techniques and including models to detect unhelpful news may help .such data might include irrelevant or inaccurate news which increases noise .last sentence.discussion	in the future we think adding more cleaning techniques and including models to detect unhelpful news may help 
0	110080	10080	our team spent 50 percent of our time on collecting and preprocessing data 20 percent on text representation and 30 percent price movement modelling and debugging .first sentence.given the challenging nature of our topic three of us worked closely during the whole process .contributions	our team spent 50 percent of our time on collecting and preprocessing data 20 percent on text representation and 30 percent price movement modelling and debugging 
0	110081	10081	given the challenging nature of our topic three of us worked closely during the whole process .our team spent 50 percent of our time on collecting and preprocessing data 20 percent on text representation and 30 percent price movement modelling and debugging .chris contributed primarily to collecting the trading data working on sentiment signal modelling using text representations and applying the models to new york times data .contributions	given the challenging nature of our topic three of us worked closely during the whole process 
0	110082	10082	chris contributed primarily to collecting the trading data working on sentiment signal modelling using text representations and applying the models to new york times data .given the challenging nature of our topic three of us worked closely during the whole process .yilun contributed primarily to collecting sentiment data and testing and debugging the rnn and cnn models .contributions	chris contributed primarily to collecting the trading data working on sentiment signal modelling using text representations and applying the models to new york times data 
0	110083	10083	yilun contributed primarily to collecting sentiment data and testing and debugging the rnn and cnn models .chris contributed primarily to collecting the trading data working on sentiment signal modelling using text representations and applying the models to new york times data .iris contributed primarily to collecting sentiment and trading data data preprocessing and applying the models to google news data .contributions	yilun contributed primarily to collecting sentiment data and testing and debugging the rnn and cnn models 
1	110084	10084	iris contributed primarily to collecting sentiment and trading data data preprocessing and applying the models to google news data .yilun contributed primarily to collecting sentiment data and testing and debugging the rnn and cnn models .we would like to thank the entire cs 229 teaching staff including our mentor atharva parulekar for providing invaluable feedback thorughout the course of the project 8 references bibliography.contributions	iris contributed primarily to collecting sentiment and trading data data preprocessing and applying the models to google news data 
0	110085	10085	we would like to thank the entire cs 229 teaching staff including our mentor atharva parulekar for providing invaluable feedback thorughout the course of the project 8 references bibliography.iris contributed primarily to collecting sentiment and trading data data preprocessing and applying the models to google news data .last sentence.contributions	we would like to thank the entire cs 229 teaching staff including our mentor atharva parulekar for providing invaluable feedback thorughout the course of the project 8 references bibliography
1	110086	10086	the finance industry has been revolutionized by the increased availability of data the rise in computing power and the popularization of machine learning algorithms .first sentence.according to the wall street journal 2017b quantitative hedge funds represented 27 of total trading activity in 2017 rivaling the 29 that represents all individual investors .i introduction	the finance industry has been revolutionized by the increased availability of data the rise in computing power and the popularization of machine learning algorithms 
0	110087	10087	according to the wall street journal 2017b quantitative hedge funds represented 27 of total trading activity in 2017 rivaling the 29 that represents all individual investors .the finance industry has been revolutionized by the increased availability of data the rise in computing power and the popularization of machine learning algorithms .most of these institutions are applying a machine learning approach to investing despite this boom in data driven strategies the literature that analyzes machine learning methods in financial forecasting is very limited with most papers focusing on stock return prediction .i introduction	according to the wall street journal 2017b quantitative hedge funds represented 27 of total trading activity in 2017 rivaling the 29 that represents all individual investors 
1	110088	10088	most of these institutions are applying a machine learning approach to investing despite this boom in data driven strategies the literature that analyzes machine learning methods in financial forecasting is very limited with most papers focusing on stock return prediction .according to the wall street journal 2017b quantitative hedge funds represented 27 of total trading activity in 2017 rivaling the 29 that represents all individual investors .the objective of this paper is to produce directional fx forecasts that are able to yield profitable investment strategies .i introduction	most of these institutions are applying a machine learning approach to investing despite this boom in data driven strategies the literature that analyzes machine learning methods in financial forecasting is very limited with most papers focusing on stock return prediction 
1	110089	10089	the objective of this paper is to produce directional fx forecasts that are able to yield profitable investment strategies .most of these institutions are applying a machine learning approach to investing despite this boom in data driven strategies the literature that analyzes machine learning methods in financial forecasting is very limited with most papers focusing on stock return prediction .hence we approach the problem from two perspectives 1 classification of long short signals 2 point forecasts of fx levels that translate into long short signals these frameworks allow us to exploit different machine learning methodologies to solve a single problem designing a profitable fx strategy based on ml generated forecasts .i introduction	the objective of this paper is to produce directional fx forecasts that are able to yield profitable investment strategies 
1	110090	10090	hence we approach the problem from two perspectives 1 classification of long short signals 2 point forecasts of fx levels that translate into long short signals these frameworks allow us to exploit different machine learning methodologies to solve a single problem designing a profitable fx strategy based on ml generated forecasts .the objective of this paper is to produce directional fx forecasts that are able to yield profitable investment strategies .last sentence.i introduction	hence we approach the problem from two perspectives 1 classification of long short signals 2 point forecasts of fx levels that translate into long short signals these frameworks allow us to exploit different machine learning methodologies to solve a single problem designing a profitable fx strategy based on ml generated forecasts 
1	110091	10091	machine learning methods have long been used in stock return prediction .first sentence.for instance variations of principal component analysis an unsupervised learning technique have been applied by nevertheless despite the large adoption of machine learning in stock return forecasting ml applications in fx prediction have been widely ignored by the literature .ii related work	machine learning methods have long been used in stock return prediction 
1	110092	10092	for instance variations of principal component analysis an unsupervised learning technique have been applied by nevertheless despite the large adoption of machine learning in stock return forecasting ml applications in fx prediction have been widely ignored by the literature .machine learning methods have long been used in stock return prediction .few exceptions are available .ii related work	for instance variations of principal component analysis an unsupervised learning technique have been applied by nevertheless despite the large adoption of machine learning in stock return forecasting ml applications in fx prediction have been widely ignored by the literature 
0	110093	10093	few exceptions are available .for instance variations of principal component analysis an unsupervised learning technique have been applied by nevertheless despite the large adoption of machine learning in stock return forecasting ml applications in fx prediction have been widely ignored by the literature .the main contribution of this paper is the assessment of the statistical and economic performance of ml generated directional forecasts iii .ii related work	few exceptions are available 
1	110094	10094	the main contribution of this paper is the assessment of the statistical and economic performance of ml generated directional forecasts iii .few exceptions are available .datasets we make use of two different datasets to explore the forecasting power of two types of variables market and fundamentals .ii related work	the main contribution of this paper is the assessment of the statistical and economic performance of ml generated directional forecasts iii 
1	110095	10095	datasets we make use of two different datasets to explore the forecasting power of two types of variables market and fundamentals .the main contribution of this paper is the assessment of the statistical and economic performance of ml generated directional forecasts iii .we define a market variable as an indicator with daily to weekly frequency that has a close relationship with traded securities .ii related work	datasets we make use of two different datasets to explore the forecasting power of two types of variables market and fundamentals 
1	110096	10096	we define a market variable as an indicator with daily to weekly frequency that has a close relationship with traded securities .datasets we make use of two different datasets to explore the forecasting power of two types of variables market and fundamentals .on the other hand we define a fundamental variable as an indicator with monthly frequency that is closely related to the macroeconomy finally we limit the scope of our project to forecasting the usdmxn which is the exchange rate between the us dollar usd and the mexican peso mxn expressed in mxn per usd .ii related work	we define a market variable as an indicator with daily to weekly frequency that has a close relationship with traded securities 
1	110097	10097	on the other hand we define a fundamental variable as an indicator with monthly frequency that is closely related to the macroeconomy finally we limit the scope of our project to forecasting the usdmxn which is the exchange rate between the us dollar usd and the mexican peso mxn expressed in mxn per usd .we define a market variable as an indicator with daily to weekly frequency that has a close relationship with traded securities .however the exercise can be generalized to other currencies .ii related work	on the other hand we define a fundamental variable as an indicator with monthly frequency that is closely related to the macroeconomy finally we limit the scope of our project to forecasting the usdmxn which is the exchange rate between the us dollar usd and the mexican peso mxn expressed in mxn per usd 
0	110098	10098	however the exercise can be generalized to other currencies .on the other hand we define a fundamental variable as an indicator with monthly frequency that is closely related to the macroeconomy finally we limit the scope of our project to forecasting the usdmxn which is the exchange rate between the us dollar usd and the mexican peso mxn expressed in mxn per usd .all data was retrieved either from bloomberg the global financial dataset or the federal reserve bank .ii related work	however the exercise can be generalized to other currencies 
1	110099	10099	all data was retrieved either from bloomberg the global financial dataset or the federal reserve bank .however the exercise can be generalized to other currencies .last sentence.ii related work	all data was retrieved either from bloomberg the global financial dataset or the federal reserve bank 
0	110100	10100	we obtained the weekly closing price of the usdmxn currency pair which we use as our target variable .first sentence.in addition we consider 25 features across both mexico and the united states .a market variables dataset	we obtained the weekly closing price of the usdmxn currency pair which we use as our target variable 
0	110101	10101	in addition we consider 25 features across both mexico and the united states .we obtained the weekly closing price of the usdmxn currency pair which we use as our target variable .a summary is shown in.a market variables dataset	in addition we consider 25 features across both mexico and the united states 
0	110102	10102	a summary is shown in.in addition we consider 25 features across both mexico and the united states .last sentence.a market variables dataset	a summary is shown in
1	110103	10103	the fundamental variables data uses the monthly closing price of the usdmxn currency pair as our target variable .first sentence.we use 27 features that describe the macroeconomic conditions of both the us and mexico between march 1990 and october 2018 .b fundamental variables dataset	the fundamental variables data uses the monthly closing price of the usdmxn currency pair as our target variable 
0	110104	10104	we use 27 features that describe the macroeconomic conditions of both the us and mexico between march 1990 and october 2018 .the fundamental variables data uses the monthly closing price of the usdmxn currency pair as our target variable .the additional features that are considered in this dataset are detailed in.b fundamental variables dataset	we use 27 features that describe the macroeconomic conditions of both the us and mexico between march 1990 and october 2018 
0	110105	10105	the additional features that are considered in this dataset are detailed in.we use 27 features that describe the macroeconomic conditions of both the us and mexico between march 1990 and october 2018 .last sentence.b fundamental variables dataset	the additional features that are considered in this dataset are detailed in
0	110106	10106	almost all data processing is identical in both datasets .first sentence.we first split the data into 60 train set 20 validation set and 20 test set .c data processing	almost all data processing is identical in both datasets 
0	110107	10107	we first split the data into 60 train set 20 validation set and 20 test set .almost all data processing is identical in both datasets .these subsets are taken sequentially in order to keep the time series nature of the data and to guarantee our algorithms train exclusively on past data to translate our problem into a classification problem we introduce the signal t variable which we set to 1 if the usdmxn was higher tomorrow than today .c data processing	we first split the data into 60 train set 20 validation set and 20 test set 
1	110108	10108	these subsets are taken sequentially in order to keep the time series nature of the data and to guarantee our algorithms train exclusively on past data to translate our problem into a classification problem we introduce the signal t variable which we set to 1 if the usdmxn was higher tomorrow than today .we first split the data into 60 train set 20 validation set and 20 test set .this is we also perform data processing on the features .c data processing	these subsets are taken sequentially in order to keep the time series nature of the data and to guarantee our algorithms train exclusively on past data to translate our problem into a classification problem we introduce the signal t variable which we set to 1 if the usdmxn was higher tomorrow than today 
1	110109	10109	this is we also perform data processing on the features .these subsets are taken sequentially in order to keep the time series nature of the data and to guarantee our algorithms train exclusively on past data to translate our problem into a classification problem we introduce the signal t variable which we set to 1 if the usdmxn was higher tomorrow than today .in particular we standardize using the mean and standard deviation of the training set for every covariate for the fundamentals dataset covariates are lagged by an additional period .c data processing	this is we also perform data processing on the features 
1	110110	10110	in particular we standardize using the mean and standard deviation of the training set for every covariate for the fundamentals dataset covariates are lagged by an additional period .this is we also perform data processing on the features .this is done to approximate the fact that it is extremely rare to obtain real time macroeconomic data .c data processing	in particular we standardize using the mean and standard deviation of the training set for every covariate for the fundamentals dataset covariates are lagged by an additional period 
0	110111	10111	this is done to approximate the fact that it is extremely rare to obtain real time macroeconomic data .in particular we standardize using the mean and standard deviation of the training set for every covariate for the fundamentals dataset covariates are lagged by an additional period .by lagging the features by one month we ensure we are not peeking into the future by including unpublished data .c data processing	this is done to approximate the fact that it is extremely rare to obtain real time macroeconomic data 
0	110112	10112	by lagging the features by one month we ensure we are not peeking into the future by including unpublished data .this is done to approximate the fact that it is extremely rare to obtain real time macroeconomic data .last sentence.c data processing	by lagging the features by one month we ensure we are not peeking into the future by including unpublished data 
1	110113	10113	first we perform binary classification on the signal t variable we constructed in the data processing step .first sentence.this essentially transforms what initially is a continuous variable problem into a classification task on a second exercise we use ml algorithms to construct point forecasts for our raw continuous target variable usdmxn t .a frameworks	first we perform binary classification on the signal t variable we constructed in the data processing step 
1	110114	10114	this essentially transforms what initially is a continuous variable problem into a classification task on a second exercise we use ml algorithms to construct point forecasts for our raw continuous target variable usdmxn t .first we perform binary classification on the signal t variable we constructed in the data processing step .we then construct an estimated long short signal by computing both strategies yield a binary signal output that we can execute as a trading strategy .a frameworks	this essentially transforms what initially is a continuous variable problem into a classification task on a second exercise we use ml algorithms to construct point forecasts for our raw continuous target variable usdmxn t 
1	110115	10115	we then construct an estimated long short signal by computing both strategies yield a binary signal output that we can execute as a trading strategy .this essentially transforms what initially is a continuous variable problem into a classification task on a second exercise we use ml algorithms to construct point forecasts for our raw continuous target variable usdmxn t .last sentence.a frameworks	we then construct an estimated long short signal by computing both strategies yield a binary signal output that we can execute as a trading strategy 
0	110116	10116	the performance of different machine learning algorithms is tested for each framework .first sentence.in particular we considered 1 logistic linear regression we use logistic and linear regression as our benchmark models 2 regularized logistic linear regression we consider l 1 and l 2 regularization applied to logistic and linear regression .b models	the performance of different machine learning algorithms is tested for each framework 
1	110117	10117	in particular we considered 1 logistic linear regression we use logistic and linear regression as our benchmark models 2 regularized logistic linear regression we consider l 1 and l 2 regularization applied to logistic and linear regression .the performance of different machine learning algorithms is tested for each framework .this allows to reduce overfitting in the validation set .b models	in particular we considered 1 logistic linear regression we use logistic and linear regression as our benchmark models 2 regularized logistic linear regression we consider l 1 and l 2 regularization applied to logistic and linear regression 
0	110118	10118	this allows to reduce overfitting in the validation set .in particular we considered 1 logistic linear regression we use logistic and linear regression as our benchmark models 2 regularized logistic linear regression we consider l 1 and l 2 regularization applied to logistic and linear regression .the hyperparameter which penalizes large coefficients is tuned using the validation set accuracy 3 support vector machines regression svm svr it is highly likely that fitting fx dynamics requires a non linear boundary .b models	this allows to reduce overfitting in the validation set 
1	110119	10119	the hyperparameter which penalizes large coefficients is tuned using the validation set accuracy 3 support vector machines regression svm svr it is highly likely that fitting fx dynamics requires a non linear boundary .this allows to reduce overfitting in the validation set .svm svr with a gaussian kernel provide the flexibility to generate a non linear boundary as a result of the infinite dimensional feature vector generated by the kernel .b models	the hyperparameter which penalizes large coefficients is tuned using the validation set accuracy 3 support vector machines regression svm svr it is highly likely that fitting fx dynamics requires a non linear boundary 
1	110120	10120	svm svr with a gaussian kernel provide the flexibility to generate a non linear boundary as a result of the infinite dimensional feature vector generated by the kernel .the hyperparameter which penalizes large coefficients is tuned using the validation set accuracy 3 support vector machines regression svm svr it is highly likely that fitting fx dynamics requires a non linear boundary .last sentence.b models	svm svr with a gaussian kernel provide the flexibility to generate a non linear boundary as a result of the infinite dimensional feature vector generated by the kernel 
1	110121	10121	tree based models allow us to capture complex interactions between the variables .first sentence.unlike random forests which require bootstrapping gbc allows us to keep the time series structure of the data while considering non linearities .4 gradient boosting classifier regression gbc gbr 	tree based models allow us to capture complex interactions between the variables 
1	110122	10122	unlike random forests which require bootstrapping gbc allows us to keep the time series structure of the data while considering non linearities .tree based models allow us to capture complex interactions between the variables .it is important to notice that gbc and gbr is just considered for the market variables dataset due to the division of work between the authors see section ix .4 gradient boosting classifier regression gbc gbr 	unlike random forests which require bootstrapping gbc allows us to keep the time series structure of the data while considering non linearities 
0	110123	10123	it is important to notice that gbc and gbr is just considered for the market variables dataset due to the division of work between the authors see section ix .unlike random forests which require bootstrapping gbc allows us to keep the time series structure of the data while considering non linearities .last sentence.4 gradient boosting classifier regression gbc gbr 	it is important to notice that gbc and gbr is just considered for the market variables dataset due to the division of work between the authors see section ix 
0	110124	10124	neural networks can model complex relationships between input features which could improve the forecasting performance .first sentence.we consider fullyconnected networks .5 neural networks nn 	neural networks can model complex relationships between input features which could improve the forecasting performance 
0	110125	10125	we consider fullyconnected networks .neural networks can model complex relationships between input features which could improve the forecasting performance .the architecture is shown in our choice for loss depends on the framework .5 neural networks nn 	we consider fullyconnected networks 
1	110126	10126	the architecture is shown in our choice for loss depends on the framework .we consider fullyconnected networks .we select logistic loss for classification and mean squared error for the continuous target variable problem .5 neural networks nn 	the architecture is shown in our choice for loss depends on the framework 
1	110127	10127	we select logistic loss for classification and mean squared error for the continuous target variable problem .the architecture is shown in our choice for loss depends on the framework .we choose the proper activations in the same fashion sigmoid is used for classification while relu is used for the continuous target variable .5 neural networks nn 	we select logistic loss for classification and mean squared error for the continuous target variable problem 
1	110128	10128	we choose the proper activations in the same fashion sigmoid is used for classification while relu is used for the continuous target variable .we select logistic loss for classification and mean squared error for the continuous target variable problem .finally we use dropout or activation regularization to avoid overfitting .5 neural networks nn 	we choose the proper activations in the same fashion sigmoid is used for classification while relu is used for the continuous target variable 
0	110129	10129	finally we use dropout or activation regularization to avoid overfitting .we choose the proper activations in the same fashion sigmoid is used for classification while relu is used for the continuous target variable .last sentence.5 neural networks nn 	finally we use dropout or activation regularization to avoid overfitting 
1	110130	10130	all model parameters are tuned using the validation set .first sentence.we use accuracy as our performance evaluation in the binary classification model and mean squared error in the continuous target variable model .v hyperparameter tuning	all model parameters are tuned using the validation set 
1	110131	10131	we use accuracy as our performance evaluation in the binary classification model and mean squared error in the continuous target variable model .all model parameters are tuned using the validation set .the resulting parameters are detailed in there is however an important caveat when interpreting the results .v hyperparameter tuning	we use accuracy as our performance evaluation in the binary classification model and mean squared error in the continuous target variable model 
1	110132	10132	the resulting parameters are detailed in there is however an important caveat when interpreting the results .we use accuracy as our performance evaluation in the binary classification model and mean squared error in the continuous target variable model .being a measurement of the fraction of predictions that we can correctly forecast accuracy does not differentiate between true positives and true negatives .v hyperparameter tuning	the resulting parameters are detailed in there is however an important caveat when interpreting the results 
1	110133	10133	being a measurement of the fraction of predictions that we can correctly forecast accuracy does not differentiate between true positives and true negatives .the resulting parameters are detailed in there is however an important caveat when interpreting the results .a successful trading strategy should exploit true positives and true negatives while minimizing false positives and false negatives to discern between these cases given the bad results of the confusion matrix for the binary classification problem we explore the results of the continuous experiments .v hyperparameter tuning	being a measurement of the fraction of predictions that we can correctly forecast accuracy does not differentiate between true positives and true negatives 
1	110134	10134	a successful trading strategy should exploit true positives and true negatives while minimizing false positives and false negatives to discern between these cases given the bad results of the confusion matrix for the binary classification problem we explore the results of the continuous experiments .being a measurement of the fraction of predictions that we can correctly forecast accuracy does not differentiate between true positives and true negatives .vii .v hyperparameter tuning	a successful trading strategy should exploit true positives and true negatives while minimizing false positives and false negatives to discern between these cases given the bad results of the confusion matrix for the binary classification problem we explore the results of the continuous experiments 
0	110135	10135	vii .a successful trading strategy should exploit true positives and true negatives while minimizing false positives and false negatives to discern between these cases given the bad results of the confusion matrix for the binary classification problem we explore the results of the continuous experiments .economic performance a model with very successful statistical performance of long short signals does not imply positive economic implications .v hyperparameter tuning	vii 
0	110136	10136	economic performance a model with very successful statistical performance of long short signals does not imply positive economic implications .vii .this is an inherent problem in directional forecasts a profitable investment strategy requires algorithms that correctly predict the direction of very large movements in the price of the asset .v hyperparameter tuning	economic performance a model with very successful statistical performance of long short signals does not imply positive economic implications 
1	110137	10137	this is an inherent problem in directional forecasts a profitable investment strategy requires algorithms that correctly predict the direction of very large movements in the price of the asset .economic performance a model with very successful statistical performance of long short signals does not imply positive economic implications .in our case if an algorithm correctly predicts most small changes but misses large jumps in the exchange rate it is very likely that it will produce negative economic performance upon execution .v hyperparameter tuning	this is an inherent problem in directional forecasts a profitable investment strategy requires algorithms that correctly predict the direction of very large movements in the price of the asset 
1	110138	10138	in our case if an algorithm correctly predicts most small changes but misses large jumps in the exchange rate it is very likely that it will produce negative economic performance upon execution .this is an inherent problem in directional forecasts a profitable investment strategy requires algorithms that correctly predict the direction of very large movements in the price of the asset .this issue has been previously assessed in the literature by at the end of every period the position is closed profits are cashed in and the strategy is repeated .v hyperparameter tuning	in our case if an algorithm correctly predicts most small changes but misses large jumps in the exchange rate it is very likely that it will produce negative economic performance upon execution 
0	110139	10139	this issue has been previously assessed in the literature by at the end of every period the position is closed profits are cashed in and the strategy is repeated .in our case if an algorithm correctly predicts most small changes but misses large jumps in the exchange rate it is very likely that it will produce negative economic performance upon execution .finally we use a longonly strategy as our benchmark for economic performance a .v hyperparameter tuning	this issue has been previously assessed in the literature by at the end of every period the position is closed profits are cashed in and the strategy is repeated 
0	110140	10140	finally we use a longonly strategy as our benchmark for economic performance a .this issue has been previously assessed in the literature by at the end of every period the position is closed profits are cashed in and the strategy is repeated .binary classification the statistically best performing model corresponds to the economically most profitable specification .v hyperparameter tuning	finally we use a longonly strategy as our benchmark for economic performance a 
0	110141	10141	binary classification the statistically best performing model corresponds to the economically most profitable specification .finally we use a longonly strategy as our benchmark for economic performance a .however it is important to notice that this positive result is mostly driven by a single correct bet made between weeks 725 and 750 .v hyperparameter tuning	binary classification the statistically best performing model corresponds to the economically most profitable specification 
0	110142	10142	however it is important to notice that this positive result is mostly driven by a single correct bet made between weeks 725 and 750 .binary classification the statistically best performing model corresponds to the economically most profitable specification .all other strategies produce profits that are equal to or worse than the long only benchmark these results can be explained by the bad performance of the models in terms of the confusion matrix .v hyperparameter tuning	however it is important to notice that this positive result is mostly driven by a single correct bet made between weeks 725 and 750 
1	110143	10143	all other strategies produce profits that are equal to or worse than the long only benchmark these results can be explained by the bad performance of the models in terms of the confusion matrix .however it is important to notice that this positive result is mostly driven by a single correct bet made between weeks 725 and 750 .due to the very low true negative rate of most models all specifications are close to the long only benchmark and the departures are a consequence of few correct or incorrect short bets .v hyperparameter tuning	all other strategies produce profits that are equal to or worse than the long only benchmark these results can be explained by the bad performance of the models in terms of the confusion matrix 
0	110144	10144	due to the very low true negative rate of most models all specifications are close to the long only benchmark and the departures are a consequence of few correct or incorrect short bets .all other strategies produce profits that are equal to or worse than the long only benchmark these results can be explained by the bad performance of the models in terms of the confusion matrix .the differences with respect to the binary classification results are once again significant .v hyperparameter tuning	due to the very low true negative rate of most models all specifications are close to the long only benchmark and the departures are a consequence of few correct or incorrect short bets 
0	110145	10145	the differences with respect to the binary classification results are once again significant .due to the very low true negative rate of most models all specifications are close to the long only benchmark and the departures are a consequence of few correct or incorrect short bets .the final cumulative return in the continuous target variable framework is around 15 higher than under the binary classification framework .v hyperparameter tuning	the differences with respect to the binary classification results are once again significant 
1	110146	10146	the final cumulative return in the continuous target variable framework is around 15 higher than under the binary classification framework .the differences with respect to the binary classification results are once again significant .furthermore all strategies outperform the long only benchmark with the best strategy being ridge regression .v hyperparameter tuning	the final cumulative return in the continuous target variable framework is around 15 higher than under the binary classification framework 
0	110147	10147	furthermore all strategies outperform the long only benchmark with the best strategy being ridge regression .the final cumulative return in the continuous target variable framework is around 15 higher than under the binary classification framework .last sentence.v hyperparameter tuning	furthermore all strategies outperform the long only benchmark with the best strategy being ridge regression 
0	110148	10148	in addition the economic effect of an improved true negative rate is considerable .first sentence.unlike the binary classification case the outperformance of all strategies with respect to the benchmark is not driven by few correct short positions .b continuous variable target	in addition the economic effect of an improved true negative rate is considerable 
1	110149	10149	unlike the binary classification case the outperformance of all strategies with respect to the benchmark is not driven by few correct short positions .in addition the economic effect of an improved true negative rate is considerable .moreover the reduction in the true positive rate observed for the continuous target variable framework does not significantly penalize cumulative profits .b continuous variable target	unlike the binary classification case the outperformance of all strategies with respect to the benchmark is not driven by few correct short positions 
0	110150	10150	moreover the reduction in the true positive rate observed for the continuous target variable framework does not significantly penalize cumulative profits .unlike the binary classification case the outperformance of all strategies with respect to the benchmark is not driven by few correct short positions .the gains of a high specificity outweigh any losses derived from the reduction in sensitivity a natural question to address is which variables explain exchange rate forecasts the most .b continuous variable target	moreover the reduction in the true positive rate observed for the continuous target variable framework does not significantly penalize cumulative profits 
1	110151	10151	the gains of a high specificity outweigh any losses derived from the reduction in sensitivity a natural question to address is which variables explain exchange rate forecasts the most .moreover the reduction in the true positive rate observed for the continuous target variable framework does not significantly penalize cumulative profits .finally another interesting insight is that the usdmxn reacts strongly to global and emerging market em fixed income indicators .b continuous variable target	the gains of a high specificity outweigh any losses derived from the reduction in sensitivity a natural question to address is which variables explain exchange rate forecasts the most 
1	110152	10152	finally another interesting insight is that the usdmxn reacts strongly to global and emerging market em fixed income indicators .the gains of a high specificity outweigh any losses derived from the reduction in sensitivity a natural question to address is which variables explain exchange rate forecasts the most .in theory the bilateral exchange rate should react strongly to the interest rate differential between the two countries .b continuous variable target	finally another interesting insight is that the usdmxn reacts strongly to global and emerging market em fixed income indicators 
0	110153	10153	in theory the bilateral exchange rate should react strongly to the interest rate differential between the two countries .finally another interesting insight is that the usdmxn reacts strongly to global and emerging market em fixed income indicators .we believe the observed result provides evidence of investor behavior .b continuous variable target	in theory the bilateral exchange rate should react strongly to the interest rate differential between the two countries 
0	110154	10154	we believe the observed result provides evidence of investor behavior .in theory the bilateral exchange rate should react strongly to the interest rate differential between the two countries .as documented in recent years by.b continuous variable target	we believe the observed result provides evidence of investor behavior 
0	110155	10155	as documented in recent years by.we believe the observed result provides evidence of investor behavior .last sentence.b continuous variable target	as documented in recent years by
1	110156	10156	this paper makes use of machine learning methods to forecast the us dollar against mexican peso exchange rate .first sentence.we use an innovative framework to find the best possible performance .viii conclusion and future work	this paper makes use of machine learning methods to forecast the us dollar against mexican peso exchange rate 
0	110157	10157	we use an innovative framework to find the best possible performance .this paper makes use of machine learning methods to forecast the us dollar against mexican peso exchange rate .first we consider a market variables dataset and a fundamentals dataset on which we train ml algorithms .viii conclusion and future work	we use an innovative framework to find the best possible performance 
1	110158	10158	first we consider a market variables dataset and a fundamentals dataset on which we train ml algorithms .we use an innovative framework to find the best possible performance .second we conduct binary classification experiments and continuous target experiments to produce the same output a binary long short signal on which we are able to execute a simple trading strategy our results suggest that continuous target prediction outperforms binary classification not only in terms of accuracy but also in terms of specificity and sensitivity .viii conclusion and future work	first we consider a market variables dataset and a fundamentals dataset on which we train ml algorithms 
1	110159	10159	second we conduct binary classification experiments and continuous target experiments to produce the same output a binary long short signal on which we are able to execute a simple trading strategy our results suggest that continuous target prediction outperforms binary classification not only in terms of accuracy but also in terms of specificity and sensitivity .first we consider a market variables dataset and a fundamentals dataset on which we train ml algorithms .the economic results are in line with this finding with all algorithms outperforming a long only benchmark .viii conclusion and future work	second we conduct binary classification experiments and continuous target experiments to produce the same output a binary long short signal on which we are able to execute a simple trading strategy our results suggest that continuous target prediction outperforms binary classification not only in terms of accuracy but also in terms of specificity and sensitivity 
0	110160	10160	the economic results are in line with this finding with all algorithms outperforming a long only benchmark .second we conduct binary classification experiments and continuous target experiments to produce the same output a binary long short signal on which we are able to execute a simple trading strategy our results suggest that continuous target prediction outperforms binary classification not only in terms of accuracy but also in terms of specificity and sensitivity .the best results are produced by svm in the binary classification case and ridge regression in the continuous target case both in terms of accuracy and cumulative profits .viii conclusion and future work	the economic results are in line with this finding with all algorithms outperforming a long only benchmark 
1	110161	10161	the best results are produced by svm in the binary classification case and ridge regression in the continuous target case both in terms of accuracy and cumulative profits .the economic results are in line with this finding with all algorithms outperforming a long only benchmark .last we find that the fundamentals dataset yields poor results future work could focus in several areas .viii conclusion and future work	the best results are produced by svm in the binary classification case and ridge regression in the continuous target case both in terms of accuracy and cumulative profits 
0	110162	10162	last we find that the fundamentals dataset yields poor results future work could focus in several areas .the best results are produced by svm in the binary classification case and ridge regression in the continuous target case both in terms of accuracy and cumulative profits .first the recursive validation procedure proposed in.viii conclusion and future work	last we find that the fundamentals dataset yields poor results future work could focus in several areas 
0	110163	10163	first the recursive validation procedure proposed in.last we find that the fundamentals dataset yields poor results future work could focus in several areas .last sentence.viii conclusion and future work	first the recursive validation procedure proposed in
0	110164	10164	the team worked on the same problem but used different datasets .first sentence.the contribution to this work was as follows christian gonz lez rojas was in charge of data collecting data processing algorithm selection and algorithm implementation on the market variables dataset for both the continuous and the binary framework .ix contributions	the team worked on the same problem but used different datasets 
1	110165	10165	the contribution to this work was as follows christian gonz lez rojas was in charge of data collecting data processing algorithm selection and algorithm implementation on the market variables dataset for both the continuous and the binary framework .the team worked on the same problem but used different datasets .he decided to consider gbc gbr as an additional model to further test the value of nonlinear relationships .ix contributions	the contribution to this work was as follows christian gonz lez rojas was in charge of data collecting data processing algorithm selection and algorithm implementation on the market variables dataset for both the continuous and the binary framework 
0	110166	10166	he decided to consider gbc gbr as an additional model to further test the value of nonlinear relationships .the contribution to this work was as follows christian gonz lez rojas was in charge of data collecting data processing algorithm selection and algorithm implementation on the market variables dataset for both the continuous and the binary framework .he was also responsible for writing the cs229 poster and the cs229 final report .ix contributions	he decided to consider gbc gbr as an additional model to further test the value of nonlinear relationships 
0	110167	10167	he was also responsible for writing the cs229 poster and the cs229 final report .he decided to consider gbc gbr as an additional model to further test the value of nonlinear relationships .his data and code can be found at this link molly herman worked on data collection data processing and algorithms for the fundamentals dataset .ix contributions	he was also responsible for writing the cs229 poster and the cs229 final report 
0	110168	10168	his data and code can be found at this link molly herman worked on data collection data processing and algorithms for the fundamentals dataset .he was also responsible for writing the cs229 poster and the cs229 final report .she was responsible for modifying the cs229 poster to create an alternative version for the cs229a presentation and was in charge of writing her own final report for cs229a the division of work for the poster and the final report was done to provide deeper insight on the results to which each author contributed the most .ix contributions	his data and code can be found at this link molly herman worked on data collection data processing and algorithms for the fundamentals dataset 
0	110169	10169	she was responsible for modifying the cs229 poster to create an alternative version for the cs229a presentation and was in charge of writing her own final report for cs229a the division of work for the poster and the final report was done to provide deeper insight on the results to which each author contributed the most .his data and code can be found at this link molly herman worked on data collection data processing and algorithms for the fundamentals dataset .last sentence.ix contributions	she was responsible for modifying the cs229 poster to create an alternative version for the cs229a presentation and was in charge of writing her own final report for cs229a the division of work for the poster and the final report was done to provide deeper insight on the results to which each author contributed the most 
0	110170	10170	the 2sigma competition at kaggle aims at advancing our understanding of how the content of news analytics might influence the performance of stock prices .first sentence.for this purpose a large set of daily market and news data is provided for a subset of us listed financial instruments .introduction	the 2sigma competition at kaggle aims at advancing our understanding of how the content of news analytics might influence the performance of stock prices 
0	110171	10171	for this purpose a large set of daily market and news data is provided for a subset of us listed financial instruments .the 2sigma competition at kaggle aims at advancing our understanding of how the content of news analytics might influence the performance of stock prices .this data shall be used to train any kind of learning algorithm deemed useful in order to predict future stock market returns the competition comprises two stages with two different evaluation periods .introduction	for this purpose a large set of daily market and news data is provided for a subset of us listed financial instruments 
0	110172	10172	this data shall be used to train any kind of learning algorithm deemed useful in order to predict future stock market returns the competition comprises two stages with two different evaluation periods .for this purpose a large set of daily market and news data is provided for a subset of us listed financial instruments .in the first stage the predictions are tested against historical data of the period 1 1 2017 to 7 31 2018 .introduction	this data shall be used to train any kind of learning algorithm deemed useful in order to predict future stock market returns the competition comprises two stages with two different evaluation periods 
0	110173	10173	in the first stage the predictions are tested against historical data of the period 1 1 2017 to 7 31 2018 .this data shall be used to train any kind of learning algorithm deemed useful in order to predict future stock market returns the competition comprises two stages with two different evaluation periods .this stage will be terminated early next year at which time the final submissions of the participating teams must be handed in .introduction	in the first stage the predictions are tested against historical data of the period 1 1 2017 to 7 31 2018 
0	110174	10174	this stage will be terminated early next year at which time the final submissions of the participating teams must be handed in .in the first stage the predictions are tested against historical data of the period 1 1 2017 to 7 31 2018 .the latter will then be evaluated against future data for about six months to identify the best performing submission which will be disclosed 7 15 2019 the objective function for this machine learning task is set the same for all participants in the competition and constructed as follows for each day t within the evaluation period the value x t is calculated aswhere for any financial asset i 1 m the term ti 1 1 stands for the predicted confidence value that it s ten day market adjusted leading return r ti r is either positive or negative .introduction	this stage will be terminated early next year at which time the final submissions of the participating teams must be handed in 
0	110175	10175	the latter will then be evaluated against future data for about six months to identify the best performing submission which will be disclosed 7 15 2019 the objective function for this machine learning task is set the same for all participants in the competition and constructed as follows for each day t within the evaluation period the value x t is calculated aswhere for any financial asset i 1 m the term ti 1 1 stands for the predicted confidence value that it s ten day market adjusted leading return r ti r is either positive or negative .this stage will be terminated early next year at which time the final submissions of the participating teams must be handed in .the universe variable u ti 0 1 controls whether the asset i is included in the evaluation at the particular evaluation day t finally the score which determines the position in the competition is composed of the mean and the standard deviation of the daily value x t with score 0 for x t 0 we apply three different algorithms to this problem logistic regression neural network and gradient boosting tree .introduction	the latter will then be evaluated against future data for about six months to identify the best performing submission which will be disclosed 7 15 2019 the objective function for this machine learning task is set the same for all participants in the competition and constructed as follows for each day t within the evaluation period the value x t is calculated aswhere for any financial asset i 1 m the term ti 1 1 stands for the predicted confidence value that it s ten day market adjusted leading return r ti r is either positive or negative 
0	110176	10176	the universe variable u ti 0 1 controls whether the asset i is included in the evaluation at the particular evaluation day t finally the score which determines the position in the competition is composed of the mean and the standard deviation of the daily value x t with score 0 for x t 0 we apply three different algorithms to this problem logistic regression neural network and gradient boosting tree .the latter will then be evaluated against future data for about six months to identify the best performing submission which will be disclosed 7 15 2019 the objective function for this machine learning task is set the same for all participants in the competition and constructed as follows for each day t within the evaluation period the value x t is calculated aswhere for any financial asset i 1 m the term ti 1 1 stands for the predicted confidence value that it s ten day market adjusted leading return r ti r is either positive or negative .last sentence.introduction	the universe variable u ti 0 1 controls whether the asset i is included in the evaluation at the particular evaluation day t finally the score which determines the position in the competition is composed of the mean and the standard deviation of the daily value x t with score 0 for x t 0 we apply three different algorithms to this problem logistic regression neural network and gradient boosting tree 
0	110177	10177	there have been multiple attempts looking into the popular topic of forecasting stock price with techniques of machine learning .first sentence.based on the works we find the focus of these research projects vary mainly in three ways .related work	there have been multiple attempts looking into the popular topic of forecasting stock price with techniques of machine learning 
0	110178	10178	based on the works we find the focus of these research projects vary mainly in three ways .there have been multiple attempts looking into the popular topic of forecasting stock price with techniques of machine learning .1 the text information used in prediction ranges from public news economy trend to exclusive information about the characteristics of the company .related work	based on the works we find the focus of these research projects vary mainly in three ways 
0	110179	10179	1 the text information used in prediction ranges from public news economy trend to exclusive information about the characteristics of the company .based on the works we find the focus of these research projects vary mainly in three ways .2 the targeting price change can be near term high frequency less than a minute short term tomorrow to a few days later and long term months later .related work	1 the text information used in prediction ranges from public news economy trend to exclusive information about the characteristics of the company 
0	110180	10180	2 the targeting price change can be near term high frequency less than a minute short term tomorrow to a few days later and long term months later .1 the text information used in prediction ranges from public news economy trend to exclusive information about the characteristics of the company .3 datasets and features.related work	2 the targeting price change can be near term high frequency less than a minute short term tomorrow to a few days later and long term months later 
0	110181	10181	3 datasets and features.2 the targeting price change can be near term high frequency less than a minute short term tomorrow to a few days later and long term months later .last sentence.related work	3 datasets and features
0	110182	10182	all the data used in the project is provided by kaggle .first sentence.two sources of data are provided one for market data and one for news data both spanning from 2007 to the end of 2016 .description	all the data used in the project is provided by kaggle 
0	110183	10183	two sources of data are provided one for market data and one for news data both spanning from 2007 to the end of 2016 .all the data used in the project is provided by kaggle .the market data contains various financial market information for 3511 us listed instruments .description	two sources of data are provided one for market data and one for news data both spanning from 2007 to the end of 2016 
0	110184	10184	the market data contains various financial market information for 3511 us listed instruments .two sources of data are provided one for market data and one for news data both spanning from 2007 to the end of 2016 .it is comprised of more than 4 million samples and 16 features the returnsopennextmktres10 column indicates the market normalized return for the next 10 days and thus serves as the ground truth value for the prediction task .description	the market data contains various financial market information for 3511 us listed instruments 
0	110185	10185	it is comprised of more than 4 million samples and 16 features the returnsopennextmktres10 column indicates the market normalized return for the next 10 days and thus serves as the ground truth value for the prediction task .the market data contains various financial market information for 3511 us listed instruments .the news data contains information at both article level and asset level .description	it is comprised of more than 4 million samples and 16 features the returnsopennextmktres10 column indicates the market normalized return for the next 10 days and thus serves as the ground truth value for the prediction task 
0	110186	10186	the news data contains information at both article level and asset level .it is comprised of more than 4 million samples and 16 features the returnsopennextmktres10 column indicates the market normalized return for the next 10 days and thus serves as the ground truth value for the prediction task .there are more 9 million samples and 35 features .description	the news data contains information at both article level and asset level 
0	110187	10187	there are more 9 million samples and 35 features .the news data contains information at both article level and asset level .most of the news features are either numerical or type indicators except the headline feature which contains text .description	there are more 9 million samples and 35 features 
0	110188	10188	most of the news features are either numerical or type indicators except the headline feature which contains text .there are more 9 million samples and 35 features .the news data provided is intentionally not normalized both data sets can be joined by using either the time stamp asset code or asset name .description	most of the news features are either numerical or type indicators except the headline feature which contains text 
0	110189	10189	the news data provided is intentionally not normalized both data sets can be joined by using either the time stamp asset code or asset name .most of the news features are either numerical or type indicators except the headline feature which contains text .last sentence.description	the news data provided is intentionally not normalized both data sets can be joined by using either the time stamp asset code or asset name 
0	110190	10190	as shown in.first sentence.last sentence.processing	as shown in
0	110191	10191	we chose logistic regression as a starting point for establishing a baseline score .first sentence.the logistic regression takes in all the features as is such that it does not include higher degree terms .logistic regression	we chose logistic regression as a starting point for establishing a baseline score 
0	110192	10192	the logistic regression takes in all the features as is such that it does not include higher degree terms .we chose logistic regression as a starting point for establishing a baseline score .because of the large size of the training data small regularization is used .logistic regression	the logistic regression takes in all the features as is such that it does not include higher degree terms 
0	110193	10193	because of the large size of the training data small regularization is used .the logistic regression takes in all the features as is such that it does not include higher degree terms .the log likely hood is.logistic regression	because of the large size of the training data small regularization is used 
0	110194	10194	the log likely hood is.because of the large size of the training data small regularization is used .last sentence.logistic regression	the log likely hood is
0	110195	10195	we implement a fully connected neural network with two inputs .first sentence.into the first input branch we feed all numerical values of the preprocessed dataset while the second input branch encodes the categorical data asset code for each sample in a trainable embedding layer .neural network	we implement a fully connected neural network with two inputs 
0	110196	10196	into the first input branch we feed all numerical values of the preprocessed dataset while the second input branch encodes the categorical data asset code for each sample in a trainable embedding layer .we implement a fully connected neural network with two inputs .after batch normalisation and two fully connected layers for the numerical part and one fully connected layer for the categorical part both branches of the network are concatenated .neural network	into the first input branch we feed all numerical values of the preprocessed dataset while the second input branch encodes the categorical data asset code for each sample in a trainable embedding layer 
0	110197	10197	after batch normalisation and two fully connected layers for the numerical part and one fully connected layer for the categorical part both branches of the network are concatenated .into the first input branch we feed all numerical values of the preprocessed dataset while the second input branch encodes the categorical data asset code for each sample in a trainable embedding layer .the concatenated data is finally fed into one more fully connected layer followed by the output layer .neural network	after batch normalisation and two fully connected layers for the numerical part and one fully connected layer for the categorical part both branches of the network are concatenated 
0	110198	10198	the concatenated data is finally fed into one more fully connected layer followed by the output layer .after batch normalisation and two fully connected layers for the numerical part and one fully connected layer for the categorical part both branches of the network are concatenated .all fully connected layers use relu activation except the output layer which has a sigmoid activation function .neural network	the concatenated data is finally fed into one more fully connected layer followed by the output layer 
0	110199	10199	all fully connected layers use relu activation except the output layer which has a sigmoid activation function .the concatenated data is finally fed into one more fully connected layer followed by the output layer .last sentence.neural network	all fully connected layers use relu activation except the output layer which has a sigmoid activation function 
0	110200	10200	gradient boosting is a technique that combines weak predicting models into an ensemble to produce a much stronger one .first sentence.it is typically implemented on decision trees .gradient boosting	gradient boosting is a technique that combines weak predicting models into an ensemble to produce a much stronger one 
0	110201	10201	it is typically implemented on decision trees .gradient boosting is a technique that combines weak predicting models into an ensemble to produce a much stronger one .like other boosting algorithms gradient boosting is an iterative operation .gradient boosting	it is typically implemented on decision trees 
0	110202	10202	like other boosting algorithms gradient boosting is an iterative operation .it is typically implemented on decision trees .at each iteration the algorithm creates a new estimator that minimizes the loss with respect to the current model .gradient boosting	like other boosting algorithms gradient boosting is an iterative operation 
0	110203	10203	at each iteration the algorithm creates a new estimator that minimizes the loss with respect to the current model .like other boosting algorithms gradient boosting is an iterative operation .this minimization can be approximated by fitting the new estimator to the gradient of loss such that where f k is the ensemble model at kth iteration r ik is the gradient residual of the loss function with respect to f k 1 for ith data h k is the new model that fits r ik for i 1 m l is the loss function binary log loss function for this project it is similar to the normal gradient descent except that the gradient descent is performed on the output of the model instead of the parameters of each weak model .gradient boosting	at each iteration the algorithm creates a new estimator that minimizes the loss with respect to the current model 
0	110204	10204	this minimization can be approximated by fitting the new estimator to the gradient of loss such that where f k is the ensemble model at kth iteration r ik is the gradient residual of the loss function with respect to f k 1 for ith data h k is the new model that fits r ik for i 1 m l is the loss function binary log loss function for this project it is similar to the normal gradient descent except that the gradient descent is performed on the output of the model instead of the parameters of each weak model .at each iteration the algorithm creates a new estimator that minimizes the loss with respect to the current model .the regularization is achieved through several ways by slowly decreasing the learning rate setting the number of minimum samples in a tree leaf limiting number of leaves or penalizing the complexity of the tree model such as l2 regularization .gradient boosting	this minimization can be approximated by fitting the new estimator to the gradient of loss such that where f k is the ensemble model at kth iteration r ik is the gradient residual of the loss function with respect to f k 1 for ith data h k is the new model that fits r ik for i 1 m l is the loss function binary log loss function for this project it is similar to the normal gradient descent except that the gradient descent is performed on the output of the model instead of the parameters of each weak model 
0	110205	10205	the regularization is achieved through several ways by slowly decreasing the learning rate setting the number of minimum samples in a tree leaf limiting number of leaves or penalizing the complexity of the tree model such as l2 regularization .this minimization can be approximated by fitting the new estimator to the gradient of loss such that where f k is the ensemble model at kth iteration r ik is the gradient residual of the loss function with respect to f k 1 for ith data h k is the new model that fits r ik for i 1 m l is the loss function binary log loss function for this project it is similar to the normal gradient descent except that the gradient descent is performed on the output of the model instead of the parameters of each weak model .lightgbm library is used to implement this algorithm in this project .gradient boosting	the regularization is achieved through several ways by slowly decreasing the learning rate setting the number of minimum samples in a tree leaf limiting number of leaves or penalizing the complexity of the tree model such as l2 regularization 
0	110206	10206	lightgbm library is used to implement this algorithm in this project .the regularization is achieved through several ways by slowly decreasing the learning rate setting the number of minimum samples in a tree leaf limiting number of leaves or penalizing the complexity of the tree model such as l2 regularization .it converts continuous features into bins which reduces memory and boosts speed and grows each tree with the priority given to the leaf with maximum delta loss leading to lower overall loss .gradient boosting	lightgbm library is used to implement this algorithm in this project 
0	110207	10207	it converts continuous features into bins which reduces memory and boosts speed and grows each tree with the priority given to the leaf with maximum delta loss leading to lower overall loss .lightgbm library is used to implement this algorithm in this project .last sentence.gradient boosting	it converts continuous features into bins which reduces memory and boosts speed and grows each tree with the priority given to the leaf with maximum delta loss leading to lower overall loss 
0	110208	10208	a auc curves the auc score for the logistic regression the fully connected neural network model and the light gbm model is 0 5 0 5799 and 0 5753 respectively as shown by.first sentence.last sentence.results and discussion	a auc curves the auc score for the logistic regression the fully connected neural network model and the light gbm model is 0 5 0 5799 and 0 5753 respectively as shown by
0	110209	10209	out of the three attempted algorithms the neural network performs the best followed closely by the gradient boosting tree while the logistic regression behaves almost like a random guess .first sentence.as the logistic regression is fairly a simple algorithm with linear mappings to each feature dimension its incapability to capture the complex relationship is expected .conclusion and future work	out of the three attempted algorithms the neural network performs the best followed closely by the gradient boosting tree while the logistic regression behaves almost like a random guess 
0	110210	10210	as the logistic regression is fairly a simple algorithm with linear mappings to each feature dimension its incapability to capture the complex relationship is expected .out of the three attempted algorithms the neural network performs the best followed closely by the gradient boosting tree while the logistic regression behaves almost like a random guess .on the other hand both the neural network and gradient boosting tree are powerful non linear algorithms with a large degree of flexibility and control making them competent to model complex situations for future work deeper dive into the feature engineering is needed .conclusion and future work	as the logistic regression is fairly a simple algorithm with linear mappings to each feature dimension its incapability to capture the complex relationship is expected 
0	110211	10211	on the other hand both the neural network and gradient boosting tree are powerful non linear algorithms with a large degree of flexibility and control making them competent to model complex situations for future work deeper dive into the feature engineering is needed .as the logistic regression is fairly a simple algorithm with linear mappings to each feature dimension its incapability to capture the complex relationship is expected .it is also worth exploring to combine neural network and gradient boosting tree in an ensemble fashion to produce a stronger model .conclusion and future work	on the other hand both the neural network and gradient boosting tree are powerful non linear algorithms with a large degree of flexibility and control making them competent to model complex situations for future work deeper dive into the feature engineering is needed 
0	110212	10212	it is also worth exploring to combine neural network and gradient boosting tree in an ensemble fashion to produce a stronger model .on the other hand both the neural network and gradient boosting tree are powerful non linear algorithms with a large degree of flexibility and control making them competent to model complex situations for future work deeper dive into the feature engineering is needed .one of the news features is text based thus natural language processing can be implemented to extract useful information from it .conclusion and future work	it is also worth exploring to combine neural network and gradient boosting tree in an ensemble fashion to produce a stronger model 
0	110213	10213	one of the news features is text based thus natural language processing can be implemented to extract useful information from it .it is also worth exploring to combine neural network and gradient boosting tree in an ensemble fashion to produce a stronger model .given the large parameter sets for the neural network and the gradient boosting tree achieve the optimum parameters is both difficult and time consuming .conclusion and future work	one of the news features is text based thus natural language processing can be implemented to extract useful information from it 
0	110214	10214	given the large parameter sets for the neural network and the gradient boosting tree achieve the optimum parameters is both difficult and time consuming .one of the news features is text based thus natural language processing can be implemented to extract useful information from it .however there is still possible room to make improvement by further tuning the parameters .conclusion and future work	given the large parameter sets for the neural network and the gradient boosting tree achieve the optimum parameters is both difficult and time consuming 
0	110215	10215	however there is still possible room to make improvement by further tuning the parameters .given the large parameter sets for the neural network and the gradient boosting tree achieve the optimum parameters is both difficult and time consuming .lastly choosing a more powerful baseline such as the support vector machine instead of the simple logistic regression should be considered .conclusion and future work	however there is still possible room to make improvement by further tuning the parameters 
0	110216	10216	lastly choosing a more powerful baseline such as the support vector machine instead of the simple logistic regression should be considered .however there is still possible room to make improvement by further tuning the parameters .last sentence.conclusion and future work	lastly choosing a more powerful baseline such as the support vector machine instead of the simple logistic regression should be considered 
0	110217	10217	as a group working on this collaborated project we contributed equally overall .first sentence.barthold albrecht has additional contribution on establishment of the logistic regression model and the fully connected neural network model .contributions	as a group working on this collaborated project we contributed equally overall 
0	110218	10218	barthold albrecht has additional contribution on establishment of the logistic regression model and the fully connected neural network model .as a group working on this collaborated project we contributed equally overall .yanzhuo wang has additional contribution on establishment of the logistic regression model and the lgbm model .contributions	barthold albrecht has additional contribution on establishment of the logistic regression model and the fully connected neural network model 
0	110219	10219	yanzhuo wang has additional contribution on establishment of the logistic regression model and the lgbm model .barthold albrecht has additional contribution on establishment of the logistic regression model and the fully connected neural network model .xiaofang zhu has additional contribution on establishing the fully connected neural network model .contributions	yanzhuo wang has additional contribution on establishment of the logistic regression model and the lgbm model 
0	110220	10220	xiaofang zhu has additional contribution on establishing the fully connected neural network model .yanzhuo wang has additional contribution on establishment of the logistic regression model and the lgbm model .last sentence.contributions	xiaofang zhu has additional contribution on establishing the fully connected neural network model 
0	110221	10221	https drive google com open id 1mnf5opuzbdvotkxl6sjkglpqabcn8v9x.first sentence.last sentence.source code	https drive google com open id 1mnf5opuzbdvotkxl6sjkglpqabcn8v9x
0	110222	10222	the real estate market in large metropolitan areas across usa and canada is characterized by high volatility .first sentence.home prices in popular technological and cultural centers such as vancouver canada have been reportedly growing over the last decade owing to a constant influx of people including immigrants attracted by a combination of career opportunities and a superb geographical setting by the ocean bay and nearby mountains .i introduction	the real estate market in large metropolitan areas across usa and canada is characterized by high volatility 
0	110223	10223	home prices in popular technological and cultural centers such as vancouver canada have been reportedly growing over the last decade owing to a constant influx of people including immigrants attracted by a combination of career opportunities and a superb geographical setting by the ocean bay and nearby mountains .the real estate market in large metropolitan areas across usa and canada is characterized by high volatility .as a result predicting home prices has become a big challenge .i introduction	home prices in popular technological and cultural centers such as vancouver canada have been reportedly growing over the last decade owing to a constant influx of people including immigrants attracted by a combination of career opportunities and a superb geographical setting by the ocean bay and nearby mountains 
0	110224	10224	as a result predicting home prices has become a big challenge .home prices in popular technological and cultural centers such as vancouver canada have been reportedly growing over the last decade owing to a constant influx of people including immigrants attracted by a combination of career opportunities and a superb geographical setting by the ocean bay and nearby mountains .real estate agents use their domain knowledge to estimate a home price aiding sellers and buyers in the transaction .i introduction	as a result predicting home prices has become a big challenge 
0	110225	10225	real estate agents use their domain knowledge to estimate a home price aiding sellers and buyers in the transaction .as a result predicting home prices has become a big challenge .this estimate is often very subjective and facilitates bubbling the home prices especially in highly attractive areas like vancouver .i introduction	real estate agents use their domain knowledge to estimate a home price aiding sellers and buyers in the transaction 
1	110226	10226	this estimate is often very subjective and facilitates bubbling the home prices especially in highly attractive areas like vancouver .real estate agents use their domain knowledge to estimate a home price aiding sellers and buyers in the transaction .therefore our main study goal was to come up with an automated way of pricetagging a home based on its characteristics including floor area number of rooms location and others the input to our algorithm is a dataset of all condo listings under 2 5mm cad in downtown vancouver between january 2016 and october 2018 containing approximately 50 features after pre processing .i introduction	this estimate is often very subjective and facilitates bubbling the home prices especially in highly attractive areas like vancouver 
1	110227	10227	therefore our main study goal was to come up with an automated way of pricetagging a home based on its characteristics including floor area number of rooms location and others the input to our algorithm is a dataset of all condo listings under 2 5mm cad in downtown vancouver between january 2016 and october 2018 containing approximately 50 features after pre processing .this estimate is often very subjective and facilitates bubbling the home prices especially in highly attractive areas like vancouver .we then use linear regression neural networks and boosted tree models to predict the expected selling price of a condo .i introduction	therefore our main study goal was to come up with an automated way of pricetagging a home based on its characteristics including floor area number of rooms location and others the input to our algorithm is a dataset of all condo listings under 2 5mm cad in downtown vancouver between january 2016 and october 2018 containing approximately 50 features after pre processing 
0	110228	10228	we then use linear regression neural networks and boosted tree models to predict the expected selling price of a condo .therefore our main study goal was to come up with an automated way of pricetagging a home based on its characteristics including floor area number of rooms location and others the input to our algorithm is a dataset of all condo listings under 2 5mm cad in downtown vancouver between january 2016 and october 2018 containing approximately 50 features after pre processing .last sentence.i introduction	we then use linear regression neural networks and boosted tree models to predict the expected selling price of a condo 
0	110229	10229	there is a good number of articles related to real estate pricing predictions .first sentence.in general it is difficult to compare the results given the diversity of features used to model the predictions and relevant error analysis .ii related work	there is a good number of articles related to real estate pricing predictions 
0	110230	10230	in general it is difficult to compare the results given the diversity of features used to model the predictions and relevant error analysis .there is a good number of articles related to real estate pricing predictions .however there are some common themes that we tried to reproduce and improve upon in our research .ii related work	in general it is difficult to compare the results given the diversity of features used to model the predictions and relevant error analysis 
0	110231	10231	however there are some common themes that we tried to reproduce and improve upon in our research .in general it is difficult to compare the results given the diversity of features used to model the predictions and relevant error analysis .more often than not the authors attempt to use linear regression and boosted trees regression algorithms another author in another example related to bay area house pricing prediction.ii related work	however there are some common themes that we tried to reproduce and improve upon in our research 
0	110232	10232	more often than not the authors attempt to use linear regression and boosted trees regression algorithms another author in another example related to bay area house pricing prediction.however there are some common themes that we tried to reproduce and improve upon in our research .last sentence.ii related work	more often than not the authors attempt to use linear regression and boosted trees regression algorithms another author in another example related to bay area house pricing prediction
0	110233	10233	the original dataset we received had exhaustive information about all condos listed for sale under 2 5mm cad in downtown vancouver between january 2016 and october 2018 .first sentence.the data was pulled from an official canadian real estate listings database called mls with the help of a local real estate agent and contained approximately 10 000 listings .a raw data	the original dataset we received had exhaustive information about all condos listed for sale under 2 5mm cad in downtown vancouver between january 2016 and october 2018 
0	110234	10234	the data was pulled from an official canadian real estate listings database called mls with the help of a local real estate agent and contained approximately 10 000 listings .the original dataset we received had exhaustive information about all condos listed for sale under 2 5mm cad in downtown vancouver between january 2016 and october 2018 .each listing had up to 237 features including immanent property characteristics like square footage number of bedrooms and bathrooms maintenance fees and relational characteristics like address vicinity to schools and public transportation and views .a raw data	the data was pulled from an official canadian real estate listings database called mls with the help of a local real estate agent and contained approximately 10 000 listings 
0	110235	10235	each listing had up to 237 features including immanent property characteristics like square footage number of bedrooms and bathrooms maintenance fees and relational characteristics like address vicinity to schools and public transportation and views .the data was pulled from an official canadian real estate listings database called mls with the help of a local real estate agent and contained approximately 10 000 listings .the features can be classified into three categories structured data e g total floor area semistructured data e g address unstructured data e g listing agent comments .a raw data	each listing had up to 237 features including immanent property characteristics like square footage number of bedrooms and bathrooms maintenance fees and relational characteristics like address vicinity to schools and public transportation and views 
0	110236	10236	the features can be classified into three categories structured data e g total floor area semistructured data e g address unstructured data e g listing agent comments .each listing had up to 237 features including immanent property characteristics like square footage number of bedrooms and bathrooms maintenance fees and relational characteristics like address vicinity to schools and public transportation and views .furthermore data can be categorized into various types interval scaled variables e g number of bedrooms year built etc .a raw data	the features can be classified into three categories structured data e g total floor area semistructured data e g address unstructured data e g listing agent comments 
0	110237	10237	furthermore data can be categorized into various types interval scaled variables e g number of bedrooms year built etc .the features can be classified into three categories structured data e g total floor area semistructured data e g address unstructured data e g listing agent comments .temporal e g date property was sold rank e g floor number boolean e g fireplace yes no categorical e g dwelling type .a raw data	furthermore data can be categorized into various types interval scaled variables e g number of bedrooms year built etc 
0	110238	10238	temporal e g date property was sold rank e g floor number boolean e g fireplace yes no categorical e g dwelling type .furthermore data can be categorized into various types interval scaled variables e g number of bedrooms year built etc .last sentence.a raw data	temporal e g date property was sold rank e g floor number boolean e g fireplace yes no categorical e g dwelling type 
0	110239	10239	our dataset did not have the geographical location of homes originally only the physical addresses .first sentence.since location is supposedly important for home value we used the google maps api 12 to geocode condo addresses to geographical coordinates latitude longitude .b feature engineering geocoding and bucketing	our dataset did not have the geographical location of homes originally only the physical addresses 
0	110240	10240	since location is supposedly important for home value we used the google maps api 12 to geocode condo addresses to geographical coordinates latitude longitude .our dataset did not have the geographical location of homes originally only the physical addresses .in addition since the area of interest was relatively small only about 9 km 2 we approximated it with a flat rectangle and converted geographic to cartesian coordinates mapping all condos to a c feature engineering view scoring.b feature engineering geocoding and bucketing	since location is supposedly important for home value we used the google maps api 12 to geocode condo addresses to geographical coordinates latitude longitude 
0	110241	10241	in addition since the area of interest was relatively small only about 9 km 2 we approximated it with a flat rectangle and converted geographic to cartesian coordinates mapping all condos to a c feature engineering view scoring.since location is supposedly important for home value we used the google maps api 12 to geocode condo addresses to geographical coordinates latitude longitude .last sentence.b feature engineering geocoding and bucketing	in addition since the area of interest was relatively small only about 9 km 2 we approximated it with a flat rectangle and converted geographic to cartesian coordinates mapping all condos to a c feature engineering view scoring
0	110242	10242	cleaning the data was essential to having an accurate model since it was originally input by real estate agents and was prone to mistakes .first sentence.we removed outliers for the numerical features using three sigma rule .d data cleaning	cleaning the data was essential to having an accurate model since it was originally input by real estate agents and was prone to mistakes 
0	110243	10243	we removed outliers for the numerical features using three sigma rule .cleaning the data was essential to having an accurate model since it was originally input by real estate agents and was prone to mistakes .we then imputed data for features that had occasionally missing data less than 1 with medians .d data cleaning	we removed outliers for the numerical features using three sigma rule 
0	110244	10244	we then imputed data for features that had occasionally missing data less than 1 with medians .we removed outliers for the numerical features using three sigma rule .lastly we standardized the data .d data cleaning	we then imputed data for features that had occasionally missing data less than 1 with medians 
0	110245	10245	lastly we standardized the data .we then imputed data for features that had occasionally missing data less than 1 with medians .the final feature set consisted of 48 features .d data cleaning	lastly we standardized the data 
0	110246	10246	the final feature set consisted of 48 features .lastly we standardized the data .last sentence.d data cleaning	the final feature set consisted of 48 features 
0	110247	10247	to improve the quality of our predictions we performed error analysis with k fold cross validation cv .first sentence.we split our dataset randomly into a training and test set 80 20 .a error analysis	to improve the quality of our predictions we performed error analysis with k fold cross validation cv 
0	110248	10248	we split our dataset randomly into a training and test set 80 20 .to improve the quality of our predictions we performed error analysis with k fold cross validation cv .the training set was used in cv setting where it was sequentially split into training and validation subsets k 5 times .a error analysis	we split our dataset randomly into a training and test set 80 20 
0	110249	10249	the training set was used in cv setting where it was sequentially split into training and validation subsets k 5 times .we split our dataset randomly into a training and test set 80 20 .the training subset was used to fit the model to the data and validation subset was used to compute errors .a error analysis	the training set was used in cv setting where it was sequentially split into training and validation subsets k 5 times 
0	110250	10250	the training subset was used to fit the model to the data and validation subset was used to compute errors .the training set was used in cv setting where it was sequentially split into training and validation subsets k 5 times .the average of k errors the cv error characterized how accurately our model performed we used two main metrics for calculating errors mean squared error and coefficient of determination r squared with y i predicted variable observations its mean x i vector of independent variables features f x the model mapping features x on y and n number of observations i 1 n .a error analysis	the training subset was used to fit the model to the data and validation subset was used to compute errors 
0	110251	10251	the average of k errors the cv error characterized how accurately our model performed we used two main metrics for calculating errors mean squared error and coefficient of determination r squared with y i predicted variable observations its mean x i vector of independent variables features f x the model mapping features x on y and n number of observations i 1 n .the training subset was used to fit the model to the data and validation subset was used to compute errors .mse characterizes the average of squared deviations of predictions from observations with mse 0 corresponding to an idealistic model exactly mimicking observations .a error analysis	the average of k errors the cv error characterized how accurately our model performed we used two main metrics for calculating errors mean squared error and coefficient of determination r squared with y i predicted variable observations its mean x i vector of independent variables features f x the model mapping features x on y and n number of observations i 1 n 
0	110252	10252	mse characterizes the average of squared deviations of predictions from observations with mse 0 corresponding to an idealistic model exactly mimicking observations .the average of k errors the cv error characterized how accurately our model performed we used two main metrics for calculating errors mean squared error and coefficient of determination r squared with y i predicted variable observations its mean x i vector of independent variables features f x the model mapping features x on y and n number of observations i 1 n .r 2 measures how well the model captures variability of observations given observed features with r 2 1 being an idealistic scenario .a error analysis	mse characterizes the average of squared deviations of predictions from observations with mse 0 corresponding to an idealistic model exactly mimicking observations 
0	110253	10253	r 2 measures how well the model captures variability of observations given observed features with r 2 1 being an idealistic scenario .mse characterizes the average of squared deviations of predictions from observations with mse 0 corresponding to an idealistic model exactly mimicking observations .one can show that these properties are closely related such thatgiving it the meaning of the fraction of explained variance in y we calculated mse and r 2 for both cv as explained above and training sets and also calculated the ratio of mse on cv and training sets to characterize how well our model generalized to new data model variance v ar m se cv m se train .a error analysis	r 2 measures how well the model captures variability of observations given observed features with r 2 1 being an idealistic scenario 
0	110254	10254	one can show that these properties are closely related such thatgiving it the meaning of the fraction of explained variance in y we calculated mse and r 2 for both cv as explained above and training sets and also calculated the ratio of mse on cv and training sets to characterize how well our model generalized to new data model variance v ar m se cv m se train .r 2 measures how well the model captures variability of observations given observed features with r 2 1 being an idealistic scenario .a good model would have this metric not much larger than 1 .a error analysis	one can show that these properties are closely related such thatgiving it the meaning of the fraction of explained variance in y we calculated mse and r 2 for both cv as explained above and training sets and also calculated the ratio of mse on cv and training sets to characterize how well our model generalized to new data model variance v ar m se cv m se train 
0	110255	10255	a good model would have this metric not much larger than 1 .one can show that these properties are closely related such thatgiving it the meaning of the fraction of explained variance in y we calculated mse and r 2 for both cv as explained above and training sets and also calculated the ratio of mse on cv and training sets to characterize how well our model generalized to new data model variance v ar m se cv m se train .a v ar 1 would mean we overfitted the data and our model would most likely perform badly on new data .a error analysis	a good model would have this metric not much larger than 1 
0	110256	10256	a v ar 1 would mean we overfitted the data and our model would most likely perform badly on new data .a good model would have this metric not much larger than 1 .finally after tuning each model and obtaining best set of coefficients we calculated mse and r 2 for test set as the final unbiased accuracy characteristic .a error analysis	a v ar 1 would mean we overfitted the data and our model would most likely perform badly on new data 
0	110257	10257	finally after tuning each model and obtaining best set of coefficients we calculated mse and r 2 for test set as the final unbiased accuracy characteristic .a v ar 1 would mean we overfitted the data and our model would most likely perform badly on new data .last sentence.a error analysis	finally after tuning each model and obtaining best set of coefficients we calculated mse and r 2 for test set as the final unbiased accuracy characteristic 
0	110258	10258	we used multiple multiple predictors linear regression as our benchmark model .first sentence.as the name suggests it assumes a linear relationship between the features and the predicted target variable and treats it as a linear combination of features f x t x b where x is the feature vector is the vector of model coefficients and b is the bias .b linear regression	we used multiple multiple predictors linear regression as our benchmark model 
0	110259	10259	as the name suggests it assumes a linear relationship between the features and the predicted target variable and treats it as a linear combination of features f x t x b where x is the feature vector is the vector of model coefficients and b is the bias .we used multiple multiple predictors linear regression as our benchmark model .to train the linear regression model one needs to find the coefficients given data x and target variable observations y as approximation of the predictions of f x .b linear regression	as the name suggests it assumes a linear relationship between the features and the predicted target variable and treats it as a linear combination of features f x t x b where x is the feature vector is the vector of model coefficients and b is the bias 
0	110260	10260	to train the linear regression model one needs to find the coefficients given data x and target variable observations y as approximation of the predictions of f x .as the name suggests it assumes a linear relationship between the features and the predicted target variable and treats it as a linear combination of features f x t x b where x is the feature vector is the vector of model coefficients and b is the bias .by minimizing the least squares cost function w r t .b linear regression	to train the linear regression model one needs to find the coefficients given data x and target variable observations y as approximation of the predictions of f x 
0	110261	10261	by minimizing the least squares cost function w r t .to train the linear regression model one needs to find the coefficients given data x and target variable observations y as approximation of the predictions of f x .coefficients they are found effectively using normal equation the added benefit of lasso regularization is that it sets coefficients of unimportant features to 0 and can be used as a feature selection technique for other methods .b linear regression	by minimizing the least squares cost function w r t 
0	110262	10262	coefficients they are found effectively using normal equation the added benefit of lasso regularization is that it sets coefficients of unimportant features to 0 and can be used as a feature selection technique for other methods .by minimizing the least squares cost function w r t .this was precisely the reason we used lasso in our research .b linear regression	coefficients they are found effectively using normal equation the added benefit of lasso regularization is that it sets coefficients of unimportant features to 0 and can be used as a feature selection technique for other methods 
0	110263	10263	this was precisely the reason we used lasso in our research .coefficients they are found effectively using normal equation the added benefit of lasso regularization is that it sets coefficients of unimportant features to 0 and can be used as a feature selection technique for other methods .last sentence.b linear regression	this was precisely the reason we used lasso in our research 
0	110264	10264	one property of neural networks that makes them a popular ml method is their ability to perform end to end learning given some input features x a network is able to determine the appropriate intermediary features and weights of those features on its own a neural network s ability to model non linear data stems from its use of activation functions in between its neuron layers .first sentence.one example of a commonly used activation function is the rectified linear unit relu function its main advantage is that it has a very simple gradient and doesn t suffer from vanishing gradients at extreme values although it can cause dead neurons when the product of the weights and inputs skews negative the leaky relu activation function addresses this shortcoming where is a small number e g 0 1 dropout layers are commonly used to address overfitting in neural networks .d neural networks	one property of neural networks that makes them a popular ml method is their ability to perform end to end learning given some input features x a network is able to determine the appropriate intermediary features and weights of those features on its own a neural network s ability to model non linear data stems from its use of activation functions in between its neuron layers 
0	110265	10265	one example of a commonly used activation function is the rectified linear unit relu function its main advantage is that it has a very simple gradient and doesn t suffer from vanishing gradients at extreme values although it can cause dead neurons when the product of the weights and inputs skews negative the leaky relu activation function addresses this shortcoming where is a small number e g 0 1 dropout layers are commonly used to address overfitting in neural networks .one property of neural networks that makes them a popular ml method is their ability to perform end to end learning given some input features x a network is able to determine the appropriate intermediary features and weights of those features on its own a neural network s ability to model non linear data stems from its use of activation functions in between its neuron layers .this is implemented by randomly dropping a small percentage of nodes in a layer during each update iteration preventing the network from over relying on any individual neuron .d neural networks	one example of a commonly used activation function is the rectified linear unit relu function its main advantage is that it has a very simple gradient and doesn t suffer from vanishing gradients at extreme values although it can cause dead neurons when the product of the weights and inputs skews negative the leaky relu activation function addresses this shortcoming where is a small number e g 0 1 dropout layers are commonly used to address overfitting in neural networks 
0	110266	10266	this is implemented by randomly dropping a small percentage of nodes in a layer during each update iteration preventing the network from over relying on any individual neuron .one example of a commonly used activation function is the rectified linear unit relu function its main advantage is that it has a very simple gradient and doesn t suffer from vanishing gradients at extreme values although it can cause dead neurons when the product of the weights and inputs skews negative the leaky relu activation function addresses this shortcoming where is a small number e g 0 1 dropout layers are commonly used to address overfitting in neural networks .neural networks learn through the backpropagation of error gradients and the weights w l at a layer l are updated by where is the learning rate l i is the cost function and l i is a loss function least squares in our case for an i th example .d neural networks	this is implemented by randomly dropping a small percentage of nodes in a layer during each update iteration preventing the network from over relying on any individual neuron 
0	110267	10267	neural networks learn through the backpropagation of error gradients and the weights w l at a layer l are updated by where is the learning rate l i is the cost function and l i is a loss function least squares in our case for an i th example .this is implemented by randomly dropping a small percentage of nodes in a layer during each update iteration preventing the network from over relying on any individual neuron .last sentence.d neural networks	neural networks learn through the backpropagation of error gradients and the weights w l at a layer l are updated by where is the learning rate l i is the cost function and l i is a loss function least squares in our case for an i th example 
0	110268	10268	as alternate methods accounting for nonlinear relationship between features and target variable we use tree based techniques random forest and gradient boosting .first sentence.the random forest rf is a special case of ensemble ml methods when individual models are combined together to produce balanced model that have higher generalization power than separate models in the case of rf individual learners are regression trees termed as where feature space is partitioned into m regions r 1 r m and the c m ave y i x i r m .e random forests and gradient boosting	as alternate methods accounting for nonlinear relationship between features and target variable we use tree based techniques random forest and gradient boosting 
0	110269	10269	the random forest rf is a special case of ensemble ml methods when individual models are combined together to produce balanced model that have higher generalization power than separate models in the case of rf individual learners are regression trees termed as where feature space is partitioned into m regions r 1 r m and the c m ave y i x i r m .as alternate methods accounting for nonlinear relationship between features and target variable we use tree based techniques random forest and gradient boosting .at each tree node the binary partition is performed on one variable .e random forests and gradient boosting	the random forest rf is a special case of ensemble ml methods when individual models are combined together to produce balanced model that have higher generalization power than separate models in the case of rf individual learners are regression trees termed as where feature space is partitioned into m regions r 1 r m and the c m ave y i x i r m 
0	110270	10270	at each tree node the binary partition is performed on one variable .the random forest rf is a special case of ensemble ml methods when individual models are combined together to produce balanced model that have higher generalization power than separate models in the case of rf individual learners are regression trees termed as where feature space is partitioned into m regions r 1 r m and the c m ave y i x i r m .finding best binary partition in terms of split variable j and split point s by minimizing sum of squares is computationally infeasible and a greedy algorithm is used when decision is being made only one step forward .e random forests and gradient boosting	at each tree node the binary partition is performed on one variable 
0	110271	10271	finding best binary partition in terms of split variable j and split point s by minimizing sum of squares is computationally infeasible and a greedy algorithm is used when decision is being made only one step forward .at each tree node the binary partition is performed on one variable .starting with all the data a pair of half planes is defined then j and s are found by solving after finding the split the data is partitioned into two regions and the splitting process is repeated on both regions and all subsequent regions until some stopping criterion is met .e random forests and gradient boosting	finding best binary partition in terms of split variable j and split point s by minimizing sum of squares is computationally infeasible and a greedy algorithm is used when decision is being made only one step forward 
0	110272	10272	starting with all the data a pair of half planes is defined then j and s are found by solving after finding the split the data is partitioned into two regions and the splitting process is repeated on both regions and all subsequent regions until some stopping criterion is met .finding best binary partition in terms of split variable j and split point s by minimizing sum of squares is computationally infeasible and a greedy algorithm is used when decision is being made only one step forward .among different criteria most popular is stopping growing a tree when minimum node size is reached .e random forests and gradient boosting	starting with all the data a pair of half planes is defined then j and s are found by solving after finding the split the data is partitioned into two regions and the splitting process is repeated on both regions and all subsequent regions until some stopping criterion is met 
0	110273	10273	among different criteria most popular is stopping growing a tree when minimum node size is reached .starting with all the data a pair of half planes is defined then j and s are found by solving after finding the split the data is partitioned into two regions and the splitting process is repeated on both regions and all subsequent regions until some stopping criterion is met .individual trees are prone to overfitting and rf method overcomes this problem by combining multiple trees grown on separate data subsets .e random forests and gradient boosting	among different criteria most popular is stopping growing a tree when minimum node size is reached 
0	110274	10274	individual trees are prone to overfitting and rf method overcomes this problem by combining multiple trees grown on separate data subsets .among different criteria most popular is stopping growing a tree when minimum node size is reached .the default approach to forming subsets in rf is bootstrap sampling with replacement when dataset size remains the same but its composition varies among samples .e random forests and gradient boosting	individual trees are prone to overfitting and rf method overcomes this problem by combining multiple trees grown on separate data subsets 
0	110275	10275	the default approach to forming subsets in rf is bootstrap sampling with replacement when dataset size remains the same but its composition varies among samples .individual trees are prone to overfitting and rf method overcomes this problem by combining multiple trees grown on separate data subsets .this way overfitting is decreased as each individual tree is learning from a different subset of data .e random forests and gradient boosting	the default approach to forming subsets in rf is bootstrap sampling with replacement when dataset size remains the same but its composition varies among samples 
0	110276	10276	this way overfitting is decreased as each individual tree is learning from a different subset of data .the default approach to forming subsets in rf is bootstrap sampling with replacement when dataset size remains the same but its composition varies among samples .moreover a random subset k rather than the whole list of features m is considered at each split where usually m k this way if few features dominate the rest in their contribution to the target variable their contribution to the final model is decreased as now the chances for them to be selected for a split are reduced another tree based ensemble technique is called boosting when power of combining weak learners is leveraged .e random forests and gradient boosting	this way overfitting is decreased as each individual tree is learning from a different subset of data 
0	110277	10277	moreover a random subset k rather than the whole list of features m is considered at each split where usually m k this way if few features dominate the rest in their contribution to the target variable their contribution to the final model is decreased as now the chances for them to be selected for a split are reduced another tree based ensemble technique is called boosting when power of combining weak learners is leveraged .this way overfitting is decreased as each individual tree is learning from a different subset of data .last sentence.e random forests and gradient boosting	moreover a random subset k rather than the whole list of features m is considered at each split where usually m k this way if few features dominate the rest in their contribution to the target variable their contribution to the final model is decreased as now the chances for them to be selected for a split are reduced another tree based ensemble technique is called boosting when power of combining weak learners is leveraged 
0	110278	10278	we start off with the linear regression lr using all features .first sentence.the reported cv mse is 0 19 this made us try nonlinear learning methods and the first one was neural networks nn .v experiments results	we start off with the linear regression lr using all features 
0	110279	10279	the reported cv mse is 0 19 this made us try nonlinear learning methods and the first one was neural networks nn .we start off with the linear regression lr using all features .there is no formula for building the perfect nn architecture .v experiments results	the reported cv mse is 0 19 this made us try nonlinear learning methods and the first one was neural networks nn 
0	110280	10280	there is no formula for building the perfect nn architecture .the reported cv mse is 0 19 this made us try nonlinear learning methods and the first one was neural networks nn .many design decisions are empirical and based on past experiences using them .v experiments results	there is no formula for building the perfect nn architecture 
0	110281	10281	many design decisions are empirical and based on past experiences using them .there is no formula for building the perfect nn architecture .as a result we experimented with various layer depths neuron counts and activation functions .v experiments results	many design decisions are empirical and based on past experiences using them 
0	110282	10282	as a result we experimented with various layer depths neuron counts and activation functions .many design decisions are empirical and based on past experiences using them .we found that in general deeper networks with smaller layers performed better than shallow networks with larger layers .v experiments results	as a result we experimented with various layer depths neuron counts and activation functions 
0	110283	10283	we found that in general deeper networks with smaller layers performed better than shallow networks with larger layers .as a result we experimented with various layer depths neuron counts and activation functions .this is supported by the nn initially suffered from significant overfitting cv mse was 10x of training mse .v experiments results	we found that in general deeper networks with smaller layers performed better than shallow networks with larger layers 
0	110284	10284	this is supported by the nn initially suffered from significant overfitting cv mse was 10x of training mse .we found that in general deeper networks with smaller layers performed better than shallow networks with larger layers .this issue was addressed by adding dropout layers to the network and choosing the best configuration 5 .v experiments results	this is supported by the nn initially suffered from significant overfitting cv mse was 10x of training mse 
0	110285	10285	this issue was addressed by adding dropout layers to the network and choosing the best configuration 5 .this is supported by the nn initially suffered from significant overfitting cv mse was 10x of training mse .as a result the train cv mse difference dropped to below 15 gradient boosting regression gb finally provided most robust and accurate model leveraging non linear nature of interaction between condo features and target variable reporting cv set mse 0 1 and test mse 0 09 with m se cv m se train being 1 25.v experiments results	this issue was addressed by adding dropout layers to the network and choosing the best configuration 5 
0	110286	10286	as a result the train cv mse difference dropped to below 15 gradient boosting regression gb finally provided most robust and accurate model leveraging non linear nature of interaction between condo features and target variable reporting cv set mse 0 1 and test mse 0 09 with m se cv m se train being 1 25.this issue was addressed by adding dropout layers to the network and choosing the best configuration 5 .last sentence.v experiments results	as a result the train cv mse difference dropped to below 15 gradient boosting regression gb finally provided most robust and accurate model leveraging non linear nature of interaction between condo features and target variable reporting cv set mse 0 1 and test mse 0 09 with m se cv m se train being 1 25
0	110287	10287	in addition to testing the three models we looked at the top features highly correlated with selling price of a condo .first sentence.they were in order 1 number of bedrooms 2 number of bathrooms 3 total floor area 4 number of parking spaces 5 maintenance fees we also noticed that the more expensive the condo was the more subjective the price appeared to be and the worse our models performed .vi discussion	in addition to testing the three models we looked at the top features highly correlated with selling price of a condo 
1	110288	10288	they were in order 1 number of bedrooms 2 number of bathrooms 3 total floor area 4 number of parking spaces 5 maintenance fees we also noticed that the more expensive the condo was the more subjective the price appeared to be and the worse our models performed .in addition to testing the three models we looked at the top features highly correlated with selling price of a condo .speaking about listing price that we intentionally excluded from the model we note that it is highly correlated with our target variable the selling price .vi discussion	they were in order 1 number of bedrooms 2 number of bathrooms 3 total floor area 4 number of parking spaces 5 maintenance fees we also noticed that the more expensive the condo was the more subjective the price appeared to be and the worse our models performed 
1	110289	10289	speaking about listing price that we intentionally excluded from the model we note that it is highly correlated with our target variable the selling price .they were in order 1 number of bedrooms 2 number of bathrooms 3 total floor area 4 number of parking spaces 5 maintenance fees we also noticed that the more expensive the condo was the more subjective the price appeared to be and the worse our models performed .the mse between them is about 6 times smaller than our best mse .vi discussion	speaking about listing price that we intentionally excluded from the model we note that it is highly correlated with our target variable the selling price 
0	110290	10290	the mse between them is about 6 times smaller than our best mse .speaking about listing price that we intentionally excluded from the model we note that it is highly correlated with our target variable the selling price .one might think that the domain knowledge of real estate agents is very thorough in estimating home value but there is a paradox .vi discussion	the mse between them is about 6 times smaller than our best mse 
1	110291	10291	one might think that the domain knowledge of real estate agents is very thorough in estimating home value but there is a paradox .the mse between them is about 6 times smaller than our best mse .when people want to sell or buy a home they first look at the listing price and therefore the resulting selling price often is very close to the listing price with listing price being a major driving factor .vi discussion	one might think that the domain knowledge of real estate agents is very thorough in estimating home value but there is a paradox 
1	110292	10292	when people want to sell or buy a home they first look at the listing price and therefore the resulting selling price often is very close to the listing price with listing price being a major driving factor .one might think that the domain knowledge of real estate agents is very thorough in estimating home value but there is a paradox .our goal on the other hand was to come up with the emotionfree algorithm that uses only bare facts about property itself and external factors .vi discussion	when people want to sell or buy a home they first look at the listing price and therefore the resulting selling price often is very close to the listing price with listing price being a major driving factor 
0	110293	10293	our goal on the other hand was to come up with the emotionfree algorithm that uses only bare facts about property itself and external factors .when people want to sell or buy a home they first look at the listing price and therefore the resulting selling price often is very close to the listing price with listing price being a major driving factor .comparing metrics of our model to those achieved by others we see that theres still room for improvement .vi discussion	our goal on the other hand was to come up with the emotionfree algorithm that uses only bare facts about property itself and external factors 
0	110294	10294	comparing metrics of our model to those achieved by others we see that theres still room for improvement .our goal on the other hand was to come up with the emotionfree algorithm that uses only bare facts about property itself and external factors .last sentence.vi discussion	comparing metrics of our model to those achieved by others we see that theres still room for improvement 
0	110295	10295	our findings show that gradient boosting had the best performance followed closely by random forest and neural networks .first sentence.this makes sense because both algorithms are useful for non linear modeling .vii conclusions future work	our findings show that gradient boosting had the best performance followed closely by random forest and neural networks 
0	110296	10296	this makes sense because both algorithms are useful for non linear modeling .our findings show that gradient boosting had the best performance followed closely by random forest and neural networks .the small data scale in our project lends itself more favorably to trees whereas it makes it more likely for a neural network to overfit resulting in slightly lower overall performance the models described in this paper can be further improved in future work through usage of additional training datasets and additional feature engineering .vii conclusions future work	this makes sense because both algorithms are useful for non linear modeling 
1	110297	10297	the small data scale in our project lends itself more favorably to trees whereas it makes it more likely for a neural network to overfit resulting in slightly lower overall performance the models described in this paper can be further improved in future work through usage of additional training datasets and additional feature engineering .this makes sense because both algorithms are useful for non linear modeling .the federal interest rate has a direct effect on the supply of money and affordability of housing which can affect the selling price .vii conclusions future work	the small data scale in our project lends itself more favorably to trees whereas it makes it more likely for a neural network to overfit resulting in slightly lower overall performance the models described in this paper can be further improved in future work through usage of additional training datasets and additional feature engineering 
0	110298	10298	the federal interest rate has a direct effect on the supply of money and affordability of housing which can affect the selling price .the small data scale in our project lends itself more favorably to trees whereas it makes it more likely for a neural network to overfit resulting in slightly lower overall performance the models described in this paper can be further improved in future work through usage of additional training datasets and additional feature engineering .over the timespan of the training dataset used in this study the federal interest rate changed from a low of 0 5 up to a high of 1 75 which could significantly affect the selling price of a condo .vii conclusions future work	the federal interest rate has a direct effect on the supply of money and affordability of housing which can affect the selling price 
1	110299	10299	over the timespan of the training dataset used in this study the federal interest rate changed from a low of 0 5 up to a high of 1 75 which could significantly affect the selling price of a condo .the federal interest rate has a direct effect on the supply of money and affordability of housing which can affect the selling price .an additional dataset that would improve the model is upcoming new condo developments .vii conclusions future work	over the timespan of the training dataset used in this study the federal interest rate changed from a low of 0 5 up to a high of 1 75 which could significantly affect the selling price of a condo 
0	110300	10300	an additional dataset that would improve the model is upcoming new condo developments .over the timespan of the training dataset used in this study the federal interest rate changed from a low of 0 5 up to a high of 1 75 which could significantly affect the selling price of a condo .large growing cities often have new real estate being built .vii conclusions future work	an additional dataset that would improve the model is upcoming new condo developments 
0	110301	10301	large growing cities often have new real estate being built .an additional dataset that would improve the model is upcoming new condo developments .additional inventory coming onto the market would affect existing condo prices negatively by increasing the available supply and alternatives for buyers lastly future work should focus on adding more temporal components to the model for example through features such as listing date number of condos sold in last n days and n day average sell price .vii conclusions future work	large growing cities often have new real estate being built 
1	110302	10302	additional inventory coming onto the market would affect existing condo prices negatively by increasing the available supply and alternatives for buyers lastly future work should focus on adding more temporal components to the model for example through features such as listing date number of condos sold in last n days and n day average sell price .large growing cities often have new real estate being built .last sentence.vii conclusions future work	additional inventory coming onto the market would affect existing condo prices negatively by increasing the available supply and alternatives for buyers lastly future work should focus on adding more temporal components to the model for example through features such as listing date number of condos sold in last n days and n day average sell price 
0	110303	10303	we d like to thank adina dragasanu from re max crest realty for providing us the data that enabled our research .first sentence.last sentence.acknowledgment	we d like to thank adina dragasanu from re max crest realty for providing us the data that enabled our research 
0	110304	10304	there are thousands of companies coming out worldwide each year .first sentence.over the past decades there has been a rapid growth in the formation of new companies both in the us and china .abstract	there are thousands of companies coming out worldwide each year 
0	110305	10305	over the past decades there has been a rapid growth in the formation of new companies both in the us and china .there are thousands of companies coming out worldwide each year .thus it is an important and challenging task to understand what makes companies successful and to predict the success of a company .abstract	over the past decades there has been a rapid growth in the formation of new companies both in the us and china 
0	110306	10306	thus it is an important and challenging task to understand what makes companies successful and to predict the success of a company .over the past decades there has been a rapid growth in the formation of new companies both in the us and china .in this project we used crunchbase data to build a predictive model through supervised learning to classify which start ups are successful and which aren t .abstract	thus it is an important and challenging task to understand what makes companies successful and to predict the success of a company 
1	110307	10307	in this project we used crunchbase data to build a predictive model through supervised learning to classify which start ups are successful and which aren t .thus it is an important and challenging task to understand what makes companies successful and to predict the success of a company .we explored k nearest neighbours knn model on this task and compared it with logistic regression lr and random forests rf model in previous work .abstract	in this project we used crunchbase data to build a predictive model through supervised learning to classify which start ups are successful and which aren t 
1	110308	10308	we explored k nearest neighbours knn model on this task and compared it with logistic regression lr and random forests rf model in previous work .in this project we used crunchbase data to build a predictive model through supervised learning to classify which start ups are successful and which aren t .we used f1 score as the metric and found that knn model has a better performance on this task which achieves 44 45 of f1 score and 73 70 of accuracy .abstract	we explored k nearest neighbours knn model on this task and compared it with logistic regression lr and random forests rf model in previous work 
1	110309	10309	we used f1 score as the metric and found that knn model has a better performance on this task which achieves 44 45 of f1 score and 73 70 of accuracy .we explored k nearest neighbours knn model on this task and compared it with logistic regression lr and random forests rf model in previous work .last sentence.abstract	we used f1 score as the metric and found that knn model has a better performance on this task which achieves 44 45 of f1 score and 73 70 of accuracy 
0	110310	10310	thousands of companies are emerging around the world each year .first sentence.among them some are merged and acquired m a or go to public ipo while others may vanish and disappear .introduction	thousands of companies are emerging around the world each year 
0	110311	10311	among them some are merged and acquired m a or go to public ipo while others may vanish and disappear .thousands of companies are emerging around the world each year .what makes this difference and leads to the different endings for each company .introduction	among them some are merged and acquired m a or go to public ipo while others may vanish and disappear 
0	110312	10312	what makes this difference and leads to the different endings for each company .among them some are merged and acquired m a or go to public ipo while others may vanish and disappear .how to predict the success of companies .introduction	what makes this difference and leads to the different endings for each company 
0	110313	10313	how to predict the success of companies .what makes this difference and leads to the different endings for each company .if the investors can know how likely the company will achieve success given their current information they can make a better decision on the investments .introduction	how to predict the success of companies 
0	110314	10314	if the investors can know how likely the company will achieve success given their current information they can make a better decision on the investments .how to predict the success of companies .therefore in this project given some key features of a company we want to predict the probability of its success .introduction	if the investors can know how likely the company will achieve success given their current information they can make a better decision on the investments 
1	110315	10315	therefore in this project given some key features of a company we want to predict the probability of its success .if the investors can know how likely the company will achieve success given their current information they can make a better decision on the investments .more specifically the input features are of two types text features such as industry category list and location and numerical features such as the amount of money a company already raised .introduction	therefore in this project given some key features of a company we want to predict the probability of its success 
1	110316	10316	more specifically the input features are of two types text features such as industry category list and location and numerical features such as the amount of money a company already raised .therefore in this project given some key features of a company we want to predict the probability of its success .we then use logistic regression random forests and k nearest neighbours to output a predicted probability of success .introduction	more specifically the input features are of two types text features such as industry category list and location and numerical features such as the amount of money a company already raised 
0	110317	10317	we then use logistic regression random forests and k nearest neighbours to output a predicted probability of success .more specifically the input features are of two types text features such as industry category list and location and numerical features such as the amount of money a company already raised .here we define the company success as the event that gives a large sum of money to the company s founders investors and early employees specifically through a process of m a merger and acquisition or an ipo initial public offering .introduction	we then use logistic regression random forests and k nearest neighbours to output a predicted probability of success 
0	110318	10318	here we define the company success as the event that gives a large sum of money to the company s founders investors and early employees specifically through a process of m a merger and acquisition or an ipo initial public offering .we then use logistic regression random forests and k nearest neighbours to output a predicted probability of success .last sentence.introduction	here we define the company success as the event that gives a large sum of money to the company s founders investors and early employees specifically through a process of m a merger and acquisition or an ipo initial public offering 
1	110319	10319	as machine learning becomes a more and more popular tool for researchers to utilize in the field of finance and investment we have found some related work to predict companies business success with machine learning and crunchbase bento lisin and nesterenko indeed these works propose a variety of efficient methods that we can use to predict the success of company .first sentence.however we notice that none of them implement k nearest neighbours model .related work	as machine learning becomes a more and more popular tool for researchers to utilize in the field of finance and investment we have found some related work to predict companies business success with machine learning and crunchbase bento lisin and nesterenko indeed these works propose a variety of efficient methods that we can use to predict the success of company 
0	110320	10320	however we notice that none of them implement k nearest neighbours model .as machine learning becomes a more and more popular tool for researchers to utilize in the field of finance and investment we have found some related work to predict companies business success with machine learning and crunchbase bento lisin and nesterenko indeed these works propose a variety of efficient methods that we can use to predict the success of company .in this project we aim to apply knn model to solving this problem .related work	however we notice that none of them implement k nearest neighbours model 
0	110321	10321	in this project we aim to apply knn model to solving this problem .however we notice that none of them implement k nearest neighbours model .last sentence.related work	in this project we aim to apply knn model to solving this problem 
1	110322	10322	the dataset we used was extracted from crunchbase data export containing 60k companies information updated to december 2015 .first sentence.there were four data files named company investments rounds and acquisition .dataset and features	the dataset we used was extracted from crunchbase data export containing 60k companies information updated to december 2015 
0	110323	10323	there were four data files named company investments rounds and acquisition .the dataset we used was extracted from crunchbase data export containing 60k companies information updated to december 2015 .the company file contains most comprehensive information of the companies while other files contains more detailed information regarding the investment operations .dataset and features	there were four data files named company investments rounds and acquisition 
0	110324	10324	the company file contains most comprehensive information of the companies while other files contains more detailed information regarding the investment operations .there were four data files named company investments rounds and acquisition .thus we chose the file company as the base and extracted meaningful features from other files to add into it .dataset and features	the company file contains most comprehensive information of the companies while other files contains more detailed information regarding the investment operations 
0	110325	10325	thus we chose the file company as the base and extracted meaningful features from other files to add into it .the company file contains most comprehensive information of the companies while other files contains more detailed information regarding the investment operations .last sentence.dataset and features	thus we chose the file company as the base and extracted meaningful features from other files to add into it 
0	110326	10326	the company dataset consists the following columns name company s name.first sentence.last sentence.dataset overview	the company dataset consists the following columns name company s name
0	110327	10327	we labeled the company that has m a with 1 otherwise 0 .first sentence.we plotted the amount of the 0 or 1 labeled data as we noticed some skewness regarding the distribution of date of funding events in this dataset as shown in.cleaning and labeling	we labeled the company that has m a with 1 otherwise 0 
1	110328	10328	we plotted the amount of the 0 or 1 labeled data as we noticed some skewness regarding the distribution of date of funding events in this dataset as shown in.we labeled the company that has m a with 1 otherwise 0 .last sentence.cleaning and labeling	we plotted the amount of the 0 or 1 labeled data as we noticed some skewness regarding the distribution of date of funding events in this dataset as shown in
1	110329	10329	we selected the most essential features to companies business success and end up with input features as category country funding rounds funding total usd and the difference between when first funding at and last funding at the training set is composed of two parts .first sentence.the first part of data is the numerical data number of funding rounds and total funding .feature selection	we selected the most essential features to companies business success and end up with input features as category country funding rounds funding total usd and the difference between when first funding at and last funding at the training set is composed of two parts 
1	110330	10330	the first part of data is the numerical data number of funding rounds and total funding .we selected the most essential features to companies business success and end up with input features as category country funding rounds funding total usd and the difference between when first funding at and last funding at the training set is composed of two parts .the second part of data is the date in string format such as first funding at final funding at and funded at columns .feature selection	the first part of data is the numerical data number of funding rounds and total funding 
0	110331	10331	the second part of data is the date in string format such as first funding at final funding at and funded at columns .the first part of data is the numerical data number of funding rounds and total funding .as there are too many missing data for funded at we finally chose first funding at and final funding at columns converted them from timestamp to numerical utc format and calculated a duration column with the subtracted data .feature selection	the second part of data is the date in string format such as first funding at final funding at and funded at columns 
1	110332	10332	as there are too many missing data for funded at we finally chose first funding at and final funding at columns converted them from timestamp to numerical utc format and calculated a duration column with the subtracted data .the second part of data is the date in string format such as first funding at final funding at and funded at columns .last sentence.feature selection	as there are too many missing data for funded at we finally chose first funding at and final funding at columns converted them from timestamp to numerical utc format and calculated a duration column with the subtracted data 
1	110333	10333	the goal of this project is to make a binary prediction on the status of start ups whether they have gone through m a or ipo .first sentence.in this project we explored logistic regression random forests and k nearest neighbors .methods	the goal of this project is to make a binary prediction on the status of start ups whether they have gone through m a or ipo 
1	110334	10334	in this project we explored logistic regression random forests and k nearest neighbors .the goal of this project is to make a binary prediction on the status of start ups whether they have gone through m a or ipo .last sentence.methods	in this project we explored logistic regression random forests and k nearest neighbors 
0	110335	10335	logistic regression is a simple algorithm that is commonly used in binary classification .first sentence.due to its efficiency it is the first model we selected to do the classification .logistic regression	logistic regression is a simple algorithm that is commonly used in binary classification 
0	110336	10336	due to its efficiency it is the first model we selected to do the classification .logistic regression is a simple algorithm that is commonly used in binary classification .the hypothesis of logistic regression algorithm is as follows the algorithm optimize by maximizing the following log likelihood function .logistic regression	due to its efficiency it is the first model we selected to do the classification 
0	110337	10337	the hypothesis of logistic regression algorithm is as follows the algorithm optimize by maximizing the following log likelihood function .due to its efficiency it is the first model we selected to do the classification .last sentence.logistic regression	the hypothesis of logistic regression algorithm is as follows the algorithm optimize by maximizing the following log likelihood function 
1	110338	10338	random forests construct a multitude of decision trees at training time and outputting the mode of the classification result of individual trees .first sentence.at each split point in the decision tree only a subset of features are selected to take into consideration by the algorithm .random forests	random forests construct a multitude of decision trees at training time and outputting the mode of the classification result of individual trees 
1	110339	10339	at each split point in the decision tree only a subset of features are selected to take into consideration by the algorithm .random forests construct a multitude of decision trees at training time and outputting the mode of the classification result of individual trees .the candidate features are generated using bootstrap .random forests	at each split point in the decision tree only a subset of features are selected to take into consideration by the algorithm 
0	110340	10340	the candidate features are generated using bootstrap .at each split point in the decision tree only a subset of features are selected to take into consideration by the algorithm .compared to an individual tree bootstrapping mitigates the variance by averaging the results of a large number of decision trees .random forests	the candidate features are generated using bootstrap 
1	110341	10341	compared to an individual tree bootstrapping mitigates the variance by averaging the results of a large number of decision trees .the candidate features are generated using bootstrap .last sentence.random forests	compared to an individual tree bootstrapping mitigates the variance by averaging the results of a large number of decision trees 
0	110342	10342	an instance is classified by a majority vote of its k nearest neighbours .first sentence.the algorithm assigns class j to x i that maximizes .k nearest neighbors	an instance is classified by a majority vote of its k nearest neighbours 
0	110343	10343	the algorithm assigns class j to x i that maximizes .an instance is classified by a majority vote of its k nearest neighbours .last sentence.k nearest neighbors	the algorithm assigns class j to x i that maximizes 
0	110344	10344	in a confusion matrix we describe the performance of a classification model .first sentence.each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class vice versa .selected metrics	in a confusion matrix we describe the performance of a classification model 
0	110345	10345	each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class vice versa .in a confusion matrix we describe the performance of a classification model .there are four basic terms in a confusion matrix here we select three metrics accuracy f1 score and auc score accuracy the proportion we have predicted right .selected metrics	each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class vice versa 
1	110346	10346	there are four basic terms in a confusion matrix here we select three metrics accuracy f1 score and auc score accuracy the proportion we have predicted right .each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class vice versa .false positive rate fpr f p f p t nf1 score auc score area under the roc curve auc score area under roc curve total area.selected metrics	there are four basic terms in a confusion matrix here we select three metrics accuracy f1 score and auc score accuracy the proportion we have predicted right 
1	110347	10347	false positive rate fpr f p f p t nf1 score auc score area under the roc curve auc score area under roc curve total area.there are four basic terms in a confusion matrix here we select three metrics accuracy f1 score and auc score accuracy the proportion we have predicted right .last sentence.selected metrics	false positive rate fpr f p f p t nf1 score auc score area under the roc curve auc score area under roc curve total area
1	110348	10348	to utilize more data in the training we split the dataset into three parts 95 data as training set 5 as cross validation set and 5 as test set .first sentence.since the dataset is quite imbalanced we up sample the minority class label 1 in the training set to balance the data but keep the cross validation set and test set untouched see we also normalize all the numerical features such as funding rounds and funding duration and use bag of words to encode the text features such as category list and country code .data processing	to utilize more data in the training we split the dataset into three parts 95 data as training set 5 as cross validation set and 5 as test set 
1	110349	10349	since the dataset is quite imbalanced we up sample the minority class label 1 in the training set to balance the data but keep the cross validation set and test set untouched see we also normalize all the numerical features such as funding rounds and funding duration and use bag of words to encode the text features such as category list and country code .to utilize more data in the training we split the dataset into three parts 95 data as training set 5 as cross validation set and 5 as test set .last sentence.data processing	since the dataset is quite imbalanced we up sample the minority class label 1 in the training set to balance the data but keep the cross validation set and test set untouched see we also normalize all the numerical features such as funding rounds and funding duration and use bag of words to encode the text features such as category list and country code 
1	110350	10350	after preprocessing the data we concatenate the two types of features and feed them to logistic regression model random forest model and k nearest neighbours model .first sentence.for random forest and k nearest neighbors model we used random search to tune the hyperparameters .hyperparameter tuning	after preprocessing the data we concatenate the two types of features and feed them to logistic regression model random forest model and k nearest neighbours model 
1	110351	10351	for random forest and k nearest neighbors model we used random search to tune the hyperparameters .after preprocessing the data we concatenate the two types of features and feed them to logistic regression model random forest model and k nearest neighbours model .a list of hyperparameters and their associated range is summarized in the table below see.hyperparameter tuning	for random forest and k nearest neighbors model we used random search to tune the hyperparameters 
0	110352	10352	a list of hyperparameters and their associated range is summarized in the table below see.for random forest and k nearest neighbors model we used random search to tune the hyperparameters .last sentence.hyperparameter tuning	a list of hyperparameters and their associated range is summarized in the table below see
1	110353	10353	5 50 k number of neighbours 10 100 we use accuracy f1 score and auc score to compare the performance of different models but the f1 score is our primary metric .first sentence.the figure below summarize the results of each model on the validation set see.hyperparameters range number of trees in rf 	5 50 k number of neighbours 10 100 we use accuracy f1 score and auc score to compare the performance of different models but the f1 score is our primary metric 
1	110354	10354	the figure below summarize the results of each model on the validation set see.5 50 k number of neighbours 10 100 we use accuracy f1 score and auc score to compare the performance of different models but the f1 score is our primary metric .last sentence.hyperparameters range number of trees in rf 	the figure below summarize the results of each model on the validation set see
0	110355	10355	in the future we should include more features of the companies and examine which features are more significant than others .first sentence.also we will try more complex models such as neural network and pre trained word embedding .future work	in the future we should include more features of the companies and examine which features are more significant than others 
1	110356	10356	also we will try more complex models such as neural network and pre trained word embedding .in the future we should include more features of the companies and examine which features are more significant than others .using kernel method to move the data to higher dimensional space is also a good direction .future work	also we will try more complex models such as neural network and pre trained word embedding 
0	110357	10357	using kernel method to move the data to higher dimensional space is also a good direction .also we will try more complex models such as neural network and pre trained word embedding .in addition more new questions are to explore such as predicting the total funding size for a company regression problem .future work	using kernel method to move the data to higher dimensional space is also a good direction 
1	110358	10358	in addition more new questions are to explore such as predicting the total funding size for a company regression problem .using kernel method to move the data to higher dimensional space is also a good direction .last sentence.future work	in addition more new questions are to explore such as predicting the total funding size for a company regression problem 
0	110359	10359	welcome to check our code here https github com chenchenpan predict success of startups.first sentence.last sentence.github repository	welcome to check our code here https github com chenchenpan predict success of startups
0	110360	10360	thank you to the cs 229 teaching staff including prof andrew ng and the tas .first sentence.last sentence.acknowledgments	thank you to the cs 229 teaching staff including prof andrew ng and the tas 
1	110361	10361	we construct using equation 1 the outcome variables the aud basis and the jpy basis .first sentence.interest rates are taken to be the fixed rate in overnight index swaps against central bank policy rates .dataset and features	we construct using equation 1 the outcome variables the aud basis and the jpy basis 
0	110362	10362	interest rates are taken to be the fixed rate in overnight index swaps against central bank policy rates .we construct using equation 1 the outcome variables the aud basis and the jpy basis .we further collect 32 data series for each of the three relevant currencies aud jpy and usd .dataset and features	interest rates are taken to be the fixed rate in overnight index swaps against central bank policy rates 
0	110363	10363	we further collect 32 data series for each of the three relevant currencies aud jpy and usd .interest rates are taken to be the fixed rate in overnight index swaps against central bank policy rates .our data capture activities in the financial markets conditions of the economy the state of international trade and the stance of economic policies see we first pre process the data by computing and using for most of the features the percentage change between two observations .dataset and features	we further collect 32 data series for each of the three relevant currencies aud jpy and usd 
1	110364	10364	our data capture activities in the financial markets conditions of the economy the state of international trade and the stance of economic policies see we first pre process the data by computing and using for most of the features the percentage change between two observations .we further collect 32 data series for each of the three relevant currencies aud jpy and usd .this is done to both normalize and extract more meaningful information .dataset and features	our data capture activities in the financial markets conditions of the economy the state of international trade and the stance of economic policies see we first pre process the data by computing and using for most of the features the percentage change between two observations 
0	110365	10365	this is done to both normalize and extract more meaningful information .our data capture activities in the financial markets conditions of the economy the state of international trade and the stance of economic policies see we first pre process the data by computing and using for most of the features the percentage change between two observations .we next augment the data in two ways .dataset and features	this is done to both normalize and extract more meaningful information 
0	110366	10366	we next augment the data in two ways .this is done to both normalize and extract more meaningful information .first while all financial markets data are reported daily other data are available only monthly or quarterly for low frequency series we impute daily observations based on the last available entry .dataset and features	we next augment the data in two ways 
0	110367	10367	first while all financial markets data are reported daily other data are available only monthly or quarterly for low frequency series we impute daily observations based on the last available entry .we next augment the data in two ways .second we interact and pairwise interact all features these additional features are used in the polynomial specification of regularized regressions .dataset and features	first while all financial markets data are reported daily other data are available only monthly or quarterly for low frequency series we impute daily observations based on the last available entry 
1	110368	10368	second we interact and pairwise interact all features these additional features are used in the polynomial specification of regularized regressions .first while all financial markets data are reported daily other data are available only monthly or quarterly for low frequency series we impute daily observations based on the last available entry .with the cleaned data set we construct two distinct samples that emphasize different aspects of the data .dataset and features	second we interact and pairwise interact all features these additional features are used in the polynomial specification of regularized regressions 
1	110369	10369	with the cleaned data set we construct two distinct samples that emphasize different aspects of the data .second we interact and pairwise interact all features these additional features are used in the polynomial specification of regularized regressions .the complete sample retains the longest possible time horizon by including series that are available between 2004 and 2017 .dataset and features	with the cleaned data set we construct two distinct samples that emphasize different aspects of the data 
0	110370	10370	the complete sample retains the longest possible time horizon by including series that are available between 2004 and 2017 .with the cleaned data set we construct two distinct samples that emphasize different aspects of the data .the post crisis finally we split each of our two samples into test vs training sets in two different ways .dataset and features	the complete sample retains the longest possible time horizon by including series that are available between 2004 and 2017 
0	110371	10371	the post crisis finally we split each of our two samples into test vs training sets in two different ways .the complete sample retains the longest possible time horizon by including series that are available between 2004 and 2017 .in both cases we arrive at a test set of 400 observations which is about a year and half in calendar days .dataset and features	the post crisis finally we split each of our two samples into test vs training sets in two different ways 
1	110372	10372	in both cases we arrive at a test set of 400 observations which is about a year and half in calendar days .the post crisis finally we split each of our two samples into test vs training sets in two different ways .the contiguous split uses as test set the last 200 observations and the middle 200 observations in the post crisis period this method emphasizes the time series nature of our outcome variable .dataset and features	in both cases we arrive at a test set of 400 observations which is about a year and half in calendar days 
1	110373	10373	the contiguous split uses as test set the last 200 observations and the middle 200 observations in the post crisis period this method emphasizes the time series nature of our outcome variable .in both cases we arrive at a test set of 400 observations which is about a year and half in calendar days .the random split uses as test set 400 randomly chosen observations in the post crisis period which reflects more of a cross sectional test of the basis predictions ultimately each of our ml models is applied to 8 distinct sample split combination for each of the aud and jpy bases we have either the complete or the post crisis sample and within each sample we split train vs test using either a contiguous or a random approach .dataset and features	the contiguous split uses as test set the last 200 observations and the middle 200 observations in the post crisis period this method emphasizes the time series nature of our outcome variable 
1	110374	10374	the random split uses as test set 400 randomly chosen observations in the post crisis period which reflects more of a cross sectional test of the basis predictions ultimately each of our ml models is applied to 8 distinct sample split combination for each of the aud and jpy bases we have either the complete or the post crisis sample and within each sample we split train vs test using either a contiguous or a random approach .the contiguous split uses as test set the last 200 observations and the middle 200 observations in the post crisis period this method emphasizes the time series nature of our outcome variable .last sentence.dataset and features	the random split uses as test set 400 randomly chosen observations in the post crisis period which reflects more of a cross sectional test of the basis predictions ultimately each of our ml models is applied to 8 distinct sample split combination for each of the aud and jpy bases we have either the complete or the post crisis sample and within each sample we split train vs test using either a contiguous or a random approach 
1	110375	10375	regularized linear regressions extend the ordinary least squares regression algorithm by allowing regularizations on the fitted model parameters .first sentence.regularization prevents the linear regression from overfitting especially when a large set of features are present .regularized regression	regularized linear regressions extend the ordinary least squares regression algorithm by allowing regularizations on the fitted model parameters 
1	110376	10376	regularization prevents the linear regression from overfitting especially when a large set of features are present .regularized linear regressions extend the ordinary least squares regression algorithm by allowing regularizations on the fitted model parameters .any regularized linear regression solves the following optimization program where is a hyperparameter that determines the strength of regularization and determines the specific regularization function employed setting 0 corresponds to regularizating l 1 norm which is also known as lasso regression .regularized regression	regularization prevents the linear regression from overfitting especially when a large set of features are present 
1	110377	10377	any regularized linear regression solves the following optimization program where is a hyperparameter that determines the strength of regularization and determines the specific regularization function employed setting 0 corresponds to regularizating l 1 norm which is also known as lasso regression .regularization prevents the linear regression from overfitting especially when a large set of features are present .this regularization will set less relevant s to zero thereby achieving model parsimony .regularized regression	any regularized linear regression solves the following optimization program where is a hyperparameter that determines the strength of regularization and determines the specific regularization function employed setting 0 corresponds to regularizating l 1 norm which is also known as lasso regression 
1	110378	10378	this regularization will set less relevant s to zero thereby achieving model parsimony .any regularized linear regression solves the following optimization program where is a hyperparameter that determines the strength of regularization and determines the specific regularization function employed setting 0 corresponds to regularizating l 1 norm which is also known as lasso regression .an 1 corresponds to using l 2 regularization or ridge regression .regularized regression	this regularization will set less relevant s to zero thereby achieving model parsimony 
1	110379	10379	an 1 corresponds to using l 2 regularization or ridge regression .this regularization will set less relevant s to zero thereby achieving model parsimony .it shrinks the coefficients of variables that are highly correlated .regularized regression	an 1 corresponds to using l 2 regularization or ridge regression 
1	110380	10380	it shrinks the coefficients of variables that are highly correlated .an 1 corresponds to using l 2 regularization or ridge regression .both of these algorithms also have a bayesian interpretation with lasso corresponding to a laplace prior and ridge to a normal prior over the regression coefficients .regularized regression	it shrinks the coefficients of variables that are highly correlated 
1	110381	10381	both of these algorithms also have a bayesian interpretation with lasso corresponding to a laplace prior and ridge to a normal prior over the regression coefficients .it shrinks the coefficients of variables that are highly correlated .finally we also consider an elastic net algorithm with 1 2 .regularized regression	both of these algorithms also have a bayesian interpretation with lasso corresponding to a laplace prior and ridge to a normal prior over the regression coefficients 
1	110382	10382	finally we also consider an elastic net algorithm with 1 2 .both of these algorithms also have a bayesian interpretation with lasso corresponding to a laplace prior and ridge to a normal prior over the regression coefficients .this trades off the two previous regularization methods in addition to linear features we also consider a second degree polynomial of the features in these regularized regressions in order to capture non linearities .regularized regression	finally we also consider an elastic net algorithm with 1 2 
1	110383	10383	this trades off the two previous regularization methods in addition to linear features we also consider a second degree polynomial of the features in these regularized regressions in order to capture non linearities .finally we also consider an elastic net algorithm with 1 2 .all features and the y are standardized when training the algorithm in order for the regularization to work as intended .regularized regression	this trades off the two previous regularization methods in addition to linear features we also consider a second degree polynomial of the features in these regularized regressions in order to capture non linearities 
1	110384	10384	all features and the y are standardized when training the algorithm in order for the regularization to work as intended .this trades off the two previous regularization methods in addition to linear features we also consider a second degree polynomial of the features in these regularized regressions in order to capture non linearities .we implement all algorithms in the statistical software r.regularized regression	all features and the y are standardized when training the algorithm in order for the regularization to work as intended 
0	110385	10385	we implement all algorithms in the statistical software r.all features and the y are standardized when training the algorithm in order for the regularization to work as intended .last sentence.regularized regression	we implement all algorithms in the statistical software r
1	110386	10386	an algorithm might need to capture higher dimensional feature interactions in order to predict well .first sentence.a regression tree allows for considering such non linear interactions among features .regression trees	an algorithm might need to capture higher dimensional feature interactions in order to predict well 
1	110387	10387	a regression tree allows for considering such non linear interactions among features .an algorithm might need to capture higher dimensional feature interactions in order to predict well .in each step the tree splits the data in one node the parent node into two subsets children nodes based on the value of one feature .regression trees	a regression tree allows for considering such non linear interactions among features 
1	110388	10388	in each step the tree splits the data in one node the parent node into two subsets children nodes based on the value of one feature .a regression tree allows for considering such non linear interactions among features .the splitting rule is chosen to minimize the purity of the children nodes .regression trees	in each step the tree splits the data in one node the parent node into two subsets children nodes based on the value of one feature 
1	110389	10389	the splitting rule is chosen to minimize the purity of the children nodes .in each step the tree splits the data in one node the parent node into two subsets children nodes based on the value of one feature .the algorithm stops once the purity of the children nodes does not improve over the purity of the parent node .regression trees	the splitting rule is chosen to minimize the purity of the children nodes 
1	110390	10390	the algorithm stops once the purity of the children nodes does not improve over the purity of the parent node .the splitting rule is chosen to minimize the purity of the children nodes .the prediction in each node j that contains m j training examples is j and the purity of a node d j is calculated with the deviance measure we implement the regression tree algorithm using the r package tree.regression trees	the algorithm stops once the purity of the children nodes does not improve over the purity of the parent node 
1	110391	10391	the prediction in each node j that contains m j training examples is j and the purity of a node d j is calculated with the deviance measure we implement the regression tree algorithm using the r package tree.the algorithm stops once the purity of the children nodes does not improve over the purity of the parent node .last sentence.regression trees	the prediction in each node j that contains m j training examples is j and the purity of a node d j is calculated with the deviance measure we implement the regression tree algorithm using the r package tree
0	110392	10392	regression tree algorithms can exhibit high variance .first sentence.this problem can be remedied using a random forest .random forest	regression tree algorithms can exhibit high variance 
0	110393	10393	this problem can be remedied using a random forest .regression tree algorithms can exhibit high variance .the random forest uses bootstrap to grow multiple trees and returns the average prediction of those trees as its final prediction .random forest	this problem can be remedied using a random forest 
1	110394	10394	the random forest uses bootstrap to grow multiple trees and returns the average prediction of those trees as its final prediction .this problem can be remedied using a random forest .it can be shown that the variance of a random forest containing n trees each with variance 2 and correlation is hence the overall variance can be decreased by choosing a high number of trees n we choose to grow 2000 and decorrelating the trees to achieve a low .random forest	the random forest uses bootstrap to grow multiple trees and returns the average prediction of those trees as its final prediction 
1	110395	10395	it can be shown that the variance of a random forest containing n trees each with variance 2 and correlation is hence the overall variance can be decreased by choosing a high number of trees n we choose to grow 2000 and decorrelating the trees to achieve a low .the random forest uses bootstrap to grow multiple trees and returns the average prediction of those trees as its final prediction .the forest decorrelates its trees in two ways .random forest	it can be shown that the variance of a random forest containing n trees each with variance 2 and correlation is hence the overall variance can be decreased by choosing a high number of trees n we choose to grow 2000 and decorrelating the trees to achieve a low 
0	110396	10396	the forest decorrelates its trees in two ways .it can be shown that the variance of a random forest containing n trees each with variance 2 and correlation is hence the overall variance can be decreased by choosing a high number of trees n we choose to grow 2000 and decorrelating the trees to achieve a low .first each tree is grown out of a bootstrapped sample which is different for each tree .random forest	the forest decorrelates its trees in two ways 
1	110397	10397	first each tree is grown out of a bootstrapped sample which is different for each tree .the forest decorrelates its trees in two ways .moreover at each node the algorithm only considers splitting on a random sub sample of all available features .random forest	first each tree is grown out of a bootstrapped sample which is different for each tree 
1	110398	10398	moreover at each node the algorithm only considers splitting on a random sub sample of all available features .first each tree is grown out of a bootstrapped sample which is different for each tree .the size of this sub sample is mtry and is a hyperparameter that we will tune .random forest	moreover at each node the algorithm only considers splitting on a random sub sample of all available features 
1	110399	10399	the size of this sub sample is mtry and is a hyperparameter that we will tune .moreover at each node the algorithm only considers splitting on a random sub sample of all available features .each tree in the forest grows until it reaches a minimum number of terminal nodes leaves and that is set to five in our case .random forest	the size of this sub sample is mtry and is a hyperparameter that we will tune 
0	110400	10400	each tree in the forest grows until it reaches a minimum number of terminal nodes leaves and that is set to five in our case .the size of this sub sample is mtry and is a hyperparameter that we will tune .these measures contribute to less correlated trees and a lower overall variance of the random forest .random forest	each tree in the forest grows until it reaches a minimum number of terminal nodes leaves and that is set to five in our case 
1	110401	10401	these measures contribute to less correlated trees and a lower overall variance of the random forest .each tree in the forest grows until it reaches a minimum number of terminal nodes leaves and that is set to five in our case .we implement the random forest algorithm using the r package ranger.random forest	these measures contribute to less correlated trees and a lower overall variance of the random forest 
1	110402	10402	we implement the random forest algorithm using the r package ranger.these measures contribute to less correlated trees and a lower overall variance of the random forest .last sentence.random forest	we implement the random forest algorithm using the r package ranger
1	110403	10403	before presenting the ml results we discuss the tuning of two key hyper parameters in regularized regressions and mtry in random forest across lasso ridge and elastic net we tune the strength of our regularization penalty via the one standard error approach .first sentence.that is the glmnet package supplies 100 s we choose the optimal as the value that s one standard error away from the that minimizes the cross validation error so as to reflect estimation errors inherent in calculating mses random forest has many degrees of freedom .results and discussions	before presenting the ml results we discuss the tuning of two key hyper parameters in regularized regressions and mtry in random forest across lasso ridge and elastic net we tune the strength of our regularization penalty via the one standard error approach 
1	110404	10404	that is the glmnet package supplies 100 s we choose the optimal as the value that s one standard error away from the that minimizes the cross validation error so as to reflect estimation errors inherent in calculating mses random forest has many degrees of freedom .before presenting the ml results we discuss the tuning of two key hyper parameters in regularized regressions and mtry in random forest across lasso ridge and elastic net we tune the strength of our regularization penalty via the one standard error approach .we focus on optimizing mtry or the number of randomly chosen features a node could split on .results and discussions	that is the glmnet package supplies 100 s we choose the optimal as the value that s one standard error away from the that minimizes the cross validation error so as to reflect estimation errors inherent in calculating mses random forest has many degrees of freedom 
1	110405	10405	we focus on optimizing mtry or the number of randomly chosen features a node could split on .that is the glmnet package supplies 100 s we choose the optimal as the value that s one standard error away from the that minimizes the cross validation error so as to reflect estimation errors inherent in calculating mses random forest has many degrees of freedom .splitting only on a subset of features at each node reduces correlation among trees and drives down variance of the overall model .results and discussions	we focus on optimizing mtry or the number of randomly chosen features a node could split on 
1	110406	10406	splitting only on a subset of features at each node reduces correlation among trees and drives down variance of the overall model .we focus on optimizing mtry or the number of randomly chosen features a node could split on .for each of the 8 random forests one on each sample split we ran ranger with mtry 5 6 7 15 .results and discussions	splitting only on a subset of features at each node reduces correlation among trees and drives down variance of the overall model 
0	110407	10407	for each of the 8 random forests one on each sample split we ran ranger with mtry 5 6 7 15 .splitting only on a subset of features at each node reduces correlation among trees and drives down variance of the overall model .the heuristic is to set mtry equal to the square root of feature dimension which would be 7 or 8 depending on the sample split .results and discussions	for each of the 8 random forests one on each sample split we ran ranger with mtry 5 6 7 15 
1	110408	10408	the heuristic is to set mtry equal to the square root of feature dimension which would be 7 or 8 depending on the sample split .for each of the 8 random forests one on each sample split we ran ranger with mtry 5 6 7 15 .we choose the optimal mtry to be the number that minimizes the out of bag mse in the training set .results and discussions	the heuristic is to set mtry equal to the square root of feature dimension which would be 7 or 8 depending on the sample split 
1	110409	10409	we choose the optimal mtry to be the number that minimizes the out of bag mse in the training set .the heuristic is to set mtry equal to the square root of feature dimension which would be 7 or 8 depending on the sample split .our final mtry s include four 12 s four 14 s one 10 and one 15 .results and discussions	we choose the optimal mtry to be the number that minimizes the out of bag mse in the training set 
0	110410	10410	our final mtry s include four 12 s four 14 s one 10 and one 15 .we choose the optimal mtry to be the number that minimizes the out of bag mse in the training set .we set the other parameters of the forest to 2000 trees and minimum of 5 leaves per node the performance of all eight of our ml models on the four contiguous splits are summarized in the mse on the random splits are not shown due to space constraint .results and discussions	our final mtry s include four 12 s four 14 s one 10 and one 15 
1	110411	10411	we set the other parameters of the forest to 2000 trees and minimum of 5 leaves per node the performance of all eight of our ml models on the four contiguous splits are summarized in the mse on the random splits are not shown due to space constraint .our final mtry s include four 12 s four 14 s one 10 and one 15 .the performance on the random splits are stunningly good the random forests generate mses on the test set of between 10 to 20 which is incredibly small compared to the 10 to 25 standard deviation of the outcome variables .results and discussions	we set the other parameters of the forest to 2000 trees and minimum of 5 leaves per node the performance of all eight of our ml models on the four contiguous splits are summarized in the mse on the random splits are not shown due to space constraint 
1	110412	10412	the performance on the random splits are stunningly good the random forests generate mses on the test set of between 10 to 20 which is incredibly small compared to the 10 to 25 standard deviation of the outcome variables .we set the other parameters of the forest to 2000 trees and minimum of 5 leaves per node the performance of all eight of our ml models on the four contiguous splits are summarized in the mse on the random splits are not shown due to space constraint .however we embrace this success with reservation as it is difficult to interpret randomly selected observations with imputed values focusing our analysis on the performance on the contiguous splits we highlight three takeaways .results and discussions	the performance on the random splits are stunningly good the random forests generate mses on the test set of between 10 to 20 which is incredibly small compared to the 10 to 25 standard deviation of the outcome variables 
1	110413	10413	however we embrace this success with reservation as it is difficult to interpret randomly selected observations with imputed values focusing our analysis on the performance on the contiguous splits we highlight three takeaways .the performance on the random splits are stunningly good the random forests generate mses on the test set of between 10 to 20 which is incredibly small compared to the 10 to 25 standard deviation of the outcome variables .first random forests achieve strong prediction performance .results and discussions	however we embrace this success with reservation as it is difficult to interpret randomly selected observations with imputed values focusing our analysis on the performance on the contiguous splits we highlight three takeaways 
0	110414	10414	first random forests achieve strong prediction performance .however we embrace this success with reservation as it is difficult to interpret randomly selected observations with imputed values focusing our analysis on the performance on the contiguous splits we highlight three takeaways .forests not only have the lowest mse in test sets in all but one sample but do so with a substantial margin .results and discussions	first random forests achieve strong prediction performance 
1	110415	10415	forests not only have the lowest mse in test sets in all but one sample but do so with a substantial margin .first random forests achieve strong prediction performance .comparing to regularized regressions forests allow non linear effects and compare to regression trees forests lower variance by bagging and splitting on only a subset of features at each node .results and discussions	forests not only have the lowest mse in test sets in all but one sample but do so with a substantial margin 
1	110416	10416	comparing to regularized regressions forests allow non linear effects and compare to regression trees forests lower variance by bagging and splitting on only a subset of features at each node .forests not only have the lowest mse in test sets in all but one sample but do so with a substantial margin .the superior performance of forests suggest that these are two important considerations we plot in second regularized regressions are informative about bias vs variance in the prediction .results and discussions	comparing to regularized regressions forests allow non linear effects and compare to regression trees forests lower variance by bagging and splitting on only a subset of features at each node 
1	110417	10417	the superior performance of forests suggest that these are two important considerations we plot in second regularized regressions are informative about bias vs variance in the prediction .comparing to regularized regressions forests allow non linear effects and compare to regression trees forests lower variance by bagging and splitting on only a subset of features at each node .looking at the mses in the training vs test sets across the linear vs polynomial specifications of regularized regressions we note that the prediction error in the aud basis is likely caused by a bias problem as the mse decreases with the inclusion of the higher dimensional features in both the training and the test set .results and discussions	the superior performance of forests suggest that these are two important considerations we plot in second regularized regressions are informative about bias vs variance in the prediction 
1	110418	10418	looking at the mses in the training vs test sets across the linear vs polynomial specifications of regularized regressions we note that the prediction error in the aud basis is likely caused by a bias problem as the mse decreases with the inclusion of the higher dimensional features in both the training and the test set .the superior performance of forests suggest that these are two important considerations we plot in second regularized regressions are informative about bias vs variance in the prediction .in contrast the results in jpy basis indicates a variance problem i e .results and discussions	looking at the mses in the training vs test sets across the linear vs polynomial specifications of regularized regressions we note that the prediction error in the aud basis is likely caused by a bias problem as the mse decreases with the inclusion of the higher dimensional features in both the training and the test set 
1	110419	10419	in contrast the results in jpy basis indicates a variance problem i e .looking at the mses in the training vs test sets across the linear vs polynomial specifications of regularized regressions we note that the prediction error in the aud basis is likely caused by a bias problem as the mse decreases with the inclusion of the higher dimensional features in both the training and the test set .overfitting as the polynomial improves the training error but increases the test error finally performance differ dramatically in the middle vs end test blocks .results and discussions	in contrast the results in jpy basis indicates a variance problem i e 
1	110420	10420	overfitting as the polynomial improves the training error but increases the test error finally performance differ dramatically in the middle vs end test blocks .in contrast the results in jpy basis indicates a variance problem i e .in results not shown due to space constraint we note that all models have respectable performance on the test block taken from the middle of the post crisis period .results and discussions	overfitting as the polynomial improves the training error but increases the test error finally performance differ dramatically in the middle vs end test blocks 
1	110421	10421	in results not shown due to space constraint we note that all models have respectable performance on the test block taken from the middle of the post crisis period .overfitting as the polynomial improves the training error but increases the test error finally performance differ dramatically in the middle vs end test blocks .yet most models struggle with predictions in the last 200 observations .results and discussions	in results not shown due to space constraint we note that all models have respectable performance on the test block taken from the middle of the post crisis period 
0	110422	10422	yet most models struggle with predictions in the last 200 observations .in results not shown due to space constraint we note that all models have respectable performance on the test block taken from the middle of the post crisis period .one potential reason is that outcome variables in this period exhibit patterns that have hitherto not been observed low variance elevated level and are thus difficult to predict via a supervised learning algorithm .results and discussions	yet most models struggle with predictions in the last 200 observations 
1	110423	10423	one potential reason is that outcome variables in this period exhibit patterns that have hitherto not been observed low variance elevated level and are thus difficult to predict via a supervised learning algorithm .yet most models struggle with predictions in the last 200 observations .last sentence.results and discussions	one potential reason is that outcome variables in this period exhibit patterns that have hitherto not been observed low variance elevated level and are thus difficult to predict via a supervised learning algorithm 
0	110424	10424	violations of the covered interest rate parity condition are important phenomena in the global foreign exchange market and a better understanding of the cross currency basis can have profound implications on the theory of asset pricing .first sentence.in this project we take a step toward this understanding by predicting bases using machine learning techniques .conclusions and future work	violations of the covered interest rate parity condition are important phenomena in the global foreign exchange market and a better understanding of the cross currency basis can have profound implications on the theory of asset pricing 
1	110425	10425	in this project we take a step toward this understanding by predicting bases using machine learning techniques .violations of the covered interest rate parity condition are important phenomena in the global foreign exchange market and a better understanding of the cross currency basis can have profound implications on the theory of asset pricing .we find that random forests achieve fairly good predictions as measured by mse on test sets that encompass two separate blocks of observations in the post crisis period .conclusions and future work	in this project we take a step toward this understanding by predicting bases using machine learning techniques 
1	110426	10426	we find that random forests achieve fairly good predictions as measured by mse on test sets that encompass two separate blocks of observations in the post crisis period .in this project we take a step toward this understanding by predicting bases using machine learning techniques .this performance likely owes to random forest s ability to flexibly introduce non linear feature effects and strike a balance between bias and variance minimization in the future we would collect more economic features and use higher order polynomial features to improve the regularized linear regressions of aud basis given the observed bias issue with the aud data .conclusions and future work	we find that random forests achieve fairly good predictions as measured by mse on test sets that encompass two separate blocks of observations in the post crisis period 
1	110427	10427	this performance likely owes to random forest s ability to flexibly introduce non linear feature effects and strike a balance between bias and variance minimization in the future we would collect more economic features and use higher order polynomial features to improve the regularized linear regressions of aud basis given the observed bias issue with the aud data .we find that random forests achieve fairly good predictions as measured by mse on test sets that encompass two separate blocks of observations in the post crisis period .we will expand the set of algorithms employed to improve the performance on the jpy data as most algorithms seem to suffer from a variance problem .conclusions and future work	this performance likely owes to random forest s ability to flexibly introduce non linear feature effects and strike a balance between bias and variance minimization in the future we would collect more economic features and use higher order polynomial features to improve the regularized linear regressions of aud basis given the observed bias issue with the aud data 
1	110428	10428	we will expand the set of algorithms employed to improve the performance on the jpy data as most algorithms seem to suffer from a variance problem .this performance likely owes to random forest s ability to flexibly introduce non linear feature effects and strike a balance between bias and variance minimization in the future we would collect more economic features and use higher order polynomial features to improve the regularized linear regressions of aud basis given the observed bias issue with the aud data .specifically we will apply the boosting technique and we will consider training a neural network overall we are encouraged to see that we found models that perform reasonably well .conclusions and future work	we will expand the set of algorithms employed to improve the performance on the jpy data as most algorithms seem to suffer from a variance problem 
1	110429	10429	specifically we will apply the boosting technique and we will consider training a neural network overall we are encouraged to see that we found models that perform reasonably well .we will expand the set of algorithms employed to improve the performance on the jpy data as most algorithms seem to suffer from a variance problem .importantly the features selected as important by our various models are intuitive and sensible .conclusions and future work	specifically we will apply the boosting technique and we will consider training a neural network overall we are encouraged to see that we found models that perform reasonably well 
0	110430	10430	importantly the features selected as important by our various models are intuitive and sensible .specifically we will apply the boosting technique and we will consider training a neural network overall we are encouraged to see that we found models that perform reasonably well .we hope to more closely examine the contribution of these features in the future and extend this analysis to a larger set of currency bases .conclusions and future work	importantly the features selected as important by our various models are intuitive and sensible 
1	110431	10431	we hope to more closely examine the contribution of these features in the future and extend this analysis to a larger set of currency bases .importantly the features selected as important by our various models are intuitive and sensible .all tasks were performed by amy and stefan in equal parts .conclusions and future work	we hope to more closely examine the contribution of these features in the future and extend this analysis to a larger set of currency bases 
0	110432	10432	all tasks were performed by amy and stefan in equal parts .we hope to more closely examine the contribution of these features in the future and extend this analysis to a larger set of currency bases .last sentence.conclusions and future work	all tasks were performed by amy and stefan in equal parts 
0	110433	10433	there are several studies evaluating the impact of increase in government spending for specific sectors in quality indexes for example the cited works by baldacci et al sutherland et al and gupta et al .first sentence.in the first example different variations of the least mean squared error regression model are tested as well as a covariance structure model based on latent variables .related work	there are several studies evaluating the impact of increase in government spending for specific sectors in quality indexes for example the cited works by baldacci et al sutherland et al and gupta et al 
0	110434	10434	in the first example different variations of the least mean squared error regression model are tested as well as a covariance structure model based on latent variables .there are several studies evaluating the impact of increase in government spending for specific sectors in quality indexes for example the cited works by baldacci et al sutherland et al and gupta et al .the models are used to find the relation between public spending and quality indexes .related work	in the first example different variations of the least mean squared error regression model are tested as well as a covariance structure model based on latent variables 
0	110435	10435	the models are used to find the relation between public spending and quality indexes .in the first example different variations of the least mean squared error regression model are tested as well as a covariance structure model based on latent variables .in all the studies there was statistical evidence of a correlation between increase in spending and increase in the indexes .related work	the models are used to find the relation between public spending and quality indexes 
0	110436	10436	in all the studies there was statistical evidence of a correlation between increase in spending and increase in the indexes .the models are used to find the relation between public spending and quality indexes .the present project aims to explore this relation to create a tool for budget planning .related work	in all the studies there was statistical evidence of a correlation between increase in spending and increase in the indexes 
0	110437	10437	the present project aims to explore this relation to create a tool for budget planning .in all the studies there was statistical evidence of a correlation between increase in spending and increase in the indexes .last sentence.related work	the present project aims to explore this relation to create a tool for budget planning 
1	110438	10438	the target variable as explained previously is the ideb score of each school .first sentence.this data can be found on the page http ideb inep gov br .dataset and features	the target variable as explained previously is the ideb score of each school 
0	110439	10439	this data can be found on the page http ideb inep gov br .the target variable as explained previously is the ideb score of each school .it contains the evolution of the in brazilian public schools for the years of 2013 2015 and 2017 .dataset and features	this data can be found on the page http ideb inep gov br 
0	110440	10440	it contains the evolution of the in brazilian public schools for the years of 2013 2015 and 2017 .this data can be found on the page http ideb inep gov br .this index combines the scores of students in a national mandatory exam with data provided by each school describing rate approvals to assess the quality of basic education in public schools .dataset and features	it contains the evolution of the in brazilian public schools for the years of 2013 2015 and 2017 
0	110441	10441	this index combines the scores of students in a national mandatory exam with data provided by each school describing rate approvals to assess the quality of basic education in public schools .it contains the evolution of the in brazilian public schools for the years of 2013 2015 and 2017 .in the year of 2017 the goals for the 2019 and 2021 were established the second data source is the brazilian school census of 2013 http portal inep gov br microdados that has survey data on every public school in brazil .dataset and features	this index combines the scores of students in a national mandatory exam with data provided by each school describing rate approvals to assess the quality of basic education in public schools 
1	110442	10442	in the year of 2017 the goals for the 2019 and 2021 were established the second data source is the brazilian school census of 2013 http portal inep gov br microdados that has survey data on every public school in brazil .this index combines the scores of students in a national mandatory exam with data provided by each school describing rate approvals to assess the quality of basic education in public schools .this dataset contains information describing many aspects of the infrastructure of the school the qualification of teachers and the profile of students .dataset and features	in the year of 2017 the goals for the 2019 and 2021 were established the second data source is the brazilian school census of 2013 http portal inep gov br microdados that has survey data on every public school in brazil 
0	110443	10443	this dataset contains information describing many aspects of the infrastructure of the school the qualification of teachers and the profile of students .in the year of 2017 the goals for the 2019 and 2021 were established the second data source is the brazilian school census of 2013 http portal inep gov br microdados that has survey data on every public school in brazil .some examples of features include total number of students number of professors by level and area of education number of laboratories computers and offices number of students per race number of classes per subject total amount of time spent by students with extracurricular activities in total after the preparation of data the dataset includes 353 features per school .dataset and features	this dataset contains information describing many aspects of the infrastructure of the school the qualification of teachers and the profile of students 
1	110444	10444	some examples of features include total number of students number of professors by level and area of education number of laboratories computers and offices number of students per race number of classes per subject total amount of time spent by students with extracurricular activities in total after the preparation of data the dataset includes 353 features per school .this dataset contains information describing many aspects of the infrastructure of the school the qualification of teachers and the profile of students .most of them are count variables as the number of professors from each educational background number of different equipment etc .dataset and features	some examples of features include total number of students number of professors by level and area of education number of laboratories computers and offices number of students per race number of classes per subject total amount of time spent by students with extracurricular activities in total after the preparation of data the dataset includes 353 features per school 
0	110445	10445	most of them are count variables as the number of professors from each educational background number of different equipment etc .some examples of features include total number of students number of professors by level and area of education number of laboratories computers and offices number of students per race number of classes per subject total amount of time spent by students with extracurricular activities in total after the preparation of data the dataset includes 353 features per school .a big part of transforming the data included counting different categories in categorical variables .dataset and features	most of them are count variables as the number of professors from each educational background number of different equipment etc 
0	110446	10446	a big part of transforming the data included counting different categories in categorical variables .most of them are count variables as the number of professors from each educational background number of different equipment etc .for example there is one entry in the original data for each student and teacher in the school in the final dataset there is only counts for the number of male female students mathematics biology chemistry teachers etc the last data source is the website https www fazenda sp gov br .dataset and features	a big part of transforming the data included counting different categories in categorical variables 
1	110447	10447	for example there is one entry in the original data for each student and teacher in the school in the final dataset there is only counts for the number of male female students mathematics biology chemistry teachers etc the last data source is the website https www fazenda sp gov br .a big part of transforming the data included counting different categories in categorical variables .it has data detailing all of the disbursement made by the state government of sao paulo since 2010 .dataset and features	for example there is one entry in the original data for each student and teacher in the school in the final dataset there is only counts for the number of male female students mathematics biology chemistry teachers etc the last data source is the website https www fazenda sp gov br 
0	110448	10448	it has data detailing all of the disbursement made by the state government of sao paulo since 2010 .for example there is one entry in the original data for each student and teacher in the school in the final dataset there is only counts for the number of male female students mathematics biology chemistry teachers etc the last data source is the website https www fazenda sp gov br .the database contains information on the targeted sector education health transportation etc .dataset and features	it has data detailing all of the disbursement made by the state government of sao paulo since 2010 
0	110449	10449	the database contains information on the targeted sector education health transportation etc .it has data detailing all of the disbursement made by the state government of sao paulo since 2010 .the subarea primary secondary higher education etc .dataset and features	the database contains information on the targeted sector education health transportation etc 
0	110450	10450	the subarea primary secondary higher education etc .the database contains information on the targeted sector education health transportation etc .as well as a more detailed classification of the purpose of the spending scholarship for poor students construction of new schools purchase of food or transportation for students etc .dataset and features	the subarea primary secondary higher education etc 
0	110451	10451	as well as a more detailed classification of the purpose of the spending scholarship for poor students construction of new schools purchase of food or transportation for students etc .the subarea primary secondary higher education etc .some of the expenditure categories include transportation for students food for students and workers.dataset and features	as well as a more detailed classification of the purpose of the spending scholarship for poor students construction of new schools purchase of food or transportation for students etc 
0	110452	10452	some of the expenditure categories include transportation for students food for students and workers.as well as a more detailed classification of the purpose of the spending scholarship for poor students construction of new schools purchase of food or transportation for students etc .last sentence.dataset and features	some of the expenditure categories include transportation for students food for students and workers
0	110453	10453	the final project includes three different models that process data in different phases .first sentence.first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 .methodology	the final project includes three different models that process data in different phases 
1	110454	10454	first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 .the final project includes three different models that process data in different phases .the purpose of this model is to reduce the number of features from the census considered in the next phase .methodology	first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 
1	110455	10455	the purpose of this model is to reduce the number of features from the census considered in the next phase .first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 .only the most important variables detected by the algorithm in this phase continue in the dataset .methodology	the purpose of this model is to reduce the number of features from the census considered in the next phase 
0	110456	10456	only the most important variables detected by the algorithm in this phase continue in the dataset .the purpose of this model is to reduce the number of features from the census considered in the next phase .the final set of variables have an accumulated feature importance of 0 99 in the model .methodology	only the most important variables detected by the algorithm in this phase continue in the dataset 
1	110457	10457	the final set of variables have an accumulated feature importance of 0 99 in the model .only the most important variables detected by the algorithm in this phase continue in the dataset .in this phase 3 different algorithms are tested svm gradient boosted trees gbt and ridge regression .methodology	the final set of variables have an accumulated feature importance of 0 99 in the model 
0	110458	10458	in this phase 3 different algorithms are tested svm gradient boosted trees gbt and ridge regression .the final set of variables have an accumulated feature importance of 0 99 in the model .for the gbt two different implementations are evaluated scikit learn and lightgbm .methodology	in this phase 3 different algorithms are tested svm gradient boosted trees gbt and ridge regression 
0	110459	10459	for the gbt two different implementations are evaluated scikit learn and lightgbm .in this phase 3 different algorithms are tested svm gradient boosted trees gbt and ridge regression .the evaluation metric chosen is the r defined as the model chosen is the one that presents greatest r in the test set 638 schools .methodology	for the gbt two different implementations are evaluated scikit learn and lightgbm 
1	110460	10460	the evaluation metric chosen is the r defined as the model chosen is the one that presents greatest r in the test set 638 schools .for the gbt two different implementations are evaluated scikit learn and lightgbm .after the selection of variables 129 features continued to the next phase the clustering algorithm .methodology	the evaluation metric chosen is the r defined as the model chosen is the one that presents greatest r in the test set 638 schools 
0	110461	10461	after the selection of variables 129 features continued to the next phase the clustering algorithm .the evaluation metric chosen is the r defined as the model chosen is the one that presents greatest r in the test set 638 schools .this second model uses the descriptive variables to separate schools in groups with similar needs .methodology	after the selection of variables 129 features continued to the next phase the clustering algorithm 
1	110462	10462	this second model uses the descriptive variables to separate schools in groups with similar needs .after the selection of variables 129 features continued to the next phase the clustering algorithm .since the final goal of the project is to define the budget and its optimal distribution for each school there is the need to isolate the effect of other variables not related to expenditure that are correlated to the ideb .methodology	this second model uses the descriptive variables to separate schools in groups with similar needs 
1	110463	10463	since the final goal of the project is to define the budget and its optimal distribution for each school there is the need to isolate the effect of other variables not related to expenditure that are correlated to the ideb .this second model uses the descriptive variables to separate schools in groups with similar needs .this is the purpose of the second stage in the data processing framework the evaluation metric for the clusters is the mean silhouette coefficient .methodology	since the final goal of the project is to define the budget and its optimal distribution for each school there is the need to isolate the effect of other variables not related to expenditure that are correlated to the ideb 
1	110464	10464	this is the purpose of the second stage in the data processing framework the evaluation metric for the clusters is the mean silhouette coefficient .since the final goal of the project is to define the budget and its optimal distribution for each school there is the need to isolate the effect of other variables not related to expenditure that are correlated to the ideb .for one sample in the train set the silhouette is given by where a is the mean intra cluster euclidean distance to the considered point and b is the euclidean distance to the nearest point in other cluster .methodology	this is the purpose of the second stage in the data processing framework the evaluation metric for the clusters is the mean silhouette coefficient 
1	110465	10465	for one sample in the train set the silhouette is given by where a is the mean intra cluster euclidean distance to the considered point and b is the euclidean distance to the nearest point in other cluster .this is the purpose of the second stage in the data processing framework the evaluation metric for the clusters is the mean silhouette coefficient .two different approaches are tested both of them use k means as the main algorithm .methodology	for one sample in the train set the silhouette is given by where a is the mean intra cluster euclidean distance to the considered point and b is the euclidean distance to the nearest point in other cluster 
0	110466	10466	two different approaches are tested both of them use k means as the main algorithm .for one sample in the train set the silhouette is given by where a is the mean intra cluster euclidean distance to the considered point and b is the euclidean distance to the nearest point in other cluster .in one of them however the original data is first transformed with principal component analysis pca in order to reduce the dimensionality of the dataset .methodology	two different approaches are tested both of them use k means as the main algorithm 
0	110467	10467	in one of them however the original data is first transformed with principal component analysis pca in order to reduce the dimensionality of the dataset .two different approaches are tested both of them use k means as the main algorithm .the model used data from 9837 schools this phase did not consider only schools administered by the state government but also those ran by the federal and city governments the final phase is a combination of multiple classifiers one for each cluster .methodology	in one of them however the original data is first transformed with principal component analysis pca in order to reduce the dimensionality of the dataset 
1	110468	10468	the model used data from 9837 schools this phase did not consider only schools administered by the state government but also those ran by the federal and city governments the final phase is a combination of multiple classifiers one for each cluster .in one of them however the original data is first transformed with principal component analysis pca in order to reduce the dimensionality of the dataset .each model predicts whether the school achieved its goal for the 2017 ideb .methodology	the model used data from 9837 schools this phase did not consider only schools administered by the state government but also those ran by the federal and city governments the final phase is a combination of multiple classifiers one for each cluster 
0	110469	10469	each model predicts whether the school achieved its goal for the 2017 ideb .the model used data from 9837 schools this phase did not consider only schools administered by the state government but also those ran by the federal and city governments the final phase is a combination of multiple classifiers one for each cluster .the input variables are the expenditure data for each school there are 3152 schools and 711 features .methodology	each model predicts whether the school achieved its goal for the 2017 ideb 
0	110470	10470	the input variables are the expenditure data for each school there are 3152 schools and 711 features .each model predicts whether the school achieved its goal for the 2017 ideb .the assumption in this phase is that after isolating the effects of descriptive variables in the ideb it is possible to find an expenditure distribution that will minimize the total sum of investments per school while allowing it to achieve its goal .methodology	the input variables are the expenditure data for each school there are 3152 schools and 711 features 
1	110471	10471	the assumption in this phase is that after isolating the effects of descriptive variables in the ideb it is possible to find an expenditure distribution that will minimize the total sum of investments per school while allowing it to achieve its goal .the input variables are the expenditure data for each school there are 3152 schools and 711 features .this distribution will be equal for all schools in the same cluster the evaluation metric is the f1 score that combines both precision and recall in order to guarantee that the model do not present good performance only for the most common class .methodology	the assumption in this phase is that after isolating the effects of descriptive variables in the ideb it is possible to find an expenditure distribution that will minimize the total sum of investments per school while allowing it to achieve its goal 
1	110472	10472	this distribution will be equal for all schools in the same cluster the evaluation metric is the f1 score that combines both precision and recall in order to guarantee that the model do not present good performance only for the most common class .the assumption in this phase is that after isolating the effects of descriptive variables in the ideb it is possible to find an expenditure distribution that will minimize the total sum of investments per school while allowing it to achieve its goal .the f1 score is given by in this phase only one algorithm was implemented derived from the first part the gbt implementation in scikit learn .methodology	this distribution will be equal for all schools in the same cluster the evaluation metric is the f1 score that combines both precision and recall in order to guarantee that the model do not present good performance only for the most common class 
0	110473	10473	the f1 score is given by in this phase only one algorithm was implemented derived from the first part the gbt implementation in scikit learn .this distribution will be equal for all schools in the same cluster the evaluation metric is the f1 score that combines both precision and recall in order to guarantee that the model do not present good performance only for the most common class .the final tool can be applied in the estimation of the budget for each school in the chosen approach first all the schools that achieved their goals and for which the model presented correct predictions are selected .methodology	the f1 score is given by in this phase only one algorithm was implemented derived from the first part the gbt implementation in scikit learn 
1	110474	10474	the final tool can be applied in the estimation of the budget for each school in the chosen approach first all the schools that achieved their goals and for which the model presented correct predictions are selected .the f1 score is given by in this phase only one algorithm was implemented derived from the first part the gbt implementation in scikit learn .the initial budget estimate for each category is the minimum value greater than 0 if there is one found for that category in this group of schools .methodology	the final tool can be applied in the estimation of the budget for each school in the chosen approach first all the schools that achieved their goals and for which the model presented correct predictions are selected 
1	110475	10475	the initial budget estimate for each category is the minimum value greater than 0 if there is one found for that category in this group of schools .the final tool can be applied in the estimation of the budget for each school in the chosen approach first all the schools that achieved their goals and for which the model presented correct predictions are selected .if the model predicts success in goal achievement with this expenditure distribution it is considered as the final budget if the model predicts fail in goal achievement one of the categories is chosen to be increased .methodology	the initial budget estimate for each category is the minimum value greater than 0 if there is one found for that category in this group of schools 
1	110476	10476	if the model predicts success in goal achievement with this expenditure distribution it is considered as the final budget if the model predicts fail in goal achievement one of the categories is chosen to be increased .the initial budget estimate for each category is the minimum value greater than 0 if there is one found for that category in this group of schools .the probability of selecting a specific category is equal to the normalized feature importance of the variable that represents this category according to the final model .methodology	if the model predicts success in goal achievement with this expenditure distribution it is considered as the final budget if the model predicts fail in goal achievement one of the categories is chosen to be increased 
1	110477	10477	the probability of selecting a specific category is equal to the normalized feature importance of the variable that represents this category according to the final model .if the model predicts success in goal achievement with this expenditure distribution it is considered as the final budget if the model predicts fail in goal achievement one of the categories is chosen to be increased .the budget for the selected category than assumes the value of the second lowest expenditure for this category in the selected subset of schools .methodology	the probability of selecting a specific category is equal to the normalized feature importance of the variable that represents this category according to the final model 
0	110478	10478	the budget for the selected category than assumes the value of the second lowest expenditure for this category in the selected subset of schools .the probability of selecting a specific category is equal to the normalized feature importance of the variable that represents this category according to the final model .this process is repeated until the model predicts success in goal achievement .methodology	the budget for the selected category than assumes the value of the second lowest expenditure for this category in the selected subset of schools 
0	110479	10479	this process is repeated until the model predicts success in goal achievement .the budget for the selected category than assumes the value of the second lowest expenditure for this category in the selected subset of schools .if a specific category achieves its maximum possible value its probability of being selected in the following iterations goes to 0 .methodology	this process is repeated until the model predicts success in goal achievement 
0	110480	10480	if a specific category achieves its maximum possible value its probability of being selected in the following iterations goes to 0 .this process is repeated until the model predicts success in goal achievement .last sentence.methodology	if a specific category achieves its maximum possible value its probability of being selected in the following iterations goes to 0 
0	110481	10481	the results for the each model tested is in.first sentence.last sentence.regression	the results for the each model tested is in
1	110482	10482	the 129 most important features in the previous model are then used in the clustering algorithm to separate schools in groups .first sentence.the results of the two models tested are in table 2 .clustering	the 129 most important features in the previous model are then used in the clustering algorithm to separate schools in groups 
0	110483	10483	the results of the two models tested are in table 2 .the 129 most important features in the previous model are then used in the clustering algorithm to separate schools in groups .the number of clusters varied from 4 to 20 and in the final model consisted of 10 clusters in a tradeoff between increase in the silhouette and guaranteeing a reasonable number of schools in each cluster .clustering	the results of the two models tested are in table 2 
1	110484	10484	the number of clusters varied from 4 to 20 and in the final model consisted of 10 clusters in a tradeoff between increase in the silhouette and guaranteeing a reasonable number of schools in each cluster .the results of the two models tested are in table 2 .still some clusters ended up with few schools to the minimum of one .clustering	the number of clusters varied from 4 to 20 and in the final model consisted of 10 clusters in a tradeoff between increase in the silhouette and guaranteeing a reasonable number of schools in each cluster 
0	110485	10485	still some clusters ended up with few schools to the minimum of one .the number of clusters varied from 4 to 20 and in the final model consisted of 10 clusters in a tradeoff between increase in the silhouette and guaranteeing a reasonable number of schools in each cluster .these consist of outliers and these clusters did not enter in the next phase .clustering	still some clusters ended up with few schools to the minimum of one 
0	110486	10486	these consist of outliers and these clusters did not enter in the next phase .still some clusters ended up with few schools to the minimum of one .last sentence.clustering	these consist of outliers and these clusters did not enter in the next phase 
0	110487	10487	silhouette k means 0 767.first sentence.last sentence.model	silhouette k means 0 767
0	110488	10488	although the silhouette for the model with pca preprocessing was higher both algorithms presented a very similar result with clusters almost identical .first sentence.the output of both cases were tested in the classification phase and the clusters from the model with pca presented a better weighted average f1 score for the classifiers .pca k means 0 805	although the silhouette for the model with pca preprocessing was higher both algorithms presented a very similar result with clusters almost identical 
1	110489	10489	the output of both cases were tested in the classification phase and the clusters from the model with pca presented a better weighted average f1 score for the classifiers .although the silhouette for the model with pca preprocessing was higher both algorithms presented a very similar result with clusters almost identical .for this reason this was the selected model .pca k means 0 805	the output of both cases were tested in the classification phase and the clusters from the model with pca presented a better weighted average f1 score for the classifiers 
0	110490	10490	for this reason this was the selected model .the output of both cases were tested in the classification phase and the clusters from the model with pca presented a better weighted average f1 score for the classifiers .as expected by separating schools into clusters the performance of the classifiers increase .pca k means 0 805	for this reason this was the selected model 
1	110491	10491	as expected by separating schools into clusters the performance of the classifiers increase .for this reason this was the selected model .this means that it is easier for the model to find patterns in expenditure data when schools with similar descriptive features are grouped together and isolated from other groups .pca k means 0 805	as expected by separating schools into clusters the performance of the classifiers increase 
1	110492	10492	this means that it is easier for the model to find patterns in expenditure data when schools with similar descriptive features are grouped together and isolated from other groups .as expected by separating schools into clusters the performance of the classifiers increase .this supports the initial hypothesis .pca k means 0 805	this means that it is easier for the model to find patterns in expenditure data when schools with similar descriptive features are grouped together and isolated from other groups 
0	110493	10493	this supports the initial hypothesis .this means that it is easier for the model to find patterns in expenditure data when schools with similar descriptive features are grouped together and isolated from other groups .however it is also possible to observe that some clusters as number 0 presented a test f1 score lower than the model with all schools together .pca k means 0 805	this supports the initial hypothesis 
1	110494	10494	however it is also possible to observe that some clusters as number 0 presented a test f1 score lower than the model with all schools together .this supports the initial hypothesis .this might be an indication that this cluster is not homogeneous in terms of characteristics that might affect the ideb .pca k means 0 805	however it is also possible to observe that some clusters as number 0 presented a test f1 score lower than the model with all schools together 
0	110495	10495	this might be an indication that this cluster is not homogeneous in terms of characteristics that might affect the ideb .however it is also possible to observe that some clusters as number 0 presented a test f1 score lower than the model with all schools together .in addition clusters as number 5 had problems with overfitting due to the small sample of schools it represents .pca k means 0 805	this might be an indication that this cluster is not homogeneous in terms of characteristics that might affect the ideb 
1	110496	10496	in addition clusters as number 5 had problems with overfitting due to the small sample of schools it represents .this might be an indication that this cluster is not homogeneous in terms of characteristics that might affect the ideb .last sentence.pca k means 0 805	in addition clusters as number 5 had problems with overfitting due to the small sample of schools it represents 
1	110497	10497	cluster number 3 had excellent performance which indicates that this cluster is homogeneous and that it is possible to find a common expenditure distribution for these schools that will allow them to achieve their goals .first sentence.for this cluster the method of budget estimation described previously was implemented .classification	cluster number 3 had excellent performance which indicates that this cluster is homogeneous and that it is possible to find a common expenditure distribution for these schools that will allow them to achieve their goals 
0	110498	10498	for this cluster the method of budget estimation described previously was implemented .cluster number 3 had excellent performance which indicates that this cluster is homogeneous and that it is possible to find a common expenditure distribution for these schools that will allow them to achieve their goals .the prediction for the initial budget estimation minimum values for each category was 1 therefore there was no need to iteratively search for the expenditure distribution applying the minimum estimated budget for each school there is a reduction of r 314 972 841 00 in the total spending of the government of sao paulo with the schools in cluster 3 .classification	for this cluster the method of budget estimation described previously was implemented 
1	110499	10499	the prediction for the initial budget estimation minimum values for each category was 1 therefore there was no need to iteratively search for the expenditure distribution applying the minimum estimated budget for each school there is a reduction of r 314 972 841 00 in the total spending of the government of sao paulo with the schools in cluster 3 .for this cluster the method of budget estimation described previously was implemented .in addition according to the model all schools would have achieved their goals using the estimated expenditure distribution while with the current budget approximately 30 of schools in this cluster did not achieve their goals .classification	the prediction for the initial budget estimation minimum values for each category was 1 therefore there was no need to iteratively search for the expenditure distribution applying the minimum estimated budget for each school there is a reduction of r 314 972 841 00 in the total spending of the government of sao paulo with the schools in cluster 3 
0	110500	10500	in addition according to the model all schools would have achieved their goals using the estimated expenditure distribution while with the current budget approximately 30 of schools in this cluster did not achieve their goals .the prediction for the initial budget estimation minimum values for each category was 1 therefore there was no need to iteratively search for the expenditure distribution applying the minimum estimated budget for each school there is a reduction of r 314 972 841 00 in the total spending of the government of sao paulo with the schools in cluster 3 .last sentence.classification	in addition according to the model all schools would have achieved their goals using the estimated expenditure distribution while with the current budget approximately 30 of schools in this cluster did not achieve their goals 
0	110501	10501	the usefulness of the tool developed in this study depends heavily on the quality of the clustering algorithm .first sentence.for the initial assumption about the clusters to hold all the relevant factors associated with the schools that do not relate to their spending and that affect the ideb must be represented in the clustering variables .conclusion and future work	the usefulness of the tool developed in this study depends heavily on the quality of the clustering algorithm 
1	110502	10502	for the initial assumption about the clusters to hold all the relevant factors associated with the schools that do not relate to their spending and that affect the ideb must be represented in the clustering variables .the usefulness of the tool developed in this study depends heavily on the quality of the clustering algorithm .when this is the case the separation of schools in groups will be able to isolate the effect of this variables and all variation observed in the ideb will be explained solely by differences in the expenditure distribution in the present project the assumption was valid for some clusters mainly number 3 .conclusion and future work	for the initial assumption about the clusters to hold all the relevant factors associated with the schools that do not relate to their spending and that affect the ideb must be represented in the clustering variables 
1	110503	10503	when this is the case the separation of schools in groups will be able to isolate the effect of this variables and all variation observed in the ideb will be explained solely by differences in the expenditure distribution in the present project the assumption was valid for some clusters mainly number 3 .for the initial assumption about the clusters to hold all the relevant factors associated with the schools that do not relate to their spending and that affect the ideb must be represented in the clustering variables .for this cluster it was possible to create a good predictor of goal achievement only with expenditure features .conclusion and future work	when this is the case the separation of schools in groups will be able to isolate the effect of this variables and all variation observed in the ideb will be explained solely by differences in the expenditure distribution in the present project the assumption was valid for some clusters mainly number 3 
1	110504	10504	for this cluster it was possible to create a good predictor of goal achievement only with expenditure features .when this is the case the separation of schools in groups will be able to isolate the effect of this variables and all variation observed in the ideb will be explained solely by differences in the expenditure distribution in the present project the assumption was valid for some clusters mainly number 3 .when this condition is present this tool can be very useful in minimizing the budget of the schools while guaranteeing they will achieve their goals however other clusters mainly number 0 are too heterogeneous to have the ideb explained only with spending data .conclusion and future work	for this cluster it was possible to create a good predictor of goal achievement only with expenditure features 
1	110505	10505	when this condition is present this tool can be very useful in minimizing the budget of the schools while guaranteeing they will achieve their goals however other clusters mainly number 0 are too heterogeneous to have the ideb explained only with spending data .for this cluster it was possible to create a good predictor of goal achievement only with expenditure features .it means that to create a good predictor for goal achievement more variables are needed .conclusion and future work	when this condition is present this tool can be very useful in minimizing the budget of the schools while guaranteeing they will achieve their goals however other clusters mainly number 0 are too heterogeneous to have the ideb explained only with spending data 
0	110506	10506	it means that to create a good predictor for goal achievement more variables are needed .when this condition is present this tool can be very useful in minimizing the budget of the schools while guaranteeing they will achieve their goals however other clusters mainly number 0 are too heterogeneous to have the ideb explained only with spending data .therefore for this clusters it is not possible to explain goal achievement only as a function of spending distribution to solve the problem described above the first step would be to incorporate new variables in the clustering phase .conclusion and future work	it means that to create a good predictor for goal achievement more variables are needed 
1	110507	10507	therefore for this clusters it is not possible to explain goal achievement only as a function of spending distribution to solve the problem described above the first step would be to incorporate new variables in the clustering phase .it means that to create a good predictor for goal achievement more variables are needed .for example sociodemographic variables of the region where the school is located are probably highly correlated with its ideb also .conclusion and future work	therefore for this clusters it is not possible to explain goal achievement only as a function of spending distribution to solve the problem described above the first step would be to incorporate new variables in the clustering phase 
1	110508	10508	for example sociodemographic variables of the region where the school is located are probably highly correlated with its ideb also .therefore for this clusters it is not possible to explain goal achievement only as a function of spending distribution to solve the problem described above the first step would be to incorporate new variables in the clustering phase .features as the average income of residents average number of people in one house and distance from the center of the city are not present in the school census data used in the first two phases of the project the brazilian census have this type of sociodemographic data .conclusion and future work	for example sociodemographic variables of the region where the school is located are probably highly correlated with its ideb also 
1	110509	10509	features as the average income of residents average number of people in one house and distance from the center of the city are not present in the school census data used in the first two phases of the project the brazilian census have this type of sociodemographic data .for example sociodemographic variables of the region where the school is located are probably highly correlated with its ideb also .however in this dataset locations are described as sectors and each sector has its own code .conclusion and future work	features as the average income of residents average number of people in one house and distance from the center of the city are not present in the school census data used in the first two phases of the project the brazilian census have this type of sociodemographic data 
0	110510	10510	however in this dataset locations are described as sectors and each sector has its own code .features as the average income of residents average number of people in one house and distance from the center of the city are not present in the school census data used in the first two phases of the project the brazilian census have this type of sociodemographic data .the problem when linking this dataset to the school census is that the last one does not have information on the code of the sector where the school is .conclusion and future work	however in this dataset locations are described as sectors and each sector has its own code 
0	110511	10511	the problem when linking this dataset to the school census is that the last one does not have information on the code of the sector where the school is .however in this dataset locations are described as sectors and each sector has its own code .this needs to be solved in order to include data from the brazilian census in the clustering algorithm a second point of improvement is aggregating redundant categories of spending .conclusion and future work	the problem when linking this dataset to the school census is that the last one does not have information on the code of the sector where the school is 
1	110512	10512	this needs to be solved in order to include data from the brazilian census in the clustering algorithm a second point of improvement is aggregating redundant categories of spending .the problem when linking this dataset to the school census is that the last one does not have information on the code of the sector where the school is .this problem was detailed previously .conclusion and future work	this needs to be solved in order to include data from the brazilian census in the clustering algorithm a second point of improvement is aggregating redundant categories of spending 
0	110513	10513	this problem was detailed previously .this needs to be solved in order to include data from the brazilian census in the clustering algorithm a second point of improvement is aggregating redundant categories of spending .because of these redundancies it might be difficult for the last model to identify the real impact of each subarea of investment on the ideb other limitation also explained previously is that the data provided by the government of sao paulo does not have detailed spending for each school .conclusion and future work	this problem was detailed previously 
1	110514	10514	because of these redundancies it might be difficult for the last model to identify the real impact of each subarea of investment on the ideb other limitation also explained previously is that the data provided by the government of sao paulo does not have detailed spending for each school .this problem was detailed previously .there certainly is in the government database this type of data however it is not open to the public finally this project did not explicitly try to find a causal relation between the input features and the target variable which is a necessary step in the design of public policies .conclusion and future work	because of these redundancies it might be difficult for the last model to identify the real impact of each subarea of investment on the ideb other limitation also explained previously is that the data provided by the government of sao paulo does not have detailed spending for each school 
1	110515	10515	there certainly is in the government database this type of data however it is not open to the public finally this project did not explicitly try to find a causal relation between the input features and the target variable which is a necessary step in the design of public policies .because of these redundancies it might be difficult for the last model to identify the real impact of each subarea of investment on the ideb other limitation also explained previously is that the data provided by the government of sao paulo does not have detailed spending for each school .a qualitative evaluation of the importance of the features in the first and third models as well as the impact they have on the target variable needs to be conducted .conclusion and future work	there certainly is in the government database this type of data however it is not open to the public finally this project did not explicitly try to find a causal relation between the input features and the target variable which is a necessary step in the design of public policies 
0	110516	10516	a qualitative evaluation of the importance of the features in the first and third models as well as the impact they have on the target variable needs to be conducted .there certainly is in the government database this type of data however it is not open to the public finally this project did not explicitly try to find a causal relation between the input features and the target variable which is a necessary step in the design of public policies .this would be better performed with the assistance of specialists in the area this tool however is a good starting point for the government to explore quantitative tools in the design of public policies .conclusion and future work	a qualitative evaluation of the importance of the features in the first and third models as well as the impact they have on the target variable needs to be conducted 
0	110517	10517	this would be better performed with the assistance of specialists in the area this tool however is a good starting point for the government to explore quantitative tools in the design of public policies .a qualitative evaluation of the importance of the features in the first and third models as well as the impact they have on the target variable needs to be conducted .in a real implementation there would be an evaluation period when the tool would suggest the expenditure distribution and after its implementation the results would be reevaluated and incorporated in the model .conclusion and future work	this would be better performed with the assistance of specialists in the area this tool however is a good starting point for the government to explore quantitative tools in the design of public policies 
1	110518	10518	in a real implementation there would be an evaluation period when the tool would suggest the expenditure distribution and after its implementation the results would be reevaluated and incorporated in the model .this would be better performed with the assistance of specialists in the area this tool however is a good starting point for the government to explore quantitative tools in the design of public policies .last sentence.conclusion and future work	in a real implementation there would be an evaluation period when the tool would suggest the expenditure distribution and after its implementation the results would be reevaluated and incorporated in the model 
1	110519	10519	pricing a rental property on airbnb is a challenging task for the owner as it determines the number of customers for the place .first sentence.on the other hand customers have to evaluate an offered price with minimal knowledge of an optimal value for the property .final report 1 introduction	pricing a rental property on airbnb is a challenging task for the owner as it determines the number of customers for the place 
1	110520	10520	on the other hand customers have to evaluate an offered price with minimal knowledge of an optimal value for the property .pricing a rental property on airbnb is a challenging task for the owner as it determines the number of customers for the place .this project aims to develop a price prediction model using a range of methods from linear regression to tree based models support vector regression svr k means clustering kmc and neural networks nns to tackle this challenge .final report 1 introduction	on the other hand customers have to evaluate an offered price with minimal knowledge of an optimal value for the property 
1	110521	10521	this project aims to develop a price prediction model using a range of methods from linear regression to tree based models support vector regression svr k means clustering kmc and neural networks nns to tackle this challenge .on the other hand customers have to evaluate an offered price with minimal knowledge of an optimal value for the property .features of the rentals owner characteristics and the customer reviews will be used to predict the price of the listing .final report 1 introduction	this project aims to develop a price prediction model using a range of methods from linear regression to tree based models support vector regression svr k means clustering kmc and neural networks nns to tackle this challenge 
1	110522	10522	features of the rentals owner characteristics and the customer reviews will be used to predict the price of the listing .this project aims to develop a price prediction model using a range of methods from linear regression to tree based models support vector regression svr k means clustering kmc and neural networks nns to tackle this challenge .last sentence.final report 1 introduction	features of the rentals owner characteristics and the customer reviews will be used to predict the price of the listing 
0	110523	10523	existing literature shows that some studies focus on non shared property purchase or rental price predictions .first sentence.in a cs229 project yu and wu this project has tried to further the experimented methods from the literature by focusing on a variety of feature selection techniques implementing neural networks and leveraging the customer reviews through sentiment analysis .related work	existing literature shows that some studies focus on non shared property purchase or rental price predictions 
1	110524	10524	in a cs229 project yu and wu this project has tried to further the experimented methods from the literature by focusing on a variety of feature selection techniques implementing neural networks and leveraging the customer reviews through sentiment analysis .existing literature shows that some studies focus on non shared property purchase or rental price predictions .the authors were unable to find the last two mentioned undertakings in the existing literature .related work	in a cs229 project yu and wu this project has tried to further the experimented methods from the literature by focusing on a variety of feature selection techniques implementing neural networks and leveraging the customer reviews through sentiment analysis 
0	110525	10525	the authors were unable to find the last two mentioned undertakings in the existing literature .in a cs229 project yu and wu this project has tried to further the experimented methods from the literature by focusing on a variety of feature selection techniques implementing neural networks and leveraging the customer reviews through sentiment analysis .last sentence.related work	the authors were unable to find the last two mentioned undertakings in the existing literature 
0	110526	10526	the main data source for this study is the public airbnb dataset for new york city 1 .first sentence.the dataset includes 50 221 entries each with 96 features .dataset	the main data source for this study is the public airbnb dataset for new york city 1 
0	110527	10527	the dataset includes 50 221 entries each with 96 features .the main data source for this study is the public airbnb dataset for new york city 1 .see.dataset	the dataset includes 50 221 entries each with 96 features 
0	110528	10528	see.the dataset includes 50 221 entries each with 96 features .last sentence.dataset	see
0	110529	10529	the reviews for each listing were analyzed using textblob 2 sentiment analysis module .first sentence.this method assigns a score between 1 and 1 to each review and the scores are averaged across each listing .sentiment analysis on the reviews	the reviews for each listing were analyzed using textblob 2 sentiment analysis module 
0	110530	10530	this method assigns a score between 1 and 1 to each review and the scores are averaged across each listing .the reviews for each listing were analyzed using textblob 2 sentiment analysis module .the final scores for each listing was included as a new feature in the model .sentiment analysis on the reviews	this method assigns a score between 1 and 1 to each review and the scores are averaged across each listing 
0	110531	10531	the final scores for each listing was included as a new feature in the model .this method assigns a score between 1 and 1 to each review and the scores are averaged across each listing .last sentence.sentiment analysis on the reviews	the final scores for each listing was included as a new feature in the model 
1	110532	10532	after data preprocessing the feature vector contained 764 elements which was deemed excessive and when fed to models resulted in a high variance of error .first sentence.consequently several feature selection techniques were used to find the features with the most predictive values to both reduce the model variances and reduce the computation time .feature selection	after data preprocessing the feature vector contained 764 elements which was deemed excessive and when fed to models resulted in a high variance of error 
1	110533	10533	consequently several feature selection techniques were used to find the features with the most predictive values to both reduce the model variances and reduce the computation time .after data preprocessing the feature vector contained 764 elements which was deemed excessive and when fed to models resulted in a high variance of error .based on prior experience with housing price estimation the first effort was manual selection of features to create a baseline for the selection process the second method was tuning the coefficient of linear regression model with lasso regularization trained on the train split from which the model with the best performance over validation split was selected .feature selection	consequently several feature selection techniques were used to find the features with the most predictive values to both reduce the model variances and reduce the computation time 
1	110534	10534	based on prior experience with housing price estimation the first effort was manual selection of features to create a baseline for the selection process the second method was tuning the coefficient of linear regression model with lasso regularization trained on the train split from which the model with the best performance over validation split was selected .consequently several feature selection techniques were used to find the features with the most predictive values to both reduce the model variances and reduce the computation time .second set of features consisted of 78 features with non zero values based on this method finally lowest p values of regular linear regression model trained on train split were used to choose the third set of features .feature selection	based on prior experience with housing price estimation the first effort was manual selection of features to create a baseline for the selection process the second method was tuning the coefficient of linear regression model with lasso regularization trained on the train split from which the model with the best performance over validation split was selected 
1	110535	10535	second set of features consisted of 78 features with non zero values based on this method finally lowest p values of regular linear regression model trained on train split were used to choose the third set of features .based on prior experience with housing price estimation the first effort was manual selection of features to create a baseline for the selection process the second method was tuning the coefficient of linear regression model with lasso regularization trained on the train split from which the model with the best performance over validation split was selected .selection was bound by the total number of features to remain less than 100 .feature selection	second set of features consisted of 78 features with non zero values based on this method finally lowest p values of regular linear regression model trained on train split were used to choose the third set of features 
0	110536	10536	selection was bound by the total number of features to remain less than 100 .second set of features consisted of 78 features with non zero values based on this method finally lowest p values of regular linear regression model trained on train split were used to choose the third set of features .the final set of features were those for which linear regression model performed best on validation split the performance of manually selected features as well as p value and lasso feature selection schemes were compared using the r 2 score of the linear regression models trained on the validation set .feature selection	selection was bound by the total number of features to remain less than 100 
1	110537	10537	the final set of features were those for which linear regression model performed best on validation split the performance of manually selected features as well as p value and lasso feature selection schemes were compared using the r 2 score of the linear regression models trained on the validation set .selection was bound by the total number of features to remain less than 100 .all models outperformed the baseline model which used the whole feature set and the second method lasso regularization yielded the highest r 2 score .feature selection	the final set of features were those for which linear regression model performed best on validation split the performance of manually selected features as well as p value and lasso feature selection schemes were compared using the r 2 score of the linear regression models trained on the validation set 
1	110538	10538	all models outperformed the baseline model which used the whole feature set and the second method lasso regularization yielded the highest r 2 score .the final set of features were those for which linear regression model performed best on validation split the performance of manually selected features as well as p value and lasso feature selection schemes were compared using the r 2 score of the linear regression models trained on the validation set .last sentence.feature selection	all models outperformed the baseline model which used the whole feature set and the second method lasso regularization yielded the highest r 2 score 
1	110539	10539	linear regression was set as a baseline model on the dataset using all of the features as model inputs .first sentence.after selecting a set of features using lasso feature selection several machine learning models were considered in order to find the optimal one .methods	linear regression was set as a baseline model on the dataset using all of the features as model inputs 
0	110540	10540	after selecting a set of features using lasso feature selection several machine learning models were considered in order to find the optimal one .linear regression was set as a baseline model on the dataset using all of the features as model inputs .all of the models except neural networks were implemented using scikit learn library.methods	after selecting a set of features using lasso feature selection several machine learning models were considered in order to find the optimal one 
0	110541	10541	all of the models except neural networks were implemented using scikit learn library.after selecting a set of features using lasso feature selection several machine learning models were considered in order to find the optimal one .last sentence.methods	all of the models except neural networks were implemented using scikit learn library
1	110542	10542	linear regression with l 2 regularization adds a penalizing term to the squared error cost function in order to help the algorithm converge for linearly separable data and reduce overfitting .first sentence.therefore ridge regression minimizes j y x 2 2 2 2 with respect to where x is a design matrix and is a hyperparameter .ridge regression	linear regression with l 2 regularization adds a penalizing term to the squared error cost function in order to help the algorithm converge for linearly separable data and reduce overfitting 
0	110543	10543	therefore ridge regression minimizes j y x 2 2 2 2 with respect to where x is a design matrix and is a hyperparameter .linear regression with l 2 regularization adds a penalizing term to the squared error cost function in order to help the algorithm converge for linearly separable data and reduce overfitting .since the baseline models were observed to have high variance ridge regression seemed to be an appropriate choice to solve the issue .ridge regression	therefore ridge regression minimizes j y x 2 2 2 2 with respect to where x is a design matrix and is a hyperparameter 
1	110544	10544	since the baseline models were observed to have high variance ridge regression seemed to be an appropriate choice to solve the issue .therefore ridge regression minimizes j y x 2 2 2 2 with respect to where x is a design matrix and is a hyperparameter .last sentence.ridge regression	since the baseline models were observed to have high variance ridge regression seemed to be an appropriate choice to solve the issue 
1	110545	10545	in order to capture the non linearity of the data the training examples were split into different clusters using k means clustering on the features and the ridge regression was run on each of the individual clusters .first sentence.the data clusters were identified using the following algorithm .k means clustering with ridge regression	in order to capture the non linearity of the data the training examples were split into different clusters using k means clustering on the features and the ridge regression was run on each of the individual clusters 
0	110546	10546	the data clusters were identified using the following algorithm .in order to capture the non linearity of the data the training examples were split into different clusters using k means clustering on the features and the ridge regression was run on each of the individual clusters .last sentence.k means clustering with ridge regression	the data clusters were identified using the following algorithm 
1	110547	10547	initialize cluster centroids i k randomly repeat assgin each point to a cluster for each centroid calculate the loss function for the assignments and check for convergence until convergence.first sentence.last sentence.algorithm 1 k means clustering	initialize cluster centroids i k randomly repeat assgin each point to a cluster for each centroid calculate the loss function for the assignments and check for convergence until convergence
1	110548	10548	in order to model the non linear relationship between the covariates the team employed support vector regression with rbf kernel to identify a linear boundary in a high dimensional feature space .first sentence.using the implementation based on libsvm paper m where c 0 0 are given parameters .support vector regression	in order to model the non linear relationship between the covariates the team employed support vector regression with rbf kernel to identify a linear boundary in a high dimensional feature space 
0	110549	10549	using the implementation based on libsvm paper m where c 0 0 are given parameters .in order to model the non linear relationship between the covariates the team employed support vector regression with rbf kernel to identify a linear boundary in a high dimensional feature space .this problem can be converted into a dual problem that does not involve x but involves k x z x z instead .support vector regression	using the implementation based on libsvm paper m where c 0 0 are given parameters 
0	110550	10550	this problem can be converted into a dual problem that does not involve x but involves k x z x z instead .using the implementation based on libsvm paper m where c 0 0 are given parameters .since we are using rbf kernel k x z exp x z 2 2 2 .support vector regression	this problem can be converted into a dual problem that does not involve x but involves k x z x z instead 
0	110551	10551	since we are using rbf kernel k x z exp x z 2 2 2 .this problem can be converted into a dual problem that does not involve x but involves k x z x z instead .last sentence.support vector regression	since we are using rbf kernel k x z exp x z 2 2 2 
1	110552	10552	neural network was used to build a model that combined the input features into high level predictors .first sentence.the architecture of the optimized network had 3 fully connected layers 20 neurons in the first hidden layer with relu activation function 5 neurons in the second hidden layer with relu activation function and 1 output neuron with a linear activation function .neural network	neural network was used to build a model that combined the input features into high level predictors 
1	110553	10553	the architecture of the optimized network had 3 fully connected layers 20 neurons in the first hidden layer with relu activation function 5 neurons in the second hidden layer with relu activation function and 1 output neuron with a linear activation function .neural network was used to build a model that combined the input features into high level predictors .last sentence.neural network	the architecture of the optimized network had 3 fully connected layers 20 neurons in the first hidden layer with relu activation function 5 neurons in the second hidden layer with relu activation function and 1 output neuron with a linear activation function 
1	110554	10554	since the relationship between the feature vector and price is non linear regression tree seemed like a proper model for this problem .first sentence.regression trees split the data points into regions according to the following formula where j is the feature the dataset is split on t is the threshold of the split r p is the parent region and r 1 and r 2 are the child regions .gradient boost tree ensemble	since the relationship between the feature vector and price is non linear regression tree seemed like a proper model for this problem 
1	110555	10555	regression trees split the data points into regions according to the following formula where j is the feature the dataset is split on t is the threshold of the split r p is the parent region and r 1 and r 2 are the child regions .since the relationship between the feature vector and price is non linear regression tree seemed like a proper model for this problem .squared error is used as the loss function since standalone regression trees have low predictive accuracies individually gradient boost tree ensemble was used to increase the models performance .gradient boost tree ensemble	regression trees split the data points into regions according to the following formula where j is the feature the dataset is split on t is the threshold of the split r p is the parent region and r 1 and r 2 are the child regions 
1	110556	10556	squared error is used as the loss function since standalone regression trees have low predictive accuracies individually gradient boost tree ensemble was used to increase the models performance .regression trees split the data points into regions according to the following formula where j is the feature the dataset is split on t is the threshold of the split r p is the parent region and r 1 and r 2 are the child regions .the idea behind a gradient boost is to improve on a previous iteration of the model by correcting its predictions using another model based on the negative gradient of the loss .gradient boost tree ensemble	squared error is used as the loss function since standalone regression trees have low predictive accuracies individually gradient boost tree ensemble was used to increase the models performance 
0	110557	10557	the idea behind a gradient boost is to improve on a previous iteration of the model by correcting its predictions using another model based on the negative gradient of the loss .squared error is used as the loss function since standalone regression trees have low predictive accuracies individually gradient boost tree ensemble was used to increase the models performance .the algorithm for the gradient boosting is the following.gradient boost tree ensemble	the idea behind a gradient boost is to improve on a previous iteration of the model by correcting its predictions using another model based on the negative gradient of the loss 
0	110558	10558	the algorithm for the gradient boosting is the following.the idea behind a gradient boost is to improve on a previous iteration of the model by correcting its predictions using another model based on the negative gradient of the loss .last sentence.gradient boost tree ensemble	the algorithm for the gradient boosting is the following
0	110559	10559	initialize f 0 to be a constant model for m 1 number of iterations do for all training examples .first sentence.last sentence.algorithm 2 gradient boosting	initialize f 0 to be a constant model for m 1 number of iterations do for all training examples 
1	110560	10560	mean absolute error mae mean squared error mse and r 2 score were used to evaluate the trained models .first sentence.training 39 980 examples and validation 4 998 examples splits were used to choose the best performing models within each category .experiments and discussion	mean absolute error mae mean squared error mse and r 2 score were used to evaluate the trained models 
1	110561	10561	training 39 980 examples and validation 4 998 examples splits were used to choose the best performing models within each category .mean absolute error mae mean squared error mse and r 2 score were used to evaluate the trained models .the test set containing 4 998 examples was used to provide an unbiased estimate of error with the final models trained on both train and validation splits .experiments and discussion	training 39 980 examples and validation 4 998 examples splits were used to choose the best performing models within each category 
1	110562	10562	the test set containing 4 998 examples was used to provide an unbiased estimate of error with the final models trained on both train and validation splits .training 39 980 examples and validation 4 998 examples splits were used to choose the best performing models within each category .results for the final models 3 are provided below .experiments and discussion	the test set containing 4 998 examples was used to provide an unbiased estimate of error with the final models trained on both train and validation splits 
0	110563	10563	results for the final models 3 are provided below .the test set containing 4 998 examples was used to provide an unbiased estimate of error with the final models trained on both train and validation splits .the outlined models had relatively similar r 2 scores which implicates that lasso feature importance analysis had made the most impact on improving the performance of the models by reducing the variance .experiments and discussion	results for the final models 3 are provided below 
1	110564	10564	the outlined models had relatively similar r 2 scores which implicates that lasso feature importance analysis had made the most impact on improving the performance of the models by reducing the variance .results for the final models 3 are provided below .even after the feature selection the resulting input vector was relatively large leaving room for model overfitting .experiments and discussion	the outlined models had relatively similar r 2 scores which implicates that lasso feature importance analysis had made the most impact on improving the performance of the models by reducing the variance 
0	110565	10565	even after the feature selection the resulting input vector was relatively large leaving room for model overfitting .the outlined models had relatively similar r 2 scores which implicates that lasso feature importance analysis had made the most impact on improving the performance of the models by reducing the variance .this explains why gradient boost a tree based model prone to high varianceperformed worse than the rest of the models despite it not performing the worst on the training set .experiments and discussion	even after the feature selection the resulting input vector was relatively large leaving room for model overfitting 
1	110566	10566	this explains why gradient boost a tree based model prone to high varianceperformed worse than the rest of the models despite it not performing the worst on the training set .even after the feature selection the resulting input vector was relatively large leaving room for model overfitting .last sentence.experiments and discussion	this explains why gradient boost a tree based model prone to high varianceperformed worse than the rest of the models despite it not performing the worst on the training set 
1	110567	10567	despite expanding the number of features in the feature vector svr with rbf kernel turned out to be the best performing model with the least mae and mse and the highest r 2 score on both train and test sets figure 2 .first sentence.rbf feature mapping was able to better model the prices of the apartments which have a non linear relationship with the apartment features .model	despite expanding the number of features in the feature vector svr with rbf kernel turned out to be the best performing model with the least mae and mse and the highest r 2 score on both train and test sets figure 2 
1	110568	10568	rbf feature mapping was able to better model the prices of the apartments which have a non linear relationship with the apartment features .despite expanding the number of features in the feature vector svr with rbf kernel turned out to be the best performing model with the least mae and mse and the highest r 2 score on both train and test sets figure 2 .since regularization is taken into account in the svr optimization problem parameter tuning ensured that the model was not overfitting .model	rbf feature mapping was able to better model the prices of the apartments which have a non linear relationship with the apartment features 
0	110569	10569	since regularization is taken into account in the svr optimization problem parameter tuning ensured that the model was not overfitting .rbf feature mapping was able to better model the prices of the apartments which have a non linear relationship with the apartment features .ridge regression neural network k means ridge regression models had similar r 2 scores even though the last two models are more complex than ridge regression .model	since regularization is taken into account in the svr optimization problem parameter tuning ensured that the model was not overfitting 
1	110570	10570	ridge regression neural network k means ridge regression models had similar r 2 scores even though the last two models are more complex than ridge regression .since regularization is taken into account in the svr optimization problem parameter tuning ensured that the model was not overfitting .the architecture complexity of neural network was limited by the insufficient number of training examples for having too many unknown weights .model	ridge regression neural network k means ridge regression models had similar r 2 scores even though the last two models are more complex than ridge regression 
0	110571	10571	the architecture complexity of neural network was limited by the insufficient number of training examples for having too many unknown weights .ridge regression neural network k means ridge regression models had similar r 2 scores even though the last two models are more complex than ridge regression .k means clustering model faced a similar issue since the frequency of some prices was greatly exceeding the frequency of others some clusters received too few training examples and drove down the overall model performance .model	the architecture complexity of neural network was limited by the insufficient number of training examples for having too many unknown weights 
1	110572	10572	k means clustering model faced a similar issue since the frequency of some prices was greatly exceeding the frequency of others some clusters received too few training examples and drove down the overall model performance .the architecture complexity of neural network was limited by the insufficient number of training examples for having too many unknown weights .last sentence.model	k means clustering model faced a similar issue since the frequency of some prices was greatly exceeding the frequency of others some clusters received too few training examples and drove down the overall model performance 
1	110573	10573	this project attempts to come up with the best model for predicting the airbnb prices based on a set of features including property specifications owner information and customer reviews on the listings .first sentence.machine learning techniques including linear regression tree based models svr and neural networks along with feature importance analyses are employed to achieve the best results in terms of mean squared error mean absolute error and r 2 score .conclusions and future work	this project attempts to come up with the best model for predicting the airbnb prices based on a set of features including property specifications owner information and customer reviews on the listings 
1	110574	10574	machine learning techniques including linear regression tree based models svr and neural networks along with feature importance analyses are employed to achieve the best results in terms of mean squared error mean absolute error and r 2 score .this project attempts to come up with the best model for predicting the airbnb prices based on a set of features including property specifications owner information and customer reviews on the listings .the initial experimentation with the baseline model proved that the abundance of features leads to high variance and weak performance of the model on the validation set compared to the training set .conclusions and future work	machine learning techniques including linear regression tree based models svr and neural networks along with feature importance analyses are employed to achieve the best results in terms of mean squared error mean absolute error and r 2 score 
1	110575	10575	the initial experimentation with the baseline model proved that the abundance of features leads to high variance and weak performance of the model on the validation set compared to the training set .machine learning techniques including linear regression tree based models svr and neural networks along with feature importance analyses are employed to achieve the best results in terms of mean squared error mean absolute error and r 2 score .lasso cross validation feature importance analysis reduced the variance and using advanced models such as svr and neural networks resulted in higher r 2 score for both the validation and test sets .conclusions and future work	the initial experimentation with the baseline model proved that the abundance of features leads to high variance and weak performance of the model on the validation set compared to the training set 
1	110576	10576	lasso cross validation feature importance analysis reduced the variance and using advanced models such as svr and neural networks resulted in higher r 2 score for both the validation and test sets .the initial experimentation with the baseline model proved that the abundance of features leads to high variance and weak performance of the model on the validation set compared to the training set .among the models tested support vector regression svr performed the best and produced an r 2 score of 69 and a mse of 0 147 defined on ln price on the test set .conclusions and future work	lasso cross validation feature importance analysis reduced the variance and using advanced models such as svr and neural networks resulted in higher r 2 score for both the validation and test sets 
1	110577	10577	among the models tested support vector regression svr performed the best and produced an r 2 score of 69 and a mse of 0 147 defined on ln price on the test set .lasso cross validation feature importance analysis reduced the variance and using advanced models such as svr and neural networks resulted in higher r 2 score for both the validation and test sets .this level of accuracy is a promising outcome given the heterogeneity of the dataset and the involved hidden factors including the personal characteristics of the owners which were impossible to consider the future works on this project can include i studying other feature selection schemes such as random forest feature importance ii further experimentation with neural net architectures and iii getting more training examples from other hospitality services such as vrbo to boost the performance of k means clustering with ridge regression model in particular .conclusions and future work	among the models tested support vector regression svr performed the best and produced an r 2 score of 69 and a mse of 0 147 defined on ln price on the test set 
1	110578	10578	this level of accuracy is a promising outcome given the heterogeneity of the dataset and the involved hidden factors including the personal characteristics of the owners which were impossible to consider the future works on this project can include i studying other feature selection schemes such as random forest feature importance ii further experimentation with neural net architectures and iii getting more training examples from other hospitality services such as vrbo to boost the performance of k means clustering with ridge regression model in particular .among the models tested support vector regression svr performed the best and produced an r 2 score of 69 and a mse of 0 147 defined on ln price on the test set .last sentence.conclusions and future work	this level of accuracy is a promising outcome given the heterogeneity of the dataset and the involved hidden factors including the personal characteristics of the owners which were impossible to consider the future works on this project can include i studying other feature selection schemes such as random forest feature importance ii further experimentation with neural net architectures and iii getting more training examples from other hospitality services such as vrbo to boost the performance of k means clustering with ridge regression model in particular 
1	110579	10579	liubov nikolenko data cleaning splitting categorical features implementing sentiment analysis of the reviews initial neural network implementation svr implementation and tuning k means ridge tuning hoormazd rezaei implementation of linear regression and tree ensembles datapreprocessing implementation of the evaluation metrics feature selection methods implementation tuning of the neural network pouya rezazadeh data cleaning and auxiliary visualization splitting categorical features result visualizations tree ensembles tuning k means ridge implementation .first sentence.last sentence.contributions	liubov nikolenko data cleaning splitting categorical features implementing sentiment analysis of the reviews initial neural network implementation svr implementation and tuning k means ridge tuning hoormazd rezaei implementation of linear regression and tree ensembles datapreprocessing implementation of the evaluation metrics feature selection methods implementation tuning of the neural network pouya rezazadeh data cleaning and auxiliary visualization splitting categorical features result visualizations tree ensembles tuning k means ridge implementation 
0	110580	10580	doodle recognition has important consequences in computer vision and pattern recognition especially in relation to the handling of noisy datasets .first sentence.in this paper we build a multi class classifier to assign hand drawn doodles from google s online game quick draw .abstract	doodle recognition has important consequences in computer vision and pattern recognition especially in relation to the handling of noisy datasets 
0	110581	10581	in this paper we build a multi class classifier to assign hand drawn doodles from google s online game quick draw .doodle recognition has important consequences in computer vision and pattern recognition especially in relation to the handling of noisy datasets .into 345 unique categories .abstract	in this paper we build a multi class classifier to assign hand drawn doodles from google s online game quick draw 
0	110582	10582	into 345 unique categories .in this paper we build a multi class classifier to assign hand drawn doodles from google s online game quick draw .to do so we implement and compare multiple variations of k nearest neighbors and a convolutional neural network which achieve 35 accuracy and 60 accuracy respectively .abstract	into 345 unique categories 
0	110583	10583	to do so we implement and compare multiple variations of k nearest neighbors and a convolutional neural network which achieve 35 accuracy and 60 accuracy respectively .into 345 unique categories .by evaluating the models performance and learned features we can identify distinct characteristics of the dataset that will prove important for future work .abstract	to do so we implement and compare multiple variations of k nearest neighbors and a convolutional neural network which achieve 35 accuracy and 60 accuracy respectively 
0	110584	10584	by evaluating the models performance and learned features we can identify distinct characteristics of the dataset that will prove important for future work .to do so we implement and compare multiple variations of k nearest neighbors and a convolutional neural network which achieve 35 accuracy and 60 accuracy respectively .last sentence.abstract	by evaluating the models performance and learned features we can identify distinct characteristics of the dataset that will prove important for future work 
0	110585	10585	in november 2016 google released an online game titled quick draw .first sentence.that challenges players to draw a given object in under 20 seconds .introduction	in november 2016 google released an online game titled quick draw 
0	110586	10586	that challenges players to draw a given object in under 20 seconds .in november 2016 google released an online game titled quick draw .however this is no ordinary game while the user is drawing an advanced neural network attempts to guess the category of the object and its predictions evolve as the user adds more and more detail beyond just the scope of quick draw the ability to recognize and classify hand drawn doodles has important implications for the development of artificial intelligence at large .introduction	that challenges players to draw a given object in under 20 seconds 
0	110587	10587	however this is no ordinary game while the user is drawing an advanced neural network attempts to guess the category of the object and its predictions evolve as the user adds more and more detail beyond just the scope of quick draw the ability to recognize and classify hand drawn doodles has important implications for the development of artificial intelligence at large .that challenges players to draw a given object in under 20 seconds .for example research in computer vision and pattern recognition especially in subfields such as optical character recognition ocr would benefit greatly from the advent of a robust classifier on high noise datasets for the purposes of this project we choose to focus on classification of the finished doodles in their entirety .introduction	however this is no ordinary game while the user is drawing an advanced neural network attempts to guess the category of the object and its predictions evolve as the user adds more and more detail beyond just the scope of quick draw the ability to recognize and classify hand drawn doodles has important implications for the development of artificial intelligence at large 
0	110588	10588	for example research in computer vision and pattern recognition especially in subfields such as optical character recognition ocr would benefit greatly from the advent of a robust classifier on high noise datasets for the purposes of this project we choose to focus on classification of the finished doodles in their entirety .however this is no ordinary game while the user is drawing an advanced neural network attempts to guess the category of the object and its predictions evolve as the user adds more and more detail beyond just the scope of quick draw the ability to recognize and classify hand drawn doodles has important implications for the development of artificial intelligence at large .while a simpler premise than that of the original game s this task remains difficult due to the large number of categories 345 wide variation of doodles within even a single category and confusing similarity between doodles across multiple categories thus we create a multi class classifier whose input is a quick draw .introduction	for example research in computer vision and pattern recognition especially in subfields such as optical character recognition ocr would benefit greatly from the advent of a robust classifier on high noise datasets for the purposes of this project we choose to focus on classification of the finished doodles in their entirety 
1	110589	10589	while a simpler premise than that of the original game s this task remains difficult due to the large number of categories 345 wide variation of doodles within even a single category and confusing similarity between doodles across multiple categories thus we create a multi class classifier whose input is a quick draw .for example research in computer vision and pattern recognition especially in subfields such as optical character recognition ocr would benefit greatly from the advent of a robust classifier on high noise datasets for the purposes of this project we choose to focus on classification of the finished doodles in their entirety .doodle and whose output is the predicted category for the depicted object .introduction	while a simpler premise than that of the original game s this task remains difficult due to the large number of categories 345 wide variation of doodles within even a single category and confusing similarity between doodles across multiple categories thus we create a multi class classifier whose input is a quick draw 
0	110590	10590	doodle and whose output is the predicted category for the depicted object .while a simpler premise than that of the original game s this task remains difficult due to the large number of categories 345 wide variation of doodles within even a single category and confusing similarity between doodles across multiple categories thus we create a multi class classifier whose input is a quick draw .last sentence.introduction	doodle and whose output is the predicted category for the depicted object 
0	110591	10591	similar to our task google engineers ha and eck used the quick draw .first sentence.online dataset to train their recurrent neural network rnn to learn sketch abstractions .related work	similar to our task google engineers ha and eck used the quick draw 
0	110592	10592	online dataset to train their recurrent neural network rnn to learn sketch abstractions .similar to our task google engineers ha and eck used the quick draw .kim and saverese experimented with svm and knn performance on image classification specifically on airplanes cars faces and motorbikes .related work	online dataset to train their recurrent neural network rnn to learn sketch abstractions 
0	110593	10593	kim and saverese experimented with svm and knn performance on image classification specifically on airplanes cars faces and motorbikes .online dataset to train their recurrent neural network rnn to learn sketch abstractions .lu and tran architected a convolutional neural network cnn to tackle sketch classification .related work	kim and saverese experimented with svm and knn performance on image classification specifically on airplanes cars faces and motorbikes 
0	110594	10594	lu and tran architected a convolutional neural network cnn to tackle sketch classification .kim and saverese experimented with svm and knn performance on image classification specifically on airplanes cars faces and motorbikes .the state of the art as of 2017 comes from a cnn developed by seddati et al .related work	lu and tran architected a convolutional neural network cnn to tackle sketch classification 
0	110595	10595	the state of the art as of 2017 comes from a cnn developed by seddati et al .lu and tran architected a convolutional neural network cnn to tackle sketch classification .with their deepsketch 3 model for sketch classification .related work	the state of the art as of 2017 comes from a cnn developed by seddati et al 
0	110596	10596	with their deepsketch 3 model for sketch classification .the state of the art as of 2017 comes from a cnn developed by seddati et al .last sentence.related work	with their deepsketch 3 model for sketch classification 
0	110597	10597	google publicly released a quick draw .first sentence.dataset containing over 50 million images across 345 categories .data	google publicly released a quick draw 
0	110598	10598	dataset containing over 50 million images across 345 categories .google publicly released a quick draw .there are multiple different representations for the images .data	dataset containing over 50 million images across 345 categories 
0	110599	10599	there are multiple different representations for the images .dataset containing over 50 million images across 345 categories .one dataset represents each drawing as a series of line vectors and another contains each image in a 28x28 grayscale matrix .data	there are multiple different representations for the images 
0	110600	10600	one dataset represents each drawing as a series of line vectors and another contains each image in a 28x28 grayscale matrix .there are multiple different representations for the images .because we focus on classification of the entire doodle in this project we use the latter version of the dataset .data	one dataset represents each drawing as a series of line vectors and another contains each image in a 28x28 grayscale matrix 
0	110601	10601	because we focus on classification of the entire doodle in this project we use the latter version of the dataset .one dataset represents each drawing as a series of line vectors and another contains each image in a 28x28 grayscale matrix .we treat each 28x28 pixel image as a 784 dimensional vector .data	because we focus on classification of the entire doodle in this project we use the latter version of the dataset 
0	110602	10602	we treat each 28x28 pixel image as a 784 dimensional vector .because we focus on classification of the entire doodle in this project we use the latter version of the dataset .to test our models we split the data into three different folds 70 for training 15 for validation and 15 for testing .data	we treat each 28x28 pixel image as a 784 dimensional vector 
0	110603	10603	to test our models we split the data into three different folds 70 for training 15 for validation and 15 for testing .we treat each 28x28 pixel image as a 784 dimensional vector .to reduce computation time and storage of the data we decided to create a smaller subset of the original dataset by randomly sampling 1 of the drawings from each category as a result we obtain approximately 350 000 examples for the training set and 75 000 examples each for the validation and testing set .data	to test our models we split the data into three different folds 70 for training 15 for validation and 15 for testing 
1	110604	10604	to reduce computation time and storage of the data we decided to create a smaller subset of the original dataset by randomly sampling 1 of the drawings from each category as a result we obtain approximately 350 000 examples for the training set and 75 000 examples each for the validation and testing set .to test our models we split the data into three different folds 70 for training 15 for validation and 15 for testing .furthermore the number of drawings in each category is balanced so this leaves approximately 1000 examples per category in the training dataset .data	to reduce computation time and storage of the data we decided to create a smaller subset of the original dataset by randomly sampling 1 of the drawings from each category as a result we obtain approximately 350 000 examples for the training set and 75 000 examples each for the validation and testing set 
0	110605	10605	furthermore the number of drawings in each category is balanced so this leaves approximately 1000 examples per category in the training dataset .to reduce computation time and storage of the data we decided to create a smaller subset of the original dataset by randomly sampling 1 of the drawings from each category as a result we obtain approximately 350 000 examples for the training set and 75 000 examples each for the validation and testing set .last sentence.data	furthermore the number of drawings in each category is balanced so this leaves approximately 1000 examples per category in the training dataset 
0	110606	10606	for our baseline we intuitively assume that all the images in a particular category should look relatively similar .first sentence.based on this assumption one way we could determine which category a given drawing belongs to is by looking at which training examples are the most nearby to the doodle under test this intuition corresponds with the k nearest neighbors knn algorithm .1 closest centroid 1 cc 	for our baseline we intuitively assume that all the images in a particular category should look relatively similar 
0	110607	10607	based on this assumption one way we could determine which category a given drawing belongs to is by looking at which training examples are the most nearby to the doodle under test this intuition corresponds with the k nearest neighbors knn algorithm .for our baseline we intuitively assume that all the images in a particular category should look relatively similar .the vanilla knn algorithm computes the k training examples that are closest in l 1 or l 2 distance to our current drawing .1 closest centroid 1 cc 	based on this assumption one way we could determine which category a given drawing belongs to is by looking at which training examples are the most nearby to the doodle under test this intuition corresponds with the k nearest neighbors knn algorithm 
0	110608	10608	the vanilla knn algorithm computes the k training examples that are closest in l 1 or l 2 distance to our current drawing .based on this assumption one way we could determine which category a given drawing belongs to is by looking at which training examples are the most nearby to the doodle under test this intuition corresponds with the k nearest neighbors knn algorithm .then it predicts the category that occurs the greatest number of times among those k neighbors .1 closest centroid 1 cc 	the vanilla knn algorithm computes the k training examples that are closest in l 1 or l 2 distance to our current drawing 
0	110609	10609	then it predicts the category that occurs the greatest number of times among those k neighbors .the vanilla knn algorithm computes the k training examples that are closest in l 1 or l 2 distance to our current drawing .however because we have 350k training examples and 75k validation examples this algorithm requires at least 3 5 10 5 7 5 10 4 784 2 10 13 operations to evaluate the entire validation set which is too slow consequently we propose a less computationally expensive variant of knn which we call 1 closest centroid 1 cc .1 closest centroid 1 cc 	then it predicts the category that occurs the greatest number of times among those k neighbors 
1	110610	10610	however because we have 350k training examples and 75k validation examples this algorithm requires at least 3 5 10 5 7 5 10 4 784 2 10 13 operations to evaluate the entire validation set which is too slow consequently we propose a less computationally expensive variant of knn which we call 1 closest centroid 1 cc .then it predicts the category that occurs the greatest number of times among those k neighbors .at a high level 1 cc equivalent to supervised kmeans clustering in which we compute a centroid for each category c using the training dataset and classify test examples according to the closest categories in more detail for each category c we calculate a centroid vector v c by taking the average of all of the vectors belonging to category c then to classify a given vector u we compute arg min c u v c 2 which seeks to minimize the squared difference in pixel values between the two images .1 closest centroid 1 cc 	however because we have 350k training examples and 75k validation examples this algorithm requires at least 3 5 10 5 7 5 10 4 784 2 10 13 operations to evaluate the entire validation set which is too slow consequently we propose a less computationally expensive variant of knn which we call 1 closest centroid 1 cc 
1	110611	10611	at a high level 1 cc equivalent to supervised kmeans clustering in which we compute a centroid for each category c using the training dataset and classify test examples according to the closest categories in more detail for each category c we calculate a centroid vector v c by taking the average of all of the vectors belonging to category c then to classify a given vector u we compute arg min c u v c 2 which seeks to minimize the squared difference in pixel values between the two images .however because we have 350k training examples and 75k validation examples this algorithm requires at least 3 5 10 5 7 5 10 4 784 2 10 13 operations to evaluate the entire validation set which is too slow consequently we propose a less computationally expensive variant of knn which we call 1 closest centroid 1 cc .effectively we are choosing the category whose mean representation vector is closest in euclidean distance to our given vector u .1 closest centroid 1 cc 	at a high level 1 cc equivalent to supervised kmeans clustering in which we compute a centroid for each category c using the training dataset and classify test examples according to the closest categories in more detail for each category c we calculate a centroid vector v c by taking the average of all of the vectors belonging to category c then to classify a given vector u we compute arg min c u v c 2 which seeks to minimize the squared difference in pixel values between the two images 
0	110612	10612	effectively we are choosing the category whose mean representation vector is closest in euclidean distance to our given vector u .at a high level 1 cc equivalent to supervised kmeans clustering in which we compute a centroid for each category c using the training dataset and classify test examples according to the closest categories in more detail for each category c we calculate a centroid vector v c by taking the average of all of the vectors belonging to category c then to classify a given vector u we compute arg min c u v c 2 which seeks to minimize the squared difference in pixel values between the two images .this reduces the number of points we look at for each u to only 345 one per category .1 closest centroid 1 cc 	effectively we are choosing the category whose mean representation vector is closest in euclidean distance to our given vector u 
0	110613	10613	this reduces the number of points we look at for each u to only 345 one per category .effectively we are choosing the category whose mean representation vector is closest in euclidean distance to our given vector u .last sentence.1 closest centroid 1 cc 	this reduces the number of points we look at for each u to only 345 one per category 
0	110614	10614	1 cc makes the simplifying assumption that all doodles in a category will be similar to each other .first sentence.however in reality there are many different ways to draw a given object .knn with k means 	1 cc makes the simplifying assumption that all doodles in a category will be similar to each other 
0	110615	10615	however in reality there are many different ways to draw a given object .1 cc makes the simplifying assumption that all doodles in a category will be similar to each other .for example bear can be drawn with multiple representations as seen in.knn with k means 	however in reality there are many different ways to draw a given object 
0	110616	10616	for example bear can be drawn with multiple representations as seen in.however in reality there are many different ways to draw a given object .last sentence.knn with k means 	for example bear can be drawn with multiple representations as seen in
0	110617	10617	we noticed that voting in knn often ended up with ties .first sentence.to mitigate ties we further extend knn to not only utilize multiple clusters per category but also to use a weighted voting schema when tallying for final predictions .knn with k means and weighted voting	we noticed that voting in knn often ended up with ties 
0	110618	10618	to mitigate ties we further extend knn to not only utilize multiple clusters per category but also to use a weighted voting schema when tallying for final predictions .we noticed that voting in knn often ended up with ties .we name this method knn weighted for short intuitively we wish to count votes from closer centroids more than votes from more distance centroids .knn with k means and weighted voting	to mitigate ties we further extend knn to not only utilize multiple clusters per category but also to use a weighted voting schema when tallying for final predictions 
0	110619	10619	we name this method knn weighted for short intuitively we wish to count votes from closer centroids more than votes from more distance centroids .to mitigate ties we further extend knn to not only utilize multiple clusters per category but also to use a weighted voting schema when tallying for final predictions .thus we experiment with two different weighting schemas distance weighting .knn with k means and weighted voting	we name this method knn weighted for short intuitively we wish to count votes from closer centroids more than votes from more distance centroids 
0	110620	10620	thus we experiment with two different weighting schemas distance weighting .we name this method knn weighted for short intuitively we wish to count votes from closer centroids more than votes from more distance centroids .with distance weighting each cen troid s c s weighted vote w i is equal towhere x i is the vector representation of the test example .knn with k means and weighted voting	thus we experiment with two different weighting schemas distance weighting 
0	110621	10621	with distance weighting each cen troid s c s weighted vote w i is equal towhere x i is the vector representation of the test example .thus we experiment with two different weighting schemas distance weighting .rank weighting .knn with k means and weighted voting	with distance weighting each cen troid s c s weighted vote w i is equal towhere x i is the vector representation of the test example 
0	110622	10622	rank weighting .with distance weighting each cen troid s c s weighted vote w i is equal towhere x i is the vector representation of the test example .with rank weighting we first sort all centroids by increasing distance to the test example .knn with k means and weighted voting	rank weighting 
0	110623	10623	with rank weighting we first sort all centroids by increasing distance to the test example .rank weighting .within this sorted order the centroid c i at rank i has a weighted vote equal to.knn with k means and weighted voting	with rank weighting we first sort all centroids by increasing distance to the test example 
0	110624	10624	within this sorted order the centroid c i at rank i has a weighted vote equal to.with rank weighting we first sort all centroids by increasing distance to the test example .last sentence.knn with k means and weighted voting	within this sorted order the centroid c i at rank i has a weighted vote equal to
1	110625	10625	as a comparison against the above knn methods we implement a convolutional neural network cnn a stateof the art model known for being able to recognize and quickly learn local features within an image to achieve the best results we perform data preprocessing .first sentence.first we calculate the mean across all training examples as well as the standard deviation .convolutional neural network	as a comparison against the above knn methods we implement a convolutional neural network cnn a stateof the art model known for being able to recognize and quickly learn local features within an image to achieve the best results we perform data preprocessing 
0	110626	10626	first we calculate the mean across all training examples as well as the standard deviation .as a comparison against the above knn methods we implement a convolutional neural network cnn a stateof the art model known for being able to recognize and quickly learn local features within an image to achieve the best results we perform data preprocessing .we then for each example training validation and test subtract and divide by .convolutional neural network	first we calculate the mean across all training examples as well as the standard deviation 
0	110627	10627	we then for each example training validation and test subtract and divide by .first we calculate the mean across all training examples as well as the standard deviation .to account for division by zero errors when dividing by we add an offset of 10 to beforehand the model architecture is shown in.convolutional neural network	we then for each example training validation and test subtract and divide by 
0	110628	10628	to account for division by zero errors when dividing by we add an offset of 10 to beforehand the model architecture is shown in.we then for each example training validation and test subtract and divide by .last sentence.convolutional neural network	to account for division by zero errors when dividing by we add an offset of 10 to beforehand the model architecture is shown in
0	110629	10629	while raw accuracy is a good measure of a model s performance it penalizes harshly for an incorrect prediction wrong predictions receive 0 points and right predictions receive 1 point .first sentence.since we have so many categories including some that are extremely similar such as cake and birthday cake we evaluate our methods not only with raw accuracy but also with a scoring metric that is more lenient of incorrect predictions thus predictions are evaluated using mean average precision 3 map 3 where u is the number of drawings in the test set p k is the precision at cutoff k and n is the number of predictions per drawing .evaluation metric	while raw accuracy is a good measure of a model s performance it penalizes harshly for an incorrect prediction wrong predictions receive 0 points and right predictions receive 1 point 
1	110630	10630	since we have so many categories including some that are extremely similar such as cake and birthday cake we evaluate our methods not only with raw accuracy but also with a scoring metric that is more lenient of incorrect predictions thus predictions are evaluated using mean average precision 3 map 3 where u is the number of drawings in the test set p k is the precision at cutoff k and n is the number of predictions per drawing .while raw accuracy is a good measure of a model s performance it penalizes harshly for an incorrect prediction wrong predictions receive 0 points and right predictions receive 1 point .put more intuitively the equation considers the top 3 predictions p 1 p 2 p 3 that the model makes for a given drawing .evaluation metric	since we have so many categories including some that are extremely similar such as cake and birthday cake we evaluate our methods not only with raw accuracy but also with a scoring metric that is more lenient of incorrect predictions thus predictions are evaluated using mean average precision 3 map 3 where u is the number of drawings in the test set p k is the precision at cutoff k and n is the number of predictions per drawing 
0	110631	10631	put more intuitively the equation considers the top 3 predictions p 1 p 2 p 3 that the model makes for a given drawing .since we have so many categories including some that are extremely similar such as cake and birthday cake we evaluate our methods not only with raw accuracy but also with a scoring metric that is more lenient of incorrect predictions thus predictions are evaluated using mean average precision 3 map 3 where u is the number of drawings in the test set p k is the precision at cutoff k and n is the number of predictions per drawing .it then assigns a score of 1 i if p i is the correct label for the image and a score of 0 if the correct label is not in the top 3 guesses .evaluation metric	put more intuitively the equation considers the top 3 predictions p 1 p 2 p 3 that the model makes for a given drawing 
0	110632	10632	it then assigns a score of 1 i if p i is the correct label for the image and a score of 0 if the correct label is not in the top 3 guesses .put more intuitively the equation considers the top 3 predictions p 1 p 2 p 3 that the model makes for a given drawing .note that map 1 is equivalent to singleprediction accuracy .evaluation metric	it then assigns a score of 1 i if p i is the correct label for the image and a score of 0 if the correct label is not in the top 3 guesses 
0	110633	10633	note that map 1 is equivalent to singleprediction accuracy .it then assigns a score of 1 i if p i is the correct label for the image and a score of 0 if the correct label is not in the top 3 guesses .last sentence.evaluation metric	note that map 1 is equivalent to singleprediction accuracy 
0	110634	10634	as seen in 1 cc performed best on the categories stairs circle and door .first sentence.knn performed best on the categories stairs the eiffel tower and bowtie .1 cc and knn analysis	as seen in 1 cc performed best on the categories stairs circle and door 
0	110635	10635	knn performed best on the categories stairs the eiffel tower and bowtie .as seen in 1 cc performed best on the categories stairs circle and door .for these categories the centroids are either simplistic circle door or are distinct in shape stairs the eiffel tower bowtie which causes the doodles to have less variance .1 cc and knn analysis	knn performed best on the categories stairs the eiffel tower and bowtie 
0	110636	10636	for these categories the centroids are either simplistic circle door or are distinct in shape stairs the eiffel tower bowtie which causes the doodles to have less variance .knn performed best on the categories stairs the eiffel tower and bowtie .thus the centroids are generally contain a clear outline of the object on the other hand 1 cc performed worst on the categories flip flops garden hose and wrist watch and knn performed worst on dog string bean and peas .1 cc and knn analysis	for these categories the centroids are either simplistic circle door or are distinct in shape stairs the eiffel tower bowtie which causes the doodles to have less variance 
1	110637	10637	thus the centroids are generally contain a clear outline of the object on the other hand 1 cc performed worst on the categories flip flops garden hose and wrist watch and knn performed worst on dog string bean and peas .for these categories the centroids are either simplistic circle door or are distinct in shape stairs the eiffel tower bowtie which causes the doodles to have less variance .the centroids for these bottom 3 categories are much more vague .1 cc and knn analysis	thus the centroids are generally contain a clear outline of the object on the other hand 1 cc performed worst on the categories flip flops garden hose and wrist watch and knn performed worst on dog string bean and peas 
0	110638	10638	the centroids for these bottom 3 categories are much more vague .thus the centroids are generally contain a clear outline of the object on the other hand 1 cc performed worst on the categories flip flops garden hose and wrist watch and knn performed worst on dog string bean and peas .for example dog was often confused with other four legged animals such as horse and cow furthermore some categories produced nearly identical centroids such as circle and octagon in.1 cc and knn analysis	the centroids for these bottom 3 categories are much more vague 
0	110639	10639	for example dog was often confused with other four legged animals such as horse and cow furthermore some categories produced nearly identical centroids such as circle and octagon in.the centroids for these bottom 3 categories are much more vague .last sentence.1 cc and knn analysis	for example dog was often confused with other four legged animals such as horse and cow furthermore some categories produced nearly identical centroids such as circle and octagon in
1	110640	10640	knn with weighted votes by rank produced the highest map 3 and map 1 scores out of all the knn models .first sentence.last sentence.knn weighted analysis	knn with weighted votes by rank produced the highest map 3 and map 1 scores out of all the knn models 
1	110641	10641	to achieve the best performance for the cnn model we tuned various hyperparameters including the number of units in each dense layer dropout rate and learning rate .first sentence.overall we found that the model producing the best map 3 score on the validation set had three dense layers with 700 500 and 400 units with each layer having a dropout rate of 0 2 .cnn analysis	to achieve the best performance for the cnn model we tuned various hyperparameters including the number of units in each dense layer dropout rate and learning rate 
1	110642	10642	overall we found that the model producing the best map 3 score on the validation set had three dense layers with 700 500 and 400 units with each layer having a dropout rate of 0 2 .to achieve the best performance for the cnn model we tuned various hyperparameters including the number of units in each dense layer dropout rate and learning rate .furthermore we trained our model with learning rate of 1 10 3 and batch size of 32 across 20 epochs .cnn analysis	overall we found that the model producing the best map 3 score on the validation set had three dense layers with 700 500 and 400 units with each layer having a dropout rate of 0 2 
0	110643	10643	furthermore we trained our model with learning rate of 1 10 3 and batch size of 32 across 20 epochs .overall we found that the model producing the best map 3 score on the validation set had three dense layers with 700 500 and 400 units with each layer having a dropout rate of 0 2 .the end architecture fits the data well as we see from the loss plot in inspecting the accuracy distribution across individual category we note from.cnn analysis	furthermore we trained our model with learning rate of 1 10 3 and batch size of 32 across 20 epochs 
0	110644	10644	the end architecture fits the data well as we see from the loss plot in inspecting the accuracy distribution across individual category we note from.furthermore we trained our model with learning rate of 1 10 3 and batch size of 32 across 20 epochs .last sentence.cnn analysis	the end architecture fits the data well as we see from the loss plot in inspecting the accuracy distribution across individual category we note from
1	110645	10645	we found that our cnn outperformed our extended knn algorithm with map 3 values of 62 1 and 34 4 respectively although both algorithms perform much better than random guessing of 0 5 but lower than human guessing of 73 0 .first sentence.although knn was able to identify multiple representations of the same category which increased accuracy compared to 1 nn knn still came short compared to our cnn due to its inability to recognize features and distinguish between apples and blueberries due to the presence of a stem for future work we would like to experiment with advanced cnn architectures such as vgg net and resnet which have already reached state of the art levels of image classification performance although not for sketches in particular .conclusion and future work	we found that our cnn outperformed our extended knn algorithm with map 3 values of 62 1 and 34 4 respectively although both algorithms perform much better than random guessing of 0 5 but lower than human guessing of 73 0 
1	110646	10646	although knn was able to identify multiple representations of the same category which increased accuracy compared to 1 nn knn still came short compared to our cnn due to its inability to recognize features and distinguish between apples and blueberries due to the presence of a stem for future work we would like to experiment with advanced cnn architectures such as vgg net and resnet which have already reached state of the art levels of image classification performance although not for sketches in particular .we found that our cnn outperformed our extended knn algorithm with map 3 values of 62 1 and 34 4 respectively although both algorithms perform much better than random guessing of 0 5 but lower than human guessing of 73 0 .additionally we have only used approximately 1 of the total quick draw .conclusion and future work	although knn was able to identify multiple representations of the same category which increased accuracy compared to 1 nn knn still came short compared to our cnn due to its inability to recognize features and distinguish between apples and blueberries due to the presence of a stem for future work we would like to experiment with advanced cnn architectures such as vgg net and resnet which have already reached state of the art levels of image classification performance although not for sketches in particular 
0	110647	10647	additionally we have only used approximately 1 of the total quick draw .although knn was able to identify multiple representations of the same category which increased accuracy compared to 1 nn knn still came short compared to our cnn due to its inability to recognize features and distinguish between apples and blueberries due to the presence of a stem for future work we would like to experiment with advanced cnn architectures such as vgg net and resnet which have already reached state of the art levels of image classification performance although not for sketches in particular .dataset and we believe training our models on the complete dataset would improve accuracy as well incorporating stroke order information and extract features such as velocity and acceleration .conclusion and future work	additionally we have only used approximately 1 of the total quick draw 
1	110648	10648	dataset and we believe training our models on the complete dataset would improve accuracy as well incorporating stroke order information and extract features such as velocity and acceleration .additionally we have only used approximately 1 of the total quick draw .finally we believe that ensembling techniques are interesting particularly for lighterweight methods such as knn .conclusion and future work	dataset and we believe training our models on the complete dataset would improve accuracy as well incorporating stroke order information and extract features such as velocity and acceleration 
0	110649	10649	finally we believe that ensembling techniques are interesting particularly for lighterweight methods such as knn .dataset and we believe training our models on the complete dataset would improve accuracy as well incorporating stroke order information and extract features such as velocity and acceleration .last sentence.conclusion and future work	finally we believe that ensembling techniques are interesting particularly for lighterweight methods such as knn 
