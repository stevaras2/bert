<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/lopez/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5-SNAPSHOT" ident="GROBID" when="2019-03-19T09:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PUBG: A Guide to Free Chicken Dinner</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-12-13">December 13, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxin</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
						</author>
						<title level="a" type="main">PUBG: A Guide to Free Chicken Dinner</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-12-13">December 13, 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract PUBG(Playerunknowns Battlegrounds) is a video game that has become popular in the past year. For this game, the final rank is the most important indicator of the player's ability. This project focuses on predicting the final rank and finding optimal strategies of the game. With data from PUBG, we uses several machine learning methods including ridge regression, lasso regression, lightGBM model, and random forest model to make relevant predictions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As the most popular multiplayer online battle arena video game, PUBG has attracted more and more players to join. The game features the last man standing deathmatch mode, in which one hundred players fight against each other in a battle royale. Players can also choose to match solo, or with small team of up to four people. In order to win the first place (a.k.a. eat Chicken Dinner), players must choose appropriate strategies during the game and make every effort to survive till the end. Various gaming strategies may include: collecting weapons and shields versus collecting sufficient medical packs, engaging in every possible fight versus avoiding battles, taking cars to relocate versus sneaking across bushes and forests, etc.</p><p>Given players statistics within a game, we plan to predict their final ranks. To be more specific, we take the data from Kaggle <ref type="bibr" target="#b0">[1]</ref> competition, which contains anonymized PUBG game stats. The inputs to our algorithms are features that influence the players finishing placement. We then use both regression models and tree models to output a predicted final rank. After predicting final ranks, we perform an additional step to classify game strategies used by top players, which may give the players a choice to adopt optimal strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>A significant part of our work is feature engineering. The source data provided by Kaggle contains 4.45 million entries of training data, with 28 features in each entry. Some entries in the data are invalid, and a few entries are generated by cheaters. We filtered out problematic data entries first, then applied techniques to remove outliers, e.g., players who had no move in the whole game but got kills, who had unreasonable head-shot rate or long-distance kills. Next, normalization of features <ref type="bibr" target="#b1">[2]</ref> is applied to numeric features, taking mean, variance and match size into consideration. Combined features are also added to help the algorithm consider team performance at the group level. Furthermore, we added cross validation to our train data <ref type="bibr">[11]</ref>.</p><p>To predict the rank of each player based on his statistic data, we firstly considered linear regression algorithms. Besides the normal linear regression, we tried Lasso regression <ref type="bibr" target="#b2">[3]</ref>, elastic net <ref type="bibr" target="#b3">[4]</ref>, Ridge regression <ref type="bibr" target="#b4">[5]</ref>, and SGD regression. Ridge regression got the best MAE because it handles regression data that suffer from multicollinearity better <ref type="bibr" target="#b5">[6]</ref>.</p><p>Tree algorithm is another series of widely-used algorithms that can be used as regression model. We explored the random forest algorithm <ref type="bibr" target="#b6">[7]</ref> first, and used fast.ai utils[8] to extract out important features. Based on these features, we compared random forest model with lightGBM <ref type="bibr" target="#b7">[9]</ref>, and got our best result (MAE = 0.0204) with the lightGBM model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset and Features</head><p>The dataset comes from Kaggle competition, which contains anonymized PUBG game stats, formatted so that each row contains one players post-game stats. The columns are some features that have impact on the players finishing rank, such as number of enemy players killed, total distance traveled on foot/in vehicle/by swimming, number of weapons picked up, etc. Two example waveform plots showing distribution of 'damageDealt' and 'walkDistance' by players. <ref type="figure">(Figure 1</ref>)</p><formula xml:id="formula_0">Figure 1: 'damageDealt' distribution (left) 'walkDistance' distribution (right)</formula><p>We first perform fully-covered data cleaning. Some observations in our dataset have really weird characteristics. The players could be cheaters, maniacs or just anomalies. Removing these outliers will most likely improve results. We inspect the data and figure out the types of players to be removed: killing without moving, anomalies in aim(more than 10 roadKills, more than 45 kills and 100% headshot rate), anomalies in total travel distance, anomalies in weapons, etc.</p><p>We apply a few feature engineering methods to process the data:</p><p>1. Add group-statistic data: As the final score is the same for all members within a group, for each group in a match, we get the mean/sum/max/min value of each feature within the same group, then rank the values within the same match to get the mean/sum/max/min value ranking as a new feature for each group.</p><p>2. Since the number of players joined in each match is different, we first normalize feature values based on the amount of players joined in a match, and then normalize the features by mean and variance.</p><p>3. Convert categorical feature such as 'match type' into one hot vector. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Regression Models</head><p>We have tried five most commonly used regression models. Linear regression was selected as our baseline model with a baseline MAE score at around 0.09, and based on that, we continued to try ridge regression, lasso regression, elastic net regression, SGD regression. Ridge regression has the best out-of-box result with the help of aforementioned feature engineering, likely because it handles multicollinearity data better.</p><p>Ridge regression is a technique for analyzing multiple regression data that suffer from multicollinearity. When multicollinearity occurs, least squares estimates are unbiased, but their variances are large so they may be far from the true value. By adding a degree of bias to the regression estimates, ridge regression reduces the standard errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Tree Models</head><p>Decision tree are a popular tool in machine learning, specifically in decision analysis, to help identify a strategy most likely to reach a goal. In this case, we have applied random forest algorithm and lightGBM (light gradient boosting machine) algorithms. LightGBM results in quite good MAE score, which shows significant improvement on linear regression models.</p><p>LightGBM is a new algorithm that combines GBDT algorithm with GOSS(Gradient-based One-Side Sampling) and EFB(Exclusive Feature Bundling). <ref type="bibr" target="#b7">[9]</ref> Specifically, LightGBM uses histogram-based algorithms, which bucket continuous feature (attribute) values into discrete bins. This speeds up training and reduces memory usage. Theoretically, in lightGBM algorithm with GOSS method, first, we rank the training instances according to their absolute values of their gradients in descending order; second, we keep the top-a * 100% instances with the larger gradients and get an instance subset A; then, for the remaining set A c consisting (1 − a) * 100% instances with smaller gradients, we further randomly sample a subset B with size b * |A c |; finally, we split the instances according to the estimated variance gain over the subset A ∪ B,</p><p>i.e., thus, the computation cost can be largely reduced. More importantly, GOSS will not lose much training accuracy and will outperform random sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Semi-supervised Mixture Gaussian</head><p>A Mixture Gaussian model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. One can think of mixture models as generalizing k-means clustering to incorporate information about the covariance structure of the data as well as the centers of the latent Gaussians. Semisupervised Mixture Gaussian model adds a portion of labeled data into the existing mixture model to help algorithm learn accurately and converge faster. Below is the update rule for GMM. <ref type="bibr" target="#b8">[10]</ref> 5 Results and Discussion</p><p>Our main motivation is to measure the accuracy of predicting players' winning placement and primary metric is MAE of predicted winning placement. The winning placement is a decimal value ranges from 0 to 1 while 0 corresponds to last place and 1 corresponds to the first place. Our best result comes from LightGBM model with a MAE of 0.0204; this result was ranked 57 in Kaggle leader board as of Dec 10.</p><p>In our baseline attempt, we feed only raw features into various models, including Linear Regression, Ridge Regression, Lasso Regression, Random Forest and Light Gradient Boosting Machine Model. Then we tune our models by adding engineered features step by step and observe that models gradually improved. The last step we perform is applying 5-fold cross validation to the data set. Please refer to the following In the Light GBM model, we used learning rate 0.05. Although we understand model could be trained faster with slightly higher rate, we choose to use a conservative number just to make sure algorithm converges properly. In order to reduce bias, we introduced k-fold cross validation and found out 5-fold performed better than 10-fold.</p><p>Another part of our motivation is to categorize the different gaming strategies of top players. Some example strategies are camping (hide in one place and wait for others to come), actively fighting (engage in more fight to collect weapons and equipment) and escape (avoid fights in order to survive till the end). We expect players who use camping strategy have less total moved distance and high final rank; on the other hand, active fighter will have high total damage and best equipment.</p><p>We first trained two unsupervised learning models, K-means and Mixture of Gaussian, and tried to observe the relationship between winning placement and strategy-related features. However, we couldn't find obvious pattern in model predicted players' categories. In other words, when plotting the strategy related features of a predicted class, there is no clear insight in data distribution. After further investigation, we realized the problem is in our data. The game data we used is post-game statistics, when means it doesn't have the power to describe what happened during the game. Given players choose strategies flexibly, we cannot expect a player stick to one strategy throughout the game. Therefore, we spend extra step in categorize players' gaming strategies holistically. By using semi-supervised Mixture of Gaussian learned in class, we manually assign 4 classes according to a few important features, and observe if these classes can differentiate winning placement. And we found the following patterns:</p><p>1. Game prefer fighters. The cluster labeled with most aggressive players has a mean winning placement over 0.95, while the most conservative players cluster only has winning placement around 0.4.</p><p>2. Don't stay in one place. Although our model indicates keep avoiding confrontation does not increase final rank, staying in one place will bring players troubles. The cluster labeled no-escape only has mean winning placement around 0.26.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>With effective feature engineering and appropriate model selection, we achive decent results and observe that lightGBM performs best in predicting final ranks. Moreover, we explore some interesting gaming strategies which we will give out to PUBG players as Free Chicken Dinner! For future work, we will train the AI to play PUBG and win. With the results we have arrived at, we will train the bots to maximize the exponentially decayed sum of future rewards with data produced entirely from self-play, that is, the bots are playing against themselves to learn to play by the strategies that maximize their chances of winning. By building AIs that succeed in complex and dynamic games like PUBG, we believe it can bring humanity closer to the ultimate goal of making AGIs that function in the messiness of the real world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Code</head><p>https://www.dropbox.com/sh/zuktbhf6h0fo0iz/AADU5PCw2lK5evL5RWaikBMYa?dl=0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Contribution</head><p>All members in our team performs extraordinary. Wenxin Wei worked on data cleaning and training with regression models. Xin Lu worked on feature engineering and training with light-GBM model. Yang Li worked on feature engineering, training with random forest, unsupervised learning algorithms and cross validation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Please refer followingFigure 2to see the clusters plotted with winning placement as Y-axis, and 'fights'/ 'escape' as X-axis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>'fights' and 'winPlacePerc' in different clusters (left) 'escape' and 'winPlacePerc' in different clusters(right)   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>4 .</head><label>4</label><figDesc>To better capture information hidden behind group of features, we add combined features such as totalDistance = rideDistance + walkDistance + swimDistance, 'healsandboosts' = 'heals' + 'boosts' 5. Define new feature 'fights' to quantify amount of battles players involved 6. Define feature 'escape' to indicate how likely a player tend to run away from danger</figDesc><table>4 Methods 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>table for detailed result we recorded from each step. For conciseness, we only selected best model from regression family and tree family, plus Linear Regression as reference.</figDesc><table>MAE on 20% validation set 
Linear Regression Ridge Regression Light GBM 

Raw features 
0.09000 
0.08989 
0.05654 
Raw + mean 
0.05736 
0.05736 
0.04158 
Raw + mean + sum + max -min 
0.04845 
0.04845 
0.02896 
Everything above + match mean + size 
0.04825 
0.04825 
0.02755 
Everything above + cross validation 
0.04812 
0.04810 
0.0204 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">PUBG Finish Placement Prediction</title>
		<ptr target="https://www.kaggle.com/c/pubg-finish-placement-prediction" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Feature Scaling</title>
		<ptr target="https://en.wikipedia.org/wiki/Featurescaling" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bayesian lasso regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<date type="published" when="2009-12-01" />
		</imprint>
	</monogr>
	<note>Pages 835845</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Regularization and variable selection via the elastic net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="301" to="320" />
			<date type="published" when="2005-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ridge Regression: Biased Estimation for Nonorthogonal Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ae Hoerl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kennard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="67" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ridge Regression</title>
	</analytic>
	<monogr>
		<title level="m">NCSS Statistical Software</title>
		<imprint>
			<biblScope unit="page">335</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Classification and Regression by randomForest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wiener</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002-12" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>R news, Page 1</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lightgbm: A highly efficient gradient boosting decision tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">On Semi-Supervised Learning of Gaussian Mixture Models for Phonetic Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jui-Ting</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<ptr target="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.164.9743rep=rep1type=pdf" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
