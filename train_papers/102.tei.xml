<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/lopez/grobid/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5-SNAPSHOT" ident="GROBID" when="2019-03-19T09:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How Real is Real? Quantitative and Qualitative comparison of GANs and supervised-learning classifiers. CS229 Project Final report</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Verzeni</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacqueline</forename><surname>Yau</surname></persName>
						</author>
						<title level="a" type="main">How Real is Real? Quantitative and Qualitative comparison of GANs and supervised-learning classifiers. CS229 Project Final report</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In this project we primarily explore how supervised learning classifiers, of increasing complexity, generalize on GANs synthetic MNIST images and secondarily how semi-supervised learning classifier performs on real images. The methods utilized encompass linear classifiers, Neural Networks and modified GANs. The supervised learning classifier seemed to generalize reasonably well on the synthetic images, while semisupervised learning classifier performed worse than expected on real images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Generative Adversarial Networks (GANs) have been introduced in 2014 combining learning generative models with game theory. In the past four years, some implementations of this new unsupervised-learning model reached quite significant results. They generated qualitatively convincing replicas of the training dataset from random noise. But how convincing are those images for a classifier trained with supervised-learning techniques? Also, could GANs, opportunely modified, improve standard supervised-learning classifiers results? To answer these questions, we evaluated how well supervised learning classifiers (see Methods first subsection), trained on the standard MNIST dataset, generalized on images generated by pre-trained GANs <ref type="bibr" target="#b0">[1]</ref>. We additionally picked the best performing classifier and converted it into a semi-supervised learning classifier modifying the standard GANs discriminator architecture as described in this OpenAI paper <ref type="bibr" target="#b4">[5]</ref>. We then proceeded comparing the accuracy of the semi-supervised learning classifiers against the original MNIST dataset (see Methods second subsection). The input of our algorithms is an MNIST image, while the output is the label of the classified image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related work</head><p>Deep neural networks and in particular convolutional neural networks have been extensively used for solving the MNIST digit classification problem and are considered state-of-theart <ref type="bibr" target="#b2">[3]</ref> <ref type="bibr" target="#b3">[4]</ref>. Current GANs research aims to improve the results of such models in creating more convincing imitations of real images, but also explores how those models could be used for performing semi-supervised learning classification <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset and Features</head><p>The real MNIST dataset has been downloaded using Keras APIs (60000 training examples, 10000 test examples), while the synthetic MNIST dataset (6336 unlabeled examples) has been generated using this Deep Convolutional Generative Adversarial Networks <ref type="bibr" target="#b0">[1]</ref>, which has been previously trained separately. The validation set splits 10% of the training set, so actual training of the supervised learning classifiers had 54000 training examples and 6000 validation examples.  We manually labeled 1000 of the synthetic images acting as ground truth creating a synthetic test dataset for the quantitative accuracy comparison. Thus, the two test datasets we used had 1000 real and 1000 synthetic examples respectively.</p><p>Each MNIST grayscale image has dimensions 1×28×28. The input features are the pixels that compose the MNIST grayscale image. For what concerns the features learned by the classifiers, the neural networks derive new features in the hidden layers, and the convolutional ones extract additional features in the convolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>For consistency we used the cross entropy loss and Adam optimization for all classifiers. Specifically we used Keras sparse categorical cross entropy loss function, which is the cross entropy function for discrete distributions with the constraint that the output labels must be integers ranging in values of the classes.</p><formula xml:id="formula_0">H(p, q) = − x p(x) log q(x)</formula><p>Softmax has been used as activation for all output layers,</p><formula xml:id="formula_1">σ(x) j = e xj K</formula><p>k=1 e x k in addition the semi-supervised classifier also uses the Sigmoid activation for its second output layer</p><formula xml:id="formula_2">g(x) = 1 1 + e −x</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised-learning classifiers</head><p>We built and trained a total of 4 supervised-learning classifiers: a linear Softmax classifier, a 2 layers fully connected neural network (NN) <ref type="figure" target="#fig_2">(Fig. 3)</ref>, a 4 layers <ref type="figure" target="#fig_3">(Fig. 4)</ref> and a 5 layers <ref type="figure" target="#fig_4">(Fig. 5</ref>) convolutional neural networks (CNN).</p><p>The fully connected NN takes in the input image with dimensions 1 x 28 x 28, then flattens it and sends it through ReLU activation at 256 neurons, and then the softmax layer at 10 neurons for classification. The CNN 4-layer starts with the input image and convolves it with 32 filters with kernel size 5. Then, it goes through max pooling with pool size 2, and the output is flattened and sent through ReLU activation (128 neurons) before the softmax output layer (10 neurons). The CNN 5-layer is identical to the CNN 4-layer architecture, but it has an additional convolution layer, with 64 filters and kernel size 5, after the first one.</p><p>For building training and testing our models we used Tensorflow Keras high-level APIs <ref type="bibr" target="#b1">[2]</ref>. The supervised learning classifiers trained on 54000 samples with 6000 validation. For comparing the accuracy of trained classifiers against synthetic images we tested them on 1000 real and the 1000 manually labeled synthetic MNIST images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semi-supervised learning classifier</head><p>The semi-supervised modified GANs discriminator has the same architecture as the CNN 5-layer, but with two output layers ( <ref type="figure" target="#fig_5">Fig. 6</ref>): a 1 neuron Sigmoid output layer for the discriminator to determine whether an image is real or synthetic, and the 10+1 neuron Softmax output layer to classify the image between 0 and 9 or synthetic which is when the   additional "+1" neuron is activated. Due to time constraints, we used previously generated synthetic images as unlabeled data, instead of also building a generator and training it along with the modified discriminator as shown by Tim S. and All <ref type="bibr" target="#b0">[1]</ref>. Since we only had around 6000 synthetic images we decided to train the semi-supervised modified GANs on two smaller datasets. The first train dataset was created combining 5400 labeled real images with 5400 unlabeled synthetic images, while the real images test dataset remained the same used for the other classifiers (1000 real images). For comparison reasons we then trained the same classifier on a 5400 labeled real images and 0 unlabeled synthetic images dataset, still testing the performances on the 1000 real images test dataset. Each of the output layers had its own respective loss function and were contributing equally in the optimization process.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments and Results Discussion</head><p>Our primary quantitative metric of evaluation was accuracy.</p><p>The accuracy was calculated as the number of correct classifications over the number of total number of classifications.</p><p>To better understand which digits were most affected by misclassification we also calculated the confusion matrix for the best performing classifier against both the real and the synthetic test dataset <ref type="figure" target="#fig_0">(Fig. 9, 10</ref>). For our hyperparameters, we chose to train each model respectively for 5, 50, and 100 epochs to give us an idea on how each one of them would have performed for increasing periods of training time and at which point they would have started showing signs of overfitting. Since we had a large number of data samples we decided to dedicate a significant amount to the validation split: 6000 as it was constituting only the 10% of the total train dataset. The baseline used for classification was the accuracy of a random classifier, which for MNIST is ten percent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised learning classifiers</head><p>The test accuracy obtained by the various supervised learning classifiers over synthetic images was overall slightly worse but comparable with the real images, where the most complex convolutional NN (CNN-5l) performed best. Overall, the various supervised classifier seemed to generalize reasonably well on GANs synthetic MNIST images. We expected the CNN to perform better than any other classifier since it is considered state-of-the-art for the MNIST digit classification problem <ref type="bibr" target="#b3">[4]</ref>. That has been indeed the case. The convolutional layers were able to extract meaningful features that were effective in identifying and then classifying the number in the MNIST image. The 5-layer CNN performed better than the 4-layer one since the extra convolution layer with 64 filters was able to find additional features that the first 32 filter convolution layer could not. However, the CNN did overfit over long training periods <ref type="figure">(Fig. 8)</ref>, and while we did not have enough time to implement regularization and test it, in future work we include what approach we would like to try to mitigate it. For what concerns the qualitative analysis: we investigated where in the real and synthetic images the CNN focused its attention on, using two types of attention visualization maps. The first type of attention map used is the saliency, which visualizes activation over the final dense layer (Softmax activation). Specifically, the saliency computes the gradient of the output category with respect to the input image to see how much the output classification changes with respect to the input image pixels. This highlights regions that contribute the most towards the output. (To properly visualize activation, the Softmax layer is swapped for a linear layer, as suggested by Keras-vis API because maximizing an output node can be done by minimizing other outputs.) The second type of attention map used is the class activation map, which instead of using the gradients with respect to output, uses the penultimate convolution layer output. It uses the nearest convolution layer to utilize spatial information lost in the dense layers. The attention maps are in the appendix <ref type="figure" target="#fig_0">(Fig. 11-14)</ref>.</p><p>For each attention map, there are three different ways we modified the backpropagation gradients. 'Vanilla' is the case when there is no modification performed on the backpropagation gradients. The 'ReLU' modifier clips negative gradient values, and the 'guided' modifier modifies the backpropagation to only propagate positive gradients for positive activations. The 'guided' modifier seems to produce the clearest attention maps.</p><p>The confusion matrix describes the performance of the classifier model and displays how many and which examples were classified correctly or incorrectly. Analyzing the results from the confusion matrix of real images <ref type="figure" target="#fig_7">(Fig. 9)</ref>, the CNN performs very well and there are not many errors. However, of the mistakes it seems that the number the CNN has most trouble classifying is 3. We observe that the CNN most frequently misclassifies 3 as 5. Comparing the results from the confusion matrix with the attention maps, we can observe that the features highlighted by the CNN to classify 5 mainly focuses on the middle, to identify where the curve starts. Since it seems like the CNN tries to classify 5 based on if there is a curve in the middle, that would explain why it would misclassify 3 as 5, since 3 has a second curve in the middle. And the one instance of a wrong prediction on 5, is a misclassification as 3.</p><p>In comparison to the real images, we observe the results for the confusion matrix on the synthetic images <ref type="figure" target="#fig_0">(Fig. 10</ref>) and see that overall the CNN 5 layer performed worse, and it especially had trouble classifying 1 and 6. It seems that the CNN most often misclassified the synthetic image 6 as 5 and 1 as 7. The synthetic 5 seems to be classified based on the top of the horizontal line and the bottom of curve. Since 6 has a bottom curve as well, that may be why 6 was incorrectly predicted to be 5. For the case of predicting 1, the saliency attention of synthetic 7 appears to concentrate on the slanted vertical line. Since 1 is a completely vertical line that is sometimes slanted, that would explain why 1 is misclassified as 7. Curiously, the classifier could predict 7 accurately, and did not misclassify 7 as 1.</p><p>Overall, based on the results of the confusion matrices, the CNN 5 layer performs very well on the real images but comparatively not as great on the synthetic ones, though total accuracy is still quite high. Interestingly, the attention maps showed that the CNN's focus features for identifying a real image may be different for the same number synthetic image. For example, the features to identify real image 5 and synthetic 5 are quite different. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semi-supervised learning classifier</head><p>The test accuracy obtained by training the semi-supervised learning classifier on the combined labeled real / unlabeled synthetic images dataset was worse than the one obtained   <ref type="table">Table 4</ref>: Semi-supervised modified GANs results on train accuracy for label classification of real images</p><p>We expected that the addition of the unlabeled synthetic data would have improved the label classifier accuracy on real MNIST images instead of worsening it. The latter seemed to be the case. This is probably because, as mentioned in the Methods section due to time constraints, we used previously generated synthetic images instead of building a generator. As a result the features extracted by the unlabeled data might not have been as relevant as they would have been if coming from the same distribution of the real data that the generator would have reproduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Future Work</head><p>We trained four supervised learning classifiers on real MNIST images and tested them on real and synthetic images for classification. We also trained a semi-supervised learning modified GANs discriminator to classify real images as well as discern whether an image is real or synthetic. The CNN 5 layer performed the best on classifying the real and synthetic images. The CNN 5 layer performed better probably because it had an extra convolution layer with 64 filters, so it could extract features that the other models could not.</p><p>For future work, the goal would be to implement some approaches to solve issues noticed during error analysis.</p><p>For the supervised learning classifiers, since the CNN was overfitting pretty severely, the next step would be to add some regularization to help reduce the overfitting, such as a dropout layer. Another approach to try out is implementing a ResNet to see if it could perform even better than a CNN.</p><p>For the semi-supervised learning GAN, the next step would be to build a complete semi-supervised learning GANs classifier, such as the one suggested <ref type="bibr" target="#b0">[1]</ref> and see if that would improve the results obtained in our partial attempt. Building the generator for the GANs would allow the adversarial training between the discriminator and the generator, which would hopefully improve the label classifier results. All the code used for this project is available at: https://github.com/ project-mnist-2018/cs229_project    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Real MNIST dig- its sample.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Synthetic MNIST digits sample, manually la- belled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Fully-connected NN classifier architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Convolutional NN 4-layer classifier architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Convolutional NN 5-layer classifier architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Particular of the architecture of the modified GANs discriminator used as semi-supervised classifier showing the two output layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>CNN-5l model accuracy Figure 8: CNN-5l model lossTo analyze the classification results for our best perform- ing model, the CNN 5 layer, we used a confusion matrix to see which examples it had the most trouble classifying and attention maps to observe which features the CNN focused on when classifying.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Confusion matrix for CNN-5l over real images of test dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Confusion matrix for CNN-5l over synthetic im- ages of test dataset by training it on the fully labeled real images dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Saliency attention for CNN-5l on a real digit sample.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>Saliency attention for CNN-5l on a synthetic digit sample.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 :</head><label>13</label><figDesc>Class Activation attention Class Activation At- tention for CNN-5l on a real digit sample.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 :</head><label>14</label><figDesc>Class Activation attention Class Activation At- tention for CNN-5l on a synthetic digit sample.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Table 1: Supervised learning classifier results on test accu- racy for real and synthetic images</figDesc><table>Models 

Epochs 
5 
50 
100 
Real 
Syn 
Real 
Syn 
Real 
Syn 
Lin softmax 0.919 0.923 0.921 0.921 0.916 0.913 
FCNN 
0.978 0.957 0.979 0.969 0.983 0.962 
CNN-4l 
0.986 0.971 0.989 0.970 0.989 0.974 
CNN-5l 
0.994 0.972 0.988 0.973 0.985 0.969 

Models 
Epochs 
5 
50 
100 
Lin softmax 0.923 0.936 0.937 
FCNN 
0.989 0.999 0.999 
CNN-4l 
0.995 1.000 1.000 
CNN-5l 
0.996 0.998 0.998 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Supervised learning classifier results on train accu- racy on real images</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Semi-supervised modified GANs results on test ac- curacy for label classification of real images Models Epochs 5 50 100 Semi-gan (all labeled) 0.995 1.000 1.000 Semi-gan (half/half) 0.974 1.000 1.000</figDesc><table>Models 
Epochs 
5 
50 
100 
</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions</head><p>Both team members worked on coding up the various classifier models with visualizations, running said models to gather results, labelling synthetic images, and writing up the proposal, milestone, poster, and final report.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A tensorflow implementation of &quot;deep convolutional generative adversarial networks</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Tensorflow keras high-level apis</title>
		<ptr target="https://www.tensorflow.org/guide/keras" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ueli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jurgen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">computer vision and pattern recognition (cvpr), 2012 ieee con-ference on</title>
		<imprint>
			<publisher>ieee</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="36421" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karen</surname></persName>
		</author>
		<idno>arxiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arxiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">S</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wojciech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Vicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1606.03498.pdf" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
