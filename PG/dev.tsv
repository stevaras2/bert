Label	id1	id2	clipping	sentence
1	120	8001	we introduce a novel approach to solving pde constrained optimization problems specifically related to aircraft design these optimization problems require running expensive computational fluid dynamics cfd simulations which have previously been approximated with a reduced order model rom to lower the computational cost instead of using a single global rom as is traditionally done we propose using multiple piecewise roms constructed and used with the aid of machine learning techniques 	these optimization problems require running expensive computational fluid dynamics cfd simulations which have previously been approximated with a reduced order model rom to lower the computational cost 
1	120	8002	we introduce a novel approach to solving pde constrained optimization problems specifically related to aircraft design these optimization problems require running expensive computational fluid dynamics cfd simulations which have previously been approximated with a reduced order model rom to lower the computational cost instead of using a single global rom as is traditionally done we propose using multiple piecewise roms constructed and used with the aid of machine learning techniques 	instead of using a single global rom as is traditionally done we propose using multiple piecewise roms constructed and used with the aid of machine learning techniques 
1	120	8003	we introduce a novel approach to solving pde constrained optimization problems specifically related to aircraft design these optimization problems require running expensive computational fluid dynamics cfd simulations which have previously been approximated with a reduced order model rom to lower the computational cost instead of using a single global rom as is traditionally done we propose using multiple piecewise roms constructed and used with the aid of machine learning techniques 	our approach consists of clustering a set of precomputed non linear partial differential equations pde solutions from which we build our piecewise roms 
1	120	8004	we introduce a novel approach to solving pde constrained optimization problems specifically related to aircraft design these optimization problems require running expensive computational fluid dynamics cfd simulations which have previously been approximated with a reduced order model rom to lower the computational cost instead of using a single global rom as is traditionally done we propose using multiple piecewise roms constructed and used with the aid of machine learning techniques 	then during the optimization problem when we need to run a simulation for a given optimization parameter we select the optimal piecewise rom to use 
0	120	8005	we introduce a novel approach to solving pde constrained optimization problems specifically related to aircraft design these optimization problems require running expensive computational fluid dynamics cfd simulations which have previously been approximated with a reduced order model rom to lower the computational cost instead of using a single global rom as is traditionally done we propose using multiple piecewise roms constructed and used with the aid of machine learning techniques 	initial results on our test dataset are promising 
1	120	8006	we introduce a novel approach to solving pde constrained optimization problems specifically related to aircraft design these optimization problems require running expensive computational fluid dynamics cfd simulations which have previously been approximated with a reduced order model rom to lower the computational cost instead of using a single global rom as is traditionally done we propose using multiple piecewise roms constructed and used with the aid of machine learning techniques 	we were able to achieve the same or better accuracy by using piecewise roms rather than a global rom while further reducing the computational cost associated with running a simulation 
1	120	8007	improving the design of aircrafts often requires solving pdeconstrained optimization problems such as maximizing the lift drag with respect to some parameters here is an optimization vector containing parameters that we want to optimize it is also common practice to have a lower bound lb and upper bound ub on this vector to find the optimal we must update it iteratively running a computational fluid dynamics cfd simulation at each optimization step 	improving the design of aircrafts often requires solving pdeconstrained optimization problems such as maximizing the lift drag with respect to some parameters 
1	120	8008	improving the design of aircrafts often requires solving pdeconstrained optimization problems such as maximizing the lift drag with respect to some parameters here is an optimization vector containing parameters that we want to optimize it is also common practice to have a lower bound lb and upper bound ub on this vector to find the optimal we must update it iteratively running a computational fluid dynamics cfd simulation at each optimization step 	here is an optimization vector containing parameters that we want to optimize 
1	120	8009	improving the design of aircrafts often requires solving pdeconstrained optimization problems such as maximizing the lift drag with respect to some parameters here is an optimization vector containing parameters that we want to optimize it is also common practice to have a lower bound lb and upper bound ub on this vector to find the optimal we must update it iteratively running a computational fluid dynamics cfd simulation at each optimization step 	it is also common practice to have a lower bound lb and upper bound ub on this vector to find the optimal we must update it iteratively running a computational fluid dynamics cfd simulation at each optimization step 
0	120	8010	fluid flow problems are governed by nonlinear partial differential equations pde solving these equations using cfd techniques such as finite volume method is equivalent to solving a set of nonlinear equations where is the set of parameters for our simulation and w is the unknown vector of dimension n w p r n called the state vector specifically a row of the state wris represents a property of the fluid flow such as pressure at point i of the cfd mesh 	fluid flow problems are governed by nonlinear partial differential equations pde 
1	120	8011	fluid flow problems are governed by nonlinear partial differential equations pde solving these equations using cfd techniques such as finite volume method is equivalent to solving a set of nonlinear equations where is the set of parameters for our simulation and w is the unknown vector of dimension n w p r n called the state vector specifically a row of the state wris represents a property of the fluid flow such as pressure at point i of the cfd mesh 	solving these equations using cfd techniques such as finite volume method is equivalent to solving a set of nonlinear equations where is the set of parameters for our simulation and w is the unknown vector of dimension n w p r n called the state vector 
0	120	8012	fluid flow problems are governed by nonlinear partial differential equations pde solving these equations using cfd techniques such as finite volume method is equivalent to solving a set of nonlinear equations where is the set of parameters for our simulation and w is the unknown vector of dimension n w p r n called the state vector specifically a row of the state wris represents a property of the fluid flow such as pressure at point i of the cfd mesh 	specifically a row of the state wris represents a property of the fluid flow such as pressure at point i of the cfd mesh 
0	120	8013	fluid flow problems are governed by nonlinear partial differential equations pde solving these equations using cfd techniques such as finite volume method is equivalent to solving a set of nonlinear equations where is the set of parameters for our simulation and w is the unknown vector of dimension n w p r n called the state vector specifically a row of the state wris represents a property of the fluid flow such as pressure at point i of the cfd mesh 	thus the cfd mesh has n points 
0	120	8014	fluid flow problems are governed by nonlinear partial differential equations pde solving these equations using cfd techniques such as finite volume method is equivalent to solving a set of nonlinear equations where is the set of parameters for our simulation and w is the unknown vector of dimension n w p r n called the state vector specifically a row of the state wris represents a property of the fluid flow such as pressure at point i of the cfd mesh 	in unfortunately this problem is very expensive to solve when n is large as it is in the case of solving cfd problems where n is in the order of thousands or millions 
1	120	8015	in order to solve cfd problems faster a reduced order model rom can be used in order to approximate the hdm where v gl p r n n denotes the global reduce order basis rob and w r p r n denotes the new vector of unknowns called the reduced state substituting eq 4 into eq 	in order to solve cfd problems faster a reduced order model rom can be used in order to approximate the hdm where v gl p r n n denotes the global reduce order basis rob and w r p r n denotes the new vector of unknowns called the reduced state 
0	120	8016	in order to solve cfd problems faster a reduced order model rom can be used in order to approximate the hdm where v gl p r n n denotes the global reduce order basis rob and w r p r n denotes the new vector of unknowns called the reduced state substituting eq 4 into eq 	 4 into eq 
0	120	8017	in order to solve cfd problems faster a reduced order model rom can be used in order to approximate the hdm where v gl p r n n denotes the global reduce order basis rob and w r p r n denotes the new vector of unknowns called the reduced state substituting eq 4 into eq 	now the least squares problem to solve is min wrpr n rpv gl w r p q q 2 2 6 
1	120	8018	to build a rom we first need to find the global reduced order basis rob v gl this is done by solving the non linear equation 2 for many optimization vectors thus given a specific vector i we can define the solution state vector therefore for a set of k optimization vectors t u k 1 we solve 3 and we get a set of state vectors twp i qu k finally we perform a singular value decomposition svd on the matrix m to compute the global rob v gl here v gl is computed by only selecting the first n columns of the matrix u and therefore v g l p r n n 	to build a rom we first need to find the global reduced order basis rob v gl 
1	120	8019	to build a rom we first need to find the global reduced order basis rob v gl this is done by solving the non linear equation 2 for many optimization vectors thus given a specific vector i we can define the solution state vector therefore for a set of k optimization vectors t u k 1 we solve 3 and we get a set of state vectors twp i qu k finally we perform a singular value decomposition svd on the matrix m to compute the global rob v gl here v gl is computed by only selecting the first n columns of the matrix u and therefore v g l p r n n 	this is done by solving the non linear equation 2 for many optimization vectors 
1	120	8020	to build a rom we first need to find the global reduced order basis rob v gl this is done by solving the non linear equation 2 for many optimization vectors thus given a specific vector i we can define the solution state vector therefore for a set of k optimization vectors t u k 1 we solve 3 and we get a set of state vectors twp i qu k finally we perform a singular value decomposition svd on the matrix m to compute the global rob v gl here v gl is computed by only selecting the first n columns of the matrix u and therefore v g l p r n n 	thus given a specific vector i we can define the solution state vector therefore for a set of k optimization vectors t u k 1 we solve 3 and we get a set of state vectors twp i qu k finally we perform a singular value decomposition svd on the matrix m to compute the global rob v gl here v gl is computed by only selecting the first n columns of the matrix u and therefore v g l p r n n 
0	120	8021	to build a rom we first need to find the global reduced order basis rob v gl this is done by solving the non linear equation 2 for many optimization vectors thus given a specific vector i we can define the solution state vector therefore for a set of k optimization vectors t u k 1 we solve 3 and we get a set of state vectors twp i qu k finally we perform a singular value decomposition svd on the matrix m to compute the global rob v gl here v gl is computed by only selecting the first n columns of the matrix u and therefore v g l p r n n 	thus the global rom has dimension n and can be used in the entire domain d wp q v gl w r p q p d 10 
1	120	8022	in this project we propose creating multiple piecewise roms in the domain d each having smaller dimensions than a global rom would these piecewise roms do not need to be accurate in the entire domain d but only in limited region of the design space d by using machine learning techniques we hypothesize that we can improve quality of the reduced order basis rob within a given design space d then using these piecewise roms will allow us to solve even cheaper least squares problems than 6 whilst maintaining a similar or better level of accuracy relative to the hdm to do this we must group the precomputed solutions twp i qu k 1 into multiple clusters and then create multiple piecewise reduced order basis tv i u c 1 where c is the number of clusters for instance choosing two clusters in figure 5 we can see a schematic comparison of a global rom versus 2 piecewise roms built by clustering twp i u k 1 into 2 clusters 	in this project we propose creating multiple piecewise roms in the domain d each having smaller dimensions than a global rom would 
1	120	8023	in this project we propose creating multiple piecewise roms in the domain d each having smaller dimensions than a global rom would these piecewise roms do not need to be accurate in the entire domain d but only in limited region of the design space d by using machine learning techniques we hypothesize that we can improve quality of the reduced order basis rob within a given design space d then using these piecewise roms will allow us to solve even cheaper least squares problems than 6 whilst maintaining a similar or better level of accuracy relative to the hdm to do this we must group the precomputed solutions twp i qu k 1 into multiple clusters and then create multiple piecewise reduced order basis tv i u c 1 where c is the number of clusters for instance choosing two clusters in figure 5 we can see a schematic comparison of a global rom versus 2 piecewise roms built by clustering twp i u k 1 into 2 clusters 	these piecewise roms do not need to be accurate in the entire domain d but only in limited region of the design space d by using machine learning techniques we hypothesize that we can improve quality of the reduced order basis rob within a given design space d then using these piecewise roms will allow us to solve even cheaper least squares problems than 6 whilst maintaining a similar or better level of accuracy relative to the hdm to do this we must group the precomputed solutions twp i qu k 1 into multiple clusters and then create multiple piecewise reduced order basis tv i u c 1 where c is the number of clusters 
1	120	8024	in this project we propose creating multiple piecewise roms in the domain d each having smaller dimensions than a global rom would these piecewise roms do not need to be accurate in the entire domain d but only in limited region of the design space d by using machine learning techniques we hypothesize that we can improve quality of the reduced order basis rob within a given design space d then using these piecewise roms will allow us to solve even cheaper least squares problems than 6 whilst maintaining a similar or better level of accuracy relative to the hdm to do this we must group the precomputed solutions twp i qu k 1 into multiple clusters and then create multiple piecewise reduced order basis tv i u c 1 where c is the number of clusters for instance choosing two clusters in figure 5 we can see a schematic comparison of a global rom versus 2 piecewise roms built by clustering twp i u k 1 into 2 clusters 	for instance choosing two clusters in figure 5 we can see a schematic comparison of a global rom versus 2 piecewise roms built by clustering twp i u k 1 into 2 clusters 
1	120	8025	in this project we propose creating multiple piecewise roms in the domain d each having smaller dimensions than a global rom would these piecewise roms do not need to be accurate in the entire domain d but only in limited region of the design space d by using machine learning techniques we hypothesize that we can improve quality of the reduced order basis rob within a given design space d then using these piecewise roms will allow us to solve even cheaper least squares problems than 6 whilst maintaining a similar or better level of accuracy relative to the hdm to do this we must group the precomputed solutions twp i qu k 1 into multiple clusters and then create multiple piecewise reduced order basis tv i u c 1 where c is the number of clusters for instance choosing two clusters in figure 5 we can see a schematic comparison of a global rom versus 2 piecewise roms built by clustering twp i u k 1 into 2 clusters 	on the left all the training solutions tw i u 10 1 computed solving 2 using t i u 10 1 are used to create v gl and therefore a global rom 
1	120	8026	in this project we propose creating multiple piecewise roms in the domain d each having smaller dimensions than a global rom would these piecewise roms do not need to be accurate in the entire domain d but only in limited region of the design space d by using machine learning techniques we hypothesize that we can improve quality of the reduced order basis rob within a given design space d then using these piecewise roms will allow us to solve even cheaper least squares problems than 6 whilst maintaining a similar or better level of accuracy relative to the hdm to do this we must group the precomputed solutions twp i qu k 1 into multiple clusters and then create multiple piecewise reduced order basis tv i u c 1 where c is the number of clusters for instance choosing two clusters in figure 5 we can see a schematic comparison of a global rom versus 2 piecewise roms built by clustering twp i u k 1 into 2 clusters 	on the right we first cluster the training solutions tw i u 10 1 into 2 clusters and then we construct 2 reduced order basis v 1 and v 2 
0	120	8027	in this project we propose creating multiple piecewise roms in the domain d each having smaller dimensions than a global rom would these piecewise roms do not need to be accurate in the entire domain d but only in limited region of the design space d by using machine learning techniques we hypothesize that we can improve quality of the reduced order basis rob within a given design space d then using these piecewise roms will allow us to solve even cheaper least squares problems than 6 whilst maintaining a similar or better level of accuracy relative to the hdm to do this we must group the precomputed solutions twp i qu k 1 into multiple clusters and then create multiple piecewise reduced order basis tv i u c 1 where c is the number of clusters for instance choosing two clusters in figure 5 we can see a schematic comparison of a global rom versus 2 piecewise roms built by clustering twp i u k 1 into 2 clusters 	v 1 is built using the solutions computed using the parameters t 1 5 6 7 10 u and v 2 using t 2 3 4 8 9 u 
0	120	8028	in this project we propose creating multiple piecewise roms in the domain d each having smaller dimensions than a global rom would these piecewise roms do not need to be accurate in the entire domain d but only in limited region of the design space d by using machine learning techniques we hypothesize that we can improve quality of the reduced order basis rob within a given design space d then using these piecewise roms will allow us to solve even cheaper least squares problems than 6 whilst maintaining a similar or better level of accuracy relative to the hdm to do this we must group the precomputed solutions twp i qu k 1 into multiple clusters and then create multiple piecewise reduced order basis tv i u c 1 where c is the number of clusters for instance choosing two clusters in figure 5 we can see a schematic comparison of a global rom versus 2 piecewise roms built by clustering twp i u k 1 into 2 clusters 	as such the global rom uses v gl p r n n 
0	120	8029	in this project we propose creating multiple piecewise roms in the domain d each having smaller dimensions than a global rom would these piecewise roms do not need to be accurate in the entire domain d but only in limited region of the design space d by using machine learning techniques we hypothesize that we can improve quality of the reduced order basis rob within a given design space d then using these piecewise roms will allow us to solve even cheaper least squares problems than 6 whilst maintaining a similar or better level of accuracy relative to the hdm to do this we must group the precomputed solutions twp i qu k 1 into multiple clusters and then create multiple piecewise reduced order basis tv i u c 1 where c is the number of clusters for instance choosing two clusters in figure 5 we can see a schematic comparison of a global rom versus 2 piecewise roms built by clustering twp i u k 1 into 2 clusters 	the two piecewise roms instead use v 1 p r n n1 and v 2 p r n n2 respectively where by construction n 1 n and n 2 n therefore the first piecewise rom makes the following approximation using v 1 wp q v 1 w r p q 11 and the second piecewise rom makes another approximation usingtherefore by using either 11 or 12 we can solve min wrpr n rpv i w r p q q 2 2where i indicates which piecewise rom i is used 
1	120	8030	in this project we propose creating multiple piecewise roms in the domain d each having smaller dimensions than a global rom would these piecewise roms do not need to be accurate in the entire domain d but only in limited region of the design space d by using machine learning techniques we hypothesize that we can improve quality of the reduced order basis rob within a given design space d then using these piecewise roms will allow us to solve even cheaper least squares problems than 6 whilst maintaining a similar or better level of accuracy relative to the hdm to do this we must group the precomputed solutions twp i qu k 1 into multiple clusters and then create multiple piecewise reduced order basis tv i u c 1 where c is the number of clusters for instance choosing two clusters in figure 5 we can see a schematic comparison of a global rom versus 2 piecewise roms built by clustering twp i u k 1 into 2 clusters 	using this method with piecewise roms gives rise to two machine learning problems that we must solve 1 
0	120	8031	in this project we propose creating multiple piecewise roms in the domain d each having smaller dimensions than a global rom would these piecewise roms do not need to be accurate in the entire domain d but only in limited region of the design space d by using machine learning techniques we hypothesize that we can improve quality of the reduced order basis rob within a given design space d then using these piecewise roms will allow us to solve even cheaper least squares problems than 6 whilst maintaining a similar or better level of accuracy relative to the hdm to do this we must group the precomputed solutions twp i qu k 1 into multiple clusters and then create multiple piecewise reduced order basis tv i u c 1 where c is the number of clusters for instance choosing two clusters in figure 5 we can see a schematic comparison of a global rom versus 2 piecewise roms built by clustering twp i u k 1 into 2 clusters 	given multiple precomputed solutions tw i u k 1 how do we cluster them most effectively into tv i u c 1 2 
1	120	8032	in this project we propose creating multiple piecewise roms in the domain d each having smaller dimensions than a global rom would these piecewise roms do not need to be accurate in the entire domain d but only in limited region of the design space d by using machine learning techniques we hypothesize that we can improve quality of the reduced order basis rob within a given design space d then using these piecewise roms will allow us to solve even cheaper least squares problems than 6 whilst maintaining a similar or better level of accuracy relative to the hdm to do this we must group the precomputed solutions twp i qu k 1 into multiple clusters and then create multiple piecewise reduced order basis tv i u c 1 where c is the number of clusters for instance choosing two clusters in figure 5 we can see a schematic comparison of a global rom versus 2 piecewise roms built by clustering twp i u k 1 into 2 clusters 	given an arbitrary which piecewise rom tv i u c 1 should we use to best represent the hdm in the next section we describe the methods we have implemented for addressing the above problems 
1	120	8033	our proposed methodology to solve a pde constrained optimization problem operates in two phases an offline phase and an online phase in the offline phase we cluster precomputed training solutions from which we build our piecewise roms that are used in the online phase in the online phase we query multiple during the optimization process 	our proposed methodology to solve a pde constrained optimization problem operates in two phases an offline phase and an online phase 
1	120	8034	our proposed methodology to solve a pde constrained optimization problem operates in two phases an offline phase and an online phase in the offline phase we cluster precomputed training solutions from which we build our piecewise roms that are used in the online phase in the online phase we query multiple during the optimization process 	in the offline phase we cluster precomputed training solutions from which we build our piecewise roms that are used in the online phase 
1	120	8035	our proposed methodology to solve a pde constrained optimization problem operates in two phases an offline phase and an online phase in the offline phase we cluster precomputed training solutions from which we build our piecewise roms that are used in the online phase in the online phase we query multiple during the optimization process 	in the online phase we query multiple during the optimization process 
1	120	8036	our proposed methodology to solve a pde constrained optimization problem operates in two phases an offline phase and an online phase in the offline phase we cluster precomputed training solutions from which we build our piecewise roms that are used in the online phase in the online phase we query multiple during the optimization process 	for each queried i we need select which piecewise rom v i to use 
0	120	8037	our proposed methodology to solve a pde constrained optimization problem operates in two phases an offline phase and an online phase in the offline phase we cluster precomputed training solutions from which we build our piecewise roms that are used in the online phase in the online phase we query multiple during the optimization process 	then we run the simulation to compute wp i q and lift drag p i q 
1	120	8038	since our goal is to break our domain d into smaller sub domains we believe clustering the training points based on the euclidean distance between features will be most effective we have applied three algorithms in order to implement this k means expectation maximization to fit a gaussian mixture model and agglomerative clustering in terms of clustering features we have considered using b a features as they provide more information on the physics problem we are trying to solve and obviously lift and drag are both related to the fluid state 	since our goal is to break our domain d into smaller sub domains we believe clustering the training points based on the euclidean distance between features will be most effective 
1	120	8039	since our goal is to break our domain d into smaller sub domains we believe clustering the training points based on the euclidean distance between features will be most effective we have applied three algorithms in order to implement this k means expectation maximization to fit a gaussian mixture model and agglomerative clustering in terms of clustering features we have considered using b a features as they provide more information on the physics problem we are trying to solve and obviously lift and drag are both related to the fluid state 	we have applied three algorithms in order to implement this k means expectation maximization to fit a gaussian mixture model and agglomerative clustering in terms of clustering features we have considered using b a features as they provide more information on the physics problem we are trying to solve and obviously lift and drag are both related to the fluid state 
1	120	8040	the number of training points used when constructing roms are relatively low when compared with other machine learning problems additionally we do not have ground truth values for our classifications and only are able to determine how well our algorithms performs after doing a rom simulation therefore we have chosen to evaluate two simple methods nearest centroid and multinomial logistic regression as our algorithms for performing classifications 	the number of training points used when constructing roms are relatively low when compared with other machine learning problems 
1	120	8041	the number of training points used when constructing roms are relatively low when compared with other machine learning problems additionally we do not have ground truth values for our classifications and only are able to determine how well our algorithms performs after doing a rom simulation therefore we have chosen to evaluate two simple methods nearest centroid and multinomial logistic regression as our algorithms for performing classifications 	additionally we do not have ground truth values for our classifications and only are able to determine how well our algorithms performs after doing a rom simulation 
1	120	8042	the number of training points used when constructing roms are relatively low when compared with other machine learning problems additionally we do not have ground truth values for our classifications and only are able to determine how well our algorithms performs after doing a rom simulation therefore we have chosen to evaluate two simple methods nearest centroid and multinomial logistic regression as our algorithms for performing classifications 	therefore we have chosen to evaluate two simple methods nearest centroid and multinomial logistic regression as our algorithms for performing classifications 
1	120	8043	the number of training points used when constructing roms are relatively low when compared with other machine learning problems additionally we do not have ground truth values for our classifications and only are able to determine how well our algorithms performs after doing a rom simulation therefore we have chosen to evaluate two simple methods nearest centroid and multinomial logistic regression as our algorithms for performing classifications 	for nearest centroid we simply select the piecewise rom whose clustered points have the closest centroid to the queried point while multinomial logistic regression is trained using a cross entropy loss and the labels output from clustering during the offline phase 
1	120	8044	the number of training points used when constructing roms are relatively low when compared with other machine learning problems additionally we do not have ground truth values for our classifications and only are able to determine how well our algorithms performs after doing a rom simulation therefore we have chosen to evaluate two simple methods nearest centroid and multinomial logistic regression as our algorithms for performing classifications 	since during the online phase we will not have access to the lift or drag for a given query point we are only able to use as a feature for classification 
1	120	8045	in order to to create a train validation test set we sampled the parameter domain d r 3 to compute 90 solutions twp i qu	in order to to create a train validation test set we sampled the parameter domain d r 3 to compute 90 solutions twp i qu
1	120	8046	for all of the following experiments we define our error to be the difference in lift drag calculated with a rom and lift drag calculated with the hdm we refer to mse as the mean squared error across our test points and max error as the highest percentage deviation from the lift drag calculated with the hdm 	for all of the following experiments we define our error to be the difference in lift drag calculated with a rom and lift drag calculated with the hdm 
1	120	8047	for all of the following experiments we define our error to be the difference in lift drag calculated with a rom and lift drag calculated with the hdm we refer to mse as the mean squared error across our test points and max error as the highest percentage deviation from the lift drag calculated with the hdm 	we refer to mse as the mean squared error across our test points and max error as the highest percentage deviation from the lift drag calculated with the hdm 
0	120	8048	for our first set of experiments we determine the best parameters for our methodology given our design space specifically we use the validation set to determine the clustering algorithm	for our first set of experiments we determine the best parameters for our methodology given our design space 
1	120	8049	for our first set of experiments we determine the best parameters for our methodology given our design space specifically we use the validation set to determine the clustering algorithm	specifically we use the validation set to determine the clustering algorithm
0	120	8050	 clustering features number of clusters for each experiment we ran three tests each with 20 training points folded from our total 50 training points and test the error on our validation set in subsection 4 3 we investigate using a predictor to automatically determine our parameters without having to run any simulations on a validation set first we tested for the best clustering algorithms fixing our classification algorithm to nearest centroid the number of clusters to 4 and clustering features to t lift drag u finally we tested the effect of using different numbers of clusters 	 clustering features number of clusters for each experiment we ran three tests each with 20 training points folded from our total 50 training points and test the error on our validation set 
1	120	8051	 clustering features number of clusters for each experiment we ran three tests each with 20 training points folded from our total 50 training points and test the error on our validation set in subsection 4 3 we investigate using a predictor to automatically determine our parameters without having to run any simulations on a validation set first we tested for the best clustering algorithms fixing our classification algorithm to nearest centroid the number of clusters to 4 and clustering features to t lift drag u finally we tested the effect of using different numbers of clusters 	in subsection 4 3 we investigate using a predictor to automatically determine our parameters without having to run any simulations on a validation set first we tested for the best clustering algorithms fixing our classification algorithm to nearest centroid the number of clusters to 4 and clustering features to t lift drag u 
0	120	8052	 clustering features number of clusters for each experiment we ran three tests each with 20 training points folded from our total 50 training points and test the error on our validation set in subsection 4 3 we investigate using a predictor to automatically determine our parameters without having to run any simulations on a validation set first we tested for the best clustering algorithms fixing our classification algorithm to nearest centroid the number of clusters to 4 and clustering features to t lift drag u finally we tested the effect of using different numbers of clusters 	finally we tested the effect of using different numbers of clusters 
0	120	8053	 clustering features number of clusters for each experiment we ran three tests each with 20 training points folded from our total 50 training points and test the error on our validation set in subsection 4 3 we investigate using a predictor to automatically determine our parameters without having to run any simulations on a validation set first we tested for the best clustering algorithms fixing our classification algorithm to nearest centroid the number of clusters to 4 and clustering features to t lift drag u finally we tested the effect of using different numbers of clusters 	we used k means for clustering nearest centroid for classification and t lift drag u as the clustering features 
1	120	8054	with our optimal clustering classification algorithms and features derived from subsection 4 1 and clusters of size 2 and 4 we tested the accuracy of our methodology versus a global rom approach for calculating the lift drag the objective function of the optimization problem for the test with two clusters we show the offline clustering phase in figure 12 in	with our optimal clustering classification algorithms and features derived from subsection 4 1 and clusters of size 2 and 4 we tested the accuracy of our methodology versus a global rom approach for calculating the lift drag the objective function of the optimization problem 
1	120	8055	with our optimal clustering classification algorithms and features derived from subsection 4 1 and clusters of size 2 and 4 we tested the accuracy of our methodology versus a global rom approach for calculating the lift drag the objective function of the optimization problem for the test with two clusters we show the offline clustering phase in figure 12 in	for the test with two clusters we show the offline clustering phase in figure 12 
1	120	8056	in practice users would not want to have to use a validation set to determine the best clustering parameters as the time required to do this may outweigh any efficiency savings from using piecewise roms therefore a predictor for a reliable set of clustering parameters is necessary for real world applications we tested different cluster scoring methods including silhouette score	in practice users would not want to have to use a validation set to determine the best clustering parameters as the time required to do this may outweigh any efficiency savings from using piecewise roms 
1	120	8057	in practice users would not want to have to use a validation set to determine the best clustering parameters as the time required to do this may outweigh any efficiency savings from using piecewise roms therefore a predictor for a reliable set of clustering parameters is necessary for real world applications we tested different cluster scoring methods including silhouette score	therefore a predictor for a reliable set of clustering parameters is necessary for real world applications 
0	120	8058	in practice users would not want to have to use a validation set to determine the best clustering parameters as the time required to do this may outweigh any efficiency savings from using piecewise roms therefore a predictor for a reliable set of clustering parameters is necessary for real world applications we tested different cluster scoring methods including silhouette score	we tested different cluster scoring methods including silhouette score
0	120	8059	from our results we see that the k means and agglomerative clustering perform somewhat similarly compared to the gaussian mixture model this makes sense as the points used in the offline phase are not necessarily gaussian distributed while k means and agglomerative clustering less strong of an assumption as for clustering features the difference between feature sets is relatively small 	from our results we see that the k means and agglomerative clustering perform somewhat similarly compared to the gaussian mixture model 
0	120	8060	from our results we see that the k means and agglomerative clustering perform somewhat similarly compared to the gaussian mixture model this makes sense as the points used in the offline phase are not necessarily gaussian distributed while k means and agglomerative clustering less strong of an assumption as for clustering features the difference between feature sets is relatively small 	this makes sense as the points used in the offline phase are not necessarily gaussian distributed while k means and agglomerative clustering less strong of an assumption 
0	120	8061	from our results we see that the k means and agglomerative clustering perform somewhat similarly compared to the gaussian mixture model this makes sense as the points used in the offline phase are not necessarily gaussian distributed while k means and agglomerative clustering less strong of an assumption as for clustering features the difference between feature sets is relatively small 	as for clustering features the difference between feature sets is relatively small 
1	120	8062	from our results we see that the k means and agglomerative clustering perform somewhat similarly compared to the gaussian mixture model this makes sense as the points used in the offline phase are not necessarily gaussian distributed while k means and agglomerative clustering less strong of an assumption as for clustering features the difference between feature sets is relatively small 	this makes sense as for many points in the design space our optimization vector will be highly correlated with the lift and drag we are also able to see an interesting trade off when it comes to the number of clusters used 
0	120	8063	from our results we see that the k means and agglomerative clustering perform somewhat similarly compared to the gaussian mixture model this makes sense as the points used in the offline phase are not necessarily gaussian distributed while k means and agglomerative clustering less strong of an assumption as for clustering features the difference between feature sets is relatively small 	from the results we can clearly see that the error decreases with the number of cluster sizes 
1	120	8064	from our results we see that the k means and agglomerative clustering perform somewhat similarly compared to the gaussian mixture model this makes sense as the points used in the offline phase are not necessarily gaussian distributed while k means and agglomerative clustering less strong of an assumption as for clustering features the difference between feature sets is relatively small 	this is sensible because as we increase the number of clusters the number of points are assigned points used to create each rob decreases decreasing the accuracy of the rom approximation 
1	120	8065	from our results we see that the k means and agglomerative clustering perform somewhat similarly compared to the gaussian mixture model this makes sense as the points used in the offline phase are not necessarily gaussian distributed while k means and agglomerative clustering less strong of an assumption as for clustering features the difference between feature sets is relatively small 	however as the number of points used to build the rob decreases so does the computation cost of running a simulation with the corresponding rom 
1	120	8066	from our results we see that the k means and agglomerative clustering perform somewhat similarly compared to the gaussian mixture model this makes sense as the points used in the offline phase are not necessarily gaussian distributed while k means and agglomerative clustering less strong of an assumption as for clustering features the difference between feature sets is relatively small 	therefore the number of clusters used should be chosen on a per application basis where the user would select the number of clusters corresponding to the acceptable error overall we can see that the our proposed methodology is superior when compared with using a global rom 
1	120	8067	from our results we see that the k means and agglomerative clustering perform somewhat similarly compared to the gaussian mixture model this makes sense as the points used in the offline phase are not necessarily gaussian distributed while k means and agglomerative clustering less strong of an assumption as for clustering features the difference between feature sets is relatively small 	we can see that we are either able to get a much higher accuracy than the global rom with a similar computational cost related to the rom size or we are able to achieve a similar accuracy with half the computation cost of the global rom with regards to predictors for parameter selection we can see that all three cluster scoring methods show some indication that they could be used as a predictor for cluster rom accuracy at least for our design space 
1	120	8068	from our results we see that the k means and agglomerative clustering perform somewhat similarly compared to the gaussian mixture model this makes sense as the points used in the offline phase are not necessarily gaussian distributed while k means and agglomerative clustering less strong of an assumption as for clustering features the difference between feature sets is relatively small 	silhouette score and the calinski harabaz index may be slightly more correlated than the davies bouldin as the distance between points on the edges of clusters are reflected in their scores rather then only accounting for the distances between cluster centroids 
1	120	8069	from our results we see that the k means and agglomerative clustering perform somewhat similarly compared to the gaussian mixture model this makes sense as the points used in the offline phase are not necessarily gaussian distributed while k means and agglomerative clustering less strong of an assumption as for clustering features the difference between feature sets is relatively small 	however more rigorous testing is needed especially we do not know if it will generalize to other pde constrained optimization problems 
1	120	8070	in conclusion we present a novel approach to solving pdeconstrained optimization problems by utilizing multiple piecewise roms this approach has proven to be both more accurate and more computationally efficient than using a single global rom it shows particularly strong promise for time constrained applications with high dimensional design spaces 	in conclusion we present a novel approach to solving pdeconstrained optimization problems by utilizing multiple piecewise roms 
0	120	8071	in conclusion we present a novel approach to solving pdeconstrained optimization problems by utilizing multiple piecewise roms this approach has proven to be both more accurate and more computationally efficient than using a single global rom it shows particularly strong promise for time constrained applications with high dimensional design spaces 	this approach has proven to be both more accurate and more computationally efficient than using a single global rom 
0	120	8072	in conclusion we present a novel approach to solving pdeconstrained optimization problems by utilizing multiple piecewise roms this approach has proven to be both more accurate and more computationally efficient than using a single global rom it shows particularly strong promise for time constrained applications with high dimensional design spaces 	it shows particularly strong promise for time constrained applications with high dimensional design spaces 
1	120	8073	in conclusion we present a novel approach to solving pdeconstrained optimization problems by utilizing multiple piecewise roms this approach has proven to be both more accurate and more computationally efficient than using a single global rom it shows particularly strong promise for time constrained applications with high dimensional design spaces 	in these scenarios the global rom would need to be very large in order to be accurate across the whole design space and thus it might not be able to meet real time deadlines 
1	120	8074	in conclusion we present a novel approach to solving pdeconstrained optimization problems by utilizing multiple piecewise roms this approach has proven to be both more accurate and more computationally efficient than using a single global rom it shows particularly strong promise for time constrained applications with high dimensional design spaces 	piecewise roms on the other hand can be more efficient and thus able to meet the timing constraints we would like to continue testing the performance of our approach in more realistic higher dimensional design spaces 50 60 parameters 
1	120	8075	in conclusion we present a novel approach to solving pdeconstrained optimization problems by utilizing multiple piecewise roms this approach has proven to be both more accurate and more computationally efficient than using a single global rom it shows particularly strong promise for time constrained applications with high dimensional design spaces 	for this project we chose a limited design space due to time constraints as running tests in higher dimensional design spaces is naturally more computationally expensive and takes more time 
1	120	8076	in conclusion we present a novel approach to solving pdeconstrained optimization problems by utilizing multiple piecewise roms this approach has proven to be both more accurate and more computationally efficient than using a single global rom it shows particularly strong promise for time constrained applications with high dimensional design spaces 	we would also like to continue research on predictors for clustering effectiveness as this is a key component for this approach to be practical in real world problems 
0	120	8077	the first part of this project was discussing and creating a new methodology for solving pde constrained optimization problem this was a significant part of the project where both forest and gabriele discussed on the optimal approach to take to implement this methodology gabriele wrote code to build and run roms from a set of training points in addition to writing code to generate the data to start the experiments 	the first part of this project was discussing and creating a new methodology for solving pde constrained optimization problem 
0	120	8078	the first part of this project was discussing and creating a new methodology for solving pde constrained optimization problem this was a significant part of the project where both forest and gabriele discussed on the optimal approach to take to implement this methodology gabriele wrote code to build and run roms from a set of training points in addition to writing code to generate the data to start the experiments 	this was a significant part of the project where both forest and gabriele discussed on the optimal approach to take 
1	120	8079	the first part of this project was discussing and creating a new methodology for solving pde constrained optimization problem this was a significant part of the project where both forest and gabriele discussed on the optimal approach to take to implement this methodology gabriele wrote code to build and run roms from a set of training points in addition to writing code to generate the data to start the experiments 	to implement this methodology gabriele wrote code to build and run roms from a set of training points in addition to writing code to generate the data to start the experiments 
0	120	8080	the first part of this project was discussing and creating a new methodology for solving pde constrained optimization problem this was a significant part of the project where both forest and gabriele discussed on the optimal approach to take to implement this methodology gabriele wrote code to build and run roms from a set of training points in addition to writing code to generate the data to start the experiments 	forest was responsible for implementing the machine learning algorithms from external libraries as well as automating testing 
1	120	8081	the first part of this project was discussing and creating a new methodology for solving pde constrained optimization problem this was a significant part of the project where both forest and gabriele discussed on the optimal approach to take to implement this methodology gabriele wrote code to build and run roms from a set of training points in addition to writing code to generate the data to start the experiments 	both gabriele and forest contributed towards research and decision making for the use of machine learning techniques in this project in addition writing routines to output and post process results for analysis unfortunately the code must be run on a super computer with many external libraries from the stanford aeronautics astronautics department 
0	120	8082	the first part of this project was discussing and creating a new methodology for solving pde constrained optimization problem this was a significant part of the project where both forest and gabriele discussed on the optimal approach to take to implement this methodology gabriele wrote code to build and run roms from a set of training points in addition to writing code to generate the data to start the experiments 	we have included a zip file containing the only the code written for this project available at https drive google com file d 1bp4iw6rir cn3hxwl58cf pi5xppsi4w view usp sharing
0	121	8083	humans are generally good at categorizing and organizing music we may create novel playlists containing songs that may seem very dissimilar according to any baseline similarity measures it is this novelty and deeper level of understanding that we attempt to learn in this work how do users put together novel playlists 	humans are generally good at categorizing and organizing music 
1	121	8084	humans are generally good at categorizing and organizing music we may create novel playlists containing songs that may seem very dissimilar according to any baseline similarity measures it is this novelty and deeper level of understanding that we attempt to learn in this work how do users put together novel playlists 	we may create novel playlists containing songs that may seem very dissimilar according to any baseline similarity measures 
1	121	8085	humans are generally good at categorizing and organizing music we may create novel playlists containing songs that may seem very dissimilar according to any baseline similarity measures it is this novelty and deeper level of understanding that we attempt to learn in this work how do users put together novel playlists 	it is this novelty and deeper level of understanding that we attempt to learn in this work how do users put together novel playlists 
0	121	8086	humans are generally good at categorizing and organizing music we may create novel playlists containing songs that may seem very dissimilar according to any baseline similarity measures it is this novelty and deeper level of understanding that we attempt to learn in this work how do users put together novel playlists 	music listeners create playlists based on a multitude of strategies and intents 
1	121	8087	humans are generally good at categorizing and organizing music we may create novel playlists containing songs that may seem very dissimilar according to any baseline similarity measures it is this novelty and deeper level of understanding that we attempt to learn in this work how do users put together novel playlists 	many users put similar sounding songs together some make playlists solely from one artist others generate heterogeneous mixes spanning multiple genres eras and soundscapes 
1	121	8088	humans are generally good at categorizing and organizing music we may create novel playlists containing songs that may seem very dissimilar according to any baseline similarity measures it is this novelty and deeper level of understanding that we attempt to learn in this work how do users put together novel playlists 	clearly the problem of playlist classification increases in difficulty as playlist moods become less concrete and or separable 
1	121	8089	humans are generally good at categorizing and organizing music we may create novel playlists containing songs that may seem very dissimilar according to any baseline similarity measures it is this novelty and deeper level of understanding that we attempt to learn in this work how do users put together novel playlists 	to tackle this problem we will be using spotify s api to gather audio artist and genre data for selected playlists 
1	121	8090	humans are generally good at categorizing and organizing music we may create novel playlists containing songs that may seem very dissimilar according to any baseline similarity measures it is this novelty and deeper level of understanding that we attempt to learn in this work how do users put together novel playlists 	with a more carefully curated and specialized fingerprint of each playlist we hope to teach an algorithm to understand what makes that user and their playlists special 
0	121	8091	humans are generally good at categorizing and organizing music we may create novel playlists containing songs that may seem very dissimilar according to any baseline similarity measures it is this novelty and deeper level of understanding that we attempt to learn in this work how do users put together novel playlists 	the playlist lengths in the dataset 
0	121	8092	humans are generally good at categorizing and organizing music we may create novel playlists containing songs that may seem very dissimilar according to any baseline similarity measures it is this novelty and deeper level of understanding that we attempt to learn in this work how do users put together novel playlists 	for example in the toy dataset section 3 m 1044 
0	121	8093	humans are generally good at categorizing and organizing music we may create novel playlists containing songs that may seem very dissimilar according to any baseline similarity measures it is this novelty and deeper level of understanding that we attempt to learn in this work how do users put together novel playlists 	each song is labeled with an integer y i c representing one of the c output classes playlists 
0	121	8094	humans are generally good at categorizing and organizing music we may create novel playlists containing songs that may seem very dissimilar according to any baseline similarity measures it is this novelty and deeper level of understanding that we attempt to learn in this work how do users put together novel playlists 	in the toy case c 13 
1	121	8095	in general we would describe our problem space as non radio playlist continuation to distinguish from real time radio based solutions there are a few core concepts in our approach first we attempt to model the theme of a playlist with minimal assumptions about the user 	in general we would describe our problem space as non radio playlist continuation to distinguish from real time radio based solutions 
0	121	8096	in general we would describe our problem space as non radio playlist continuation to distinguish from real time radio based solutions there are a few core concepts in our approach first we attempt to model the theme of a playlist with minimal assumptions about the user 	there are a few core concepts in our approach 
1	121	8097	in general we would describe our problem space as non radio playlist continuation to distinguish from real time radio based solutions there are a few core concepts in our approach first we attempt to model the theme of a playlist with minimal assumptions about the user 	first we attempt to model the theme of a playlist with minimal assumptions about the user 
1	121	8098	in general we would describe our problem space as non radio playlist continuation to distinguish from real time radio based solutions there are a few core concepts in our approach first we attempt to model the theme of a playlist with minimal assumptions about the user 	second the model should be scalable and generalizable to any user and any song on a service we use spotify 
0	121	8099	in general we would describe our problem space as non radio playlist continuation to distinguish from real time radio based solutions there are a few core concepts in our approach first we attempt to model the theme of a playlist with minimal assumptions about the user 	third the model should be novel and specific to each user 
1	121	8100	in general we would describe our problem space as non radio playlist continuation to distinguish from real time radio based solutions there are a few core concepts in our approach first we attempt to model the theme of a playlist with minimal assumptions about the user 	these concepts are informed by three fundamental pieces of literature in the space which conclude the following individuals apply a wide variety of methods and reasoning for how and why their playlists are created model playlists using random walks on a hypergraph of song nodes especially due to large data these implementations would not be fully appropriate for our application however they introduce important core concepts and algorithms that we have borrowed such as hybrid feature sets both collaborative filtering and contentbased features 
1	121	8101	for our first study we started with data we perceived to be most separable a toy set of c 1 13 spotify curated playlists country by the grace of god spread the gospel for a total of m 1 1044 tracks the first experiment is an idealized fully supervised setting we also conduct a second study generalizing the most successful model on real user data to truly challenge the models for each playlist we used spotify s api to pull tracks audio features per track and genre tags per artist in the dataset spotify has developed 2095 genre tags spanning from broad ex 	for our first study we started with data we perceived to be most separable a toy set of c 1 13 spotify curated playlists country by the grace of god spread the gospel for a total of m 1 1044 tracks 
0	121	8102	for our first study we started with data we perceived to be most separable a toy set of c 1 13 spotify curated playlists country by the grace of god spread the gospel for a total of m 1 1044 tracks the first experiment is an idealized fully supervised setting we also conduct a second study generalizing the most successful model on real user data to truly challenge the models for each playlist we used spotify s api to pull tracks audio features per track and genre tags per artist in the dataset spotify has developed 2095 genre tags spanning from broad ex 	the first experiment is an idealized fully supervised setting 
1	121	8103	for our first study we started with data we perceived to be most separable a toy set of c 1 13 spotify curated playlists country by the grace of god spread the gospel for a total of m 1 1044 tracks the first experiment is an idealized fully supervised setting we also conduct a second study generalizing the most successful model on real user data to truly challenge the models for each playlist we used spotify s api to pull tracks audio features per track and genre tags per artist in the dataset spotify has developed 2095 genre tags spanning from broad ex 	we also conduct a second study generalizing the most successful model on real user data to truly challenge the models for each playlist we used spotify s api to pull tracks audio features per track and genre tags per artist in the dataset spotify has developed 2095 genre tags spanning from broad ex 
0	121	8104	for our first study we started with data we perceived to be most separable a toy set of c 1 13 spotify curated playlists country by the grace of god spread the gospel for a total of m 1 1044 tracks the first experiment is an idealized fully supervised setting we also conduct a second study generalizing the most successful model on real user data to truly challenge the models for each playlist we used spotify s api to pull tracks audio features per track and genre tags per artist in the dataset spotify has developed 2095 genre tags spanning from broad ex 	 pop or r b to extremely granular ex 
0	121	8105	for our first study we started with data we perceived to be most separable a toy set of c 1 13 spotify curated playlists country by the grace of god spread the gospel for a total of m 1 1044 tracks the first experiment is an idealized fully supervised setting we also conduct a second study generalizing the most successful model on real user data to truly challenge the models for each playlist we used spotify s api to pull tracks audio features per track and genre tags per artist in the dataset spotify has developed 2095 genre tags spanning from broad ex 	 australian dance and big room 
1	121	8106	for our first study we started with data we perceived to be most separable a toy set of c 1 13 spotify curated playlists country by the grace of god spread the gospel for a total of m 1 1044 tracks the first experiment is an idealized fully supervised setting we also conduct a second study generalizing the most successful model on real user data to truly challenge the models for each playlist we used spotify s api to pull tracks audio features per track and genre tags per artist in the dataset spotify has developed 2095 genre tags spanning from broad ex 	these audio features and genre tags are created with spotify s internal machine listening and analysis capabilities acquired mostly from echonest for both experiments we split the dataset 80 20 80 train and 20 test 
0	121	8107	for our first study we started with data we perceived to be most separable a toy set of c 1 13 spotify curated playlists country by the grace of god spread the gospel for a total of m 1 1044 tracks the first experiment is an idealized fully supervised setting we also conduct a second study generalizing the most successful model on real user data to truly challenge the models for each playlist we used spotify s api to pull tracks audio features per track and genre tags per artist in the dataset spotify has developed 2095 genre tags spanning from broad ex 	in the future we may explore data augmentation techniques to expand the size of these datasets most of the audio features ex 
0	121	8108	for our first study we started with data we perceived to be most separable a toy set of c 1 13 spotify curated playlists country by the grace of god spread the gospel for a total of m 1 1044 tracks the first experiment is an idealized fully supervised setting we also conduct a second study generalizing the most successful model on real user data to truly challenge the models for each playlist we used spotify s api to pull tracks audio features per track and genre tags per artist in the dataset spotify has developed 2095 genre tags spanning from broad ex 	 danceability energy and speechiness are measures between 0 1 
0	121	8109	for our first study we started with data we perceived to be most separable a toy set of c 1 13 spotify curated playlists country by the grace of god spread the gospel for a total of m 1 1044 tracks the first experiment is an idealized fully supervised setting we also conduct a second study generalizing the most successful model on real user data to truly challenge the models for each playlist we used spotify s api to pull tracks audio features per track and genre tags per artist in the dataset spotify has developed 2095 genre tags spanning from broad ex 	however key takes on an integer value 0 11 loudness is a float 60 0 measuring average decibel db reading across a track and tempo is a float representing beats per minute bpm 
0	121	8110	for our first study we started with data we perceived to be most separable a toy set of c 1 13 spotify curated playlists country by the grace of god spread the gospel for a total of m 1 1044 tracks the first experiment is an idealized fully supervised setting we also conduct a second study generalizing the most successful model on real user data to truly challenge the models for each playlist we used spotify s api to pull tracks audio features per track and genre tags per artist in the dataset spotify has developed 2095 genre tags spanning from broad ex 	as such we explore the effects of applying the standard preprocessing step of subtracting the mean and scaling by the variance of each feature 
0	121	8111	for our first study we started with data we perceived to be most separable a toy set of c 1 13 spotify curated playlists country by the grace of god spread the gospel for a total of m 1 1044 tracks the first experiment is an idealized fully supervised setting we also conduct a second study generalizing the most successful model on real user data to truly challenge the models for each playlist we used spotify s api to pull tracks audio features per track and genre tags per artist in the dataset spotify has developed 2095 genre tags spanning from broad ex 	this ensures that the relative scale of these features do not negatively skew the results 
0	121	8112	for our first study we started with data we perceived to be most separable a toy set of c 1 13 spotify curated playlists country by the grace of god spread the gospel for a total of m 1 1044 tracks the first experiment is an idealized fully supervised setting we also conduct a second study generalizing the most successful model on real user data to truly challenge the models for each playlist we used spotify s api to pull tracks audio features per track and genre tags per artist in the dataset spotify has developed 2095 genre tags spanning from broad ex 	a summary of our results is in section 5 
1	121	8113	the models we elected to compare are perceptron regression with one versus all binary classification for each class various svms polynomial sigmoid and rbf kernels and two neural network architectures the perceptron update rule given a sample prediction h x i is given bythis update rule comes a generalization of the gradient descent update on the negative log likelihood for logistic regression svms attempt to find a boundary which maximizes the geometric margin between classes specifically denoting the geometric margin we look to solve the following constrained maximization problem w 1 i e find the separating boundary given by w and b such that all samples are a distance of at least away from the boundary 	the models we elected to compare are perceptron regression with one versus all binary classification for each class various svms polynomial sigmoid and rbf kernels and two neural network architectures the perceptron update rule given a sample prediction h x i is given bythis update rule comes a generalization of the gradient descent update on the negative log likelihood for logistic regression 
0	121	8114	the models we elected to compare are perceptron regression with one versus all binary classification for each class various svms polynomial sigmoid and rbf kernels and two neural network architectures the perceptron update rule given a sample prediction h x i is given bythis update rule comes a generalization of the gradient descent update on the negative log likelihood for logistic regression svms attempt to find a boundary which maximizes the geometric margin between classes specifically denoting the geometric margin we look to solve the following constrained maximization problem w 1 i e find the separating boundary given by w and b such that all samples are a distance of at least away from the boundary 	svms attempt to find a boundary which maximizes the geometric margin between classes 
0	121	8115	the models we elected to compare are perceptron regression with one versus all binary classification for each class various svms polynomial sigmoid and rbf kernels and two neural network architectures the perceptron update rule given a sample prediction h x i is given bythis update rule comes a generalization of the gradient descent update on the negative log likelihood for logistic regression svms attempt to find a boundary which maximizes the geometric margin between classes specifically denoting the geometric margin we look to solve the following constrained maximization problem w 1 i e find the separating boundary given by w and b such that all samples are a distance of at least away from the boundary 	specifically denoting the geometric margin we look to solve the following constrained maximization problem w 1 i e find the separating boundary given by w and b such that all samples are a distance of at least away from the boundary 
0	121	8116	the models we elected to compare are perceptron regression with one versus all binary classification for each class various svms polynomial sigmoid and rbf kernels and two neural network architectures the perceptron update rule given a sample prediction h x i is given bythis update rule comes a generalization of the gradient descent update on the negative log likelihood for logistic regression svms attempt to find a boundary which maximizes the geometric margin between classes specifically denoting the geometric margin we look to solve the following constrained maximization problem w 1 i e find the separating boundary given by w and b such that all samples are a distance of at least away from the boundary 	the true algorithm solves the dual form of this primal problem as the problem as stated above is non convex 
1	121	8117	the models we elected to compare are perceptron regression with one versus all binary classification for each class various svms polynomial sigmoid and rbf kernels and two neural network architectures the perceptron update rule given a sample prediction h x i is given bythis update rule comes a generalization of the gradient descent update on the negative log likelihood for logistic regression svms attempt to find a boundary which maximizes the geometric margin between classes specifically denoting the geometric margin we look to solve the following constrained maximization problem w 1 i e find the separating boundary given by w and b such that all samples are a distance of at least away from the boundary 	the library used for the regression and svms is scikit learn python s machine learning platform 
0	121	8118	the models we elected to compare are perceptron regression with one versus all binary classification for each class various svms polynomial sigmoid and rbf kernels and two neural network architectures the perceptron update rule given a sample prediction h x i is given bythis update rule comes a generalization of the gradient descent update on the negative log likelihood for logistic regression svms attempt to find a boundary which maximizes the geometric margin between classes specifically denoting the geometric margin we look to solve the following constrained maximization problem w 1 i e find the separating boundary given by w and b such that all samples are a distance of at least away from the boundary 	lastly a neural network can be thought of as a sequence of linear transformations non linear functions applied to an input set of samples x r m n to obtain a prediction vector y r n 
0	121	8119	the models we elected to compare are perceptron regression with one versus all binary classification for each class various svms polynomial sigmoid and rbf kernels and two neural network architectures the perceptron update rule given a sample prediction h x i is given bythis update rule comes a generalization of the gradient descent update on the negative log likelihood for logistic regression svms attempt to find a boundary which maximizes the geometric margin between classes specifically denoting the geometric margin we look to solve the following constrained maximization problem w 1 i e find the separating boundary given by w and b such that all samples are a distance of at least away from the boundary 	y a h a 2 where w i is a linear transformation a i is a non linear activation function applied element wise to its argument and h denotes the number of hidden layers 
1	121	8120	the models we elected to compare are perceptron regression with one versus all binary classification for each class various svms polynomial sigmoid and rbf kernels and two neural network architectures the perceptron update rule given a sample prediction h x i is given bythis update rule comes a generalization of the gradient descent update on the negative log likelihood for logistic regression svms attempt to find a boundary which maximizes the geometric margin between classes specifically denoting the geometric margin we look to solve the following constrained maximization problem w 1 i e find the separating boundary given by w and b such that all samples are a distance of at least away from the boundary 	the neural network attempts to learn a weighting of the input features w i s that best classifies the output 
1	121	8121	the models we elected to compare are perceptron regression with one versus all binary classification for each class various svms polynomial sigmoid and rbf kernels and two neural network architectures the perceptron update rule given a sample prediction h x i is given bythis update rule comes a generalization of the gradient descent update on the negative log likelihood for logistic regression svms attempt to find a boundary which maximizes the geometric margin between classes specifically denoting the geometric margin we look to solve the following constrained maximization problem w 1 i e find the separating boundary given by w and b such that all samples are a distance of at least away from the boundary 	the neural networks were implemented in pytorch though we test 6 models architectures in this work we have elected to focus most of our efforts on the neural networks to solve the problem 
0	121	8122	the models we elected to compare are perceptron regression with one versus all binary classification for each class various svms polynomial sigmoid and rbf kernels and two neural network architectures the perceptron update rule given a sample prediction h x i is given bythis update rule comes a generalization of the gradient descent update on the negative log likelihood for logistic regression svms attempt to find a boundary which maximizes the geometric margin between classes specifically denoting the geometric margin we look to solve the following constrained maximization problem w 1 i e find the separating boundary given by w and b such that all samples are a distance of at least away from the boundary 	it is difficult sometimes even for humans to discern exactly what holds a playlist together 
1	121	8123	the models we elected to compare are perceptron regression with one versus all binary classification for each class various svms polynomial sigmoid and rbf kernels and two neural network architectures the perceptron update rule given a sample prediction h x i is given bythis update rule comes a generalization of the gradient descent update on the negative log likelihood for logistic regression svms attempt to find a boundary which maximizes the geometric margin between classes specifically denoting the geometric margin we look to solve the following constrained maximization problem w 1 i e find the separating boundary given by w and b such that all samples are a distance of at least away from the boundary 	furthermore the relationship between song features and playlists will not lend itself well to geometric separation in a feature space further complicated by the fact that one data point song may have multiple labels belong to multiple playlists 
0	121	8124	the models we elected to compare are perceptron regression with one versus all binary classification for each class various svms polynomial sigmoid and rbf kernels and two neural network architectures the perceptron update rule given a sample prediction h x i is given bythis update rule comes a generalization of the gradient descent update on the negative log likelihood for logistic regression svms attempt to find a boundary which maximizes the geometric margin between classes specifically denoting the geometric margin we look to solve the following constrained maximization problem w 1 i e find the separating boundary given by w and b such that all samples are a distance of at least away from the boundary 	the complexity of this problem indicates that less complex algorithms may not be suited to solving the problem effectively 
0	121	8125	for all tests below we took test accuracy to be our primary metric of success ta correctly classified total of samples many models achieved high training accuracy but unless the test accuracy matched this level the model was likely overfitted 	for all tests below we took test accuracy to be our primary metric of success 
0	121	8126	for all tests below we took test accuracy to be our primary metric of success ta correctly classified total of samples many models achieved high training accuracy but unless the test accuracy matched this level the model was likely overfitted 	ta correctly classified total of samples many models achieved high training accuracy but unless the test accuracy matched this level the model was likely overfitted 
1	121	8127	we compared perceptron with polynomial sigmoid and rbf kerneled svms with and without the preprocessing step of rescaling the features on the toy dataset 13 spotify curated playlists we performed k fold cross validation with k 5 the results are summarized in without performing the preprocessing step the accuracy of the results takes a significant hit 	we compared perceptron with polynomial sigmoid and rbf kerneled svms with and without the preprocessing step of rescaling the features on the toy dataset 13 spotify curated playlists 
0	121	8128	we compared perceptron with polynomial sigmoid and rbf kerneled svms with and without the preprocessing step of rescaling the features on the toy dataset 13 spotify curated playlists we performed k fold cross validation with k 5 the results are summarized in without performing the preprocessing step the accuracy of the results takes a significant hit 	we performed k fold cross validation with k 5 
0	121	8129	we compared perceptron with polynomial sigmoid and rbf kerneled svms with and without the preprocessing step of rescaling the features on the toy dataset 13 spotify curated playlists we performed k fold cross validation with k 5 the results are summarized in without performing the preprocessing step the accuracy of the results takes a significant hit 	the results are summarized in without performing the preprocessing step the accuracy of the results takes a significant hit 
0	121	8130	we compared perceptron with polynomial sigmoid and rbf kerneled svms with and without the preprocessing step of rescaling the features on the toy dataset 13 spotify curated playlists we performed k fold cross validation with k 5 the results are summarized in without performing the preprocessing step the accuracy of the results takes a significant hit 	on preprocessed data rbf kerneled svm reliably performed the best achieved the highest test accuracy 
0	121	8131	we compared perceptron with polynomial sigmoid and rbf kerneled svms with and without the preprocessing step of rescaling the features on the toy dataset 13 spotify curated playlists we performed k fold cross validation with k 5 the results are summarized in without performing the preprocessing step the accuracy of the results takes a significant hit 	tuning the penalty parameter on the error term we obtain a final test accuracy of 0 80 0 05 
1	121	8132	we compared perceptron with polynomial sigmoid and rbf kerneled svms with and without the preprocessing step of rescaling the features on the toy dataset 13 spotify curated playlists we performed k fold cross validation with k 5 the results are summarized in without performing the preprocessing step the accuracy of the results takes a significant hit 	the precision recall f score and support results are tabulated in note that the playlists celtic punk and spread the gospel achieved 100 precision by the tuned svm all corresponding tracks in the test set were correctly classified into these playlists 
0	121	8133	we compared perceptron with polynomial sigmoid and rbf kerneled svms with and without the preprocessing step of rescaling the features on the toy dataset 13 spotify curated playlists we performed k fold cross validation with k 5 the results are summarized in without performing the preprocessing step the accuracy of the results takes a significant hit 	this demonstrates the efficacy of these methods on more unique genre specific lists 
0	121	8134	we compared perceptron with polynomial sigmoid and rbf kerneled svms with and without the preprocessing step of rescaling the features on the toy dataset 13 spotify curated playlists we performed k fold cross validation with k 5 the results are summarized in without performing the preprocessing step the accuracy of the results takes a significant hit 	 kitchen swagger on the other hand is a harder playlist to classify as it does not match any one genre 
1	121	8135	we considered many parameters of our network number of hidden layers activation functions sigmoid relu identity softmax logsoftmax with vs without l 2 regulation number of iterations stochastic batch full gradient descent loss function mse vs nll etc we ultimately found that a neural network minimizing nll loss via full gradient descent with one hidden layer of c neurons and a logsoftmax output layer performs best on the toy set 	we considered many parameters of our network number of hidden layers activation functions sigmoid relu identity softmax logsoftmax with vs without l 2 regulation number of iterations stochastic batch full gradient descent loss function mse vs nll etc 
1	121	8136	we considered many parameters of our network number of hidden layers activation functions sigmoid relu identity softmax logsoftmax with vs without l 2 regulation number of iterations stochastic batch full gradient descent loss function mse vs nll etc we ultimately found that a neural network minimizing nll loss via full gradient descent with one hidden layer of c neurons and a logsoftmax output layer performs best on the toy set 	we ultimately found that a neural network minimizing nll loss via full gradient descent with one hidden layer of c neurons and a logsoftmax output layer performs best on the toy set 
1	121	8137	train test 2 identity sigmoid 0 91 0 77 1 sigmoid 0 89 0 82 with the architecture finalized we tested on a handful of real users again training on 80 of their playlists and testing on the remaining 20 the results are summarized in	train test 2 identity sigmoid 0 91 0 77 1 sigmoid 0 89 0 82 with the architecture finalized we tested on a handful of real users again training on 80 of their playlists and testing on the remaining 20 
0	121	8138	train test 2 identity sigmoid 0 91 0 77 1 sigmoid 0 89 0 82 with the architecture finalized we tested on a handful of real users again training on 80 of their playlists and testing on the remaining 20 the results are summarized in	the results are summarized in
1	121	8139	train test a jacob s playlists b myles playlists we see that certain users are more challenging to classify than others ex myles vs jacob 	train test a jacob s playlists b myles playlists we see that certain users are more challenging to classify than others ex 
0	121	8140	train test a jacob s playlists b myles playlists we see that certain users are more challenging to classify than others ex myles vs jacob 	myles vs jacob 
0	121	8141	in some ways the observed results are not surprising we have observed that a relatively minimal neural network does a better job at classifying playlists than perceptron and svms do this is understandable as the nature of curating a playlist is rather subjective and playlists are not always separable 	in some ways the observed results are not surprising 
1	121	8142	in some ways the observed results are not surprising we have observed that a relatively minimal neural network does a better job at classifying playlists than perceptron and svms do this is understandable as the nature of curating a playlist is rather subjective and playlists are not always separable 	we have observed that a relatively minimal neural network does a better job at classifying playlists than perceptron and svms do 
0	121	8143	in some ways the observed results are not surprising we have observed that a relatively minimal neural network does a better job at classifying playlists than perceptron and svms do this is understandable as the nature of curating a playlist is rather subjective and playlists are not always separable 	this is understandable as the nature of curating a playlist is rather subjective and playlists are not always separable 
0	121	8144	in some ways the observed results are not surprising we have observed that a relatively minimal neural network does a better job at classifying playlists than perceptron and svms do this is understandable as the nature of curating a playlist is rather subjective and playlists are not always separable 	even so the neural network struggled against a real user set 
0	121	8145	in some ways the observed results are not surprising we have observed that a relatively minimal neural network does a better job at classifying playlists than perceptron and svms do this is understandable as the nature of curating a playlist is rather subjective and playlists are not always separable 	there are multiple factors that play into this larger labeled datasets 
0	121	8146	in some ways the observed results are not surprising we have observed that a relatively minimal neural network does a better job at classifying playlists than perceptron and svms do this is understandable as the nature of curating a playlist is rather subjective and playlists are not always separable 	in many instances the user data we were training testing on consisted of only hundreds sometimes tens of samples 
0	121	8147	in some ways the observed results are not surprising we have observed that a relatively minimal neural network does a better job at classifying playlists than perceptron and svms do this is understandable as the nature of curating a playlist is rather subjective and playlists are not always separable 	training on only 80 of this dataset decreases this number further 
1	121	8148	in some ways the observed results are not surprising we have observed that a relatively minimal neural network does a better job at classifying playlists than perceptron and svms do this is understandable as the nature of curating a playlist is rather subjective and playlists are not always separable 	this is not enough information to train a neural network classifier on nor to gather meaningful test results on given our limited features 
1	121	8149	in some ways the observed results are not surprising we have observed that a relatively minimal neural network does a better job at classifying playlists than perceptron and svms do this is understandable as the nature of curating a playlist is rather subjective and playlists are not always separable 	in the future we may either select larger user playlists or consider applying data augmentation techniques to increase the size of our set segmentation 
1	121	8150	in some ways the observed results are not surprising we have observed that a relatively minimal neural network does a better job at classifying playlists than perceptron and svms do this is understandable as the nature of curating a playlist is rather subjective and playlists are not always separable 	it is easier to classify a song into a playlist when the number of output classes c is small 
0	121	8151	in some ways the observed results are not surprising we have observed that a relatively minimal neural network does a better job at classifying playlists than perceptron and svms do this is understandable as the nature of curating a playlist is rather subjective and playlists are not always separable 	this concept is known as segmentation different algorithm 
0	121	8152	in some ways the observed results are not surprising we have observed that a relatively minimal neural network does a better job at classifying playlists than perceptron and svms do this is understandable as the nature of curating a playlist is rather subjective and playlists are not always separable 	it may be the case that even neural networks are not the best option for this problem though literature in the field suggests otherwise more track features 
1	121	8153	in some ways the observed results are not surprising we have observed that a relatively minimal neural network does a better job at classifying playlists than perceptron and svms do this is understandable as the nature of curating a playlist is rather subjective and playlists are not always separable 	we have built a classifier with only audio features and one hot vectors of genre tags for each track 
0	121	8154	in some ways the observed results are not surprising we have observed that a relatively minimal neural network does a better job at classifying playlists than perceptron and svms do this is understandable as the nature of curating a playlist is rather subjective and playlists are not always separable 	we are missing vital data such as artist info release date etc section 7 
0	121	8155	playlist curation and music recommendation is an art that trained music analysts and disc jockeys have perfected over decades with the rise of massive listener data and the wealth of statistical and algorithmic developments we have begun to see a rise in computergenerated playlists and mixes at this stage however there is massive room for improvement 	playlist curation and music recommendation is an art that trained music analysts and disc jockeys have perfected over decades 
0	121	8156	playlist curation and music recommendation is an art that trained music analysts and disc jockeys have perfected over decades with the rise of massive listener data and the wealth of statistical and algorithmic developments we have begun to see a rise in computergenerated playlists and mixes at this stage however there is massive room for improvement 	with the rise of massive listener data and the wealth of statistical and algorithmic developments we have begun to see a rise in computergenerated playlists and mixes 
0	121	8157	playlist curation and music recommendation is an art that trained music analysts and disc jockeys have perfected over decades with the rise of massive listener data and the wealth of statistical and algorithmic developments we have begun to see a rise in computergenerated playlists and mixes at this stage however there is massive room for improvement 	at this stage however there is massive room for improvement 
1	121	8158	playlist curation and music recommendation is an art that trained music analysts and disc jockeys have perfected over decades with the rise of massive listener data and the wealth of statistical and algorithmic developments we have begun to see a rise in computergenerated playlists and mixes at this stage however there is massive room for improvement 	we have attempted to build a playlist classifier using audio features and genre tag data from spotify s public api node2vec 
1	121	8159	playlist curation and music recommendation is an art that trained music analysts and disc jockeys have perfected over decades with the rise of massive listener data and the wealth of statistical and algorithmic developments we have begun to see a rise in computergenerated playlists and mixes at this stage however there is massive room for improvement 	through spotify s api we have access to relatedartists data each artist comes with a list of 20 similar artists 
1	121	8160	playlist curation and music recommendation is an art that trained music analysts and disc jockeys have perfected over decades with the rise of massive listener data and the wealth of statistical and algorithmic developments we have begun to see a rise in computergenerated playlists and mixes at this stage however there is massive room for improvement 	we have gathered this information and built a related artists graph where each node represents an artist and edges link like artists 
0	121	8161	 google has published an open source library for computing vector embeddings of words in the future we hope to integrate this into our framework taking advantage of large free corpuses such as wikipedia and music website scrapes 	google has published an open source library for computing vector embeddings of words 
1	121	8162	 google has published an open source library for computing vector embeddings of words in the future we hope to integrate this into our framework taking advantage of large free corpuses such as wikipedia and music website scrapes 	in the future we hope to integrate this into our framework taking advantage of large free corpuses such as wikipedia and music website scrapes 
1	121	8163	 google has published an open source library for computing vector embeddings of words in the future we hope to integrate this into our framework taking advantage of large free corpuses such as wikipedia and music website scrapes 	results from echonest show that nlp understanding of the text around music are highly effective in music classification for example the stevie wonder vector may be near the words soul michael jackson piano or motown in the vector space 
1	121	8164	 google has published an open source library for computing vector embeddings of words in the future we hope to integrate this into our framework taking advantage of large free corpuses such as wikipedia and music website scrapes 	candidates for word2vec representations in our data include artist names user generated tags both from our users and from open sources like acousticbrainz genre tags playlist titles song names etc fasttext 
0	121	8165	 google has published an open source library for computing vector embeddings of words in the future we hope to integrate this into our framework taking advantage of large free corpuses such as wikipedia and music website scrapes 	facebook has developed the fasttext library for document classification 
1	121	8166	 google has published an open source library for computing vector embeddings of words in the future we hope to integrate this into our framework taking advantage of large free corpuses such as wikipedia and music website scrapes 	we hope to try using it with the variety of texts that we hope to collect including the wikipedia pages for our artists user generated tags genre tags and a variety of other textual metadata from track information artist information and more 
0	121	8167	 google has published an open source library for computing vector embeddings of words in the future we hope to integrate this into our framework taking advantage of large free corpuses such as wikipedia and music website scrapes 	in this case a playlist can be considered a document category and we will attempt to classify song documents as belonging to the document category 
0	121	8168	the data used for this problem comes from spotify s web api kevin obtained the necessary authorization with spotify automated the request process for all playlists tracks and feature data and built a postgres database from scratch via sequelize an object relational manager orm for node js once all the data for the toy set was in the database the duo implemented the baseline perceptron svm tests on the toy set using scikit learn 	the data used for this problem comes from spotify s web api 
0	121	8169	the data used for this problem comes from spotify s web api kevin obtained the necessary authorization with spotify automated the request process for all playlists tracks and feature data and built a postgres database from scratch via sequelize an object relational manager orm for node js once all the data for the toy set was in the database the duo implemented the baseline perceptron svm tests on the toy set using scikit learn 	kevin obtained the necessary authorization with spotify automated the request process for all playlists tracks and feature data and built a postgres database from scratch via sequelize an object relational manager orm for node js 
1	121	8170	the data used for this problem comes from spotify s web api kevin obtained the necessary authorization with spotify automated the request process for all playlists tracks and feature data and built a postgres database from scratch via sequelize an object relational manager orm for node js once all the data for the toy set was in the database the duo implemented the baseline perceptron svm tests on the toy set using scikit learn 	once all the data for the toy set was in the database the duo implemented the baseline perceptron svm tests on the toy set using scikit learn 
0	121	8171	the data used for this problem comes from spotify s web api kevin obtained the necessary authorization with spotify automated the request process for all playlists tracks and feature data and built a postgres database from scratch via sequelize an object relational manager orm for node js once all the data for the toy set was in the database the duo implemented the baseline perceptron svm tests on the toy set using scikit learn 	amel designed and tested our neural networks using pytorch wrote script to get pca visualizations 
1	121	8172	the data used for this problem comes from spotify s web api kevin obtained the necessary authorization with spotify automated the request process for all playlists tracks and feature data and built a postgres database from scratch via sequelize an object relational manager orm for node js once all the data for the toy set was in the database the duo implemented the baseline perceptron svm tests on the toy set using scikit learn 	kevin took on advanced data collection building a related artists graph and computing vector embeddings of each node using node2vec 
1	122	8173	understanding fluid flow in porous media at the microscale is relevant to many fields such as oil and gas recovery geothermal energy and geological co 2 storage properties such as porosity and permeability are often calculated from laboratory measurements or direct imaging of the microstructure however due to acquisition times and experimental costs it is difficult to evaluate the variability due to rock heterogeneity 	understanding fluid flow in porous media at the microscale is relevant to many fields such as oil and gas recovery geothermal energy and geological co 2 storage 
1	122	8174	understanding fluid flow in porous media at the microscale is relevant to many fields such as oil and gas recovery geothermal energy and geological co 2 storage properties such as porosity and permeability are often calculated from laboratory measurements or direct imaging of the microstructure however due to acquisition times and experimental costs it is difficult to evaluate the variability due to rock heterogeneity 	properties such as porosity and permeability are often calculated from laboratory measurements or direct imaging of the microstructure 
1	122	8175	understanding fluid flow in porous media at the microscale is relevant to many fields such as oil and gas recovery geothermal energy and geological co 2 storage properties such as porosity and permeability are often calculated from laboratory measurements or direct imaging of the microstructure however due to acquisition times and experimental costs it is difficult to evaluate the variability due to rock heterogeneity 	however due to acquisition times and experimental costs it is difficult to evaluate the variability due to rock heterogeneity 
1	122	8176	understanding fluid flow in porous media at the microscale is relevant to many fields such as oil and gas recovery geothermal energy and geological co 2 storage properties such as porosity and permeability are often calculated from laboratory measurements or direct imaging of the microstructure however due to acquisition times and experimental costs it is difficult to evaluate the variability due to rock heterogeneity 	instead researchers often use statistical methods to reconstruct porous media based on two point or multi point statistics recent advances in deep learning have shown promising use of generative adversarial networks gans for rapid generation of 3d images with no a priori model
0	122	8177	gans are made of two components 1 a generator g that creates a synthetic training image and 2 a discriminator d that tries to differentiate between the synthetic and real training image as the gan is trained the generator tries to create more realistic training images to fool the discriminator while the discriminator tries to become better at classifying real label 1 vs fake label 0 images z is the latent space vector sampled from a normal distribution so g z maps the latent vector to the image space 	gans are made of two components 1 a generator g that creates a synthetic training image and 2 a discriminator d that tries to differentiate between the synthetic and real training image 
0	122	8178	gans are made of two components 1 a generator g that creates a synthetic training image and 2 a discriminator d that tries to differentiate between the synthetic and real training image as the gan is trained the generator tries to create more realistic training images to fool the discriminator while the discriminator tries to become better at classifying real label 1 vs fake label 0 images z is the latent space vector sampled from a normal distribution so g z maps the latent vector to the image space 	as the gan is trained the generator tries to create more realistic training images to fool the discriminator while the discriminator tries to become better at classifying real label 1 vs fake label 0 images 
0	122	8179	gans are made of two components 1 a generator g that creates a synthetic training image and 2 a discriminator d that tries to differentiate between the synthetic and real training image as the gan is trained the generator tries to create more realistic training images to fool the discriminator while the discriminator tries to become better at classifying real label 1 vs fake label 0 images z is the latent space vector sampled from a normal distribution so g z maps the latent vector to the image space 	z is the latent space vector sampled from a normal distribution so g z maps the latent vector to the image space 
0	122	8180	gans are made of two components 1 a generator g that creates a synthetic training image and 2 a discriminator d that tries to differentiate between the synthetic and real training image as the gan is trained the generator tries to create more realistic training images to fool the discriminator while the discriminator tries to become better at classifying real label 1 vs fake label 0 images z is the latent space vector sampled from a normal distribution so g z maps the latent vector to the image space 	x is the data from an image real or fake and d g z is the probability that the generated image is real 
0	122	8181	gans are made of two components 1 a generator g that creates a synthetic training image and 2 a discriminator d that tries to differentiate between the synthetic and real training image as the gan is trained the generator tries to create more realistic training images to fool the discriminator while the discriminator tries to become better at classifying real label 1 vs fake label 0 images z is the latent space vector sampled from a normal distribution so g z maps the latent vector to the image space 	the two competing objectives result in the following value function researchers have shown that using a deep convolutional network in the generator and discriminator models can improve synthetic image generation and training of the network 
1	122	8182	gans are made of two components 1 a generator g that creates a synthetic training image and 2 a discriminator d that tries to differentiate between the synthetic and real training image as the gan is trained the generator tries to create more realistic training images to fool the discriminator while the discriminator tries to become better at classifying real label 1 vs fake label 0 images z is the latent space vector sampled from a normal distribution so g z maps the latent vector to the image space 	the overall guidelines for training a dcgan usually involes 1 using strided convolutions instead of pooling so that energy resources engineering stanford university the network can learn its own pooling functions 2 using batch normalization to improve gradient flow 3 removing fully connected hidden layers and 4 using a specific set of activation functions in the generator and discriminator explained further in the methods 
0	122	8183	the main objective of this project is to investigate the accuracy and feasibility of generating 2d 3d sandstone images through training a dcgan model while this has been already studied in the literature it is a relatively new field with many ongoing areas of interest such as ways to improve training stability image quality and incorporating grayscale and multiscale images this project first aims to successfully create and train a 2d gan before eventually training a 3d gan we can then evaluate how modifying the gan architecture affects the loss and accuracy of the generated images 	the main objective of this project is to investigate the accuracy and feasibility of generating 2d 3d sandstone images through training a dcgan model 
0	122	8184	the main objective of this project is to investigate the accuracy and feasibility of generating 2d 3d sandstone images through training a dcgan model while this has been already studied in the literature it is a relatively new field with many ongoing areas of interest such as ways to improve training stability image quality and incorporating grayscale and multiscale images this project first aims to successfully create and train a 2d gan before eventually training a 3d gan we can then evaluate how modifying the gan architecture affects the loss and accuracy of the generated images 	while this has been already studied in the literature it is a relatively new field with many ongoing areas of interest such as ways to improve training stability image quality and incorporating grayscale and multiscale images this project first aims to successfully create and train a 2d gan before eventually training a 3d gan 
0	122	8185	the main objective of this project is to investigate the accuracy and feasibility of generating 2d 3d sandstone images through training a dcgan model while this has been already studied in the literature it is a relatively new field with many ongoing areas of interest such as ways to improve training stability image quality and incorporating grayscale and multiscale images this project first aims to successfully create and train a 2d gan before eventually training a 3d gan we can then evaluate how modifying the gan architecture affects the loss and accuracy of the generated images 	we can then evaluate how modifying the gan architecture affects the loss and accuracy of the generated images 
0	122	8186	the main objective of this project is to investigate the accuracy and feasibility of generating 2d 3d sandstone images through training a dcgan model while this has been already studied in the literature it is a relatively new field with many ongoing areas of interest such as ways to improve training stability image quality and incorporating grayscale and multiscale images this project first aims to successfully create and train a 2d gan before eventually training a 3d gan we can then evaluate how modifying the gan architecture affects the loss and accuracy of the generated images 	once trained these images would then be able to be used as inputs into digital rock physics calculations of properties such as permeability and capillary pressure 
1	122	8187	the main objective of this project is to investigate the accuracy and feasibility of generating 2d 3d sandstone images through training a dcgan model while this has been already studied in the literature it is a relatively new field with many ongoing areas of interest such as ways to improve training stability image quality and incorporating grayscale and multiscale images this project first aims to successfully create and train a 2d gan before eventually training a 3d gan we can then evaluate how modifying the gan architecture affects the loss and accuracy of the generated images 	understanding how permeability is affected by variations in porosity and connectivity is necessary in many research areas involving fluid flow through porous media 
0	122	8188	the dataset was obtained from micro x ray tomography scans of a bentheimer sandstone the original data is a greyscale representation of the 3d solid and has been processed and binarized into the pore and solid phases the full dataset is 512 3 voxels large with a voxel size of 3 06 m 	the dataset was obtained from micro x ray tomography scans of a bentheimer sandstone 
0	122	8189	the dataset was obtained from micro x ray tomography scans of a bentheimer sandstone the original data is a greyscale representation of the 3d solid and has been processed and binarized into the pore and solid phases the full dataset is 512 3 voxels large with a voxel size of 3 06 m 	the original data is a greyscale representation of the 3d solid and has been processed and binarized into the pore and solid phases 
0	122	8190	the dataset was obtained from micro x ray tomography scans of a bentheimer sandstone the original data is a greyscale representation of the 3d solid and has been processed and binarized into the pore and solid phases the full dataset is 512 3 voxels large with a voxel size of 3 06 m 	the full dataset is 512 3 voxels large with a voxel size of 3 06 m 
0	122	8191	the dataset was obtained from micro x ray tomography scans of a bentheimer sandstone the original data is a greyscale representation of the 3d solid and has been processed and binarized into the pore and solid phases the full dataset is 512 3 voxels large with a voxel size of 3 06 m 	in order for the training image 64 x 64 voxels to capture an adequate area the image was downsampled to 256 3 voxels with a voxel size of 6 12 m 
0	122	8192	the dataset was obtained from micro x ray tomography scans of a bentheimer sandstone the original data is a greyscale representation of the 3d solid and has been processed and binarized into the pore and solid phases the full dataset is 512 3 voxels large with a voxel size of 3 06 m 	for data augmentation subvolumes were extracted every 16 voxels to yield 36 864 training images 
0	122	8193	the dataset was obtained from micro x ray tomography scans of a bentheimer sandstone the original data is a greyscale representation of the 3d solid and has been processed and binarized into the pore and solid phases the full dataset is 512 3 voxels large with a voxel size of 3 06 m 	initial tests were also done on a larger dataset of 72 728 images and yielded comparable results 
0	122	8194	the dataset was obtained from micro x ray tomography scans of a bentheimer sandstone the original data is a greyscale representation of the 3d solid and has been processed and binarized into the pore and solid phases the full dataset is 512 3 voxels large with a voxel size of 3 06 m 	to reduce training time we used the smaller training set for the majority of our tests 
0	122	8195	we used a deep convolutional gan dcgan which uses convolutional transpose layers in the generator and convolutional layers in the discriminator as our network model	we used a deep convolutional gan dcgan which uses convolutional transpose layers in the generator and convolutional layers in the discriminator as our network model
0	122	8196	to train d and g we used two different loss functions we first use the binary cross entropy loss function model dcgan 1 training is performed in two steps 1 train the discriminator to maximizewhile keeping the generator fixed and 2 train the generator to minimizewhile keeping the discriminator fixed in practice due to the effect of vanishing gradients it is easier to maximizeconvergence is theoretically reached when the generator can generate a distribution p g x that equal to p data x which corresponds to a discriminator output of 0 5 	to train d and g we used two different loss functions 
0	122	8197	to train d and g we used two different loss functions we first use the binary cross entropy loss function model dcgan 1 training is performed in two steps 1 train the discriminator to maximizewhile keeping the generator fixed and 2 train the generator to minimizewhile keeping the discriminator fixed in practice due to the effect of vanishing gradients it is easier to maximizeconvergence is theoretically reached when the generator can generate a distribution p g x that equal to p data x which corresponds to a discriminator output of 0 5 	we first use the binary cross entropy loss function model dcgan 1 training is performed in two steps 1 train the discriminator to maximizewhile keeping the generator fixed and 2 train the generator to minimizewhile keeping the discriminator fixed 
0	122	8198	to train d and g we used two different loss functions we first use the binary cross entropy loss function model dcgan 1 training is performed in two steps 1 train the discriminator to maximizewhile keeping the generator fixed and 2 train the generator to minimizewhile keeping the discriminator fixed in practice due to the effect of vanishing gradients it is easier to maximizeconvergence is theoretically reached when the generator can generate a distribution p g x that equal to p data x which corresponds to a discriminator output of 0 5 	in practice due to the effect of vanishing gradients it is easier to maximizeconvergence is theoretically reached when the generator can generate a distribution p g x that equal to p data x which corresponds to a discriminator output of 0 5 
0	122	8199	to train d and g we used two different loss functions we first use the binary cross entropy loss function model dcgan 1 training is performed in two steps 1 train the discriminator to maximizewhile keeping the generator fixed and 2 train the generator to minimizewhile keeping the discriminator fixed in practice due to the effect of vanishing gradients it is easier to maximizeconvergence is theoretically reached when the generator can generate a distribution p g x that equal to p data x which corresponds to a discriminator output of 0 5 	further details about the training steps can be found in we also investigated the effect of using the wasserstein distance as the loss function instead model dcgan 2 the primary advantages of using the wasserstein loss are that it can prevent mode collapse in the generator and allow for better convergence 
0	122	8200	to train d and g we used two different loss functions we first use the binary cross entropy loss function model dcgan 1 training is performed in two steps 1 train the discriminator to maximizewhile keeping the generator fixed and 2 train the generator to minimizewhile keeping the discriminator fixed in practice due to the effect of vanishing gradients it is easier to maximizeconvergence is theoretically reached when the generator can generate a distribution p g x that equal to p data x which corresponds to a discriminator output of 0 5 	the wasserstein distance measures the distance between two probability functions and the discriminator now becomes a critic that evaluates the wasserstein distance between the real and synthetic images 
0	122	8201	to train d and g we used two different loss functions we first use the binary cross entropy loss function model dcgan 1 training is performed in two steps 1 train the discriminator to maximizewhile keeping the generator fixed and 2 train the generator to minimizewhile keeping the discriminator fixed in practice due to the effect of vanishing gradients it is easier to maximizeconvergence is theoretically reached when the generator can generate a distribution p g x that equal to p data x which corresponds to a discriminator output of 0 5 	the distance is calculated by enforcing a lipschitz constraint on the critic s model either through weight clipping or a gradient penalty where is the gradient penalty coefficient and is set to 10 for our model 
0	122	8202	the objective of using a dcgan with our dataset is to recreate a pore network image with similar morphological properties as the original porous media to evaluate the accuracy of our model we use a set of morphological descriptors known as minkowski functionals in 2d there are 3 minkowski functionals that describe the surface area perimeter and euler characteristic 	the objective of using a dcgan with our dataset is to recreate a pore network image with similar morphological properties as the original porous media 
0	122	8203	the objective of using a dcgan with our dataset is to recreate a pore network image with similar morphological properties as the original porous media to evaluate the accuracy of our model we use a set of morphological descriptors known as minkowski functionals in 2d there are 3 minkowski functionals that describe the surface area perimeter and euler characteristic 	to evaluate the accuracy of our model we use a set of morphological descriptors known as minkowski functionals 
0	122	8204	the objective of using a dcgan with our dataset is to recreate a pore network image with similar morphological properties as the original porous media to evaluate the accuracy of our model we use a set of morphological descriptors known as minkowski functionals in 2d there are 3 minkowski functionals that describe the surface area perimeter and euler characteristic 	in 2d there are 3 minkowski functionals that describe the surface area perimeter and euler characteristic 
1	122	8205	the objective of using a dcgan with our dataset is to recreate a pore network image with similar morphological properties as the original porous media to evaluate the accuracy of our model we use a set of morphological descriptors known as minkowski functionals in 2d there are 3 minkowski functionals that describe the surface area perimeter and euler characteristic 	the area is simply the percent of pixels that are labeled as pore n pore divided by the total number of pixels n total area n pore n total finally the euler characteristic in 2d describes the connectivity of the surface and is defined as the difference between the number of connected regions solid and the number of holes pores a region with a negative euler characteristic will have more holes than connected regions which indicates low connectivity across the area 
0	122	8206	the objective of using a dcgan with our dataset is to recreate a pore network image with similar morphological properties as the original porous media to evaluate the accuracy of our model we use a set of morphological descriptors known as minkowski functionals in 2d there are 3 minkowski functionals that describe the surface area perimeter and euler characteristic 	a region with a positive euler characteristic will have more connected regions and therefore a high connectivity across the area which can allow for fluid flow for evaluation after the model is fully trained we create 64 realizations of 100 2 pixel images using the generator and randomly select the same number of images from our training set 
0	122	8207	the objective of using a dcgan with our dataset is to recreate a pore network image with similar morphological properties as the original porous media to evaluate the accuracy of our model we use a set of morphological descriptors known as minkowski functionals in 2d there are 3 minkowski functionals that describe the surface area perimeter and euler characteristic 	the generator outputs images with values between 1 1 
0	122	8208	the objective of using a dcgan with our dataset is to recreate a pore network image with similar morphological properties as the original porous media to evaluate the accuracy of our model we use a set of morphological descriptors known as minkowski functionals in 2d there are 3 minkowski functionals that describe the surface area perimeter and euler characteristic 	the images are normalized filtered using a median filter with a neighborhood of 2 pixels and binarized using otsu s method 
0	122	8209	the objective of using a dcgan with our dataset is to recreate a pore network image with similar morphological properties as the original porous media to evaluate the accuracy of our model we use a set of morphological descriptors known as minkowski functionals in 2d there are 3 minkowski functionals that describe the surface area perimeter and euler characteristic 	an example of the final synthetic image used for evaluation is shown in we next show the effect of varying different model parameters during the training process 
0	122	8210	we were able to successfully implement and train a 2d dcgan model to generate reasonable images with similar morphological properties to the original rock sample however it is still unclear if the generator is accurately capturing the underlying probability distribution of the real data further investigation could involve using the wasserstein loss as it is a measurement of the distance between two probability distributions 	we were able to successfully implement and train a 2d dcgan model to generate reasonable images with similar morphological properties to the original rock sample 
0	122	8211	we were able to successfully implement and train a 2d dcgan model to generate reasonable images with similar morphological properties to the original rock sample however it is still unclear if the generator is accurately capturing the underlying probability distribution of the real data further investigation could involve using the wasserstein loss as it is a measurement of the distance between two probability distributions 	however it is still unclear if the generator is accurately capturing the underlying probability distribution of the real data 
0	122	8212	we were able to successfully implement and train a 2d dcgan model to generate reasonable images with similar morphological properties to the original rock sample however it is still unclear if the generator is accurately capturing the underlying probability distribution of the real data further investigation could involve using the wasserstein loss as it is a measurement of the distance between two probability distributions 	further investigation could involve using the wasserstein loss as it is a measurement of the distance between two probability distributions 
0	122	8213	we were able to successfully implement and train a 2d dcgan model to generate reasonable images with similar morphological properties to the original rock sample however it is still unclear if the generator is accurately capturing the underlying probability distribution of the real data further investigation could involve using the wasserstein loss as it is a measurement of the distance between two probability distributions 	while our model using the wasserstein loss did not perform as well there have been extensive studies on ways to improve gans and dcgans and only some of the suggestions have been implemented here the ultimate goal of this work is to create a 3d gan to create 3d pore networks for use in digital rock physics 
0	122	8214	we were able to successfully implement and train a 2d dcgan model to generate reasonable images with similar morphological properties to the original rock sample however it is still unclear if the generator is accurately capturing the underlying probability distribution of the real data further investigation could involve using the wasserstein loss as it is a measurement of the distance between two probability distributions 	the major challenge when scaling from 2d to 3d is expected to be in the computational train required to train the 3d network 
0	122	8215	we were able to successfully implement and train a 2d dcgan model to generate reasonable images with similar morphological properties to the original rock sample however it is still unclear if the generator is accurately capturing the underlying probability distribution of the real data further investigation could involve using the wasserstein loss as it is a measurement of the distance between two probability distributions 	therefore it is still important to understand the underlying architecture in 2d and knowledge gained from this project will be invaluable when constructing the 3d model 
0	122	8216	thanks to tim anderson and prof anthony kovscek for their guidance on this project part of this work was performed at the stanford nano shared facilities snsf supported by the national science foundation under award eccs 1542152 	thanks to tim anderson and prof anthony kovscek for their guidance on this project 
0	122	8217	thanks to tim anderson and prof anthony kovscek for their guidance on this project part of this work was performed at the stanford nano shared facilities snsf supported by the national science foundation under award eccs 1542152 	part of this work was performed at the stanford nano shared facilities snsf supported by the national science foundation under award eccs 1542152 
0	122	8218	project code can be downloaded by clicking here stanford google drive 	project code can be downloaded by clicking here stanford google drive 
1	123	8219	to provide a spacecraft with such pareto optimal autonomous planning capabilities is a huge challenge in this project the solution approach to the problem combines machine learning both reinforcement and supervised and numerical multi objective optimization in particular assuming a value for the nea dynamics parameters an heuristic multi objective optimization algorithm is used to generate a pareto front describing the trade off offered by various motion plans according to two conflicting cost functions 	to provide a spacecraft with such pareto optimal autonomous planning capabilities is a huge challenge 
1	123	8220	to provide a spacecraft with such pareto optimal autonomous planning capabilities is a huge challenge in this project the solution approach to the problem combines machine learning both reinforcement and supervised and numerical multi objective optimization in particular assuming a value for the nea dynamics parameters an heuristic multi objective optimization algorithm is used to generate a pareto front describing the trade off offered by various motion plans according to two conflicting cost functions 	in this project the solution approach to the problem combines machine learning both reinforcement and supervised and numerical multi objective optimization 
1	123	8221	to provide a spacecraft with such pareto optimal autonomous planning capabilities is a huge challenge in this project the solution approach to the problem combines machine learning both reinforcement and supervised and numerical multi objective optimization in particular assuming a value for the nea dynamics parameters an heuristic multi objective optimization algorithm is used to generate a pareto front describing the trade off offered by various motion plans according to two conflicting cost functions 	in particular assuming a value for the nea dynamics parameters an heuristic multi objective optimization algorithm is used to generate a pareto front describing the trade off offered by various motion plans according to two conflicting cost functions 
1	123	8222	to provide a spacecraft with such pareto optimal autonomous planning capabilities is a huge challenge in this project the solution approach to the problem combines machine learning both reinforcement and supervised and numerical multi objective optimization in particular assuming a value for the nea dynamics parameters an heuristic multi objective optimization algorithm is used to generate a pareto front describing the trade off offered by various motion plans according to two conflicting cost functions 	the two cost functions to be minimized provide metric of 1 the control effort required to realize the motion plan j v 2 the inverse of the quality quantity of scientific output perceivable through realization of the motion plan j science 
1	123	8223	to provide a spacecraft with such pareto optimal autonomous planning capabilities is a huge challenge in this project the solution approach to the problem combines machine learning both reinforcement and supervised and numerical multi objective optimization in particular assuming a value for the nea dynamics parameters an heuristic multi objective optimization algorithm is used to generate a pareto front describing the trade off offered by various motion plans according to two conflicting cost functions 	the pareto front obtained relies on the assumption of the nea dynamics parameters when these parameters are changed different pareto fronts are obtained 
1	123	8224	to provide a spacecraft with such pareto optimal autonomous planning capabilities is a huge challenge in this project the solution approach to the problem combines machine learning both reinforcement and supervised and numerical multi objective optimization in particular assuming a value for the nea dynamics parameters an heuristic multi objective optimization algorithm is used to generate a pareto front describing the trade off offered by various motion plans according to two conflicting cost functions 	to identify a specific point on a pareto front which corresponds to a specific trade off between the two conflicting costs the multi objective problem can be scalarized
1	123	8225	a spacecraft sc capable to autonomously plan its motion while accounting for conflicting mission objectives in a pareto optimal way would permit to accomplish complex mission tasks around highly uncharacterized celestial bodies such as near earth asteroids neas the two most common conflicting objectives of a space exploration mission are the maximization of the scientific output and the minimization of the control effort i e the propellant required on board 	a spacecraft sc capable to autonomously plan its motion while accounting for conflicting mission objectives in a pareto optimal way would permit to accomplish complex mission tasks around highly uncharacterized celestial bodies such as near earth asteroids neas 
1	123	8226	a spacecraft sc capable to autonomously plan its motion while accounting for conflicting mission objectives in a pareto optimal way would permit to accomplish complex mission tasks around highly uncharacterized celestial bodies such as near earth asteroids neas the two most common conflicting objectives of a space exploration mission are the maximization of the scientific output and the minimization of the control effort i e the propellant required on board 	the two most common conflicting objectives of a space exploration mission are the maximization of the scientific output and the minimization of the control effort i e 
1	123	8227	a spacecraft sc capable to autonomously plan its motion while accounting for conflicting mission objectives in a pareto optimal way would permit to accomplish complex mission tasks around highly uncharacterized celestial bodies such as near earth asteroids neas the two most common conflicting objectives of a space exploration mission are the maximization of the scientific output and the minimization of the control effort i e the propellant required on board 	the propellant required on board 
1	123	8228	a spacecraft sc capable to autonomously plan its motion while accounting for conflicting mission objectives in a pareto optimal way would permit to accomplish complex mission tasks around highly uncharacterized celestial bodies such as near earth asteroids neas the two most common conflicting objectives of a space exploration mission are the maximization of the scientific output and the minimization of the control effort i e the propellant required on board 	if the targeted celestial body is sufficiently known and has a predictable orbital environment both numerical and analytical tools can be leveraged on ground to design spacecraft motion plans that account for the trade off 
1	123	8229	a spacecraft sc capable to autonomously plan its motion while accounting for conflicting mission objectives in a pareto optimal way would permit to accomplish complex mission tasks around highly uncharacterized celestial bodies such as near earth asteroids neas the two most common conflicting objectives of a space exploration mission are the maximization of the scientific output and the minimization of the control effort i e the propellant required on board 	on the contrary if the celestial body and its orbit environment are largely uncertain all plans elaborated on ground may fail dramatically when implemented in space 
1	123	8230	a spacecraft sc capable to autonomously plan its motion while accounting for conflicting mission objectives in a pareto optimal way would permit to accomplish complex mission tasks around highly uncharacterized celestial bodies such as near earth asteroids neas the two most common conflicting objectives of a space exploration mission are the maximization of the scientific output and the minimization of the control effort i e the propellant required on board 	a clear example are missions around neas having relevant dynamics parameters i e 
1	123	8231	a spacecraft sc capable to autonomously plan its motion while accounting for conflicting mission objectives in a pareto optimal way would permit to accomplish complex mission tasks around highly uncharacterized celestial bodies such as near earth asteroids neas the two most common conflicting objectives of a space exploration mission are the maximization of the scientific output and the minimization of the control effort i e the propellant required on board 	mass shape rotation axis orientation and gravity coefficients largely uncertain 
1	123	8232	a spacecraft sc capable to autonomously plan its motion while accounting for conflicting mission objectives in a pareto optimal way would permit to accomplish complex mission tasks around highly uncharacterized celestial bodies such as near earth asteroids neas the two most common conflicting objectives of a space exploration mission are the maximization of the scientific output and the minimization of the control effort i e the propellant required on board 	in these missions a spacecraft should be capable of autonomously plan its motion when an updated knowledge of the nea environment is provided by the sensors and the navigation filter 
1	123	8233	a spacecraft sc capable to autonomously plan its motion while accounting for conflicting mission objectives in a pareto optimal way would permit to accomplish complex mission tasks around highly uncharacterized celestial bodies such as near earth asteroids neas the two most common conflicting objectives of a space exploration mission are the maximization of the scientific output and the minimization of the control effort i e the propellant required on board 	in addition the generated motion plan should account for the trade off science output vs control effort in an optimal sense 
1	123	8234	the multi objective optimizer used is the multi objective particle swarm optimization mopso which is shown in literature to provide high level performances in terms of time of convergence and full reconstruction of the global pareto front	the multi objective optimizer used is the multi objective particle swarm optimization mopso which is shown in literature to provide high level performances in terms of time of convergence and full reconstruction of the global pareto front
1	123	8235	the neural network nn weights have been trained on the training set using a mean square error mse loss function the nn hyperparameters have been tuned and optimized according to the performances provided on the development set	the neural network nn weights have been trained on the training set using a mean square error mse loss function 
1	123	8236	the neural network nn weights have been trained on the training set using a mean square error mse loss function the nn hyperparameters have been tuned and optimized according to the performances provided on the development set	the nn hyperparameters have been tuned and optimized according to the performances provided on the development set
1	123	8237	this project explores the use of neural networks nn to provide a spacecraft with autonomous multiobjective motion planning capabilities around a near earth asteroid nea the trained nn has been shown to provide interesting but still moderate accuracy results to improve the performances the first way to follow is to enlarge the dataset 	this project explores the use of neural networks nn to provide a spacecraft with autonomous multiobjective motion planning capabilities around a near earth asteroid nea 
1	123	8238	this project explores the use of neural networks nn to provide a spacecraft with autonomous multiobjective motion planning capabilities around a near earth asteroid nea the trained nn has been shown to provide interesting but still moderate accuracy results to improve the performances the first way to follow is to enlarge the dataset 	the trained nn has been shown to provide interesting but still moderate accuracy results 
1	123	8239	this project explores the use of neural networks nn to provide a spacecraft with autonomous multiobjective motion planning capabilities around a near earth asteroid nea the trained nn has been shown to provide interesting but still moderate accuracy results to improve the performances the first way to follow is to enlarge the dataset 	to improve the performances the first way to follow is to enlarge the dataset 
1	123	8240	this project explores the use of neural networks nn to provide a spacecraft with autonomous multiobjective motion planning capabilities around a near earth asteroid nea the trained nn has been shown to provide interesting but still moderate accuracy results to improve the performances the first way to follow is to enlarge the dataset 	in addition future work will explore ways to leverage information about the covariance of the estimated state that the navigation filter outputs 
1	123	8241	this project explores the use of neural networks nn to provide a spacecraft with autonomous multiobjective motion planning capabilities around a near earth asteroid nea the trained nn has been shown to provide interesting but still moderate accuracy results to improve the performances the first way to follow is to enlarge the dataset 	in this sense a possible way to go is to reformulate the problem as a stochastic markov decision process 
0	123	8242	i have developed this project advised by prof simone d amico the topic of this study has been motivated by the on going research project autonomous nanosatellite swarming using radio frequency and optical navigation developed at stanford s space rendezvous laboratory in collaboration with nasa ames research center and sponsored by nasa sstp small spacecraft technology program 	i have developed this project advised by prof simone d amico 
1	123	8243	i have developed this project advised by prof simone d amico the topic of this study has been motivated by the on going research project autonomous nanosatellite swarming using radio frequency and optical navigation developed at stanford s space rendezvous laboratory in collaboration with nasa ames research center and sponsored by nasa sstp small spacecraft technology program 	the topic of this study has been motivated by the on going research project autonomous nanosatellite swarming using radio frequency and optical navigation developed at stanford s space rendezvous laboratory in collaboration with nasa ames research center and sponsored by nasa sstp small spacecraft technology program 
1	124	8244	the increasing deployment of distributed energy resources e g solar electric vehicles in power distribution systems will result in greater uncertainty in power demand one method to mitigate this uncertainty and maintain grid reliability is to enable more control of demand side resources through demand response programs 	the increasing deployment of distributed energy resources e g 
0	124	8245	the increasing deployment of distributed energy resources e g solar electric vehicles in power distribution systems will result in greater uncertainty in power demand one method to mitigate this uncertainty and maintain grid reliability is to enable more control of demand side resources through demand response programs 	solar electric vehicles in power distribution systems will result in greater uncertainty in power demand 
0	124	8246	the increasing deployment of distributed energy resources e g solar electric vehicles in power distribution systems will result in greater uncertainty in power demand one method to mitigate this uncertainty and maintain grid reliability is to enable more control of demand side resources through demand response programs 	one method to mitigate this uncertainty and maintain grid reliability is to enable more control of demand side resources through demand response programs 
1	124	8247	the increasing deployment of distributed energy resources e g solar electric vehicles in power distribution systems will result in greater uncertainty in power demand one method to mitigate this uncertainty and maintain grid reliability is to enable more control of demand side resources through demand response programs 	demand response dr is a reduction or shift in power consumption relative to baseline behavior during peak loads or high prices 
0	124	8248	the increasing deployment of distributed energy resources e g solar electric vehicles in power distribution systems will result in greater uncertainty in power demand one method to mitigate this uncertainty and maintain grid reliability is to enable more control of demand side resources through demand response programs 	while dr programs have historically focused on the industrial and commercial customers residential dr programs are expanding 
1	124	8249	the increasing deployment of distributed energy resources e g solar electric vehicles in power distribution systems will result in greater uncertainty in power demand one method to mitigate this uncertainty and maintain grid reliability is to enable more control of demand side resources through demand response programs 	these programs are run by utility companies or third party aggregators and generally focus on control of specific types of residential appliances such as air conditioners or pool pumps the effectiveness of a dr program depends on the power consumption patterns of consumers and their responsiveness to prices or incentives 
0	124	8250	the increasing deployment of distributed energy resources e g solar electric vehicles in power distribution systems will result in greater uncertainty in power demand one method to mitigate this uncertainty and maintain grid reliability is to enable more control of demand side resources through demand response programs 	power consumption at the household level is extremely volatile and can vary significantly from one household to another given heterogeneity in consumer behavior and the stochastic nature of exogenous variables e g 
0	124	8251	the increasing deployment of distributed energy resources e g solar electric vehicles in power distribution systems will result in greater uncertainty in power demand one method to mitigate this uncertainty and maintain grid reliability is to enable more control of demand side resources through demand response programs 	direct targeting of consumers with behavior patterns well suited for dr would be highly beneficial and cost effective for utility companies 
0	124	8252	past research has focused on using smart meter data for consumer segmentation to identify households with similar power consumption patterns using unsupervised learning kwac et al utilized a combination of adaptive k means and hierarchical clustering to develop a load profile dictionary from a dataset of 220 000 consumers in ca	past research has focused on using smart meter data for consumer segmentation to identify households with similar power consumption patterns using unsupervised learning kwac et al 
0	124	8253	past research has focused on using smart meter data for consumer segmentation to identify households with similar power consumption patterns using unsupervised learning kwac et al utilized a combination of adaptive k means and hierarchical clustering to develop a load profile dictionary from a dataset of 220 000 consumers in ca	utilized a combination of adaptive k means and hierarchical clustering to develop a load profile dictionary from a dataset of 220 000 consumers in ca
1	124	8254	the objective of this project is to segment consumers based on their appliance level power consumption with respect to three factors consumer 1 availability 2 variability and 3 flexibility in contrast with previous studies we focus on appliance level power consumption availability refers to the tendency of a consumer group to consume power during periods of peak system demand when a utility would be most interested in curtailing power consumption 	the objective of this project is to segment consumers based on their appliance level power consumption with respect to three factors consumer 1 availability 2 variability and 3 flexibility 
0	124	8255	the objective of this project is to segment consumers based on their appliance level power consumption with respect to three factors consumer 1 availability 2 variability and 3 flexibility in contrast with previous studies we focus on appliance level power consumption availability refers to the tendency of a consumer group to consume power during periods of peak system demand when a utility would be most interested in curtailing power consumption 	in contrast with previous studies we focus on appliance level power consumption 
0	124	8256	the objective of this project is to segment consumers based on their appliance level power consumption with respect to three factors consumer 1 availability 2 variability and 3 flexibility in contrast with previous studies we focus on appliance level power consumption availability refers to the tendency of a consumer group to consume power during periods of peak system demand when a utility would be most interested in curtailing power consumption 	availability refers to the tendency of a consumer group to consume power during periods of peak system demand when a utility would be most interested in curtailing power consumption 
0	124	8257	the objective of this project is to segment consumers based on their appliance level power consumption with respect to three factors consumer 1 availability 2 variability and 3 flexibility in contrast with previous studies we focus on appliance level power consumption availability refers to the tendency of a consumer group to consume power during periods of peak system demand when a utility would be most interested in curtailing power consumption 	we evaluate consumer availability using unsupervised learning to cluster consumers into groups with similar temporal use patterns for each appliance 
0	124	8258	the objective of this project is to segment consumers based on their appliance level power consumption with respect to three factors consumer 1 availability 2 variability and 3 flexibility in contrast with previous studies we focus on appliance level power consumption availability refers to the tendency of a consumer group to consume power during periods of peak system demand when a utility would be most interested in curtailing power consumption 	variability is associated with the consistency of power consumption patterns 
0	124	8259	the objective of this project is to segment consumers based on their appliance level power consumption with respect to three factors consumer 1 availability 2 variability and 3 flexibility in contrast with previous studies we focus on appliance level power consumption availability refers to the tendency of a consumer group to consume power during periods of peak system demand when a utility would be most interested in curtailing power consumption 	consistent use patterns typically result in more accurate power demand forecasts which improve the effectiveness of a dr program 
0	124	8260	the objective of this project is to segment consumers based on their appliance level power consumption with respect to three factors consumer 1 availability 2 variability and 3 flexibility in contrast with previous studies we focus on appliance level power consumption availability refers to the tendency of a consumer group to consume power during periods of peak system demand when a utility would be most interested in curtailing power consumption 	we evaluate variability by using unsupervised learning to cluster load profiles into discrete groups and analyze the entropy of the distribution of the load profile assignments for each consumer 
0	124	8261	the objective of this project is to segment consumers based on their appliance level power consumption with respect to three factors consumer 1 availability 2 variability and 3 flexibility in contrast with previous studies we focus on appliance level power consumption availability refers to the tendency of a consumer group to consume power during periods of peak system demand when a utility would be most interested in curtailing power consumption 	flexibility refers to the willingness of a consumer to shift power consumption from a peak load period to an off peak period 
0	124	8262	the objective of this project is to segment consumers based on their appliance level power consumption with respect to three factors consumer 1 availability 2 variability and 3 flexibility in contrast with previous studies we focus on appliance level power consumption availability refers to the tendency of a consumer group to consume power during periods of peak system demand when a utility would be most interested in curtailing power consumption 	we use supervised learning to predict the responsiveness of consumers to changes in price based on household characteristics and features extracted from power consumption profiles 
1	124	8263	appliance and meter level real power consumption data was obtained from the open source pecan street database for the availability and variability analyses twelve months of minute level data from 2014 2015 and 2016 were used for the training validation and tests sets respectively availability analysis was applied only to deferrable loads which are appliances that are user initiated the power consumption of these appliances is primarily dependent on consumer use patterns 	appliance and meter level real power consumption data was obtained from the open source pecan street database for the availability and variability analyses twelve months of minute level data from 2014 2015 and 2016 were used for the training validation and tests sets respectively 
0	124	8264	appliance and meter level real power consumption data was obtained from the open source pecan street database for the availability and variability analyses twelve months of minute level data from 2014 2015 and 2016 were used for the training validation and tests sets respectively availability analysis was applied only to deferrable loads which are appliances that are user initiated the power consumption of these appliances is primarily dependent on consumer use patterns 	availability analysis was applied only to deferrable loads which are appliances that are user initiated 
0	124	8265	appliance and meter level real power consumption data was obtained from the open source pecan street database for the availability and variability analyses twelve months of minute level data from 2014 2015 and 2016 were used for the training validation and tests sets respectively availability analysis was applied only to deferrable loads which are appliances that are user initiated the power consumption of these appliances is primarily dependent on consumer use patterns 	the power consumption of these appliances is primarily dependent on consumer use patterns 
1	124	8266	appliance and meter level real power consumption data was obtained from the open source pecan street database for the availability and variability analyses twelve months of minute level data from 2014 2015 and 2016 were used for the training validation and tests sets respectively availability analysis was applied only to deferrable loads which are appliances that are user initiated the power consumption of these appliances is primarily dependent on consumer use patterns 	we extracted the start times of each appliance use event from the raw data using thresholding heuristics based on changes in the moving average of power consumption 
0	124	8267	appliance and meter level real power consumption data was obtained from the open source pecan street database for the availability and variability analyses twelve months of minute level data from 2014 2015 and 2016 were used for the training validation and tests sets respectively availability analysis was applied only to deferrable loads which are appliances that are user initiated the power consumption of these appliances is primarily dependent on consumer use patterns 	a multinomial distribution of the start time over the hours of the day was fit for each home and appliance using maximum likelihood estimation with laplace smoothing 
0	124	8268	appliance and meter level real power consumption data was obtained from the open source pecan street database for the availability and variability analyses twelve months of minute level data from 2014 2015 and 2016 were used for the training validation and tests sets respectively availability analysis was applied only to deferrable loads which are appliances that are user initiated the power consumption of these appliances is primarily dependent on consumer use patterns 	given an extracted set of appliance start times s s 1 
0	124	8269	appliance and meter level real power consumption data was obtained from the open source pecan street database for the availability and variability analyses twelve months of minute level data from 2014 2015 and 2016 were used for the training validation and tests sets respectively availability analysis was applied only to deferrable loads which are appliances that are user initiated the power consumption of these appliances is primarily dependent on consumer use patterns 	 s n the probability that an appliance use event for house j occurs during hour h is given bywherewe analyzed the variability of all nine appliance types listed in for the flexibility analysis data was obtained from a critical peak pricing study conducted in 2013 on participants in the pecan street project
0	124	8270	four different unsupervised learning methods were utilized for the availability analysis k means clustering hierarchical clustering latent dirichlet allocation lda and gaussian mixture models gmm for k means clustering we ran the algorithm 10 times each time re initializing the cluster centers and chose the cluster assignments with the lowest intra cluster variation for hierarchical clustering we used a symmetrized version of the kl divergence which is equal to twice the jensenshannon divergence as a similarity measure between the starttime probability distributions of consumer i and consumer j for each appliance using laplace smoothing in the parameter estimation of the probability distributions ensured that p j s 0 s 0 	four different unsupervised learning methods were utilized for the availability analysis k means clustering hierarchical clustering latent dirichlet allocation lda and gaussian mixture models gmm for k means clustering we ran the algorithm 10 times each time re initializing the cluster centers and chose the cluster assignments with the lowest intra cluster variation for hierarchical clustering we used a symmetrized version of the kl divergence which is equal to twice the jensenshannon divergence as a similarity measure between the starttime probability distributions of consumer i and consumer j for each appliance using laplace smoothing in the parameter estimation of the probability distributions ensured that p j s 0 s 0 
0	124	8271	four different unsupervised learning methods were utilized for the availability analysis k means clustering hierarchical clustering latent dirichlet allocation lda and gaussian mixture models gmm for k means clustering we ran the algorithm 10 times each time re initializing the cluster centers and chose the cluster assignments with the lowest intra cluster variation for hierarchical clustering we used a symmetrized version of the kl divergence which is equal to twice the jensenshannon divergence as a similarity measure between the starttime probability distributions of consumer i and consumer j for each appliance using laplace smoothing in the parameter estimation of the probability distributions ensured that p j s 0 s 0 	 23 such that equation 1 is always defined 
0	124	8272	four different unsupervised learning methods were utilized for the availability analysis k means clustering hierarchical clustering latent dirichlet allocation lda and gaussian mixture models gmm for k means clustering we ran the algorithm 10 times each time re initializing the cluster centers and chose the cluster assignments with the lowest intra cluster variation for hierarchical clustering we used a symmetrized version of the kl divergence which is equal to twice the jensenshannon divergence as a similarity measure between the starttime probability distributions of consumer i and consumer j for each appliance using laplace smoothing in the parameter estimation of the probability distributions ensured that p j s 0 s 0 	for hierarchical clustering we used agglomerative methods with the ward variance minimization algorithm the gmm was modeled as a mixture of 24 dimensional multivariate gaussians each with tied covariance matrices 
0	124	8273	four different unsupervised learning methods were utilized for the availability analysis k means clustering hierarchical clustering latent dirichlet allocation lda and gaussian mixture models gmm for k means clustering we ran the algorithm 10 times each time re initializing the cluster centers and chose the cluster assignments with the lowest intra cluster variation for hierarchical clustering we used a symmetrized version of the kl divergence which is equal to twice the jensenshannon divergence as a similarity measure between the starttime probability distributions of consumer i and consumer j for each appliance using laplace smoothing in the parameter estimation of the probability distributions ensured that p j s 0 s 0 	this was required since the 24 features are elements of a discrete probability distribution and are thus not independent of each other 
1	124	8274	four different unsupervised learning methods were utilized for the availability analysis k means clustering hierarchical clustering latent dirichlet allocation lda and gaussian mixture models gmm for k means clustering we ran the algorithm 10 times each time re initializing the cluster centers and chose the cluster assignments with the lowest intra cluster variation for hierarchical clustering we used a symmetrized version of the kl divergence which is equal to twice the jensenshannon divergence as a similarity measure between the starttime probability distributions of consumer i and consumer j for each appliance using laplace smoothing in the parameter estimation of the probability distributions ensured that p j s 0 s 0 	the model was trained using expectation maximization both probabilistic models were each run 10 times for 2 14 clusters and the assignments with the lowest intra cluster variation as measured by symmetrized kl divergence were selected for each of the analyzed cluster sizes all four unsupervised learning methods were implemented using the sci kit learn three different metrics were used to evaluate the performance of each method 
0	124	8275	four different unsupervised learning methods were utilized for the availability analysis k means clustering hierarchical clustering latent dirichlet allocation lda and gaussian mixture models gmm for k means clustering we ran the algorithm 10 times each time re initializing the cluster centers and chose the cluster assignments with the lowest intra cluster variation for hierarchical clustering we used a symmetrized version of the kl divergence which is equal to twice the jensenshannon divergence as a similarity measure between the starttime probability distributions of consumer i and consumer j for each appliance using laplace smoothing in the parameter estimation of the probability distributions ensured that p j s 0 s 0 	suppose the training and validation set both contain n consumers and k clusters are obtained from both sets such that cluster c t 0 
0	124	8276	four different unsupervised learning methods were utilized for the availability analysis k means clustering hierarchical clustering latent dirichlet allocation lda and gaussian mixture models gmm for k means clustering we ran the algorithm 10 times each time re initializing the cluster centers and chose the cluster assignments with the lowest intra cluster variation for hierarchical clustering we used a symmetrized version of the kl divergence which is equal to twice the jensenshannon divergence as a similarity measure between the starttime probability distributions of consumer i and consumer j for each appliance using laplace smoothing in the parameter estimation of the probability distributions ensured that p j s 0 s 0 	 k from the training set contains n c t consumers and cluster c d 0 
0	124	8277	four different unsupervised learning methods were utilized for the availability analysis k means clustering hierarchical clustering latent dirichlet allocation lda and gaussian mixture models gmm for k means clustering we ran the algorithm 10 times each time re initializing the cluster centers and chose the cluster assignments with the lowest intra cluster variation for hierarchical clustering we used a symmetrized version of the kl divergence which is equal to twice the jensenshannon divergence as a similarity measure between the starttime probability distributions of consumer i and consumer j for each appliance using laplace smoothing in the parameter estimation of the probability distributions ensured that p j s 0 s 0 	 k from the validation set contains n c d consumers 
0	124	8278	four different unsupervised learning methods were utilized for the availability analysis k means clustering hierarchical clustering latent dirichlet allocation lda and gaussian mixture models gmm for k means clustering we ran the algorithm 10 times each time re initializing the cluster centers and chose the cluster assignments with the lowest intra cluster variation for hierarchical clustering we used a symmetrized version of the kl divergence which is equal to twice the jensenshannon divergence as a similarity measure between the starttime probability distributions of consumer i and consumer j for each appliance using laplace smoothing in the parameter estimation of the probability distributions ensured that p j s 0 s 0 	supposec j is the cluster assignment associated with consumer j 
0	124	8279	four different unsupervised learning methods were utilized for the availability analysis k means clustering hierarchical clustering latent dirichlet allocation lda and gaussian mixture models gmm for k means clustering we ran the algorithm 10 times each time re initializing the cluster centers and chose the cluster assignments with the lowest intra cluster variation for hierarchical clustering we used a symmetrized version of the kl divergence which is equal to twice the jensenshannon divergence as a similarity measure between the starttime probability distributions of consumer i and consumer j for each appliance using laplace smoothing in the parameter estimation of the probability distributions ensured that p j s 0 s 0 	we define the availability of a cluster of appliances during hour h as the mean power consumption during hour h for the entire cluster 
0	124	8280	four different unsupervised learning methods were utilized for the availability analysis k means clustering hierarchical clustering latent dirichlet allocation lda and gaussian mixture models gmm for k means clustering we ran the algorithm 10 times each time re initializing the cluster centers and chose the cluster assignments with the lowest intra cluster variation for hierarchical clustering we used a symmetrized version of the kl divergence which is equal to twice the jensenshannon divergence as a similarity measure between the starttime probability distributions of consumer i and consumer j for each appliance using laplace smoothing in the parameter estimation of the probability distributions ensured that p j s 0 s 0 	the increase in availability from consumer segmentation is the maximum availability over all clusters divided by the availability of all of the appliances in the entire datasetwhere is the set of all time indices associated with hour h and p j t is the power consumption of consumer j at time t in the test set 
1	124	8281	four different unsupervised learning methods were utilized for the availability analysis k means clustering hierarchical clustering latent dirichlet allocation lda and gaussian mixture models gmm for k means clustering we ran the algorithm 10 times each time re initializing the cluster centers and chose the cluster assignments with the lowest intra cluster variation for hierarchical clustering we used a symmetrized version of the kl divergence which is equal to twice the jensenshannon divergence as a similarity measure between the starttime probability distributions of consumer i and consumer j for each appliance using laplace smoothing in the parameter estimation of the probability distributions ensured that p j s 0 s 0 	an effective consumer segmentation algorithm should yield a large increase in availability the completeness score where n c t c d is the number of consumers assigned to cluster c t in the training set and cluster c d in the validation set 
1	124	8282	four different unsupervised learning methods were utilized for the availability analysis k means clustering hierarchical clustering latent dirichlet allocation lda and gaussian mixture models gmm for k means clustering we ran the algorithm 10 times each time re initializing the cluster centers and chose the cluster assignments with the lowest intra cluster variation for hierarchical clustering we used a symmetrized version of the kl divergence which is equal to twice the jensenshannon divergence as a similarity measure between the starttime probability distributions of consumer i and consumer j for each appliance using laplace smoothing in the parameter estimation of the probability distributions ensured that p j s 0 s 0 	this analysis assumes that consumer power consumption patterns remain similar between the training validation and test sets such that a perfect clustering algorithm would recover identical clusters finally we used the intra cluster variation of the samples as a measure of cluster quality 
0	124	8283	four different unsupervised learning methods were utilized for the availability analysis k means clustering hierarchical clustering latent dirichlet allocation lda and gaussian mixture models gmm for k means clustering we ran the algorithm 10 times each time re initializing the cluster centers and chose the cluster assignments with the lowest intra cluster variation for hierarchical clustering we used a symmetrized version of the kl divergence which is equal to twice the jensenshannon divergence as a similarity measure between the starttime probability distributions of consumer i and consumer j for each appliance using laplace smoothing in the parameter estimation of the probability distributions ensured that p j s 0 s 0 	the kl divergence as defined in equation 1 was used a distance measure between samples 
0	124	8284	four different unsupervised learning methods were utilized for the availability analysis k means clustering hierarchical clustering latent dirichlet allocation lda and gaussian mixture models gmm for k means clustering we ran the algorithm 10 times each time re initializing the cluster centers and chose the cluster assignments with the lowest intra cluster variation for hierarchical clustering we used a symmetrized version of the kl divergence which is equal to twice the jensenshannon divergence as a similarity measure between the starttime probability distributions of consumer i and consumer j for each appliance using laplace smoothing in the parameter estimation of the probability distributions ensured that p j s 0 s 0 	the elbow method
0	124	8285	the variability of consumer power consumption patterns was also analyzed using unsupervised learning first k means clustering was used to cluster the 24 hour power consumption profiles of all homes for each specific appliance into k load shape types this resulted in 365 load shape cluster assignments for each home and appliance 	the variability of consumer power consumption patterns was also analyzed using unsupervised learning 
1	124	8286	the variability of consumer power consumption patterns was also analyzed using unsupervised learning first k means clustering was used to cluster the 24 hour power consumption profiles of all homes for each specific appliance into k load shape types this resulted in 365 load shape cluster assignments for each home and appliance 	first k means clustering was used to cluster the 24 hour power consumption profiles of all homes for each specific appliance into k load shape types 
0	124	8287	the variability of consumer power consumption patterns was also analyzed using unsupervised learning first k means clustering was used to cluster the 24 hour power consumption profiles of all homes for each specific appliance into k load shape types this resulted in 365 load shape cluster assignments for each home and appliance 	this resulted in 365 load shape cluster assignments for each home and appliance 
0	124	8288	the variability of consumer power consumption patterns was also analyzed using unsupervised learning first k means clustering was used to cluster the 24 hour power consumption profiles of all homes for each specific appliance into k load shape types this resulted in 365 load shape cluster assignments for each home and appliance 	the distribution over these load shape types q j r k for each home and appliance over the entire training set was calculated using laplace smoothing 
0	124	8289	the variability of consumer power consumption patterns was also analyzed using unsupervised learning first k means clustering was used to cluster the 24 hour power consumption profiles of all homes for each specific appliance into k load shape types this resulted in 365 load shape cluster assignments for each home and appliance 	the entropy of q j gives a measure of the variability of consumer use patterns 
1	124	8290	three methods were compared for predicting the responsiveness of the power consumption of each consumer to changes in electricity price linear regression with recursive feature selection k nearest neighbors knn regression and random forests with recursive feature selection the tuning parameters for each algorithm were selected to minimize the mean squared error mse in the validation set tuning parameters included the number of features for linear regression and random forests the number of neighbors for knn and the number of estimators and the maximum tree depth for random forests 	three methods were compared for predicting the responsiveness of the power consumption of each consumer to changes in electricity price linear regression with recursive feature selection k nearest neighbors knn regression and random forests with recursive feature selection 
0	124	8291	three methods were compared for predicting the responsiveness of the power consumption of each consumer to changes in electricity price linear regression with recursive feature selection k nearest neighbors knn regression and random forests with recursive feature selection the tuning parameters for each algorithm were selected to minimize the mean squared error mse in the validation set tuning parameters included the number of features for linear regression and random forests the number of neighbors for knn and the number of estimators and the maximum tree depth for random forests 	the tuning parameters for each algorithm were selected to minimize the mean squared error mse in the validation set 
0	124	8292	three methods were compared for predicting the responsiveness of the power consumption of each consumer to changes in electricity price linear regression with recursive feature selection k nearest neighbors knn regression and random forests with recursive feature selection the tuning parameters for each algorithm were selected to minimize the mean squared error mse in the validation set tuning parameters included the number of features for linear regression and random forests the number of neighbors for knn and the number of estimators and the maximum tree depth for random forests 	tuning parameters included the number of features for linear regression and random forests the number of neighbors for knn and the number of estimators and the maximum tree depth for random forests 
1	124	8293	in this project we developed methods for segmenting residential consumers based on their appliance level power consumption with respect to their availability variability and flexibility results indicated that segmentation using unsupervised learning methods can increase load availability for dr programs by up to a factor of two hierarchical clustering applied to the start time probability distributions of deferrable appliances produced the most consistent performance 	in this project we developed methods for segmenting residential consumers based on their appliance level power consumption with respect to their availability variability and flexibility 
0	124	8294	in this project we developed methods for segmenting residential consumers based on their appliance level power consumption with respect to their availability variability and flexibility results indicated that segmentation using unsupervised learning methods can increase load availability for dr programs by up to a factor of two hierarchical clustering applied to the start time probability distributions of deferrable appliances produced the most consistent performance 	results indicated that segmentation using unsupervised learning methods can increase load availability for dr programs by up to a factor of two 
0	124	8295	in this project we developed methods for segmenting residential consumers based on their appliance level power consumption with respect to their availability variability and flexibility results indicated that segmentation using unsupervised learning methods can increase load availability for dr programs by up to a factor of two hierarchical clustering applied to the start time probability distributions of deferrable appliances produced the most consistent performance 	hierarchical clustering applied to the start time probability distributions of deferrable appliances produced the most consistent performance 
1	124	8296	in this project we developed methods for segmenting residential consumers based on their appliance level power consumption with respect to their availability variability and flexibility results indicated that segmentation using unsupervised learning methods can increase load availability for dr programs by up to a factor of two hierarchical clustering applied to the start time probability distributions of deferrable appliances produced the most consistent performance 	results indicated that cluster assignments can vary significantly from one appliance to another and can differ from the cluster assignments obtained by only analyzing the total power consumption of each home 
0	124	8297	in this project we developed methods for segmenting residential consumers based on their appliance level power consumption with respect to their availability variability and flexibility results indicated that segmentation using unsupervised learning methods can increase load availability for dr programs by up to a factor of two hierarchical clustering applied to the start time probability distributions of deferrable appliances produced the most consistent performance 	this highlights the importance of performing consumer segmentation based on appliance level power consumption data 
0	124	8298	in this project we developed methods for segmenting residential consumers based on their appliance level power consumption with respect to their availability variability and flexibility results indicated that segmentation using unsupervised learning methods can increase load availability for dr programs by up to a factor of two hierarchical clustering applied to the start time probability distributions of deferrable appliances produced the most consistent performance 	power consumption variability was assessed by calculating the entropy of the distribution of load profile types for individual consumers identified using kmeans clustering 
0	124	8299	in this project we developed methods for segmenting residential consumers based on their appliance level power consumption with respect to their availability variability and flexibility results indicated that segmentation using unsupervised learning methods can increase load availability for dr programs by up to a factor of two hierarchical clustering applied to the start time probability distributions of deferrable appliances produced the most consistent performance 	results indicated notable differences in the variability of power consumption of different appliance types and segments of the population which could be exploited by a dr program provider 
1	124	8300	in this project we developed methods for segmenting residential consumers based on their appliance level power consumption with respect to their availability variability and flexibility results indicated that segmentation using unsupervised learning methods can increase load availability for dr programs by up to a factor of two hierarchical clustering applied to the start time probability distributions of deferrable appliances produced the most consistent performance 	we tested three different supervised learning approaches for predicting consumer responsiveness to electricity prices and found that low variance models such as linear regression paired with recursive feature selection resulted in the lowest test error future work may investigate incorporating additional variables such as day of the week and season into the availability analysis 
0	124	8301	in this project we developed methods for segmenting residential consumers based on their appliance level power consumption with respect to their availability variability and flexibility results indicated that segmentation using unsupervised learning methods can increase load availability for dr programs by up to a factor of two hierarchical clustering applied to the start time probability distributions of deferrable appliances produced the most consistent performance 	expanding the analysis to a larger dataset may provide more insight into the generalizability of the results code for this project can be found at https github com ebuech cs229 
1	124	8302	lily buechler performed feature extraction on the raw data implemented k means clustering and hierarchical clustering for the availability and variability analysis implemented the three supervised learning methods for the flexibility analysis and contributed to the poster and report zonghe chua implemented the gmm and lda models and performed the cluster number selection analysis using the elbow method and silhouette scores on all the unsupervised algorithms for the availability segmentation he also contributed to the poster and report 	lily buechler performed feature extraction on the raw data implemented k means clustering and hierarchical clustering for the availability and variability analysis implemented the three supervised learning methods for the flexibility analysis and contributed to the poster and report zonghe chua implemented the gmm and lda models and performed the cluster number selection analysis using the elbow method and silhouette scores on all the unsupervised algorithms for the availability segmentation 
0	124	8303	lily buechler performed feature extraction on the raw data implemented k means clustering and hierarchical clustering for the availability and variability analysis implemented the three supervised learning methods for the flexibility analysis and contributed to the poster and report zonghe chua implemented the gmm and lda models and performed the cluster number selection analysis using the elbow method and silhouette scores on all the unsupervised algorithms for the availability segmentation he also contributed to the poster and report 	he also contributed to the poster and report 
1	125	8304	in this project we seek to take a time series of current amplitudes collected by by phononsensitive detectors called qets quasiparticle trapping assisted electrothermal feedback transitionedge sensors and identify the start time of the pulse currently our techniques for registering a pulse are not accurate and often classify detector noise as a pulse this issue is significant because the separation of signal and noise is critical to the success of the experiment and is easily generalized to any other detector of this type 	in this project we seek to take a time series of current amplitudes collected by by phononsensitive detectors called qets quasiparticle trapping assisted electrothermal feedback transitionedge sensors and identify the start time of the pulse 
0	125	8305	in this project we seek to take a time series of current amplitudes collected by by phononsensitive detectors called qets quasiparticle trapping assisted electrothermal feedback transitionedge sensors and identify the start time of the pulse currently our techniques for registering a pulse are not accurate and often classify detector noise as a pulse this issue is significant because the separation of signal and noise is critical to the success of the experiment and is easily generalized to any other detector of this type 	currently our techniques for registering a pulse are not accurate and often classify detector noise as a pulse 
1	125	8306	in this project we seek to take a time series of current amplitudes collected by by phononsensitive detectors called qets quasiparticle trapping assisted electrothermal feedback transitionedge sensors and identify the start time of the pulse currently our techniques for registering a pulse are not accurate and often classify detector noise as a pulse this issue is significant because the separation of signal and noise is critical to the success of the experiment and is easily generalized to any other detector of this type 	this issue is significant because the separation of signal and noise is critical to the success of the experiment and is easily generalized to any other detector of this type 
1	125	8307	in this project we seek to take a time series of current amplitudes collected by by phononsensitive detectors called qets quasiparticle trapping assisted electrothermal feedback transitionedge sensors and identify the start time of the pulse currently our techniques for registering a pulse are not accurate and often classify detector noise as a pulse this issue is significant because the separation of signal and noise is critical to the success of the experiment and is easily generalized to any other detector of this type 	furthermore the problem of processing data to find pulses and characterizing them occur in many other fields were signals need to processed therefore it has the potential for a wider range of uses 
1	125	8308	in this project we seek to take a time series of current amplitudes collected by by phononsensitive detectors called qets quasiparticle trapping assisted electrothermal feedback transitionedge sensors and identify the start time of the pulse currently our techniques for registering a pulse are not accurate and often classify detector noise as a pulse this issue is significant because the separation of signal and noise is critical to the success of the experiment and is easily generalized to any other detector of this type 	we implement various machine learning models and found the most success with pca fcnn 
0	125	8309	the cryogenic dark matter search cdms research group seeks to directly detect the most frequent form of matter in the universe dark matter to do so we study the behaviour inside semiconducting crystals at cryogenic temperatures when a dark matter particle or another form of radiation interacts with the crystal a cloud of electrons and holes is produced at the interaction site 	the cryogenic dark matter search cdms research group seeks to directly detect the most frequent form of matter in the universe dark matter 
0	125	8310	the cryogenic dark matter search cdms research group seeks to directly detect the most frequent form of matter in the universe dark matter to do so we study the behaviour inside semiconducting crystals at cryogenic temperatures when a dark matter particle or another form of radiation interacts with the crystal a cloud of electrons and holes is produced at the interaction site 	to do so we study the behaviour inside semiconducting crystals at cryogenic temperatures 
0	125	8311	the cryogenic dark matter search cdms research group seeks to directly detect the most frequent form of matter in the universe dark matter to do so we study the behaviour inside semiconducting crystals at cryogenic temperatures when a dark matter particle or another form of radiation interacts with the crystal a cloud of electrons and holes is produced at the interaction site 	when a dark matter particle or another form of radiation interacts with the crystal a cloud of electrons and holes is produced at the interaction site 
0	125	8312	the cryogenic dark matter search cdms research group seeks to directly detect the most frequent form of matter in the universe dark matter to do so we study the behaviour inside semiconducting crystals at cryogenic temperatures when a dark matter particle or another form of radiation interacts with the crystal a cloud of electrons and holes is produced at the interaction site 	these charges are then drifted through the crystal by an applied electric field and produce phonons that are collected by phonon sensitive detectors called qets quasiparticle trappingassisted electrothermal feedback transition edge sensors 
1	125	8313	the cryogenic dark matter search cdms research group seeks to directly detect the most frequent form of matter in the universe dark matter to do so we study the behaviour inside semiconducting crystals at cryogenic temperatures when a dark matter particle or another form of radiation interacts with the crystal a cloud of electrons and holes is produced at the interaction site 	once the a signal pulse is received we seek to determine the start time of the interaction from the raw data 
1	125	8314	the cryogenic dark matter search cdms research group seeks to directly detect the most frequent form of matter in the universe dark matter to do so we study the behaviour inside semiconducting crystals at cryogenic temperatures when a dark matter particle or another form of radiation interacts with the crystal a cloud of electrons and holes is produced at the interaction site 	we use logistic regression a shallow fully connected neural networks fcnn linear and kernelized principle component analysis pca with fcnn and convolutional recurrent neural networks cnn rnn 
0	125	8315	dr andrew watson s dissertation	dr andrew watson s dissertation
1	125	8316	while we dont have enough real data 1 to train on we have do have a monte carlo simulation of our experiment built with g4cmp 2 combining the results from simulating with real noise we created our dataset as represented by	while we dont have enough real data 1 to train on we have do have a monte carlo simulation of our experiment built with g4cmp 2 
1	125	8317	while we dont have enough real data 1 to train on we have do have a monte carlo simulation of our experiment built with g4cmp 2 combining the results from simulating with real noise we created our dataset as represented by	combining the results from simulating with real noise we created our dataset as represented by
1	125	8318	in input features were traces with two channels a i r 2048 2 for the logistic regression and fcnn we flattened the trace to a 4096 dimensional vector for the cnn rnn we kept the shape of the trace 	in input features were traces with two channels a i r 2048 2 
1	125	8319	in input features were traces with two channels a i r 2048 2 for the logistic regression and fcnn we flattened the trace to a 4096 dimensional vector for the cnn rnn we kept the shape of the trace 	for the logistic regression and fcnn we flattened the trace to a 4096 dimensional vector 
1	125	8320	in input features were traces with two channels a i r 2048 2 for the logistic regression and fcnn we flattened the trace to a 4096 dimensional vector for the cnn rnn we kept the shape of the trace 	for the cnn rnn we kept the shape of the trace 
1	125	8321	in input features were traces with two channels a i r 2048 2 for the logistic regression and fcnn we flattened the trace to a 4096 dimensional vector for the cnn rnn we kept the shape of the trace 	for pca fcnn model we flattened the traces and used pca to find the first 1024 principle of the components pcs using 20 of the training set which explain 89 83 of the variance 
1	125	8322	in input features were traces with two channels a i r 2048 2 for the logistic regression and fcnn we flattened the trace to a 4096 dimensional vector for the cnn rnn we kept the shape of the trace 	we decided to try pca because we plotted the correlation matrix of the traces 1 by real data we mean data that is produced by the physical detector instead of by the monter carlo simulation 2 https github com kelseymh g4cmp 3 each simulated event has an associated trace for each channel 
0	125	8323	in input features were traces with two channels a i r 2048 2 for the logistic regression and fcnn we flattened the trace to a 4096 dimensional vector for the cnn rnn we kept the shape of the trace 	a trace is a time series defined by an array of 2048 values where each value represents a current measured by the detector 
1	125	8324	in input features were traces with two channels a i r 2048 2 for the logistic regression and fcnn we flattened the trace to a 4096 dimensional vector for the cnn rnn we kept the shape of the trace 	radial basis kernel and 1024 pcs 
1	125	8325	in input features were traces with two channels a i r 2048 2 for the logistic regression and fcnn we flattened the trace to a 4096 dimensional vector for the cnn rnn we kept the shape of the trace 	we tried this too because we thought the relationship between the projected features could be non linear 
1	125	8326	we represent a trance example input as a i r 2048 2 the flattened version as a i flatten a i r 4096 the true value of the time as t i and the prediction ast although for applied purposes we are more interested in reporting the mean absolute error mae 	we represent a trance example input as a i r 2048 2 the flattened version as a i flatten a i r 4096 the true value of the time as t i and the prediction ast although for applied purposes we are more interested in reporting the mean absolute error mae 
1	125	8327	we begin the training modelling phase by fitting a logistic regression as our most basic baseline we use a i s as inputs and t i discretized into one hot encoded vector with 1048 classes as outputs as a secondary baseline and to get a sense of the power of neural networks on this task we make a fcnn with one layer with 512 nodes 	we begin the training modelling phase by fitting a logistic regression as our most basic baseline 
0	125	8328	we begin the training modelling phase by fitting a logistic regression as our most basic baseline we use a i s as inputs and t i discretized into one hot encoded vector with 1048 classes as outputs as a secondary baseline and to get a sense of the power of neural networks on this task we make a fcnn with one layer with 512 nodes 	we use a i s as inputs and t i discretized into one hot encoded vector with 1048 classes as outputs 
1	125	8329	we begin the training modelling phase by fitting a logistic regression as our most basic baseline we use a i s as inputs and t i discretized into one hot encoded vector with 1048 classes as outputs as a secondary baseline and to get a sense of the power of neural networks on this task we make a fcnn with one layer with 512 nodes 	as a secondary baseline and to get a sense of the power of neural networks on this task we make a fcnn with one layer with 512 nodes 
1	125	8330	we perform pca by finding the eigen basis of the correlation matrix c and then finding the projections by where v k t is the k th principle we then feed these projections into a fcnn explained in the next subsection 	we perform pca by finding the eigen basis of the correlation matrix c and then finding the projections by where v k t is the k th principle 
0	125	8331	we perform pca by finding the eigen basis of the correlation matrix c and then finding the projections by where v k t is the k th principle we then feed these projections into a fcnn explained in the next subsection 	we then feed these projections into a fcnn explained in the next subsection 
1	125	8332	we perform the kernel trick on linear pca as follows this was the structure used	we perform the kernel trick on linear pca as follows this was the structure used
1	125	8333	base on the suggestion of ta ashwini ramamoorthy we implemented a long short term memory lstm neural network as this type of nn is especially suited to dealing with time series data we built the model with two layers of max pooling with a stride of 4 followed by two dense hidden layers of 256 and 16 nodes respectively training the lstm proved to be much slower than previous methods 	base on the suggestion of ta ashwini ramamoorthy we implemented a long short term memory lstm neural network as this type of nn is especially suited to dealing with time series data 
1	125	8334	base on the suggestion of ta ashwini ramamoorthy we implemented a long short term memory lstm neural network as this type of nn is especially suited to dealing with time series data we built the model with two layers of max pooling with a stride of 4 followed by two dense hidden layers of 256 and 16 nodes respectively training the lstm proved to be much slower than previous methods 	we built the model with two layers of max pooling with a stride of 4 followed by two dense hidden layers of 256 and 16 nodes respectively 
0	125	8335	base on the suggestion of ta ashwini ramamoorthy we implemented a long short term memory lstm neural network as this type of nn is especially suited to dealing with time series data we built the model with two layers of max pooling with a stride of 4 followed by two dense hidden layers of 256 and 16 nodes respectively training the lstm proved to be much slower than previous methods 	training the lstm proved to be much slower than previous methods 
1	125	8336	base on the suggestion of ta ashwini ramamoorthy we implemented a long short term memory lstm neural network as this type of nn is especially suited to dealing with time series data we built the model with two layers of max pooling with a stride of 4 followed by two dense hidden layers of 256 and 16 nodes respectively training the lstm proved to be much slower than previous methods 	on the other hand the lstm predictions would converge withing a relatively smaller number of epochs less than 20 so little training was required 
1	125	8337	base on the suggestion of ta ashwini ramamoorthy we implemented a long short term memory lstm neural network as this type of nn is especially suited to dealing with time series data we built the model with two layers of max pooling with a stride of 4 followed by two dense hidden layers of 256 and 16 nodes respectively training the lstm proved to be much slower than previous methods 	unfortunately despite trying different structures of the model we were unable to attain an mean average error with this method that came close to that of pca fcnn 
0	125	8338	the results table represents the average mae scaled to 2048 the total number of bins from this project were able to develop a methodology to construct an effective tool that we might use as part of physics experiment to determine the start time of pulses measured by our detector after constructing our dataset from our monte carlo simulations we trained logistic regression fcnn cnn lstm a standard pca fed into fcnn and kpca with a radial basis kernel fed into a fcnn to predict t i and had the best test set mae with the standard pca fcnn method 	the results table represents the average mae scaled to 2048 the total number of bins 
1	125	8339	the results table represents the average mae scaled to 2048 the total number of bins from this project were able to develop a methodology to construct an effective tool that we might use as part of physics experiment to determine the start time of pulses measured by our detector after constructing our dataset from our monte carlo simulations we trained logistic regression fcnn cnn lstm a standard pca fed into fcnn and kpca with a radial basis kernel fed into a fcnn to predict t i and had the best test set mae with the standard pca fcnn method 	from this project were able to develop a methodology to construct an effective tool that we might use as part of physics experiment to determine the start time of pulses measured by our detector 
1	125	8340	the results table represents the average mae scaled to 2048 the total number of bins from this project were able to develop a methodology to construct an effective tool that we might use as part of physics experiment to determine the start time of pulses measured by our detector after constructing our dataset from our monte carlo simulations we trained logistic regression fcnn cnn lstm a standard pca fed into fcnn and kpca with a radial basis kernel fed into a fcnn to predict t i and had the best test set mae with the standard pca fcnn method 	after constructing our dataset from our monte carlo simulations we trained logistic regression fcnn cnn lstm a standard pca fed into fcnn and kpca with a radial basis kernel fed into a fcnn to predict t i and had the best test set mae with the standard pca fcnn method 
1	125	8341	the results table represents the average mae scaled to 2048 the total number of bins from this project were able to develop a methodology to construct an effective tool that we might use as part of physics experiment to determine the start time of pulses measured by our detector after constructing our dataset from our monte carlo simulations we trained logistic regression fcnn cnn lstm a standard pca fed into fcnn and kpca with a radial basis kernel fed into a fcnn to predict t i and had the best test set mae with the standard pca fcnn method 	our goal was to get to a mae around 1 or 2 however the lowest we ever got on training was 4 
1	125	8342	the results table represents the average mae scaled to 2048 the total number of bins from this project were able to develop a methodology to construct an effective tool that we might use as part of physics experiment to determine the start time of pulses measured by our detector after constructing our dataset from our monte carlo simulations we trained logistic regression fcnn cnn lstm a standard pca fed into fcnn and kpca with a radial basis kernel fed into a fcnn to predict t i and had the best test set mae with the standard pca fcnn method 	this is likely due to the fact that the pulses are so noisy which is why we chose this challenging problem in the first place 
1	125	8343	the results table represents the average mae scaled to 2048 the total number of bins from this project were able to develop a methodology to construct an effective tool that we might use as part of physics experiment to determine the start time of pulses measured by our detector after constructing our dataset from our monte carlo simulations we trained logistic regression fcnn cnn lstm a standard pca fed into fcnn and kpca with a radial basis kernel fed into a fcnn to predict t i and had the best test set mae with the standard pca fcnn method 	an important insight from this project was that more complex models dont always produce better results as can be seen comparing the lstm cnn and kpca fcnn with the pca fcnn 
1	125	8344	the results table represents the average mae scaled to 2048 the total number of bins from this project were able to develop a methodology to construct an effective tool that we might use as part of physics experiment to determine the start time of pulses measured by our detector after constructing our dataset from our monte carlo simulations we trained logistic regression fcnn cnn lstm a standard pca fed into fcnn and kpca with a radial basis kernel fed into a fcnn to predict t i and had the best test set mae with the standard pca fcnn method 	another lesson we learned was that producing the dataset and preparing it for training can be the most time intensive step 
1	125	8345	the results table represents the average mae scaled to 2048 the total number of bins from this project were able to develop a methodology to construct an effective tool that we might use as part of physics experiment to determine the start time of pulses measured by our detector after constructing our dataset from our monte carlo simulations we trained logistic regression fcnn cnn lstm a standard pca fed into fcnn and kpca with a radial basis kernel fed into a fcnn to predict t i and had the best test set mae with the standard pca fcnn method 	finally while we didnt accomplish exactly what we set out to do we are content with out results and will continue improving on them 
1	125	8346	the results table represents the average mae scaled to 2048 the total number of bins from this project were able to develop a methodology to construct an effective tool that we might use as part of physics experiment to determine the start time of pulses measured by our detector after constructing our dataset from our monte carlo simulations we trained logistic regression fcnn cnn lstm a standard pca fed into fcnn and kpca with a radial basis kernel fed into a fcnn to predict t i and had the best test set mae with the standard pca fcnn method 	if we had more time we would try other models tune hyper parameters more methodically keeping track of all results and use a larger dataset with more examples per energy and also a wider range of energies 
1	125	8347	we would like to thank to chin yu for suggesting this project and providing key insights about machine learning the northwestern supercdms team for providing the mc simulation and the sample noise traces from which we generated our dataset and finally the cs299 tas and professors for giving us the opportunity to do this project and teaching us all the material 	we would like to thank to chin yu for suggesting this project and providing key insights about machine learning the northwestern supercdms team for providing the mc simulation and the sample noise traces from which we generated our dataset and finally the cs299 tas and professors for giving us the opportunity to do this project and teaching us all the material 
0	126	8348	we present an unsupervised machine learning model for computing approximate electromagnetic fields in a cavity containing an arbitrary spatial dielectric permittivity distribution our model achieves good predictive performance and is over 10 faster than identically sized finite difference frequency domain simulations suggesting possible applications for accelerating optical inverse design algorithms 	we present an unsupervised machine learning model for computing approximate electromagnetic fields in a cavity containing an arbitrary spatial dielectric permittivity distribution 
0	126	8349	we present an unsupervised machine learning model for computing approximate electromagnetic fields in a cavity containing an arbitrary spatial dielectric permittivity distribution our model achieves good predictive performance and is over 10 faster than identically sized finite difference frequency domain simulations suggesting possible applications for accelerating optical inverse design algorithms 	our model achieves good predictive performance and is over 10 faster than identically sized finite difference frequency domain simulations suggesting possible applications for accelerating optical inverse design algorithms 
0	126	8350	 inverse design problems computational design of structures by specifying an objective functionare pervasive throughout physics especially in photonics where inverse design methods have been used to design many highly compact optical components the iterative fdfd simulations although exact can be computationally expensive and scale poorly with the design dimensions for many applications an approximate field solution is sufficient a machine learning model which could quickly compute approximate electromagnetic fields for a dielectric structure could reduce this computational bottleneck allowing for much faster inverse design processes 	 inverse design problems computational design of structures by specifying an objective functionare pervasive throughout physics especially in photonics where inverse design methods have been used to design many highly compact optical components the iterative fdfd simulations although exact can be computationally expensive and scale poorly with the design dimensions 
0	126	8351	 inverse design problems computational design of structures by specifying an objective functionare pervasive throughout physics especially in photonics where inverse design methods have been used to design many highly compact optical components the iterative fdfd simulations although exact can be computationally expensive and scale poorly with the design dimensions for many applications an approximate field solution is sufficient a machine learning model which could quickly compute approximate electromagnetic fields for a dielectric structure could reduce this computational bottleneck allowing for much faster inverse design processes 	for many applications an approximate field solution is sufficient 
0	126	8352	 inverse design problems computational design of structures by specifying an objective functionare pervasive throughout physics especially in photonics where inverse design methods have been used to design many highly compact optical components the iterative fdfd simulations although exact can be computationally expensive and scale poorly with the design dimensions for many applications an approximate field solution is sufficient a machine learning model which could quickly compute approximate electromagnetic fields for a dielectric structure could reduce this computational bottleneck allowing for much faster inverse design processes 	a machine learning model which could quickly compute approximate electromagnetic fields for a dielectric structure could reduce this computational bottleneck allowing for much faster inverse design processes 
0	126	8353	we were able to find a small body of existing work related to this problem shan et al lagaris et al 	we were able to find a small body of existing work related to this problem 
0	126	8354	we were able to find a small body of existing work related to this problem shan et al lagaris et al 	shan et al 
0	126	8355	we were able to find a small body of existing work related to this problem shan et al lagaris et al 	lagaris et al 
0	126	8356	our model computes approximate electromagnetic field solutions for a specific type of scenario formalized here suppose we have a perfectly reflective d dimensional 2 cavity of length l with an electromagnetic source at the center the cavity contains material forming an arbitrary spatial distribution of dielectric permittivity x 	our model computes approximate electromagnetic field solutions for a specific type of scenario formalized here 
0	126	8357	our model computes approximate electromagnetic field solutions for a specific type of scenario formalized here suppose we have a perfectly reflective d dimensional 2 cavity of length l with an electromagnetic source at the center the cavity contains material forming an arbitrary spatial distribution of dielectric permittivity x 	suppose we have a perfectly reflective d dimensional 2 cavity of length l with an electromagnetic source at the center 
0	126	8358	our model computes approximate electromagnetic field solutions for a specific type of scenario formalized here suppose we have a perfectly reflective d dimensional 2 cavity of length l with an electromagnetic source at the center the cavity contains material forming an arbitrary spatial distribution of dielectric permittivity x 	the cavity contains material forming an arbitrary spatial distribution of dielectric permittivity x 
0	126	8359	our model computes approximate electromagnetic field solutions for a specific type of scenario formalized here suppose we have a perfectly reflective d dimensional 2 cavity of length l with an electromagnetic source at the center the cavity contains material forming an arbitrary spatial distribution of dielectric permittivity x 	discretizing the cavity into pixels of size l the permittivity at each point in space can be expressed as a vector of size n l l d 
0	126	8360	our model computes approximate electromagnetic field solutions for a specific type of scenario formalized here suppose we have a perfectly reflective d dimensional 2 cavity of length l with an electromagnetic source at the center the cavity contains material forming an arbitrary spatial distribution of dielectric permittivity x 	given an input permittivity vector and knowledge of the source location the model outputs an identicallysized vector e pred representing the electric field amplitude at each point in space 
0	126	8361	our model computes approximate electromagnetic field solutions for a specific type of scenario formalized here suppose we have a perfectly reflective d dimensional 2 cavity of length l with an electromagnetic source at the center the cavity contains material forming an arbitrary spatial distribution of dielectric permittivity x 	the cavity scenario was chosen to impose dirichlet boundary conditions of e 0 at the cavity edges ensuring the electric fields are standing waves and thus real up to a global phase 
0	126	8362	our model computes approximate electromagnetic field solutions for a specific type of scenario formalized here suppose we have a perfectly reflective d dimensional 2 cavity of length l with an electromagnetic source at the center the cavity contains material forming an arbitrary spatial distribution of dielectric permittivity x 	the structure of the model is loosely analogous to a generative adversarial network pred 
0	126	8363	our model computes approximate electromagnetic field solutions for a specific type of scenario formalized here suppose we have a perfectly reflective d dimensional 2 cavity of length l with an electromagnetic source at the center the cavity contains material forming an arbitrary spatial distribution of dielectric permittivity x 	the second part is a discriminator 5 which computes the maxwell residual of the predicted field as d providing a measure of how physically realistic the generator s outputs are 
0	126	8364	our model computes approximate electromagnetic field solutions for a specific type of scenario formalized here suppose we have a perfectly reflective d dimensional 2 cavity of length l with an electromagnetic source at the center the cavity contains material forming an arbitrary spatial distribution of dielectric permittivity x 	in both cases the loss of the total model is
0	126	8365	maxwell s equations govern the dynamics and propagation of electromagnetic fields in materials and form the foundation for classical electromagnetism where e b are electric and magnetic fields at a given point in space and time are the permittivity and permeability of the material t is time is charge density and j is current density in a nonmagnetic electrically neutral linear material such as many cases of interest 0 0 and these equations can be simplified to where h b 0 is the magnetizing field 	maxwell s equations govern the dynamics and propagation of electromagnetic fields in materials and form the foundation for classical electromagnetism 
0	126	8366	maxwell s equations govern the dynamics and propagation of electromagnetic fields in materials and form the foundation for classical electromagnetism where e b are electric and magnetic fields at a given point in space and time are the permittivity and permeability of the material t is time is charge density and j is current density in a nonmagnetic electrically neutral linear material such as many cases of interest 0 0 and these equations can be simplified to where h b 0 is the magnetizing field 	where e b are electric and magnetic fields at a given point in space and time are the permittivity and permeability of the material t is time is charge density and j is current density 
0	126	8367	maxwell s equations govern the dynamics and propagation of electromagnetic fields in materials and form the foundation for classical electromagnetism where e b are electric and magnetic fields at a given point in space and time are the permittivity and permeability of the material t is time is charge density and j is current density in a nonmagnetic electrically neutral linear material such as many cases of interest 0 0 and these equations can be simplified to where h b 0 is the magnetizing field 	in a nonmagnetic electrically neutral linear material such as many cases of interest 0 0 and these equations can be simplified to where h b 0 is the magnetizing field 
0	126	8368	maxwell s equations govern the dynamics and propagation of electromagnetic fields in materials and form the foundation for classical electromagnetism where e b are electric and magnetic fields at a given point in space and time are the permittivity and permeability of the material t is time is charge density and j is current density in a nonmagnetic electrically neutral linear material such as many cases of interest 0 0 and these equations can be simplified to where h b 0 is the magnetizing field 	in a steady state frequency domain solution such as the ones found with fdfd methods e t ee i t so t where is frequency 
0	126	8369	maxwell s equations govern the dynamics and propagation of electromagnetic fields in materials and form the foundation for classical electromagnetism where e b are electric and magnetic fields at a given point in space and time are the permittivity and permeability of the material t is time is charge density and j is current density in a nonmagnetic electrically neutral linear material such as many cases of interest 0 0 and these equations can be simplified to where h b 0 is the magnetizing field 	we can combine if the electromagnetic field is polarized say with e z polarization then e e and j j at each point in space 
0	126	8370	maxwell s equations govern the dynamics and propagation of electromagnetic fields in materials and form the foundation for classical electromagnetism where e b are electric and magnetic fields at a given point in space and time are the permittivity and permeability of the material t is time is charge density and j is current density in a nonmagnetic electrically neutral linear material such as many cases of interest 0 0 and these equations can be simplified to where h b 0 is the magnetizing field 	we can then vectorize this such that e and j are the electric field and free current amplitudes in the direction and is the dielectric permittivity at each point in space 
0	126	8371	maxwell s equations govern the dynamics and propagation of electromagnetic fields in materials and form the foundation for classical electromagnetism where e b are electric and magnetic fields at a given point in space and time are the permittivity and permeability of the material t is time is charge density and j is current density in a nonmagnetic electrically neutral linear material such as many cases of interest 0 0 and these equations can be simplified to where h b 0 is the magnetizing field 	if we have a model which takes in a permittivity distribution and a source term j and returns a predicted field e pred then we use eq 
0	126	8372	maxwell s equations govern the dynamics and propagation of electromagnetic fields in materials and form the foundation for classical electromagnetism where e b are electric and magnetic fields at a given point in space and time are the permittivity and permeability of the material t is time is charge density and j is current density in a nonmagnetic electrically neutral linear material such as many cases of interest 0 0 and these equations can be simplified to where h b 0 is the magnetizing field 	4 to define the maxwell residual l m as the maxwell residual provides an element wise measure of the physical realism of the predicted field e pred a measure of how far the predicted solution is from satisfying maxwell s equations at each point 
0	126	8373	maxwell s equations govern the dynamics and propagation of electromagnetic fields in materials and form the foundation for classical electromagnetism where e b are electric and magnetic fields at a given point in space and time are the permittivity and permeability of the material t is time is charge density and j is current density in a nonmagnetic electrically neutral linear material such as many cases of interest 0 0 and these equations can be simplified to where h b 0 is the magnetizing field 	if the model can sufficiently minimize l m then it can produce solutions which approximately satisfy maxwell s equations at each point and thus are approximate global electromagnetic field solutions for the system described by and j 
0	126	8374	maxwell s equations govern the dynamics and propagation of electromagnetic fields in materials and form the foundation for classical electromagnetism where e b are electric and magnetic fields at a given point in space and time are the permittivity and permeability of the material t is time is charge density and j is current density in a nonmagnetic electrically neutral linear material such as many cases of interest 0 0 and these equations can be simplified to where h b 0 is the magnetizing field 	this training does not require the model to ever see the exact fdfd field solution the outputs it attempts to replicate and is thus unsupervised 
1	126	8375	we found that when trained or more precisely overfit to predict the field of a single permittivity distribution virtually any network architecture would allow the predicted field to converge to the true field given enough training time for more on this see section 4 1 the challenge was finding a network architecture with the correct structure to capture the generalized transformation e when trained on a large number of possible permittivities we tested many different network architectures for this project 	we found that when trained or more precisely overfit to predict the field of a single permittivity distribution virtually any network architecture would allow the predicted field to converge to the true field given enough training time 
0	126	8376	we found that when trained or more precisely overfit to predict the field of a single permittivity distribution virtually any network architecture would allow the predicted field to converge to the true field given enough training time for more on this see section 4 1 the challenge was finding a network architecture with the correct structure to capture the generalized transformation e when trained on a large number of possible permittivities we tested many different network architectures for this project 	 for more on this see section 4 1 
0	126	8377	we found that when trained or more precisely overfit to predict the field of a single permittivity distribution virtually any network architecture would allow the predicted field to converge to the true field given enough training time for more on this see section 4 1 the challenge was finding a network architecture with the correct structure to capture the generalized transformation e when trained on a large number of possible permittivities we tested many different network architectures for this project 	the challenge was finding a network architecture with the correct structure to capture the generalized transformation e when trained on a large number of possible permittivities we tested many different network architectures for this project 
0	126	8378	we found that when trained or more precisely overfit to predict the field of a single permittivity distribution virtually any network architecture would allow the predicted field to converge to the true field given enough training time for more on this see section 4 1 the challenge was finding a network architecture with the correct structure to capture the generalized transformation e when trained on a large number of possible permittivities we tested many different network architectures for this project 	purely convolutional architectures like the ones used by ref 
0	126	8379	we found that when trained or more precisely overfit to predict the field of a single permittivity distribution virtually any network architecture would allow the predicted field to converge to the true field given enough training time for more on this see section 4 1 the challenge was finding a network architecture with the correct structure to capture the generalized transformation e when trained on a large number of possible permittivities we tested many different network architectures for this project 	our final network architecture employed a hybrid convolutional dense deconvolutional approach and is shown in during training the network outputs the maxwell residual l m e 
0	126	8380	we found that when trained or more precisely overfit to predict the field of a single permittivity distribution virtually any network architecture would allow the predicted field to converge to the true field given enough training time for more on this see section 4 1 the challenge was finding a network architecture with the correct structure to capture the generalized transformation e when trained on a large number of possible permittivities we tested many different network architectures for this project 	dropout layers with p 0 1 and relu activations are present after every layer except the last one 
0	126	8381	we found that when trained or more precisely overfit to predict the field of a single permittivity distribution virtually any network architecture would allow the predicted field to converge to the true field given enough training time for more on this see section 4 1 the challenge was finding a network architecture with the correct structure to capture the generalized transformation e when trained on a large number of possible permittivities we tested many different network architectures for this project 	our model was implemented using pytorch
0	126	8382	as an initial experiment we trained the model to predict the field of only a single input using the maxwell residual method described in section 3 2 the evolution of the predicted field as the network is trained on a sample permittivity is shown in	as an initial experiment we trained the model to predict the field of only a single input using the maxwell residual method described in section 3 2 
0	126	8383	as an initial experiment we trained the model to predict the field of only a single input using the maxwell residual method described in section 3 2 the evolution of the predicted field as the network is trained on a sample permittivity is shown in	the evolution of the predicted field as the network is trained on a sample permittivity is shown in
0	126	8384	for the main experiment in this paper we trained a model with the architecture described in to evaluate the results of the trained model a test set of 10 4 new permittivity samples was generated using the same generation procedure the model was run on each of these inputs the loss for each sample was calculated average loss of 8 8 10 4 and the results were sorted from best to worst example good best 10 10000 typical middle 10 10000 and bad worst 10 10000 field predictions from the test set are shown in the first three panels of finally we tested the model s capability to generalize to inputs outside of the training distributionthat is permittivities representing a different set of structures than the ones generated for the training and test sets 	for the main experiment in this paper we trained a model with the architecture described in to evaluate the results of the trained model a test set of 10 4 new permittivity samples was generated using the same generation procedure 
0	126	8385	for the main experiment in this paper we trained a model with the architecture described in to evaluate the results of the trained model a test set of 10 4 new permittivity samples was generated using the same generation procedure the model was run on each of these inputs the loss for each sample was calculated average loss of 8 8 10 4 and the results were sorted from best to worst example good best 10 10000 typical middle 10 10000 and bad worst 10 10000 field predictions from the test set are shown in the first three panels of finally we tested the model s capability to generalize to inputs outside of the training distributionthat is permittivities representing a different set of structures than the ones generated for the training and test sets 	the model was run on each of these inputs the loss for each sample was calculated average loss of 8 8 10 4 and the results were sorted from best to worst 
1	126	8386	for the main experiment in this paper we trained a model with the architecture described in to evaluate the results of the trained model a test set of 10 4 new permittivity samples was generated using the same generation procedure the model was run on each of these inputs the loss for each sample was calculated average loss of 8 8 10 4 and the results were sorted from best to worst example good best 10 10000 typical middle 10 10000 and bad worst 10 10000 field predictions from the test set are shown in the first three panels of finally we tested the model s capability to generalize to inputs outside of the training distributionthat is permittivities representing a different set of structures than the ones generated for the training and test sets 	example good best 10 10000 typical middle 10 10000 and bad worst 10 10000 field predictions from the test set are shown in the first three panels of finally we tested the model s capability to generalize to inputs outside of the training distributionthat is permittivities representing a different set of structures than the ones generated for the training and test sets 
0	126	8387	for the main experiment in this paper we trained a model with the architecture described in to evaluate the results of the trained model a test set of 10 4 new permittivity samples was generated using the same generation procedure the model was run on each of these inputs the loss for each sample was calculated average loss of 8 8 10 4 and the results were sorted from best to worst example good best 10 10000 typical middle 10 10000 and bad worst 10 10000 field predictions from the test set are shown in the first three panels of finally we tested the model s capability to generalize to inputs outside of the training distributionthat is permittivities representing a different set of structures than the ones generated for the training and test sets 	as an example the predicted field amplitudes for a sample where each point in space has a permittivity value randomly chosen between vacuum and silicon is shown in the last panel of
0	126	8388	in this paper we presented a machine learning model capable of computing approximate solutions to maxwell s equations over an arbitrary dielectric permittivity in a cavity the model was trained using an unsupervised approach where it learned to minimize the maxwell residual of its predicted fields thereby maximizing the physical realism of the solutions our model demonstrates good predictive performance and is over 10 faster than comparable fdfd simulations suggesting applications for accelerating optical inverse design algorithms for future work we would like to implement a complex valued model to solve the more general problem of predicting fields outside of a cavity environment 	in this paper we presented a machine learning model capable of computing approximate solutions to maxwell s equations over an arbitrary dielectric permittivity in a cavity 
0	126	8389	in this paper we presented a machine learning model capable of computing approximate solutions to maxwell s equations over an arbitrary dielectric permittivity in a cavity the model was trained using an unsupervised approach where it learned to minimize the maxwell residual of its predicted fields thereby maximizing the physical realism of the solutions our model demonstrates good predictive performance and is over 10 faster than comparable fdfd simulations suggesting applications for accelerating optical inverse design algorithms for future work we would like to implement a complex valued model to solve the more general problem of predicting fields outside of a cavity environment 	the model was trained using an unsupervised approach where it learned to minimize the maxwell residual of its predicted fields thereby maximizing the physical realism of the solutions 
1	126	8390	in this paper we presented a machine learning model capable of computing approximate solutions to maxwell s equations over an arbitrary dielectric permittivity in a cavity the model was trained using an unsupervised approach where it learned to minimize the maxwell residual of its predicted fields thereby maximizing the physical realism of the solutions our model demonstrates good predictive performance and is over 10 faster than comparable fdfd simulations suggesting applications for accelerating optical inverse design algorithms for future work we would like to implement a complex valued model to solve the more general problem of predicting fields outside of a cavity environment 	our model demonstrates good predictive performance and is over 10 faster than comparable fdfd simulations suggesting applications for accelerating optical inverse design algorithms for future work we would like to implement a complex valued model to solve the more general problem of predicting fields outside of a cavity environment 
0	126	8391	in this paper we presented a machine learning model capable of computing approximate solutions to maxwell s equations over an arbitrary dielectric permittivity in a cavity the model was trained using an unsupervised approach where it learned to minimize the maxwell residual of its predicted fields thereby maximizing the physical realism of the solutions our model demonstrates good predictive performance and is over 10 faster than comparable fdfd simulations suggesting applications for accelerating optical inverse design algorithms for future work we would like to implement a complex valued model to solve the more general problem of predicting fields outside of a cavity environment 	our choice of the cavity problem was driven primarily by pytorch s lack of support for complex tensors 
0	126	8392	in this paper we presented a machine learning model capable of computing approximate solutions to maxwell s equations over an arbitrary dielectric permittivity in a cavity the model was trained using an unsupervised approach where it learned to minimize the maxwell residual of its predicted fields thereby maximizing the physical realism of the solutions our model demonstrates good predictive performance and is over 10 faster than comparable fdfd simulations suggesting applications for accelerating optical inverse design algorithms for future work we would like to implement a complex valued model to solve the more general problem of predicting fields outside of a cavity environment 	 in the project repository we have an initial implementation of this which explicitly parameterizes e and e although this approach was only mildly successful 
0	126	8393	in this paper we presented a machine learning model capable of computing approximate solutions to maxwell s equations over an arbitrary dielectric permittivity in a cavity the model was trained using an unsupervised approach where it learned to minimize the maxwell residual of its predicted fields thereby maximizing the physical realism of the solutions our model demonstrates good predictive performance and is over 10 faster than comparable fdfd simulations suggesting applications for accelerating optical inverse design algorithms for future work we would like to implement a complex valued model to solve the more general problem of predicting fields outside of a cavity environment 	we would also like to explore using our model for dimensionality reduction especially for 2d and 3d problems 
1	126	8394	in this paper we presented a machine learning model capable of computing approximate solutions to maxwell s equations over an arbitrary dielectric permittivity in a cavity the model was trained using an unsupervised approach where it learned to minimize the maxwell residual of its predicted fields thereby maximizing the physical realism of the solutions our model demonstrates good predictive performance and is over 10 faster than comparable fdfd simulations suggesting applications for accelerating optical inverse design algorithms for future work we would like to implement a complex valued model to solve the more general problem of predicting fields outside of a cavity environment 	we were able to achieve a 1 16 dimensionality reduction with our model applied to a 32 32 2d input of permittivities by adjusting the network parameters to force a 64 value chokepoint in the middle dense layers of the network 
0	126	8395	in this paper we presented a machine learning model capable of computing approximate solutions to maxwell s equations over an arbitrary dielectric permittivity in a cavity the model was trained using an unsupervised approach where it learned to minimize the maxwell residual of its predicted fields thereby maximizing the physical realism of the solutions our model demonstrates good predictive performance and is over 10 faster than comparable fdfd simulations suggesting applications for accelerating optical inverse design algorithms for future work we would like to implement a complex valued model to solve the more general problem of predicting fields outside of a cavity environment 	 this figure is present in the poster but omitted here due to length constraints 
0	126	8396	in this paper we presented a machine learning model capable of computing approximate solutions to maxwell s equations over an arbitrary dielectric permittivity in a cavity the model was trained using an unsupervised approach where it learned to minimize the maxwell residual of its predicted fields thereby maximizing the physical realism of the solutions our model demonstrates good predictive performance and is over 10 faster than comparable fdfd simulations suggesting applications for accelerating optical inverse design algorithms for future work we would like to implement a complex valued model to solve the more general problem of predicting fields outside of a cavity environment 	this could force the model to learn more efficient representations of the relationships between permittivities and fields 
0	126	8397	we would like to thank shanhui fan sunil pai and tyler hughes for several illuminating discussions relating to this work all source code used for this paper is available at https github com bencbartlett neural maxwell trained model parameters were too large to include in the repository but are are available upon request 	we would like to thank shanhui fan sunil pai and tyler hughes for several illuminating discussions relating to this work all source code used for this paper is available at https github com bencbartlett neural maxwell 
0	126	8398	we would like to thank shanhui fan sunil pai and tyler hughes for several illuminating discussions relating to this work all source code used for this paper is available at https github com bencbartlett neural maxwell trained model parameters were too large to include in the repository but are are available upon request 	trained model parameters were too large to include in the repository but are are available upon request 
1	127	8399	since the 1970s the seismic bridge design process has gone through a great change from capacity based design to performance based design and the performance of bridges has become a great concern to engineers the safety margins of the contributive factors vary from case to case and the trends are still unclear partially if not all because of the change in the design logics in the past decades therefore having an on hand trained model on bridge performance prediction would be to some extent helpful for knowing how well badly an existing bridge would perform in a future earthquake as well as guiding the design of a new bridge to survive a future earthquake in this project we are trying to train a prediction model for bridge performance under earthquakes with supervised learning 	since the 1970s the seismic bridge design process has gone through a great change from capacity based design to performance based design and the performance of bridges has become a great concern to engineers 
1	127	8400	since the 1970s the seismic bridge design process has gone through a great change from capacity based design to performance based design and the performance of bridges has become a great concern to engineers the safety margins of the contributive factors vary from case to case and the trends are still unclear partially if not all because of the change in the design logics in the past decades therefore having an on hand trained model on bridge performance prediction would be to some extent helpful for knowing how well badly an existing bridge would perform in a future earthquake as well as guiding the design of a new bridge to survive a future earthquake in this project we are trying to train a prediction model for bridge performance under earthquakes with supervised learning 	the safety margins of the contributive factors vary from case to case and the trends are still unclear partially if not all because of the change in the design logics in the past decades 
1	127	8401	since the 1970s the seismic bridge design process has gone through a great change from capacity based design to performance based design and the performance of bridges has become a great concern to engineers the safety margins of the contributive factors vary from case to case and the trends are still unclear partially if not all because of the change in the design logics in the past decades therefore having an on hand trained model on bridge performance prediction would be to some extent helpful for knowing how well badly an existing bridge would perform in a future earthquake as well as guiding the design of a new bridge to survive a future earthquake in this project we are trying to train a prediction model for bridge performance under earthquakes with supervised learning 	therefore having an on hand trained model on bridge performance prediction would be to some extent helpful for knowing how well badly an existing bridge would perform in a future earthquake as well as guiding the design of a new bridge to survive a future earthquake in this project we are trying to train a prediction model for bridge performance under earthquakes with supervised learning 
1	127	8402	since the 1970s the seismic bridge design process has gone through a great change from capacity based design to performance based design and the performance of bridges has become a great concern to engineers the safety margins of the contributive factors vary from case to case and the trends are still unclear partially if not all because of the change in the design logics in the past decades therefore having an on hand trained model on bridge performance prediction would be to some extent helpful for knowing how well badly an existing bridge would perform in a future earthquake as well as guiding the design of a new bridge to survive a future earthquake in this project we are trying to train a prediction model for bridge performance under earthquakes with supervised learning 	the inputs to our algorithm are the age of a bridge the magnitude of the earthquake and the distance between that bridge and the epicenter 
1	127	8403	since the 1970s the seismic bridge design process has gone through a great change from capacity based design to performance based design and the performance of bridges has become a great concern to engineers the safety margins of the contributive factors vary from case to case and the trends are still unclear partially if not all because of the change in the design logics in the past decades therefore having an on hand trained model on bridge performance prediction would be to some extent helpful for knowing how well badly an existing bridge would perform in a future earthquake as well as guiding the design of a new bridge to survive a future earthquake in this project we are trying to train a prediction model for bridge performance under earthquakes with supervised learning 	we then use logistic regression quadratic discriminative analysis and k nearest neighbor classifier to output a predicted performance categorized to be positive damaged or negative undamaged of the bridge under the given earthquake 
0	127	8404	prof kiremidjian s paper greatly inspires our interest on data driven earthquake engineering when trying to select the related features we refer to a comprehensive study of u s bridge failures from mceer technical report we also learn the advantages of bootstrap resampling in managing unbalanced data from dupret and koda s paper the problem we encountered is actually a sub problem of other projects on application of machine learning in earthquake engineering in the previous cs 229 projects 	prof kiremidjian s paper greatly inspires our interest on data driven earthquake engineering 
0	127	8405	prof kiremidjian s paper greatly inspires our interest on data driven earthquake engineering when trying to select the related features we refer to a comprehensive study of u s bridge failures from mceer technical report we also learn the advantages of bootstrap resampling in managing unbalanced data from dupret and koda s paper the problem we encountered is actually a sub problem of other projects on application of machine learning in earthquake engineering in the previous cs 229 projects 	when trying to select the related features we refer to a comprehensive study of u s bridge failures from mceer technical report 
1	127	8406	prof kiremidjian s paper greatly inspires our interest on data driven earthquake engineering when trying to select the related features we refer to a comprehensive study of u s bridge failures from mceer technical report we also learn the advantages of bootstrap resampling in managing unbalanced data from dupret and koda s paper the problem we encountered is actually a sub problem of other projects on application of machine learning in earthquake engineering in the previous cs 229 projects 	we also learn the advantages of bootstrap resampling in managing unbalanced data from dupret and koda s paper the problem we encountered is actually a sub problem of other projects on application of machine learning in earthquake engineering in the previous cs 229 projects 
1	127	8407	prof kiremidjian s paper greatly inspires our interest on data driven earthquake engineering when trying to select the related features we refer to a comprehensive study of u s bridge failures from mceer technical report we also learn the advantages of bootstrap resampling in managing unbalanced data from dupret and koda s paper the problem we encountered is actually a sub problem of other projects on application of machine learning in earthquake engineering in the previous cs 229 projects 	in this project we implement some similar methodologies such as knn but with a different approach to select the optimal k value 
0	127	8408	the data collection is a big challenge for us because we have limited records of bridge failure due to earthquakes therefore we make an assumption in advance if an earthquake occurs near the site of a bridge but there is no record showing that the bridge is damaged then this should be identified as an example of no damage negative examples we first search for bridge failure records in previous famous earthquakes in the reports published by the u s government find the site location of the bridge and mark it as a positive example next we search for other earthquake records happened in bridge s history near the site location on usgs earthquake search catalog and mark them as negative examples 	the data collection is a big challenge for us because we have limited records of bridge failure due to earthquakes 
1	127	8409	the data collection is a big challenge for us because we have limited records of bridge failure due to earthquakes therefore we make an assumption in advance if an earthquake occurs near the site of a bridge but there is no record showing that the bridge is damaged then this should be identified as an example of no damage negative examples we first search for bridge failure records in previous famous earthquakes in the reports published by the u s government find the site location of the bridge and mark it as a positive example next we search for other earthquake records happened in bridge s history near the site location on usgs earthquake search catalog and mark them as negative examples 	therefore we make an assumption in advance if an earthquake occurs near the site of a bridge but there is no record showing that the bridge is damaged then this should be identified as an example of no damage negative examples we first search for bridge failure records in previous famous earthquakes in the reports published by the u s government find the site location of the bridge and mark it as a positive example 
1	127	8410	the data collection is a big challenge for us because we have limited records of bridge failure due to earthquakes therefore we make an assumption in advance if an earthquake occurs near the site of a bridge but there is no record showing that the bridge is damaged then this should be identified as an example of no damage negative examples we first search for bridge failure records in previous famous earthquakes in the reports published by the u s government find the site location of the bridge and mark it as a positive example next we search for other earthquake records happened in bridge s history near the site location on usgs earthquake search catalog and mark them as negative examples 	next we search for other earthquake records happened in bridge s history near the site location on usgs earthquake search catalog and mark them as negative examples 
1	127	8411	the data collection is a big challenge for us because we have limited records of bridge failure due to earthquakes therefore we make an assumption in advance if an earthquake occurs near the site of a bridge but there is no record showing that the bridge is damaged then this should be identified as an example of no damage negative examples we first search for bridge failure records in previous famous earthquakes in the reports published by the u s government find the site location of the bridge and mark it as a positive example next we search for other earthquake records happened in bridge s history near the site location on usgs earthquake search catalog and mark them as negative examples 	finally we split the dataset into training 70 and test 30 sets because of the intrinsic scarcity of positive examples our dataset is unbalanced 
0	127	8412	the data collection is a big challenge for us because we have limited records of bridge failure due to earthquakes therefore we make an assumption in advance if an earthquake occurs near the site of a bridge but there is no record showing that the bridge is damaged then this should be identified as an example of no damage negative examples we first search for bridge failure records in previous famous earthquakes in the reports published by the u s government find the site location of the bridge and mark it as a positive example next we search for other earthquake records happened in bridge s history near the site location on usgs earthquake search catalog and mark them as negative examples 	the positive tonegative ratio is about 1 10 
1	127	8413	the data collection is a big challenge for us because we have limited records of bridge failure due to earthquakes therefore we make an assumption in advance if an earthquake occurs near the site of a bridge but there is no record showing that the bridge is damaged then this should be identified as an example of no damage negative examples we first search for bridge failure records in previous famous earthquakes in the reports published by the u s government find the site location of the bridge and mark it as a positive example next we search for other earthquake records happened in bridge s history near the site location on usgs earthquake search catalog and mark them as negative examples 	to mitigate this problem we use bootstrap resampling with replacement to up sample the positive class and generate training sets with more reasonable positive to negative ratio 
1	127	8414	the data collection is a big challenge for us because we have limited records of bridge failure due to earthquakes therefore we make an assumption in advance if an earthquake occurs near the site of a bridge but there is no record showing that the bridge is damaged then this should be identified as an example of no damage negative examples we first search for bridge failure records in previous famous earthquakes in the reports published by the u s government find the site location of the bridge and mark it as a positive example next we search for other earthquake records happened in bridge s history near the site location on usgs earthquake search catalog and mark them as negative examples 	in our project this ratio varies from 0 2 to 1 0 the raw features selected for the model are the age of the bridge the magnitude of the earthquake and the distance between the bridge and the epicenter all of which are continuous variables 
0	127	8415	the data collection is a big challenge for us because we have limited records of bridge failure due to earthquakes therefore we make an assumption in advance if an earthquake occurs near the site of a bridge but there is no record showing that the bridge is damaged then this should be identified as an example of no damage negative examples we first search for bridge failure records in previous famous earthquakes in the reports published by the u s government find the site location of the bridge and mark it as a positive example next we search for other earthquake records happened in bridge s history near the site location on usgs earthquake search catalog and mark them as negative examples 	for future work we plan to explore more features continuous and discrete such as the material type the structure type the annual average daily traffic etc 
1	127	8416	the goal of the algorithm for this binary classification problem is to predict the correct or more likely bridge performance given data on the earthquake and the bridge itself we use logistic regression quadratic discriminative analysis qda and k nearest neighbor classifier knn to train the model respectively and independently 	the goal of the algorithm for this binary classification problem is to predict the correct or more likely bridge performance given data on the earthquake and the bridge itself we use logistic regression quadratic discriminative analysis qda and k nearest neighbor classifier knn to train the model respectively and independently 
0	127	8417	the logistic regression updates a set of parameters that maximize the log likelihood 2 quadratic discriminative analysis qda qda is one of the most commonly used generative models where we assume that the results from each class are normally distributed in binary case with means 0 1 and 0 1 the log likelihood can be computed as 	the logistic regression updates a set of parameters that maximize the log likelihood 2 
0	127	8418	the logistic regression updates a set of parameters that maximize the log likelihood 2 quadratic discriminative analysis qda qda is one of the most commonly used generative models where we assume that the results from each class are normally distributed in binary case with means 0 1 and 0 1 the log likelihood can be computed as 	quadratic discriminative analysis qda qda is one of the most commonly used generative models where we assume that the results from each class are normally distributed 
0	127	8419	the logistic regression updates a set of parameters that maximize the log likelihood 2 quadratic discriminative analysis qda qda is one of the most commonly used generative models where we assume that the results from each class are normally distributed in binary case with means 0 1 and 0 1 the log likelihood can be computed as 	in binary case with means 0 1 and 0 1 the log likelihood can be computed as 
0	127	8420	the k nearest neighbor classifier considers k neighbors of the current data point compares the number of positives and negatives within the k neighbors and outputs the label i e positive or negative of the one that has a greater number than the other to determine the optimal value of k we use a cross validation set and plot its accuracy with respect to the value of k from 1 to 10 as shown below based on the figure shown above the optimal k value in this case is 2 	the k nearest neighbor classifier considers k neighbors of the current data point compares the number of positives and negatives within the k neighbors and outputs the label i e 
0	127	8421	the k nearest neighbor classifier considers k neighbors of the current data point compares the number of positives and negatives within the k neighbors and outputs the label i e positive or negative of the one that has a greater number than the other to determine the optimal value of k we use a cross validation set and plot its accuracy with respect to the value of k from 1 to 10 as shown below based on the figure shown above the optimal k value in this case is 2 	positive or negative of the one that has a greater number than the other 
1	127	8422	the k nearest neighbor classifier considers k neighbors of the current data point compares the number of positives and negatives within the k neighbors and outputs the label i e positive or negative of the one that has a greater number than the other to determine the optimal value of k we use a cross validation set and plot its accuracy with respect to the value of k from 1 to 10 as shown below based on the figure shown above the optimal k value in this case is 2 	to determine the optimal value of k we use a cross validation set and plot its accuracy with respect to the value of k from 1 to 10 as shown below based on the figure shown above the optimal k value in this case is 2 
1	127	8423	the following training and testing accuracy are the average results from 5 cases with different positive to negative ratio after resampling since bootstrap resampling is a randomized sampling algorithm we run 100 iterations for each model and plot both training and testing accuracy vs numbers of iterations with different positive to negative ratio as tabulated below 	the following training and testing accuracy are the average results from 5 cases with different positive to negative ratio after resampling since bootstrap resampling is a randomized sampling algorithm we run 100 iterations for each model and plot both training and testing accuracy vs numbers of iterations with different positive to negative ratio as tabulated below 
0	127	8424	algorithm pos neg 0 2 pos neg 1 0	algorithm pos neg 0 2 pos neg 1 0
0	127	8425	there are a few observations from these plots 1 the training accuracy is relatively stable in terms of number of iterations and only fluctuates about 2 5 for each algorithm 2 	there are a few observations from these plots 1 
1	127	8426	there are a few observations from these plots 1 the training accuracy is relatively stable in terms of number of iterations and only fluctuates about 2 5 for each algorithm 2 	the training accuracy is relatively stable in terms of number of iterations and only fluctuates about 2 5 for each algorithm 
1	127	8427	there are a few observations from these plots 1 the training accuracy is relatively stable in terms of number of iterations and only fluctuates about 2 5 for each algorithm 2 	the accuracy decreases as the size of resampling increases 
1	127	8428	there are a few observations from these plots 1 the training accuracy is relatively stable in terms of number of iterations and only fluctuates about 2 5 for each algorithm 2 	however since the test set is also unbalanced a very high testing accuracy may not be meaningful as it always tends to predict the result to be negative 
1	127	8429	there are a few observations from these plots 1 the training accuracy is relatively stable in terms of number of iterations and only fluctuates about 2 5 for each algorithm 2 	additionally simply resampling the size does not truly add any valuable data point into the data set so the scarcity of positive examples is not improved 
1	127	8430	there are a few observations from these plots 1 the training accuracy is relatively stable in terms of number of iterations and only fluctuates about 2 5 for each algorithm 2 	the testing accuracies of knn for a positive to negative ratio of 1 0 appear to be a straight line or closely which indicates that knn algorithm tends to make the same predictions for random input of positive examples 
1	127	8431	there are a few observations from these plots 1 the training accuracy is relatively stable in terms of number of iterations and only fluctuates about 2 5 for each algorithm 2 	this also makes sense because in case of 2 a larger positive example size will make the output of knn more stable as it only uses the nearest 2 neighbors for prediction from all the plots and tables shown above qda seems to be the most accurate one among the 3 models we choose for this project 
1	127	8432	there are a few observations from these plots 1 the training accuracy is relatively stable in terms of number of iterations and only fluctuates about 2 5 for each algorithm 2 	also its accuracy does not decrease too much as we expand the size of our positive class 
1	127	8433	there are a few observations from these plots 1 the training accuracy is relatively stable in terms of number of iterations and only fluctuates about 2 5 for each algorithm 2 	the testing accuracy is even more stable than that of qda always around 80 but it behaves poorly when making predictions on the positive class 
1	127	8434	in this project we pre process the raw data set with bootstrap resampling and implement 3 supervised learning models on the training set during training process we observe that the accuracy decreases as we increase the size of resampling however since the test set is also unbalanced a very high testing accuracy may not be meaningful as it always tends to predict the result to be negative for future work we expect to expand the size of dataset more explicitly to increase the number of positive examples to run tests on other generative models and to implement multi class classification so we may obtain a more meaningful and practical model as we desired thinking about the nature of the problem helps us understand the observations above 	in this project we pre process the raw data set with bootstrap resampling and implement 3 supervised learning models on the training set 
1	127	8435	in this project we pre process the raw data set with bootstrap resampling and implement 3 supervised learning models on the training set during training process we observe that the accuracy decreases as we increase the size of resampling however since the test set is also unbalanced a very high testing accuracy may not be meaningful as it always tends to predict the result to be negative for future work we expect to expand the size of dataset more explicitly to increase the number of positive examples to run tests on other generative models and to implement multi class classification so we may obtain a more meaningful and practical model as we desired thinking about the nature of the problem helps us understand the observations above 	during training process we observe that the accuracy decreases as we increase the size of resampling 
1	127	8436	in this project we pre process the raw data set with bootstrap resampling and implement 3 supervised learning models on the training set during training process we observe that the accuracy decreases as we increase the size of resampling however since the test set is also unbalanced a very high testing accuracy may not be meaningful as it always tends to predict the result to be negative for future work we expect to expand the size of dataset more explicitly to increase the number of positive examples to run tests on other generative models and to implement multi class classification so we may obtain a more meaningful and practical model as we desired thinking about the nature of the problem helps us understand the observations above 	however since the test set is also unbalanced a very high testing accuracy may not be meaningful as it always tends to predict the result to be negative for future work we expect to expand the size of dataset more explicitly to increase the number of positive examples to run tests on other generative models and to implement multi class classification so we may obtain a more meaningful and practical model as we desired thinking about the nature of the problem helps us understand the observations above 
1	127	8437	in this project we pre process the raw data set with bootstrap resampling and implement 3 supervised learning models on the training set during training process we observe that the accuracy decreases as we increase the size of resampling however since the test set is also unbalanced a very high testing accuracy may not be meaningful as it always tends to predict the result to be negative for future work we expect to expand the size of dataset more explicitly to increase the number of positive examples to run tests on other generative models and to implement multi class classification so we may obtain a more meaningful and practical model as we desired thinking about the nature of the problem helps us understand the observations above 	in a civil engineering perspective seismic performance of structures is highly uncertain and most of them are hard to predict 
1	127	8438	in this project we pre process the raw data set with bootstrap resampling and implement 3 supervised learning models on the training set during training process we observe that the accuracy decreases as we increase the size of resampling however since the test set is also unbalanced a very high testing accuracy may not be meaningful as it always tends to predict the result to be negative for future work we expect to expand the size of dataset more explicitly to increase the number of positive examples to run tests on other generative models and to implement multi class classification so we may obtain a more meaningful and practical model as we desired thinking about the nature of the problem helps us understand the observations above 	to view this study in a broader scope it may be observed that there are usually lots of different constraints on features data size physical meanings of results etc 
1	127	8439	in this project we pre process the raw data set with bootstrap resampling and implement 3 supervised learning models on the training set during training process we observe that the accuracy decreases as we increase the size of resampling however since the test set is also unbalanced a very high testing accuracy may not be meaningful as it always tends to predict the result to be negative for future work we expect to expand the size of dataset more explicitly to increase the number of positive examples to run tests on other generative models and to implement multi class classification so we may obtain a more meaningful and practical model as we desired thinking about the nature of the problem helps us understand the observations above 	in civil engineering scenarios which may impact the practicality of machine learning in such kind of studies 
0	127	8440	xiao and ziyang came up with the topic and scope of the project together we both engaged in data collection and feature selection ziyang mainly ran the algorithms to train the model and test the model 	xiao and ziyang came up with the topic and scope of the project together 
0	127	8441	xiao and ziyang came up with the topic and scope of the project together we both engaged in data collection and feature selection ziyang mainly ran the algorithms to train the model and test the model 	we both engaged in data collection and feature selection 
0	127	8442	xiao and ziyang came up with the topic and scope of the project together we both engaged in data collection and feature selection ziyang mainly ran the algorithms to train the model and test the model 	ziyang mainly ran the algorithms to train the model and test the model 
0	127	8443	xiao and ziyang came up with the topic and scope of the project together we both engaged in data collection and feature selection ziyang mainly ran the algorithms to train the model and test the model 	xiao and ziyang interpreted the results and made observations together 
0	127	8444	xiao and ziyang came up with the topic and scope of the project together we both engaged in data collection and feature selection ziyang mainly ran the algorithms to train the model and test the model 	ziyang prepared the proposal and the milestone whereas xiao made much of the poster and the final report 
0	127	8445	the link to the github repository containing all the datasets codes and output are given below https github com jzy95310 cs229 fall 2018 final report	the link to the github repository containing all the datasets codes and output are given below https github com jzy95310 cs229 fall 2018 final report
1	128	8446	we are motivated by the opportunity of using machine learning on novel vr based mental healthcare datasets to improve understanding and diagnosis of mood and emotional disorders recently researchers have established a relationship between degree frequency and type of head movement and the degree of clinical depression experienced by an individual we are given the time series data gathered from oculus sensors for 148 engage study participants the time series represent two channels of yaw pitch and roll values of the participant s head location 	we are motivated by the opportunity of using machine learning on novel vr based mental healthcare datasets to improve understanding and diagnosis of mood and emotional disorders 
1	128	8447	we are motivated by the opportunity of using machine learning on novel vr based mental healthcare datasets to improve understanding and diagnosis of mood and emotional disorders recently researchers have established a relationship between degree frequency and type of head movement and the degree of clinical depression experienced by an individual we are given the time series data gathered from oculus sensors for 148 engage study participants the time series represent two channels of yaw pitch and roll values of the participant s head location 	recently researchers have established a relationship between degree frequency and type of head movement and the degree of clinical depression experienced by an individual we are given the time series data gathered from oculus sensors for 148 engage study participants 
0	128	8448	we are motivated by the opportunity of using machine learning on novel vr based mental healthcare datasets to improve understanding and diagnosis of mood and emotional disorders recently researchers have established a relationship between degree frequency and type of head movement and the degree of clinical depression experienced by an individual we are given the time series data gathered from oculus sensors for 148 engage study participants the time series represent two channels of yaw pitch and roll values of the participant s head location 	the time series represent two channels of yaw pitch and roll values of the participant s head location 
0	128	8449	we are motivated by the opportunity of using machine learning on novel vr based mental healthcare datasets to improve understanding and diagnosis of mood and emotional disorders recently researchers have established a relationship between degree frequency and type of head movement and the degree of clinical depression experienced by an individual we are given the time series data gathered from oculus sensors for 148 engage study participants the time series represent two channels of yaw pitch and roll values of the participant s head location 	this is our input data and our output labels are binary true if the participant s gad7 a score for level of anxiety value is above 10 false otherwise 
1	128	8450	we are motivated by the opportunity of using machine learning on novel vr based mental healthcare datasets to improve understanding and diagnosis of mood and emotional disorders recently researchers have established a relationship between degree frequency and type of head movement and the degree of clinical depression experienced by an individual we are given the time series data gathered from oculus sensors for 148 engage study participants the time series represent two channels of yaw pitch and roll values of the participant s head location 	we featurize this data in two ways 1 summary statistics across time and 2 a 30 point discrete fourier transform 
1	128	8451	we are motivated by the opportunity of using machine learning on novel vr based mental healthcare datasets to improve understanding and diagnosis of mood and emotional disorders recently researchers have established a relationship between degree frequency and type of head movement and the degree of clinical depression experienced by an individual we are given the time series data gathered from oculus sensors for 148 engage study participants the time series represent two channels of yaw pitch and roll values of the participant s head location 	we feed both of these input featurizations to three different classifiers logistic regression naive bayes and decision tree 
0	128	8452	we are motivated by the opportunity of using machine learning on novel vr based mental healthcare datasets to improve understanding and diagnosis of mood and emotional disorders recently researchers have established a relationship between degree frequency and type of head movement and the degree of clinical depression experienced by an individual we are given the time series data gathered from oculus sensors for 148 engage study participants the time series represent two channels of yaw pitch and roll values of the participant s head location 	so in total there are six classification nodes 
1	128	8453	we are motivated by the opportunity of using machine learning on novel vr based mental healthcare datasets to improve understanding and diagnosis of mood and emotional disorders recently researchers have established a relationship between degree frequency and type of head movement and the degree of clinical depression experienced by an individual we are given the time series data gathered from oculus sensors for 148 engage study participants the time series represent two channels of yaw pitch and roll values of the participant s head location 	the outputs of these 6 classifiers are fed into an ensembled learned weights voting node and the output of that is our final prediction 
1	128	8454	we are motivated by the opportunity of using machine learning on novel vr based mental healthcare datasets to improve understanding and diagnosis of mood and emotional disorders recently researchers have established a relationship between degree frequency and type of head movement and the degree of clinical depression experienced by an individual we are given the time series data gathered from oculus sensors for 148 engage study participants the time series represent two channels of yaw pitch and roll values of the participant s head location 	we compare the efficacy of this approach for several weighting threshold schemes against a convolutional neural network using the unfeaturized time series as input and also against predict 1 and predict random baselines finding our best model to improve upon baselines 
1	128	8455	to our knowledge this is the first time the task of psychological disorder prediction using machine learning has been explored for time series head movement datasets gathered from virtual reality experiences that said we drew inspiration from discussion with members of the panlab and chose machine learning methods based on their success with similar tasks we chose to use a convolutional neural network in hope that it captures patterns that hand crafted features do not 	to our knowledge this is the first time the task of psychological disorder prediction using machine learning has been explored for time series head movement datasets gathered from virtual reality experiences 
1	128	8456	to our knowledge this is the first time the task of psychological disorder prediction using machine learning has been explored for time series head movement datasets gathered from virtual reality experiences that said we drew inspiration from discussion with members of the panlab and chose machine learning methods based on their success with similar tasks we chose to use a convolutional neural network in hope that it captures patterns that hand crafted features do not 	that said we drew inspiration from discussion with members of the panlab and chose machine learning methods based on their success with similar tasks 
1	128	8457	to our knowledge this is the first time the task of psychological disorder prediction using machine learning has been explored for time series head movement datasets gathered from virtual reality experiences that said we drew inspiration from discussion with members of the panlab and chose machine learning methods based on their success with similar tasks we chose to use a convolutional neural network in hope that it captures patterns that hand crafted features do not 	we chose to use a convolutional neural network in hope that it captures patterns that hand crafted features do not 
1	128	8458	to our knowledge this is the first time the task of psychological disorder prediction using machine learning has been explored for time series head movement datasets gathered from virtual reality experiences that said we drew inspiration from discussion with members of the panlab and chose machine learning methods based on their success with similar tasks we chose to use a convolutional neural network in hope that it captures patterns that hand crafted features do not 	a study done by hoppe and bulling proposes a convolutional neural network for learning featurizations for classification tasks on eye movement data
1	128	8459	the data we have available to us is head angle data recorded over time by two sensors of an oculus vr the data is recorded at 0 2 6 and 12 month sessions and there are five types of virtual reality experiences during each session an example of the time series yaw pitch roll data for a single vr experience can be seen in in our experiments we attempt to predict the gad7 score an indicator of anxiety based on the feature vector discussed above 	the data we have available to us is head angle data recorded over time by two sensors of an oculus vr 
0	128	8460	the data we have available to us is head angle data recorded over time by two sensors of an oculus vr the data is recorded at 0 2 6 and 12 month sessions and there are five types of virtual reality experiences during each session an example of the time series yaw pitch roll data for a single vr experience can be seen in in our experiments we attempt to predict the gad7 score an indicator of anxiety based on the feature vector discussed above 	the data is recorded at 0 2 6 and 12 month sessions and there are five types of virtual reality experiences during each session 
1	128	8461	the data we have available to us is head angle data recorded over time by two sensors of an oculus vr the data is recorded at 0 2 6 and 12 month sessions and there are five types of virtual reality experiences during each session an example of the time series yaw pitch roll data for a single vr experience can be seen in in our experiments we attempt to predict the gad7 score an indicator of anxiety based on the feature vector discussed above 	an example of the time series yaw pitch roll data for a single vr experience can be seen in in our experiments we attempt to predict the gad7 score an indicator of anxiety based on the feature vector discussed above 
1	128	8462	the data we have available to us is head angle data recorded over time by two sensors of an oculus vr the data is recorded at 0 2 6 and 12 month sessions and there are five types of virtual reality experiences during each session an example of the time series yaw pitch roll data for a single vr experience can be seen in in our experiments we attempt to predict the gad7 score an indicator of anxiety based on the feature vector discussed above 	computing these features requires that we have the associated score labels and the tracking data for each experiment type for a given participant and month v gad7 148 is the number of pairs of participant month for which we have the required data 
1	128	8463	the data we have available to us is head angle data recorded over time by two sensors of an oculus vr the data is recorded at 0 2 6 and 12 month sessions and there are five types of virtual reality experiences during each session an example of the time series yaw pitch roll data for a single vr experience can be seen in in our experiments we attempt to predict the gad7 score an indicator of anxiety based on the feature vector discussed above 	because of the small number of examples we use a hold one out cross validation scheme and for testing we set aside 30 test examples 
1	128	8464	the data we have available to us is head angle data recorded over time by two sensors of an oculus vr the data is recorded at 0 2 6 and 12 month sessions and there are five types of virtual reality experiences during each session an example of the time series yaw pitch roll data for a single vr experience can be seen in in our experiments we attempt to predict the gad7 score an indicator of anxiety based on the feature vector discussed above 	this leaves us with a training matrix x gad7 and a label matrix y gad7 whose rows correspond respectively to the feature and label for all p m leftover for training 
1	128	8465	the data we have available to us is head angle data recorded over time by two sensors of an oculus vr the data is recorded at 0 2 6 and 12 month sessions and there are five types of virtual reality experiences during each session an example of the time series yaw pitch roll data for a single vr experience can be seen in in our experiments we attempt to predict the gad7 score an indicator of anxiety based on the feature vector discussed above 	in our experiments x gad7 has 118 observations and 120 summary statistics features or 360 frequency domain features depending on the featurization scheme used 
1	128	8466	we compute summary statistics on both the time series and the differences between subsequent elements of the time series as our features each piece of head tracking data is a matrix t whose columns are the roll pitch yaw gathered by sensor 1 concatenated with that of sensor 2 now we compute a difference matrix then for every column of t we compute the mean and variance and every column of d we compute the sum and variance we concatenate these statistics across all experience types to form a feature vector for each participant month pair 	we compute summary statistics on both the time series and the differences between subsequent elements of the time series as our features each piece of head tracking data is a matrix t whose columns are the roll pitch yaw gathered by sensor 1 concatenated with that of sensor 2 
1	128	8467	we compute summary statistics on both the time series and the differences between subsequent elements of the time series as our features each piece of head tracking data is a matrix t whose columns are the roll pitch yaw gathered by sensor 1 concatenated with that of sensor 2 now we compute a difference matrix then for every column of t we compute the mean and variance and every column of d we compute the sum and variance we concatenate these statistics across all experience types to form a feature vector for each participant month pair 	now we compute a difference matrix then for every column of t we compute the mean and variance and every column of d we compute the sum and variance 
1	128	8468	we compute summary statistics on both the time series and the differences between subsequent elements of the time series as our features each piece of head tracking data is a matrix t whose columns are the roll pitch yaw gathered by sensor 1 concatenated with that of sensor 2 now we compute a difference matrix then for every column of t we compute the mean and variance and every column of d we compute the sum and variance we concatenate these statistics across all experience types to form a feature vector for each participant month pair 	we concatenate these statistics across all experience types to form a feature vector for each participant month pair 
0	128	8469	we use a 30 pt discrete fourier transform dft computed on each time axis for our second featurization the n point dft is defined as follows where x k is called the discrete fourier transform of the sequence x n x k can be thought of as periodic with period n or as of length n hence it is called the n point dft and for our purposes is used to compute n features there exists an aptly named algorithm called the fast fourier transform fft which is a computationally efficient implementation of an n point dft 	we use a 30 pt discrete fourier transform dft computed on each time axis for our second featurization 
0	128	8470	we use a 30 pt discrete fourier transform dft computed on each time axis for our second featurization the n point dft is defined as follows where x k is called the discrete fourier transform of the sequence x n x k can be thought of as periodic with period n or as of length n hence it is called the n point dft and for our purposes is used to compute n features there exists an aptly named algorithm called the fast fourier transform fft which is a computationally efficient implementation of an n point dft 	the n point dft is defined as follows where x k is called the discrete fourier transform of the sequence x n 
1	128	8471	we use a 30 pt discrete fourier transform dft computed on each time axis for our second featurization the n point dft is defined as follows where x k is called the discrete fourier transform of the sequence x n x k can be thought of as periodic with period n or as of length n hence it is called the n point dft and for our purposes is used to compute n features there exists an aptly named algorithm called the fast fourier transform fft which is a computationally efficient implementation of an n point dft 	x k can be thought of as periodic with period n or as of length n hence it is called the n point dft and for our purposes is used to compute n features there exists an aptly named algorithm called the fast fourier transform fft which is a computationally efficient implementation of an n point dft 
0	128	8472	we use a 30 pt discrete fourier transform dft computed on each time axis for our second featurization the n point dft is defined as follows where x k is called the discrete fourier transform of the sequence x n x k can be thought of as periodic with period n or as of length n hence it is called the n point dft and for our purposes is used to compute n features there exists an aptly named algorithm called the fast fourier transform fft which is a computationally efficient implementation of an n point dft 	we use numpy s fft implementation to featurize our time series head movement data across each each channel s yaw pitch and roll time axis 
1	128	8473	the primary challenge of our task beyond uncertainty in featurization due to the novelty of the task is data scarcity we have only 118 participants on which to train our systems and a complex phenomenon to model we pursue three broad avenues to tackle these challenges 1 simple classifiers with different inductive biases and trained off of each of our two featurizations 2 ensembles of simple classifiers with vote weighting determined through random search and 3 a small convolutional neural network with pooling across time simple models we train multinomial naive bayes classifiers decision trees and logistic regression classifiers on each of our 2 featurization types for a total of 6 simple classifiers 	the primary challenge of our task beyond uncertainty in featurization due to the novelty of the task is data scarcity 
0	128	8474	the primary challenge of our task beyond uncertainty in featurization due to the novelty of the task is data scarcity we have only 118 participants on which to train our systems and a complex phenomenon to model we pursue three broad avenues to tackle these challenges 1 simple classifiers with different inductive biases and trained off of each of our two featurizations 2 ensembles of simple classifiers with vote weighting determined through random search and 3 a small convolutional neural network with pooling across time simple models we train multinomial naive bayes classifiers decision trees and logistic regression classifiers on each of our 2 featurization types for a total of 6 simple classifiers 	we have only 118 participants on which to train our systems and a complex phenomenon to model 
1	128	8475	the primary challenge of our task beyond uncertainty in featurization due to the novelty of the task is data scarcity we have only 118 participants on which to train our systems and a complex phenomenon to model we pursue three broad avenues to tackle these challenges 1 simple classifiers with different inductive biases and trained off of each of our two featurizations 2 ensembles of simple classifiers with vote weighting determined through random search and 3 a small convolutional neural network with pooling across time simple models we train multinomial naive bayes classifiers decision trees and logistic regression classifiers on each of our 2 featurization types for a total of 6 simple classifiers 	we pursue three broad avenues to tackle these challenges 1 simple classifiers with different inductive biases and trained off of each of our two featurizations 2 ensembles of simple classifiers with vote weighting determined through random search and 3 a small convolutional neural network with pooling across time simple models we train multinomial naive bayes classifiers decision trees and logistic regression classifiers on each of our 2 featurization types for a total of 6 simple classifiers 
1	128	8476	the primary challenge of our task beyond uncertainty in featurization due to the novelty of the task is data scarcity we have only 118 participants on which to train our systems and a complex phenomenon to model we pursue three broad avenues to tackle these challenges 1 simple classifiers with different inductive biases and trained off of each of our two featurizations 2 ensembles of simple classifiers with vote weighting determined through random search and 3 a small convolutional neural network with pooling across time simple models we train multinomial naive bayes classifiers decision trees and logistic regression classifiers on each of our 2 featurization types for a total of 6 simple classifiers 	in short the naive bayes models each observation as having been generated by sampling the class and then sampling all features independently given the class the decision tree iteratively splits the data to minimize the gini loss based on individual parameter differences and the logisitic regression learns a linear combination of the features to minimize the difference between the true and predicted probabilities of anxiety disorder for each patient 
1	128	8477	the primary challenge of our task beyond uncertainty in featurization due to the novelty of the task is data scarcity we have only 118 participants on which to train our systems and a complex phenomenon to model we pursue three broad avenues to tackle these challenges 1 simple classifiers with different inductive biases and trained off of each of our two featurizations 2 ensembles of simple classifiers with vote weighting determined through random search and 3 a small convolutional neural network with pooling across time simple models we train multinomial naive bayes classifiers decision trees and logistic regression classifiers on each of our 2 featurization types for a total of 6 simple classifiers 	we ran manual search in development deciding to use a max tree depth of 5 and a regularization precision of 1 for our logistic regression ensemble weighting through random search because of the high variance nature of the data and the different inductive biases in each of our featurizations and simple model architectures we hypothesized that an ensemble of models may improve over the performance of any individual model 
0	128	8478	the primary challenge of our task beyond uncertainty in featurization due to the novelty of the task is data scarcity we have only 118 participants on which to train our systems and a complex phenomenon to model we pursue three broad avenues to tackle these challenges 1 simple classifiers with different inductive biases and trained off of each of our two featurizations 2 ensembles of simple classifiers with vote weighting determined through random search and 3 a small convolutional neural network with pooling across time simple models we train multinomial naive bayes classifiers decision trees and logistic regression classifiers on each of our 2 featurization types for a total of 6 simple classifiers 	we start with a simple majority vote ensembling baseline 
1	128	8479	the primary challenge of our task beyond uncertainty in featurization due to the novelty of the task is data scarcity we have only 118 participants on which to train our systems and a complex phenomenon to model we pursue three broad avenues to tackle these challenges 1 simple classifiers with different inductive biases and trained off of each of our two featurizations 2 ensembles of simple classifiers with vote weighting determined through random search and 3 a small convolutional neural network with pooling across time simple models we train multinomial naive bayes classifiers decision trees and logistic regression classifiers on each of our 2 featurization types for a total of 6 simple classifiers 	to leverage the intuition that 1 the individual simple models are not of the same quality and 2 the tradeoff between recall and precision may be controlled through the voting threshold for predicting anxiety we run a random search on our development set to find high quality model weights and decision thresholds for precision recall and f1 specifically for each of the 6 simple models we draw a value from a gamma distribution with parameters shape 2 scale 1 and then normalize the weights by the sum of all 
1	128	8480	the primary challenge of our task beyond uncertainty in featurization due to the novelty of the task is data scarcity we have only 118 participants on which to train our systems and a complex phenomenon to model we pursue three broad avenues to tackle these challenges 1 simple classifiers with different inductive biases and trained off of each of our two featurizations 2 ensembles of simple classifiers with vote weighting determined through random search and 3 a small convolutional neural network with pooling across time simple models we train multinomial naive bayes classifiers decision trees and logistic regression classifiers on each of our 2 featurization types for a total of 6 simple classifiers 	we chose this distribution because it should give some variation between model weights without deviation in extremes 
1	128	8481	the primary challenge of our task beyond uncertainty in featurization due to the novelty of the task is data scarcity we have only 118 participants on which to train our systems and a complex phenomenon to model we pursue three broad avenues to tackle these challenges 1 simple classifiers with different inductive biases and trained off of each of our two featurizations 2 ensembles of simple classifiers with vote weighting determined through random search and 3 a small convolutional neural network with pooling across time simple models we train multinomial naive bayes classifiers decision trees and logistic regression classifiers on each of our 2 featurization types for a total of 6 simple classifiers 	for the threshold we sampled from a uniform between 4 and 6 as we found that sampling from a greater range led to degenerate results like extremely high recall by predicting all participants to have high anxiety 
1	128	8482	the primary challenge of our task beyond uncertainty in featurization due to the novelty of the task is data scarcity we have only 118 participants on which to train our systems and a complex phenomenon to model we pursue three broad avenues to tackle these challenges 1 simple classifiers with different inductive biases and trained off of each of our two featurizations 2 ensembles of simple classifiers with vote weighting determined through random search and 3 a small convolutional neural network with pooling across time simple models we train multinomial naive bayes classifiers decision trees and logistic regression classifiers on each of our 2 featurization types for a total of 6 simple classifiers 	for each sampled set of hyperparameters we ran all models 5 times using hold one out evaluation and averaged the scores 
1	128	8483	the primary challenge of our task beyond uncertainty in featurization due to the novelty of the task is data scarcity we have only 118 participants on which to train our systems and a complex phenomenon to model we pursue three broad avenues to tackle these challenges 1 simple classifiers with different inductive biases and trained off of each of our two featurizations 2 ensembles of simple classifiers with vote weighting determined through random search and 3 a small convolutional neural network with pooling across time simple models we train multinomial naive bayes classifiers decision trees and logistic regression classifiers on each of our 2 featurization types for a total of 6 simple classifiers 	we then picked the set of hyperparameters for each of f1 precision and recall that led to the best development result to run on the test set 
0	128	8484	the primary challenge of our task beyond uncertainty in featurization due to the novelty of the task is data scarcity we have only 118 participants on which to train our systems and a complex phenomenon to model we pursue three broad avenues to tackle these challenges 1 simple classifiers with different inductive biases and trained off of each of our two featurizations 2 ensembles of simple classifiers with vote weighting determined through random search and 3 a small convolutional neural network with pooling across time simple models we train multinomial naive bayes classifiers decision trees and logistic regression classifiers on each of our 2 featurization types for a total of 6 simple classifiers 	this model is visualized in
1	128	8485	for our second model we consider a small 1 dimensional convolutional neural network that uses the unfeaturized raw head movement data across the 6 channels of roll pitch yaw for both the right and left sensors we felt that a 1d cnn model was well suited for our data given that we were working with time series data where the exact time of head movement may not be as important as the amount or speed of movement in short intervals for predicting anxiety levels our cnn architecture consists of five layers 	for our second model we consider a small 1 dimensional convolutional neural network that uses the unfeaturized raw head movement data across the 6 channels of roll pitch yaw for both the right and left sensors 
1	128	8486	for our second model we consider a small 1 dimensional convolutional neural network that uses the unfeaturized raw head movement data across the 6 channels of roll pitch yaw for both the right and left sensors we felt that a 1d cnn model was well suited for our data given that we were working with time series data where the exact time of head movement may not be as important as the amount or speed of movement in short intervals for predicting anxiety levels our cnn architecture consists of five layers 	we felt that a 1d cnn model was well suited for our data given that we were working with time series data where the exact time of head movement may not be as important as the amount or speed of movement in short intervals for predicting anxiety levels 
0	128	8487	for our second model we consider a small 1 dimensional convolutional neural network that uses the unfeaturized raw head movement data across the 6 channels of roll pitch yaw for both the right and left sensors we felt that a 1d cnn model was well suited for our data given that we were working with time series data where the exact time of head movement may not be as important as the amount or speed of movement in short intervals for predicting anxiety levels our cnn architecture consists of five layers 	our cnn architecture consists of five layers 
1	128	8488	for our second model we consider a small 1 dimensional convolutional neural network that uses the unfeaturized raw head movement data across the 6 channels of roll pitch yaw for both the right and left sensors we felt that a 1d cnn model was well suited for our data given that we were working with time series data where the exact time of head movement may not be as important as the amount or speed of movement in short intervals for predicting anxiety levels our cnn architecture consists of five layers 	the first layer is a 1 dimensional convolutional layer followed by an average pooling layer a relu activation layer and two dense layers that also have relu activations 
1	128	8489	for our second model we consider a small 1 dimensional convolutional neural network that uses the unfeaturized raw head movement data across the 6 channels of roll pitch yaw for both the right and left sensors we felt that a 1d cnn model was well suited for our data given that we were working with time series data where the exact time of head movement may not be as important as the amount or speed of movement in short intervals for predicting anxiety levels our cnn architecture consists of five layers 	we included dropout as a form of regularization as well and chose the dropout rate during our hyperparameter search on the development set 
0	128	8490	for our second model we consider a small 1 dimensional convolutional neural network that uses the unfeaturized raw head movement data across the 6 channels of roll pitch yaw for both the right and left sensors we felt that a 1d cnn model was well suited for our data given that we were working with time series data where the exact time of head movement may not be as important as the amount or speed of movement in short intervals for predicting anxiety levels our cnn architecture consists of five layers 	this model s layout is visualized in
1	128	8491	because the split of high anxiety to low anxiety participants was roughly 20 to 80 we report precision recall and f1 score across all experiments 	because the split of high anxiety to low anxiety participants was roughly 20 to 80 we report precision recall and f1 score across all experiments 
1	128	8492	in our random search we sampled 50 ensemble configurations a few interesting patterns emerged the top recall ensemble simply chose to ignore all models but the naive bayes on summary statistic features 	in our random search we sampled 50 ensemble configurations 
0	128	8493	in our random search we sampled 50 ensemble configurations a few interesting patterns emerged the top recall ensemble simply chose to ignore all models but the naive bayes on summary statistic features 	a few interesting patterns emerged 
1	128	8494	in our random search we sampled 50 ensemble configurations a few interesting patterns emerged the top recall ensemble simply chose to ignore all models but the naive bayes on summary statistic features 	the top recall ensemble simply chose to ignore all models but the naive bayes on summary statistic features 
1	128	8495	in our random search we sampled 50 ensemble configurations a few interesting patterns emerged the top recall ensemble simply chose to ignore all models but the naive bayes on summary statistic features 	the top precision ensemble as might be expected used the highest threshold of the three at 60 of voting weight required to predict high anxiety 
1	128	8496	in our random search we sampled 50 ensemble configurations a few interesting patterns emerged the top recall ensemble simply chose to ignore all models but the naive bayes on summary statistic features 	the top f1 ensemble assigned almost all its weight equally across the summary statistic models 
1	128	8497	a convolutional neural network layer passes the same feature detector across all spatial steps of the data and in our case uses a pooling function to aggregate features across all timesteps for our cnn we found that the set of hyperparameters that resulted in the highest f1 score on the development set was a filter count of 16 a kernel size of 10 and a dropout rate of 0 5 and ultimately used this choice of values for our final model we report numbers on cnns trained only the first 400 timesteps of the data as the results were robust to the number of timesteps used 	a convolutional neural network layer passes the same feature detector across all spatial steps of the data and in our case uses a pooling function to aggregate features across all timesteps 
1	128	8498	a convolutional neural network layer passes the same feature detector across all spatial steps of the data and in our case uses a pooling function to aggregate features across all timesteps for our cnn we found that the set of hyperparameters that resulted in the highest f1 score on the development set was a filter count of 16 a kernel size of 10 and a dropout rate of 0 5 and ultimately used this choice of values for our final model we report numbers on cnns trained only the first 400 timesteps of the data as the results were robust to the number of timesteps used 	for our cnn we found that the set of hyperparameters that resulted in the highest f1 score on the development set was a filter count of 16 a kernel size of 10 and a dropout rate of 0 5 and ultimately used this choice of values for our final model 
1	128	8499	a convolutional neural network layer passes the same feature detector across all spatial steps of the data and in our case uses a pooling function to aggregate features across all timesteps for our cnn we found that the set of hyperparameters that resulted in the highest f1 score on the development set was a filter count of 16 a kernel size of 10 and a dropout rate of 0 5 and ultimately used this choice of values for our final model we report numbers on cnns trained only the first 400 timesteps of the data as the results were robust to the number of timesteps used 	we report numbers on cnns trained only the first 400 timesteps of the data as the results were robust to the number of timesteps used 
1	128	8500	a convolutional neural network layer passes the same feature detector across all spatial steps of the data and in our case uses a pooling function to aggregate features across all timesteps for our cnn we found that the set of hyperparameters that resulted in the highest f1 score on the development set was a filter count of 16 a kernel size of 10 and a dropout rate of 0 5 and ultimately used this choice of values for our final model we report numbers on cnns trained only the first 400 timesteps of the data as the results were robust to the number of timesteps used 	one interesting takeaway from our hyperparameter search was that smaller models tended to do best reflecting small data problem but our smallest models started to degrade as well perhaps signaling limitations of raw head movement feature format with so little data 
1	128	8501	our results are summarized in our ensembles underperform our expectations perhaps due to the high variation in model quality and the bias of our random search to being close to uniform however while they do not in general outperform the individual models we do see the desired behavior of each weighted ensemble f1 precision recall tending to bias towards that metric while not sacrificing too much on the other metrics finally our cnn model underperforms all baselines despite hyperparameter tuning to attempt to avoid overfitting 	our results are summarized in our ensembles underperform our expectations perhaps due to the high variation in model quality and the bias of our random search to being close to uniform 
1	128	8502	our results are summarized in our ensembles underperform our expectations perhaps due to the high variation in model quality and the bias of our random search to being close to uniform however while they do not in general outperform the individual models we do see the desired behavior of each weighted ensemble f1 precision recall tending to bias towards that metric while not sacrificing too much on the other metrics finally our cnn model underperforms all baselines despite hyperparameter tuning to attempt to avoid overfitting 	however while they do not in general outperform the individual models we do see the desired behavior of each weighted ensemble f1 precision recall tending to bias towards that metric while not sacrificing too much on the other metrics 
1	128	8503	our results are summarized in our ensembles underperform our expectations perhaps due to the high variation in model quality and the bias of our random search to being close to uniform however while they do not in general outperform the individual models we do see the desired behavior of each weighted ensemble f1 precision recall tending to bias towards that metric while not sacrificing too much on the other metrics finally our cnn model underperforms all baselines despite hyperparameter tuning to attempt to avoid overfitting 	finally our cnn model underperforms all baselines despite hyperparameter tuning to attempt to avoid overfitting 
1	128	8504	our results are summarized in our ensembles underperform our expectations perhaps due to the high variation in model quality and the bias of our random search to being close to uniform however while they do not in general outperform the individual models we do see the desired behavior of each weighted ensemble f1 precision recall tending to bias towards that metric while not sacrificing too much on the other metrics finally our cnn model underperforms all baselines despite hyperparameter tuning to attempt to avoid overfitting 	we expect that this is because the small amount of data and somewhat abstract problem make joint feature detection and generalization infeasible finally we conducted qualitative error exploration on our development set 
0	128	8505	our results are summarized in our ensembles underperform our expectations perhaps due to the high variation in model quality and the bias of our random search to being close to uniform however while they do not in general outperform the individual models we do see the desired behavior of each weighted ensemble f1 precision recall tending to bias towards that metric while not sacrificing too much on the other metrics finally our cnn model underperforms all baselines despite hyperparameter tuning to attempt to avoid overfitting 	we sampled two highanxiety participants one of which our naive bayes classified correctly and the other incorrectly 
0	128	8506	our results are summarized in our ensembles underperform our expectations perhaps due to the high variation in model quality and the bias of our random search to being close to uniform however while they do not in general outperform the individual models we do see the desired behavior of each weighted ensemble f1 precision recall tending to bias towards that metric while not sacrificing too much on the other metrics finally our cnn model underperforms all baselines despite hyperparameter tuning to attempt to avoid overfitting 	qualitatively the examples seem quite similar in the amount of movement as seen in
1	128	8507	in this work we explored head movements a noisy signal in the medical domain which we confirm to be useful for predicting patient anxiety disorder we faced an inherently small data problem since controlled participation in a vr experience is costly to collect as such we focused on featurization and model comparison to determine what features and methods are promising for evaluating anxiety through head movement 	in this work we explored head movements a noisy signal in the medical domain which we confirm to be useful for predicting patient anxiety disorder 
1	128	8508	in this work we explored head movements a noisy signal in the medical domain which we confirm to be useful for predicting patient anxiety disorder we faced an inherently small data problem since controlled participation in a vr experience is costly to collect as such we focused on featurization and model comparison to determine what features and methods are promising for evaluating anxiety through head movement 	we faced an inherently small data problem since controlled participation in a vr experience is costly to collect 
1	128	8509	in this work we explored head movements a noisy signal in the medical domain which we confirm to be useful for predicting patient anxiety disorder we faced an inherently small data problem since controlled participation in a vr experience is costly to collect as such we focused on featurization and model comparison to determine what features and methods are promising for evaluating anxiety through head movement 	as such we focused on featurization and model comparison to determine what features and methods are promising for evaluating anxiety through head movement 
1	128	8510	in this work we explored head movements a noisy signal in the medical domain which we confirm to be useful for predicting patient anxiety disorder we faced an inherently small data problem since controlled participation in a vr experience is costly to collect as such we focused on featurization and model comparison to determine what features and methods are promising for evaluating anxiety through head movement 	our best model improved on an informationless baseline by 10 2 points f1 a modest but potentially useful result when combined with other predictors of anxiety in a hypothetical future system 
1	128	8511	in this work we explored head movements a noisy signal in the medical domain which we confirm to be useful for predicting patient anxiety disorder we faced an inherently small data problem since controlled participation in a vr experience is costly to collect as such we focused on featurization and model comparison to determine what features and methods are promising for evaluating anxiety through head movement 	by ensembling models and running a random search on the ensemble voting weights and decision threshold we were able to control the tradeoff between precision and recall but not improve upon the f1 score of individual models a mixed result 
1	128	8512	in this work we explored head movements a noisy signal in the medical domain which we confirm to be useful for predicting patient anxiety disorder we faced an inherently small data problem since controlled participation in a vr experience is costly to collect as such we focused on featurization and model comparison to determine what features and methods are promising for evaluating anxiety through head movement 	for thoroughness we compared our featurizations and simple models to a low parameter cnn finding as we expected that the cnn underperformed models with hand crafted features 
0	128	8513	in this work we explored head movements a noisy signal in the medical domain which we confirm to be useful for predicting patient anxiety disorder we faced an inherently small data problem since controlled participation in a vr experience is costly to collect as such we focused on featurization and model comparison to determine what features and methods are promising for evaluating anxiety through head movement 	we hypothesize this was due to the rather small data setting 
1	128	8514	in this work we explored head movements a noisy signal in the medical domain which we confirm to be useful for predicting patient anxiety disorder we faced an inherently small data problem since controlled participation in a vr experience is costly to collect as such we focused on featurization and model comparison to determine what features and methods are promising for evaluating anxiety through head movement 	our findings suggest that head movement data has signal for predicting anxiety disorder and suggest that future work may leverage richer representations of each patient in combination with head tracking to improve predictiveness and eventually improve professionals ability to care for patients 
1	129	8515	in this paper we explore the applications of machine learning to sports betting by focusing on predicting the number of total points scored by both teams in an nba game we use neural networks as well as recurrent models for this task and manage to achieve results that are similar to those of the sports books on average our best models can beat the house 51 5 of the time 	in this paper we explore the applications of machine learning to sports betting by focusing on predicting the number of total points scored by both teams in an nba game 
1	129	8516	in this paper we explore the applications of machine learning to sports betting by focusing on predicting the number of total points scored by both teams in an nba game we use neural networks as well as recurrent models for this task and manage to achieve results that are similar to those of the sports books on average our best models can beat the house 51 5 of the time 	we use neural networks as well as recurrent models for this task and manage to achieve results that are similar to those of the sports books 
0	129	8517	in this paper we explore the applications of machine learning to sports betting by focusing on predicting the number of total points scored by both teams in an nba game we use neural networks as well as recurrent models for this task and manage to achieve results that are similar to those of the sports books on average our best models can beat the house 51 5 of the time 	on average our best models can beat the house 51 5 of the time 
0	129	8518	last may the united states supreme court legalized sports betting while it is still up to every state to pass legislation on the issues many have already been working on bills as tracked by espn eight states have already legalized sports gambling with two more states having signed bills that will legalize gambling in the near future 	last may the united states supreme court legalized sports betting 
0	129	8519	last may the united states supreme court legalized sports betting while it is still up to every state to pass legislation on the issues many have already been working on bills as tracked by espn eight states have already legalized sports gambling with two more states having signed bills that will legalize gambling in the near future 	while it is still up to every state to pass legislation on the issues many have already been working on bills 
0	129	8520	last may the united states supreme court legalized sports betting while it is still up to every state to pass legislation on the issues many have already been working on bills as tracked by espn eight states have already legalized sports gambling with two more states having signed bills that will legalize gambling in the near future 	as tracked by espn eight states have already legalized sports gambling with two more states having signed bills that will legalize gambling in the near future 
1	129	8521	last may the united states supreme court legalized sports betting while it is still up to every state to pass legislation on the issues many have already been working on bills as tracked by espn eight states have already legalized sports gambling with two more states having signed bills that will legalize gambling in the near future 	with its ruling the supreme court paved way to the opening of whole new legal and thus taxable market of size 150 billion according to the american gaming association in this project we will explore the applications of machine learning in the field of sports betting using a case study of the national basketball association 
0	129	8522	last may the united states supreme court legalized sports betting while it is still up to every state to pass legislation on the issues many have already been working on bills as tracked by espn eight states have already legalized sports gambling with two more states having signed bills that will legalize gambling in the near future 	more specifically we wish to design sets of models that predict betting indicators for nba matches 
0	129	8523	last may the united states supreme court legalized sports betting while it is still up to every state to pass legislation on the issues many have already been working on bills as tracked by espn eight states have already legalized sports gambling with two more states having signed bills that will legalize gambling in the near future 	out of the three main indicators overunder money line and point spread we focus on the over under 
0	129	8524	last may the united states supreme court legalized sports betting while it is still up to every state to pass legislation on the issues many have already been working on bills as tracked by espn eight states have already legalized sports gambling with two more states having signed bills that will legalize gambling in the near future 	in other words we predict for every nba game how many combined points the two teams will score 
1	129	8525	last may the united states supreme court legalized sports betting while it is still up to every state to pass legislation on the issues many have already been working on bills as tracked by espn eight states have already legalized sports gambling with two more states having signed bills that will legalize gambling in the near future 	ideally we would be able to come up with an estimate for this indicator that is more accurate than that of some betting platforms and use this to our advantage to place bets 
0	129	8526	the paper predicting margin of victory in nfl games the paper football match prediction using deep learning 	the paper predicting margin of victory in nfl games the paper football match prediction using deep learning 
1	129	8527	the data collected so far can be classified into two groups betting odds data and game data which consists of both data describing team performance and player performance we found odds data on sports book review online a website that compiles betting data for every nba games since the 2007 2008 season the website offers a downloadable excel file for each season using a short script we were able to retrieve all the necessary target variables and the corresponding betting odds offered 	the data collected so far can be classified into two groups betting odds data and game data which consists of both data describing team performance and player performance we found odds data on sports book review online a website that compiles betting data for every nba games since the 2007 2008 season 
0	129	8528	the data collected so far can be classified into two groups betting odds data and game data which consists of both data describing team performance and player performance we found odds data on sports book review online a website that compiles betting data for every nba games since the 2007 2008 season the website offers a downloadable excel file for each season using a short script we were able to retrieve all the necessary target variables and the corresponding betting odds offered 	the website offers a downloadable excel file for each season 
0	129	8529	the data collected so far can be classified into two groups betting odds data and game data which consists of both data describing team performance and player performance we found odds data on sports book review online a website that compiles betting data for every nba games since the 2007 2008 season the website offers a downloadable excel file for each season using a short script we were able to retrieve all the necessary target variables and the corresponding betting odds offered 	using a short script we were able to retrieve all the necessary target variables and the corresponding betting odds offered 
1	129	8530	the data collected so far can be classified into two groups betting odds data and game data which consists of both data describing team performance and player performance we found odds data on sports book review online a website that compiles betting data for every nba games since the 2007 2008 season the website offers a downloadable excel file for each season using a short script we were able to retrieve all the necessary target variables and the corresponding betting odds offered 	the betting indicators scraped were the following i over under the total number of points scored in a game ii spread the number of points by which the home team wins or loses iii money line a number encoding the amount of money won from placing a bet on the winning team of a game in the rest of this project we focus on the total number of points scored and we use the over under data collected to solely evaluate our models and do not include the above data in our features for game data we retrieved data from basketball reference using fran goitia s nba crawler
0	129	8531	using the retrieved data from basketball reference we build a featurized dataset for every matchup between two teams we decide to look at both team s past three games we included simple features such as points scored points scored against or total rebounds and also more complicated metrics such as offensive rating and plus minus 	using the retrieved data from basketball reference we build a featurized dataset 
0	129	8532	using the retrieved data from basketball reference we build a featurized dataset for every matchup between two teams we decide to look at both team s past three games we included simple features such as points scored points scored against or total rebounds and also more complicated metrics such as offensive rating and plus minus 	for every matchup between two teams we decide to look at both team s past three games 
1	129	8533	using the retrieved data from basketball reference we build a featurized dataset for every matchup between two teams we decide to look at both team s past three games we included simple features such as points scored points scored against or total rebounds and also more complicated metrics such as offensive rating and plus minus 	we included simple features such as points scored points scored against or total rebounds and also more complicated metrics such as offensive rating and plus minus 
1	129	8534	using the retrieved data from basketball reference we build a featurized dataset for every matchup between two teams we decide to look at both team s past three games we included simple features such as points scored points scored against or total rebounds and also more complicated metrics such as offensive rating and plus minus 	in order to account for opponent strength we also added each opponent s season averages in all above metrics 
1	129	8535	using the retrieved data from basketball reference we build a featurized dataset for every matchup between two teams we decide to look at both team s past three games we included simple features such as points scored points scored against or total rebounds and also more complicated metrics such as offensive rating and plus minus 	finally to account for player fatigue we added the number of days since the last game as well as the distance traveled see we trained a random forest as a baseline before moving on to more complex models 
1	129	8536	this problem is similar to the user item rating prediction information about the outcome of previous games between other teams can inform our predicted output for the current matchup to leverage this we used singular value decomposition we hope that the rows of matrices u and v capture information about teams at home and away respectively and we then predict u i v t j total points scored in a game between teams i home and j away where b k is the k th row of matrix b 	this problem is similar to the user item rating prediction information about the outcome of previous games between other teams can inform our predicted output for the current matchup 
0	129	8537	this problem is similar to the user item rating prediction information about the outcome of previous games between other teams can inform our predicted output for the current matchup to leverage this we used singular value decomposition we hope that the rows of matrices u and v capture information about teams at home and away respectively and we then predict u i v t j total points scored in a game between teams i home and j away where b k is the k th row of matrix b 	to leverage this we used singular value decomposition we hope that the rows of matrices u and v capture information about teams at home and away respectively and we then predict u i v t j total points scored in a game between teams i home and j away where b k is the k th row of matrix b 
1	129	8538	like the collaborative filtering model the neural network captures information from the outcomes of previous games between other teams as during training the network is provided the results of previous games as input along with the identity of the two teams involved in the game however the neural network has an advantage over collaborative filtering in that it is also able to take features of both teams involved as inputs therefore it can draw on not only the outcomes of previous training examples but also the offensive rating of each of the teams involved over their past three games etc passing in the feature set created in 3 2 we train a neural network with fully connected layers and relu activations 	like the collaborative filtering model the neural network captures information from the outcomes of previous games between other teams as during training the network is provided the results of previous games as input along with the identity of the two teams involved in the game 
0	129	8539	like the collaborative filtering model the neural network captures information from the outcomes of previous games between other teams as during training the network is provided the results of previous games as input along with the identity of the two teams involved in the game however the neural network has an advantage over collaborative filtering in that it is also able to take features of both teams involved as inputs therefore it can draw on not only the outcomes of previous training examples but also the offensive rating of each of the teams involved over their past three games etc passing in the feature set created in 3 2 we train a neural network with fully connected layers and relu activations 	however the neural network has an advantage over collaborative filtering in that it is also able to take features of both teams involved as inputs 
1	129	8540	like the collaborative filtering model the neural network captures information from the outcomes of previous games between other teams as during training the network is provided the results of previous games as input along with the identity of the two teams involved in the game however the neural network has an advantage over collaborative filtering in that it is also able to take features of both teams involved as inputs therefore it can draw on not only the outcomes of previous training examples but also the offensive rating of each of the teams involved over their past three games etc passing in the feature set created in 3 2 we train a neural network with fully connected layers and relu activations 	therefore it can draw on not only the outcomes of previous training examples but also the offensive rating of each of the teams involved over their past three games etc passing in the feature set created in 3 2 we train a neural network with fully connected layers and relu activations 
0	129	8541	like the collaborative filtering model the neural network captures information from the outcomes of previous games between other teams as during training the network is provided the results of previous games as input along with the identity of the two teams involved in the game however the neural network has an advantage over collaborative filtering in that it is also able to take features of both teams involved as inputs therefore it can draw on not only the outcomes of previous training examples but also the offensive rating of each of the teams involved over their past three games etc passing in the feature set created in 3 2 we train a neural network with fully connected layers and relu activations 	the input data is flattened into a size of 1524 features 
0	129	8542	like the collaborative filtering model the neural network captures information from the outcomes of previous games between other teams as during training the network is provided the results of previous games as input along with the identity of the two teams involved in the game however the neural network has an advantage over collaborative filtering in that it is also able to take features of both teams involved as inputs therefore it can draw on not only the outcomes of previous training examples but also the offensive rating of each of the teams involved over their past three games etc passing in the feature set created in 3 2 we train a neural network with fully connected layers and relu activations 	the relu activations are used for ease of training and to reduce the likelihood of gradient vanishing see
1	129	8543	given that games are played sequentially we decided to use an lstm to process the past three games one by one as described in long short term memory for our task we implemented an lstm model with a fully connected layer at the end to output the number of points scored by the two teams for the desired game see	given that games are played sequentially we decided to use an lstm to process the past three games one by one 
1	129	8544	given that games are played sequentially we decided to use an lstm to process the past three games one by one as described in long short term memory for our task we implemented an lstm model with a fully connected layer at the end to output the number of points scored by the two teams for the desired game see	as described in long short term memory for our task we implemented an lstm model with a fully connected layer at the end to output the number of points scored by the two teams for the desired game see
1	129	8545	after tuning our models on the validation set to minimize validation mse we found the following architectures to perform best ii neural network we use four fully connected layers reducing the input of dimension 1524 to size 500 then 100 then 20 then finally a 1 dimensional output that predicts the overunder of the desired game see iii lstm we use one layer with hidden dimension 20 and a fully connected layer at the end see we evaluate the performance of our models primarily through the mean squared error mse between the over under value predicted by the model and the true point total observed in the game the over under values predicted by the sports books can serve as a benchmark as calculating the mse between the sports books predictions and the observed point totals gives us a sense of how well our models are performing relative to the books in addition we can also calculate the percentage of the time that our model would have correctly predicted that the true point total was either over or under the over under number provided by the sports books we found that the neural network correctly chooses over or under around 51 5 of the time while the collaborative filtering beats the line 51 of the time on average 	after tuning our models on the validation set to minimize validation mse we found the following architectures to perform best ii neural network we use four fully connected layers reducing the input of dimension 1524 to size 500 then 100 then 20 then finally a 1 dimensional output that predicts the overunder of the desired game see iii lstm we use one layer with hidden dimension 20 and a fully connected layer at the end see we evaluate the performance of our models primarily through the mean squared error mse between the over under value predicted by the model and the true point total observed in the game 
1	129	8546	after tuning our models on the validation set to minimize validation mse we found the following architectures to perform best ii neural network we use four fully connected layers reducing the input of dimension 1524 to size 500 then 100 then 20 then finally a 1 dimensional output that predicts the overunder of the desired game see iii lstm we use one layer with hidden dimension 20 and a fully connected layer at the end see we evaluate the performance of our models primarily through the mean squared error mse between the over under value predicted by the model and the true point total observed in the game the over under values predicted by the sports books can serve as a benchmark as calculating the mse between the sports books predictions and the observed point totals gives us a sense of how well our models are performing relative to the books in addition we can also calculate the percentage of the time that our model would have correctly predicted that the true point total was either over or under the over under number provided by the sports books we found that the neural network correctly chooses over or under around 51 5 of the time while the collaborative filtering beats the line 51 of the time on average 	the over under values predicted by the sports books can serve as a benchmark as calculating the mse between the sports books predictions and the observed point totals gives us a sense of how well our models are performing relative to the books 
1	129	8547	after tuning our models on the validation set to minimize validation mse we found the following architectures to perform best ii neural network we use four fully connected layers reducing the input of dimension 1524 to size 500 then 100 then 20 then finally a 1 dimensional output that predicts the overunder of the desired game see iii lstm we use one layer with hidden dimension 20 and a fully connected layer at the end see we evaluate the performance of our models primarily through the mean squared error mse between the over under value predicted by the model and the true point total observed in the game the over under values predicted by the sports books can serve as a benchmark as calculating the mse between the sports books predictions and the observed point totals gives us a sense of how well our models are performing relative to the books in addition we can also calculate the percentage of the time that our model would have correctly predicted that the true point total was either over or under the over under number provided by the sports books we found that the neural network correctly chooses over or under around 51 5 of the time while the collaborative filtering beats the line 51 of the time on average 	in addition we can also calculate the percentage of the time that our model would have correctly predicted that the true point total was either over or under the over under number provided by the sports books we found that the neural network correctly chooses over or under around 51 5 of the time while the collaborative filtering beats the line 51 of the time on average 
1	129	8548	due to the rapidly changing nature of the nba it is difficult to acquire sufficient training data that reflects the way the game is currently played this can be seen most prominently in the increasing frequency with which nba teams are attempting and making three point shots according to the nba teams around the league combined for around 15 000 three pointers made in the 2009 2010 season and broke the 25 000 mark last year 2017 2018 when we were training the models we found while while we had data available starting from it is worth noting that the collaborative filtering model which doesn t use any team features significantly outperformed the random forest 	due to the rapidly changing nature of the nba it is difficult to acquire sufficient training data that reflects the way the game is currently played 
0	129	8549	due to the rapidly changing nature of the nba it is difficult to acquire sufficient training data that reflects the way the game is currently played this can be seen most prominently in the increasing frequency with which nba teams are attempting and making three point shots according to the nba teams around the league combined for around 15 000 three pointers made in the 2009 2010 season and broke the 25 000 mark last year 2017 2018 when we were training the models we found while while we had data available starting from it is worth noting that the collaborative filtering model which doesn t use any team features significantly outperformed the random forest 	this can be seen most prominently in the increasing frequency with which nba teams are attempting and making three point shots 
1	129	8550	due to the rapidly changing nature of the nba it is difficult to acquire sufficient training data that reflects the way the game is currently played this can be seen most prominently in the increasing frequency with which nba teams are attempting and making three point shots according to the nba teams around the league combined for around 15 000 three pointers made in the 2009 2010 season and broke the 25 000 mark last year 2017 2018 when we were training the models we found while while we had data available starting from it is worth noting that the collaborative filtering model which doesn t use any team features significantly outperformed the random forest 	according to the nba teams around the league combined for around 15 000 three pointers made in the 2009 2010 season and broke the 25 000 mark last year 2017 2018 when we were training the models we found while while we had data available starting from it is worth noting that the collaborative filtering model which doesn t use any team features significantly outperformed the random forest 
1	129	8551	due to the rapidly changing nature of the nba it is difficult to acquire sufficient training data that reflects the way the game is currently played this can be seen most prominently in the increasing frequency with which nba teams are attempting and making three point shots according to the nba teams around the league combined for around 15 000 three pointers made in the 2009 2010 season and broke the 25 000 mark last year 2017 2018 when we were training the models we found while while we had data available starting from it is worth noting that the collaborative filtering model which doesn t use any team features significantly outperformed the random forest 	this shows the high variance in our data as well as the strong seasonal trends that a model needs to encompass in order to be accurate on this task on we achieved a test mse of 369 84 for our best model the neural network 
1	129	8552	due to the rapidly changing nature of the nba it is difficult to acquire sufficient training data that reflects the way the game is currently played this can be seen most prominently in the increasing frequency with which nba teams are attempting and making three point shots according to the nba teams around the league combined for around 15 000 three pointers made in the 2009 2010 season and broke the 25 000 mark last year 2017 2018 when we were training the models we found while while we had data available starting from it is worth noting that the collaborative filtering model which doesn t use any team features significantly outperformed the random forest 	this value is higher than the mse of the sports books predictions which is 320 70 but is relatively close to the level of accuracy of the books 
1	129	8553	due to the rapidly changing nature of the nba it is difficult to acquire sufficient training data that reflects the way the game is currently played this can be seen most prominently in the increasing frequency with which nba teams are attempting and making three point shots according to the nba teams around the league combined for around 15 000 three pointers made in the 2009 2010 season and broke the 25 000 mark last year 2017 2018 when we were training the models we found while while we had data available starting from it is worth noting that the collaborative filtering model which doesn t use any team features significantly outperformed the random forest 	when considering that nba teams are scoring roughly a combined 200 points per game mse values of over 300 from even sports books may seem high 
1	129	8554	due to the rapidly changing nature of the nba it is difficult to acquire sufficient training data that reflects the way the game is currently played this can be seen most prominently in the increasing frequency with which nba teams are attempting and making three point shots according to the nba teams around the league combined for around 15 000 three pointers made in the 2009 2010 season and broke the 25 000 mark last year 2017 2018 when we were training the models we found while while we had data available starting from it is worth noting that the collaborative filtering model which doesn t use any team features significantly outperformed the random forest 	however this is simply a reflection of the high variance nature of nba games where anything from injuries to players to a strong shooting night or the decision of a coach to rest his star players after building a lead can all lead to huge swings in the overall point totals of a game 
1	129	8555	due to the rapidly changing nature of the nba it is difficult to acquire sufficient training data that reflects the way the game is currently played this can be seen most prominently in the increasing frequency with which nba teams are attempting and making three point shots according to the nba teams around the league combined for around 15 000 three pointers made in the 2009 2010 season and broke the 25 000 mark last year 2017 2018 when we were training the models we found while while we had data available starting from it is worth noting that the collaborative filtering model which doesn t use any team features significantly outperformed the random forest 	the variance of the data was also reflected in the need for a relatively large weight decay parameter while training the neural network to prevent overfitting encouragingly when placing bets against the sports books on games in the test dataset using the neural network the predictions made were correct with respect to the actual outcomes 51 5 of the time as mentioned earlier 
1	129	8556	due to the rapidly changing nature of the nba it is difficult to acquire sufficient training data that reflects the way the game is currently played this can be seen most prominently in the increasing frequency with which nba teams are attempting and making three point shots according to the nba teams around the league combined for around 15 000 three pointers made in the 2009 2010 season and broke the 25 000 mark last year 2017 2018 when we were training the models we found while while we had data available starting from it is worth noting that the collaborative filtering model which doesn t use any team features significantly outperformed the random forest 	however note that in the real world the fees associated with betting mean that successful long term betting patterns need to be correct at least 52 53 of the time 
1	129	8557	due to the rapidly changing nature of the nba it is difficult to acquire sufficient training data that reflects the way the game is currently played this can be seen most prominently in the increasing frequency with which nba teams are attempting and making three point shots according to the nba teams around the league combined for around 15 000 three pointers made in the 2009 2010 season and broke the 25 000 mark last year 2017 2018 when we were training the models we found while while we had data available starting from it is worth noting that the collaborative filtering model which doesn t use any team features significantly outperformed the random forest 	in the future it would be interesting to explore if directly predicting whether the true result of the game is over of under a sports book s over under prediction would lead to higher levels of accuracy in this secondary metric 
1	129	8558	overall we were able to achieve encouraging results using our models as our best predictions rivaled the accuracy of sports books indeed our neural network had a mse of 369 84 compared to the 320 696 mse of sports book on average our model was able to pick the correct side of the over under 51 5 of the time in the future there is definitely potential to improve the model through augmenting our data set 	overall we were able to achieve encouraging results using our models as our best predictions rivaled the accuracy of sports books 
1	129	8559	overall we were able to achieve encouraging results using our models as our best predictions rivaled the accuracy of sports books indeed our neural network had a mse of 369 84 compared to the 320 696 mse of sports book on average our model was able to pick the correct side of the over under 51 5 of the time in the future there is definitely potential to improve the model through augmenting our data set 	indeed our neural network had a mse of 369 84 compared to the 320 696 mse of sports book 
1	129	8560	overall we were able to achieve encouraging results using our models as our best predictions rivaled the accuracy of sports books indeed our neural network had a mse of 369 84 compared to the 320 696 mse of sports book on average our model was able to pick the correct side of the over under 51 5 of the time in the future there is definitely potential to improve the model through augmenting our data set 	on average our model was able to pick the correct side of the over under 51 5 of the time in the future there is definitely potential to improve the model through augmenting our data set 
1	129	8561	overall we were able to achieve encouraging results using our models as our best predictions rivaled the accuracy of sports books indeed our neural network had a mse of 369 84 compared to the 320 696 mse of sports book on average our model was able to pick the correct side of the over under 51 5 of the time in the future there is definitely potential to improve the model through augmenting our data set 	first of all the odds lines offered by sports books themselves offer a large amount of information on the potential outcomes of games including data that indicates the direction the odds lines are moving in the hours before the game 
1	129	8562	overall we were able to achieve encouraging results using our models as our best predictions rivaled the accuracy of sports books indeed our neural network had a mse of 369 84 compared to the 320 696 mse of sports book on average our model was able to pick the correct side of the over under 51 5 of the time in the future there is definitely potential to improve the model through augmenting our data set 	it is entirely possible that the odds lines would indicate that the sports books are very good at predicting the outcomes of games involving certain teams but tend to skew in some direction when trying to predict the outcomes of games involving other teams 
1	129	8563	overall we were able to achieve encouraging results using our models as our best predictions rivaled the accuracy of sports books indeed our neural network had a mse of 369 84 compared to the 320 696 mse of sports book on average our model was able to pick the correct side of the over under 51 5 of the time in the future there is definitely potential to improve the model through augmenting our data set 	in addition while our current models only use team level data incorporating player level data could add additional layers of nuance while accounting for the impact of injuries or player fatigue further exploration of model architectures could potentially improve our results as well 
1	129	8564	overall we were able to achieve encouraging results using our models as our best predictions rivaled the accuracy of sports books indeed our neural network had a mse of 369 84 compared to the 320 696 mse of sports book on average our model was able to pick the correct side of the over under 51 5 of the time in the future there is definitely potential to improve the model through augmenting our data set 	due to the similarity of our current problem predicting betting indicators from the home team and away team to classical recommendation systems predicting ratings for a given user and item we could definitely explore adapting algorithms used by netflix amazon and others for recommendation 
0	129	8565	in this project both vishnu and alexandre contributed equally to the writing of this report alexandre completed around two thirds of the data retrieval cleaning process built the collaborative filtering model and the lstm network vishnu completed the remaining third of the data retrieval cleaning process built the baseline models as well as the neural network 	in this project both vishnu and alexandre contributed equally to the writing of this report 
0	129	8566	in this project both vishnu and alexandre contributed equally to the writing of this report alexandre completed around two thirds of the data retrieval cleaning process built the collaborative filtering model and the lstm network vishnu completed the remaining third of the data retrieval cleaning process built the baseline models as well as the neural network 	alexandre completed around two thirds of the data retrieval cleaning process built the collaborative filtering model and the lstm network 
0	129	8567	in this project both vishnu and alexandre contributed equally to the writing of this report alexandre completed around two thirds of the data retrieval cleaning process built the collaborative filtering model and the lstm network vishnu completed the remaining third of the data retrieval cleaning process built the baseline models as well as the neural network 	vishnu completed the remaining third of the data retrieval cleaning process built the baseline models as well as the neural network 
0	129	8568	in this project both vishnu and alexandre contributed equally to the writing of this report alexandre completed around two thirds of the data retrieval cleaning process built the collaborative filtering model and the lstm network vishnu completed the remaining third of the data retrieval cleaning process built the baseline models as well as the neural network 	the rest was achieved jointly by both members 
1	130	8569	several prominent pathologies such as cerebral palsy parkinson s disease and alzheimer s disease can manifest themselves in an abnormal walking gait abnormal gait is characterized by irregular patterns in step length step cadence joint angles and poor balance early detection of these abnormalities can help in diagnosis treatment and effective post treatment monitoring of patients a comprehensive metric used to assess the extent of gait pathology is the gait deviation index score or gdi the current gold standard method of measuring gdi marker based motion capture places a set of reflective markers on body segments and tracks their trajectories over time 	several prominent pathologies such as cerebral palsy parkinson s disease and alzheimer s disease can manifest themselves in an abnormal walking gait 
0	130	8570	several prominent pathologies such as cerebral palsy parkinson s disease and alzheimer s disease can manifest themselves in an abnormal walking gait abnormal gait is characterized by irregular patterns in step length step cadence joint angles and poor balance early detection of these abnormalities can help in diagnosis treatment and effective post treatment monitoring of patients a comprehensive metric used to assess the extent of gait pathology is the gait deviation index score or gdi the current gold standard method of measuring gdi marker based motion capture places a set of reflective markers on body segments and tracks their trajectories over time 	abnormal gait is characterized by irregular patterns in step length step cadence joint angles and poor balance 
1	130	8571	several prominent pathologies such as cerebral palsy parkinson s disease and alzheimer s disease can manifest themselves in an abnormal walking gait abnormal gait is characterized by irregular patterns in step length step cadence joint angles and poor balance early detection of these abnormalities can help in diagnosis treatment and effective post treatment monitoring of patients a comprehensive metric used to assess the extent of gait pathology is the gait deviation index score or gdi the current gold standard method of measuring gdi marker based motion capture places a set of reflective markers on body segments and tracks their trajectories over time 	early detection of these abnormalities can help in diagnosis treatment and effective post treatment monitoring of patients a comprehensive metric used to assess the extent of gait pathology is the gait deviation index score or gdi the current gold standard method of measuring gdi marker based motion capture places a set of reflective markers on body segments and tracks their trajectories over time 
0	130	8572	several prominent pathologies such as cerebral palsy parkinson s disease and alzheimer s disease can manifest themselves in an abnormal walking gait abnormal gait is characterized by irregular patterns in step length step cadence joint angles and poor balance early detection of these abnormalities can help in diagnosis treatment and effective post treatment monitoring of patients a comprehensive metric used to assess the extent of gait pathology is the gait deviation index score or gdi the current gold standard method of measuring gdi marker based motion capture places a set of reflective markers on body segments and tracks their trajectories over time 	sessions with patients can last multiple hours and cost hundreds or thousands of dollars 
0	130	8573	several prominent pathologies such as cerebral palsy parkinson s disease and alzheimer s disease can manifest themselves in an abnormal walking gait abnormal gait is characterized by irregular patterns in step length step cadence joint angles and poor balance early detection of these abnormalities can help in diagnosis treatment and effective post treatment monitoring of patients a comprehensive metric used to assess the extent of gait pathology is the gait deviation index score or gdi the current gold standard method of measuring gdi marker based motion capture places a set of reflective markers on body segments and tracks their trajectories over time 	a potential less expensive and less time consuming alternative is to analyze video captured by commodity devices i e 
1	130	8574	several prominent pathologies such as cerebral palsy parkinson s disease and alzheimer s disease can manifest themselves in an abnormal walking gait abnormal gait is characterized by irregular patterns in step length step cadence joint angles and poor balance early detection of these abnormalities can help in diagnosis treatment and effective post treatment monitoring of patients a comprehensive metric used to assess the extent of gait pathology is the gait deviation index score or gdi the current gold standard method of measuring gdi marker based motion capture places a set of reflective markers on body segments and tracks their trajectories over time 	mobile camera phone using machine learning algorithms to predict gdi previous attempts have been made to predict gdi from monocular video footage using a projection of joint centers onto the two dimensional plane of the camera 
0	130	8575	several prominent pathologies such as cerebral palsy parkinson s disease and alzheimer s disease can manifest themselves in an abnormal walking gait abnormal gait is characterized by irregular patterns in step length step cadence joint angles and poor balance early detection of these abnormalities can help in diagnosis treatment and effective post treatment monitoring of patients a comprehensive metric used to assess the extent of gait pathology is the gait deviation index score or gdi the current gold standard method of measuring gdi marker based motion capture places a set of reflective markers on body segments and tracks their trajectories over time 	in this project we leverage cutting edge computer vision tactics to extract three dimensional features from each frame of video 
1	130	8576	several prominent pathologies such as cerebral palsy parkinson s disease and alzheimer s disease can manifest themselves in an abnormal walking gait abnormal gait is characterized by irregular patterns in step length step cadence joint angles and poor balance early detection of these abnormalities can help in diagnosis treatment and effective post treatment monitoring of patients a comprehensive metric used to assess the extent of gait pathology is the gait deviation index score or gdi the current gold standard method of measuring gdi marker based motion capture places a set of reflective markers on body segments and tracks their trajectories over time 	by stacking processed frames into a video sequence relevant spatiotemporal features can be modeled for gait characterization 
1	130	8577	several prominent pathologies such as cerebral palsy parkinson s disease and alzheimer s disease can manifest themselves in an abnormal walking gait abnormal gait is characterized by irregular patterns in step length step cadence joint angles and poor balance early detection of these abnormalities can help in diagnosis treatment and effective post treatment monitoring of patients a comprehensive metric used to assess the extent of gait pathology is the gait deviation index score or gdi the current gold standard method of measuring gdi marker based motion capture places a set of reflective markers on body segments and tracks their trajectories over time 	thus the project goal is to use monocular video footage to predict gdi score with lower root mean squared error rmse than existing methods 
1	130	8578	our work builds on the efforts of many machine learning scientists who developed models to extract spatiotemporal features from video as well as biomechanists who have analyzed human motion to help physicians assess neuromuscular pathology a leader in this field is lukasz kidzinski a member of stanford s mobilize center who was our advisor for this project we met with lukasz throughout the project for guidance on data processing and model generation 	our work builds on the efforts of many machine learning scientists who developed models to extract spatiotemporal features from video as well as biomechanists who have analyzed human motion to help physicians assess neuromuscular pathology 
0	130	8579	our work builds on the efforts of many machine learning scientists who developed models to extract spatiotemporal features from video as well as biomechanists who have analyzed human motion to help physicians assess neuromuscular pathology a leader in this field is lukasz kidzinski a member of stanford s mobilize center who was our advisor for this project we met with lukasz throughout the project for guidance on data processing and model generation 	a leader in this field is lukasz kidzinski a member of stanford s mobilize center who was our advisor for this project 
0	130	8580	our work builds on the efforts of many machine learning scientists who developed models to extract spatiotemporal features from video as well as biomechanists who have analyzed human motion to help physicians assess neuromuscular pathology a leader in this field is lukasz kidzinski a member of stanford s mobilize center who was our advisor for this project we met with lukasz throughout the project for guidance on data processing and model generation 	we met with lukasz throughout the project for guidance on data processing and model generation 
1	130	8581	our work builds on the efforts of many machine learning scientists who developed models to extract spatiotemporal features from video as well as biomechanists who have analyzed human motion to help physicians assess neuromuscular pathology a leader in this field is lukasz kidzinski a member of stanford s mobilize center who was our advisor for this project we met with lukasz throughout the project for guidance on data processing and model generation 	in 2017 lukasz and his team used a temporal convolutional network built on videos processed through openpose to predict gdi a critical component to our analysis is the refeaturization of images into a spatial representation of human pose 
1	130	8582	our work builds on the efforts of many machine learning scientists who developed models to extract spatiotemporal features from video as well as biomechanists who have analyzed human motion to help physicians assess neuromuscular pathology a leader in this field is lukasz kidzinski a member of stanford s mobilize center who was our advisor for this project we met with lukasz throughout the project for guidance on data processing and model generation 	specifically we leveraged the densepose algorithm which converts red green blue images toeach pixel in an image to one of thousands of surface locations on a modeled human mesh 
1	130	8583	our work builds on the efforts of many machine learning scientists who developed models to extract spatiotemporal features from video as well as biomechanists who have analyzed human motion to help physicians assess neuromuscular pathology a leader in this field is lukasz kidzinski a member of stanford s mobilize center who was our advisor for this project we met with lukasz throughout the project for guidance on data processing and model generation 	densepose builds on prior work in human pose estimation most notably the skinned multi person linear model our models and experiments were motivated by researchers who have used machine learning to extract spatial and spatiotemporal features from video 
1	130	8584	our work builds on the efforts of many machine learning scientists who developed models to extract spatiotemporal features from video as well as biomechanists who have analyzed human motion to help physicians assess neuromuscular pathology a leader in this field is lukasz kidzinski a member of stanford s mobilize center who was our advisor for this project we met with lukasz throughout the project for guidance on data processing and model generation 	ibm used a cnn with a multi layer perceptron to classify images into one of many types a guiding work for extracting spatiotemporal features from images was harvey s blog post
1	130	8585	our dataset comprises of 3 000 videos of patients walking in a room at gillette children s specialty healthcare center for gait and motion analysis the videos have a resolution of 640x480 and are 25 frames per second each frame is processed using densepose which maps all pixels of an rgb image to the surface of a modeled human mesh	our dataset comprises of 3 000 videos of patients walking in a room at gillette children s specialty healthcare center for gait and motion analysis the videos have a resolution of 640x480 and are 25 frames per second 
1	130	8586	our dataset comprises of 3 000 videos of patients walking in a room at gillette children s specialty healthcare center for gait and motion analysis the videos have a resolution of 640x480 and are 25 frames per second each frame is processed using densepose which maps all pixels of an rgb image to the surface of a modeled human mesh	each frame is processed using densepose which maps all pixels of an rgb image to the surface of a modeled human mesh
1	130	8587	densepose right each pixel is assigned to a corresponding point on a human skin mesh the processed frames consist of three channels i part index u and v coordinates and are passed as inputs to the different learning models in the case of a model with a temporal component 10 outputs i e 	each pixel is assigned to a corresponding point on a human skin mesh the processed frames consist of three channels i part index u and v coordinates and are passed as inputs to the different learning models 
0	130	8588	densepose right each pixel is assigned to a corresponding point on a human skin mesh the processed frames consist of three channels i part index u and v coordinates and are passed as inputs to the different learning models in the case of a model with a temporal component 10 outputs i e 	in the case of a model with a temporal component 10 outputs i e 
1	130	8589	densepose right each pixel is assigned to a corresponding point on a human skin mesh the processed frames consist of three channels i part index u and v coordinates and are passed as inputs to the different learning models in the case of a model with a temporal component 10 outputs i e 	processed frames are concatenated in sequence per training example before running any initial experiments substantial work was performed to process data 
0	130	8590	densepose right each pixel is assigned to a corresponding point on a human skin mesh the processed frames consist of three channels i part index u and v coordinates and are passed as inputs to the different learning models in the case of a model with a temporal component 10 outputs i e 	this included running densepose algorithm on top of thousands of videos and organizing them into folders 
0	130	8591	densepose right each pixel is assigned to a corresponding point on a human skin mesh the processed frames consist of three channels i part index u and v coordinates and are passed as inputs to the different learning models in the case of a model with a temporal component 10 outputs i e 	subsequently the folders were assigned a gdi score based on the corresponding examid from a joined file of physician assessments 
0	130	8592	densepose right each pixel is assigned to a corresponding point on a human skin mesh the processed frames consist of three channels i part index u and v coordinates and are passed as inputs to the different learning models in the case of a model with a temporal component 10 outputs i e 	due to the massive data volume and limits of memory we did not leverage the entire dataset in our experiments 
0	130	8593	densepose right each pixel is assigned to a corresponding point on a human skin mesh the processed frames consist of three channels i part index u and v coordinates and are passed as inputs to the different learning models in the case of a model with a temporal component 10 outputs i e 	we typically accessed 500 1 500 videos depending on the model s computational demands 
0	130	8594	densepose right each pixel is assigned to a corresponding point on a human skin mesh the processed frames consist of three channels i part index u and v coordinates and are passed as inputs to the different learning models in the case of a model with a temporal component 10 outputs i e 	as such though we considered using video slicing image mirroring or other data augmentation techniques we decided not to implement these as generating additional augmented data was not necessary 
1	130	8595	patient videos are transformed into i u v channels by densepose and are then passed as inputs to our model which we call gdi net gdi net is a machine learning algorithm with spatial and temporal components to predict a patient s gdi score gdi net was implemented in keras an open source neural network library which runs on top of tensorflow environment our initial implementations were quick and simple 	patient videos are transformed into i u v channels by densepose and are then passed as inputs to our model which we call gdi net 
1	130	8596	patient videos are transformed into i u v channels by densepose and are then passed as inputs to our model which we call gdi net gdi net is a machine learning algorithm with spatial and temporal components to predict a patient s gdi score gdi net was implemented in keras an open source neural network library which runs on top of tensorflow environment our initial implementations were quick and simple 	gdi net is a machine learning algorithm with spatial and temporal components to predict a patient s gdi score 
0	130	8597	patient videos are transformed into i u v channels by densepose and are then passed as inputs to our model which we call gdi net gdi net is a machine learning algorithm with spatial and temporal components to predict a patient s gdi score gdi net was implemented in keras an open source neural network library which runs on top of tensorflow environment our initial implementations were quick and simple 	gdi net was implemented in keras an open source neural network library which runs on top of tensorflow environment our initial implementations were quick and simple 
1	130	8598	patient videos are transformed into i u v channels by densepose and are then passed as inputs to our model which we call gdi net gdi net is a machine learning algorithm with spatial and temporal components to predict a patient s gdi score gdi net was implemented in keras an open source neural network library which runs on top of tensorflow environment our initial implementations were quick and simple 	we built a linear regression model implemented in scikit learn package using 5 frames of 480x640x3 resolution for each training example as the model complexity was gradually increased a sole spatial component was added to gdi net architecture 
0	130	8599	patient videos are transformed into i u v channels by densepose and are then passed as inputs to our model which we call gdi net gdi net is a machine learning algorithm with spatial and temporal components to predict a patient s gdi score gdi net was implemented in keras an open source neural network library which runs on top of tensorflow environment our initial implementations were quick and simple 	we trained the spatial component which consisted of vgg16 an off the shelf cnn architecture or a custom 2d cnn with 3 834 training frames of 480x640x3 dimension 
1	130	8600	patient videos are transformed into i u v channels by densepose and are then passed as inputs to our model which we call gdi net gdi net is a machine learning algorithm with spatial and temporal components to predict a patient s gdi score gdi net was implemented in keras an open source neural network library which runs on top of tensorflow environment our initial implementations were quick and simple 	the model was validated against 951 examples to quantify performance on unseen examples the vgg16 model is pre trained and only the last layer of the model was replaced by a linear function and trained to perform our regression task 
0	130	8601	patient videos are transformed into i u v channels by densepose and are then passed as inputs to our model which we call gdi net gdi net is a machine learning algorithm with spatial and temporal components to predict a patient s gdi score gdi net was implemented in keras an open source neural network library which runs on top of tensorflow environment our initial implementations were quick and simple 	the motivation behind using the pre trained weights was to investigate the possibility of transfer learning 
0	130	8602	patient videos are transformed into i u v channels by densepose and are then passed as inputs to our model which we call gdi net gdi net is a machine learning algorithm with spatial and temporal components to predict a patient s gdi score gdi net was implemented in keras an open source neural network library which runs on top of tensorflow environment our initial implementations were quick and simple 	since collecting patient data and building custom models is expensive and cumbersome transfer learning is desirable and could reduce the lead time of any application development 
1	130	8603	patient videos are transformed into i u v channels by densepose and are then passed as inputs to our model which we call gdi net gdi net is a machine learning algorithm with spatial and temporal components to predict a patient s gdi score gdi net was implemented in keras an open source neural network library which runs on top of tensorflow environment our initial implementations were quick and simple 	after receiving advice from cs229 course assistants at the course poster session we also ran a vgg16 model in which all weights were re trained a challenge in executing vgg16 was that it requires an image with a standard size of 224x224x3 as an input 
1	130	8604	patient videos are transformed into i u v channels by densepose and are then passed as inputs to our model which we call gdi net gdi net is a machine learning algorithm with spatial and temporal components to predict a patient s gdi score gdi net was implemented in keras an open source neural network library which runs on top of tensorflow environment our initial implementations were quick and simple 	since the densepose output has a resolution of 480x640x3 the outputs had to be cropped before being passed to vgg16 
0	130	8605	patient videos are transformed into i u v channels by densepose and are then passed as inputs to our model which we call gdi net gdi net is a machine learning algorithm with spatial and temporal components to predict a patient s gdi score gdi net was implemented in keras an open source neural network library which runs on top of tensorflow environment our initial implementations were quick and simple 	the crop was made in a manner to preserve as much information as possible i e 
1	130	8606	patient videos are transformed into i u v channels by densepose and are then passed as inputs to our model which we call gdi net gdi net is a machine learning algorithm with spatial and temporal components to predict a patient s gdi score gdi net was implemented in keras an open source neural network library which runs on top of tensorflow environment our initial implementations were quick and simple 	by selecting pixel values where patients are most likely to appear in the frame however some loss is inevitable densepose outputs were passed whole to the custom cnn 
0	130	8607	patient videos are transformed into i u v channels by densepose and are then passed as inputs to our model which we call gdi net gdi net is a machine learning algorithm with spatial and temporal components to predict a patient s gdi score gdi net was implemented in keras an open source neural network library which runs on top of tensorflow environment our initial implementations were quick and simple 	the inspiration behind the cnn s architecture was mahendran et al 
0	130	8608	patient videos are transformed into i u v channels by densepose and are then passed as inputs to our model which we call gdi net gdi net is a machine learning algorithm with spatial and temporal components to predict a patient s gdi score gdi net was implemented in keras an open source neural network library which runs on top of tensorflow environment our initial implementations were quick and simple 	 s model that leveraged a cnn to predict the pose of a vehicle within a continuous regression framework 
1	130	8609	patient videos are transformed into i u v channels by densepose and are then passed as inputs to our model which we call gdi net gdi net is a machine learning algorithm with spatial and temporal components to predict a patient s gdi score gdi net was implemented in keras an open source neural network library which runs on top of tensorflow environment our initial implementations were quick and simple 	in addition lukasz kidzinski provided instrumental guidance for making important architecture choices in the cnn although the spatial component identifies human poses in a frame it cannot track the pose trajectories over time 
1	130	8610	patient videos are transformed into i u v channels by densepose and are then passed as inputs to our model which we call gdi net gdi net is a machine learning algorithm with spatial and temporal components to predict a patient s gdi score gdi net was implemented in keras an open source neural network library which runs on top of tensorflow environment our initial implementations were quick and simple 	to detect temporal characteristics a temporal model either an lstm or 1d cnn was added to gdi net 
1	130	8611	patient videos are transformed into i u v channels by densepose and are then passed as inputs to our model which we call gdi net gdi net is a machine learning algorithm with spatial and temporal components to predict a patient s gdi score gdi net was implemented in keras an open source neural network library which runs on top of tensorflow environment our initial implementations were quick and simple 	the input to a temporal model had 2 920 training examples of 10 frames of 480x640x3 resolution concatenated into a single array 
1	130	8612	patient videos are transformed into i u v channels by densepose and are then passed as inputs to our model which we call gdi net gdi net is a machine learning algorithm with spatial and temporal components to predict a patient s gdi score gdi net was implemented in keras an open source neural network library which runs on top of tensorflow environment our initial implementations were quick and simple 	hyperparameter tuning was performed to further optimize models mainly tuning learning rate batch size and dropout 
0	130	8613	patient videos are transformed into i u v channels by densepose and are then passed as inputs to our model which we call gdi net gdi net is a machine learning algorithm with spatial and temporal components to predict a patient s gdi score gdi net was implemented in keras an open source neural network library which runs on top of tensorflow environment our initial implementations were quick and simple 	the majority of this effort focused on tuning hyperparameters for our most promising model the cnn with lstm 
1	130	8614	patient videos are transformed into i u v channels by densepose and are then passed as inputs to our model which we call gdi net gdi net is a machine learning algorithm with spatial and temporal components to predict a patient s gdi score gdi net was implemented in keras an open source neural network library which runs on top of tensorflow environment our initial implementations were quick and simple 	the complete architecture of the highest performing model is outlined in the results and discussion section of this paper the immense size of the dataset and the desire to concatenate frames sometimes led to issues with memory overload 
0	130	8615	patient videos are transformed into i u v channels by densepose and are then passed as inputs to our model which we call gdi net gdi net is a machine learning algorithm with spatial and temporal components to predict a patient s gdi score gdi net was implemented in keras an open source neural network library which runs on top of tensorflow environment our initial implementations were quick and simple 	in those scenarios hyperparameters such as batch size and kernel size were adjusted to avoid memory limits 
0	130	8616	patient videos are transformed into i u v channels by densepose and are then passed as inputs to our model which we call gdi net gdi net is a machine learning algorithm with spatial and temporal components to predict a patient s gdi score gdi net was implemented in keras an open source neural network library which runs on top of tensorflow environment our initial implementations were quick and simple 	further for computationally expensive models we leveraged sherlock a high performance computing cluster available to stanford university affiliates to reduce run time
0	130	8617	one of the interesting findings from our experiments was the relatively poor performance from using an off the shelf model the initial goal in these experiments was to exploit transfer learning to build complex networks with millions of parameters that are pre trained we used vgg16 a model that is readily available within the keras api and has reported success with image classification tasks frame specific models that only captured spatial features of a given frame did not perform well 	one of the interesting findings from our experiments was the relatively poor performance from using an off the shelf model 
0	130	8618	one of the interesting findings from our experiments was the relatively poor performance from using an off the shelf model the initial goal in these experiments was to exploit transfer learning to build complex networks with millions of parameters that are pre trained we used vgg16 a model that is readily available within the keras api and has reported success with image classification tasks frame specific models that only captured spatial features of a given frame did not perform well 	the initial goal in these experiments was to exploit transfer learning to build complex networks with millions of parameters that are pre trained 
1	130	8619	one of the interesting findings from our experiments was the relatively poor performance from using an off the shelf model the initial goal in these experiments was to exploit transfer learning to build complex networks with millions of parameters that are pre trained we used vgg16 a model that is readily available within the keras api and has reported success with image classification tasks frame specific models that only captured spatial features of a given frame did not perform well 	we used vgg16 a model that is readily available within the keras api and has reported success with image classification tasks frame specific models that only captured spatial features of a given frame did not perform well 
1	130	8620	one of the interesting findings from our experiments was the relatively poor performance from using an off the shelf model the initial goal in these experiments was to exploit transfer learning to build complex networks with millions of parameters that are pre trained we used vgg16 a model that is readily available within the keras api and has reported success with image classification tasks frame specific models that only captured spatial features of a given frame did not perform well 	this is expected as gdi is largely determined by trajectories of body parts and individual frames do not hold temporal information describing how the patient moves over time 
1	130	8621	one of the interesting findings from our experiments was the relatively poor performance from using an off the shelf model the initial goal in these experiments was to exploit transfer learning to build complex networks with millions of parameters that are pre trained we used vgg16 a model that is readily available within the keras api and has reported success with image classification tasks frame specific models that only captured spatial features of a given frame did not perform well 	as such we spent most of our time and effort experimenting with spatiotemporal models the best performing model combined a 2d cnn on each frame and an lstm to capture temporal patterns 
0	130	8622	one of the interesting findings from our experiments was the relatively poor performance from using an off the shelf model the initial goal in these experiments was to exploit transfer learning to build complex networks with millions of parameters that are pre trained we used vgg16 a model that is readily available within the keras api and has reported success with image classification tasks frame specific models that only captured spatial features of a given frame did not perform well 	the learning curve regression plot and detailed architecture are outlined in
0	130	8623	in this study we implemented a machine learning approach to develop a cheap and automated way to assess gait abnormalities the results especially that of a cnn with lstm model are very promising we are excited by our model s potential to help diagnose and track the progression of neuromuscular disease although the cnn with lstm model was able to achieve a low rmse error analysis should be performed to investigate the deviations between the model and ground truth 	in this study we implemented a machine learning approach to develop a cheap and automated way to assess gait abnormalities 
0	130	8624	in this study we implemented a machine learning approach to develop a cheap and automated way to assess gait abnormalities the results especially that of a cnn with lstm model are very promising we are excited by our model s potential to help diagnose and track the progression of neuromuscular disease although the cnn with lstm model was able to achieve a low rmse error analysis should be performed to investigate the deviations between the model and ground truth 	the results especially that of a cnn with lstm model are very promising 
1	130	8625	in this study we implemented a machine learning approach to develop a cheap and automated way to assess gait abnormalities the results especially that of a cnn with lstm model are very promising we are excited by our model s potential to help diagnose and track the progression of neuromuscular disease although the cnn with lstm model was able to achieve a low rmse error analysis should be performed to investigate the deviations between the model and ground truth 	we are excited by our model s potential to help diagnose and track the progression of neuromuscular disease although the cnn with lstm model was able to achieve a low rmse error analysis should be performed to investigate the deviations between the model and ground truth 
1	130	8626	in this study we implemented a machine learning approach to develop a cheap and automated way to assess gait abnormalities the results especially that of a cnn with lstm model are very promising we are excited by our model s potential to help diagnose and track the progression of neuromuscular disease although the cnn with lstm model was able to achieve a low rmse error analysis should be performed to investigate the deviations between the model and ground truth 	once the nature of these errors is understood the model and the architecture can be fine tuned for better accuracy an option to enhance our architecture is to implement 3d convolution blocks instead of a separate spatial and temporal component 
1	130	8627	in this study we implemented a machine learning approach to develop a cheap and automated way to assess gait abnormalities the results especially that of a cnn with lstm model are very promising we are excited by our model s potential to help diagnose and track the progression of neuromuscular disease although the cnn with lstm model was able to achieve a low rmse error analysis should be performed to investigate the deviations between the model and ground truth 	we hypothesize that it may perform better than the spatiotemporal model as it can capture lower level features in time and is not affected by the way data is passed from the spatial component to the temporal component in our experiments we observed a growing gap between training error and validation error as training progressed which suggests overfitting 
0	130	8628	in this study we implemented a machine learning approach to develop a cheap and automated way to assess gait abnormalities the results especially that of a cnn with lstm model are very promising we are excited by our model s potential to help diagnose and track the progression of neuromuscular disease although the cnn with lstm model was able to achieve a low rmse error analysis should be performed to investigate the deviations between the model and ground truth 	although we attempted to mitigate this using dropout more could be done to generalize our model 
1	130	8629	in this study we implemented a machine learning approach to develop a cheap and automated way to assess gait abnormalities the results especially that of a cnn with lstm model are very promising we are excited by our model s potential to help diagnose and track the progression of neuromuscular disease although the cnn with lstm model was able to achieve a low rmse error analysis should be performed to investigate the deviations between the model and ground truth 	future experiments can focus on reducing model complexity or incorporating l2 regularization an interesting approach we would like to implement is building a classification network by bucketing gdi scores to the nearest integer 
0	130	8630	in this study we implemented a machine learning approach to develop a cheap and automated way to assess gait abnormalities the results especially that of a cnn with lstm model are very promising we are excited by our model s potential to help diagnose and track the progression of neuromuscular disease although the cnn with lstm model was able to achieve a low rmse error analysis should be performed to investigate the deviations between the model and ground truth 	using a softmax layer we can take the probability weighted sum of bucket values to determine a scalar gdi score 
0	130	8631	in this study we implemented a machine learning approach to develop a cheap and automated way to assess gait abnormalities the results especially that of a cnn with lstm model are very promising we are excited by our model s potential to help diagnose and track the progression of neuromuscular disease although the cnn with lstm model was able to achieve a low rmse error analysis should be performed to investigate the deviations between the model and ground truth 	in other words we can train the network as a classification task but derive the scalar gdi score using the appropriate weighted sum of the softmax output 
0	130	8632	in this study we implemented a machine learning approach to develop a cheap and automated way to assess gait abnormalities the results especially that of a cnn with lstm model are very promising we are excited by our model s potential to help diagnose and track the progression of neuromuscular disease although the cnn with lstm model was able to achieve a low rmse error analysis should be performed to investigate the deviations between the model and ground truth 	this option also opens the opportunity to use off the shelf classification frameworks for our task our capacity to experiment was constrained by memory overload issues 
0	130	8633	in this study we implemented a machine learning approach to develop a cheap and automated way to assess gait abnormalities the results especially that of a cnn with lstm model are very promising we are excited by our model s potential to help diagnose and track the progression of neuromuscular disease although the cnn with lstm model was able to achieve a low rmse error analysis should be performed to investigate the deviations between the model and ground truth 	the efficient management of memory and resource utilization would allow for more rapid experimentation 
1	130	8634	in this study we implemented a machine learning approach to develop a cheap and automated way to assess gait abnormalities the results especially that of a cnn with lstm model are very promising we are excited by our model s potential to help diagnose and track the progression of neuromuscular disease although the cnn with lstm model was able to achieve a low rmse error analysis should be performed to investigate the deviations between the model and ground truth 	chunking memory swapping or simply accessing machines with larger random access memory can be applied to address this issue future experiments should explore refeaturizing the processed densepose outputs to global x y z coordinates 
1	130	8635	in this study we implemented a machine learning approach to develop a cheap and automated way to assess gait abnormalities the results especially that of a cnn with lstm model are very promising we are excited by our model s potential to help diagnose and track the progression of neuromuscular disease although the cnn with lstm model was able to achieve a low rmse error analysis should be performed to investigate the deviations between the model and ground truth 	this would allow us to manually engineer additional relevant features such as knee flexion angle that are expected correlates of gdi score 
1	130	8636	in this study we implemented a machine learning approach to develop a cheap and automated way to assess gait abnormalities the results especially that of a cnn with lstm model are very promising we are excited by our model s potential to help diagnose and track the progression of neuromuscular disease although the cnn with lstm model was able to achieve a low rmse error analysis should be performed to investigate the deviations between the model and ground truth 	we can further compress our data by considering only the x y z coordinates of the most relevant body landmarks as movements of the hip knee and ankle are particularly important for gait analysis 
1	130	8637	in this study we implemented a machine learning approach to develop a cheap and automated way to assess gait abnormalities the results especially that of a cnn with lstm model are very promising we are excited by our model s potential to help diagnose and track the progression of neuromuscular disease although the cnn with lstm model was able to achieve a low rmse error analysis should be performed to investigate the deviations between the model and ground truth 	this process would require the manipulation of densepose outputs to a customized smpl human body model the potential for future work is enormous as we have just scratched the surface of densepose s capabilities 
0	130	8638	in this study we implemented a machine learning approach to develop a cheap and automated way to assess gait abnormalities the results especially that of a cnn with lstm model are very promising we are excited by our model s potential to help diagnose and track the progression of neuromuscular disease although the cnn with lstm model was able to achieve a low rmse error analysis should be performed to investigate the deviations between the model and ground truth 	a determined effort can lead to the development of a robust reliable and low cost alternative to analyzing human gait 
0	130	8639	adam gotlin collaborated closely with the stanford mobilize center and other project partners to secure data and share knowledge he orchestrated meetings with advisors and experts and helped identify best practices for time series and movement data he led efforts on building temporal cnn models apurva pancholi spearheaded initial experiments and was involved in data preprocessing 	adam gotlin collaborated closely with the stanford mobilize center and other project partners to secure data and share knowledge 
0	130	8640	adam gotlin collaborated closely with the stanford mobilize center and other project partners to secure data and share knowledge he orchestrated meetings with advisors and experts and helped identify best practices for time series and movement data he led efforts on building temporal cnn models apurva pancholi spearheaded initial experiments and was involved in data preprocessing 	he orchestrated meetings with advisors and experts and helped identify best practices for time series and movement data 
0	130	8641	adam gotlin collaborated closely with the stanford mobilize center and other project partners to secure data and share knowledge he orchestrated meetings with advisors and experts and helped identify best practices for time series and movement data he led efforts on building temporal cnn models apurva pancholi spearheaded initial experiments and was involved in data preprocessing 	he led efforts on building temporal cnn models apurva pancholi spearheaded initial experiments and was involved in data preprocessing 
0	130	8642	adam gotlin collaborated closely with the stanford mobilize center and other project partners to secure data and share knowledge he orchestrated meetings with advisors and experts and helped identify best practices for time series and movement data he led efforts on building temporal cnn models apurva pancholi spearheaded initial experiments and was involved in data preprocessing 	he owned data transfer from the mobilize center to our project team 
0	130	8643	adam gotlin collaborated closely with the stanford mobilize center and other project partners to secure data and share knowledge he orchestrated meetings with advisors and experts and helped identify best practices for time series and movement data he led efforts on building temporal cnn models apurva pancholi spearheaded initial experiments and was involved in data preprocessing 	apurva developed a custom cnn that was the primary spatial component for our highest performing model 
0	130	8644	adam gotlin collaborated closely with the stanford mobilize center and other project partners to secure data and share knowledge he orchestrated meetings with advisors and experts and helped identify best practices for time series and movement data he led efforts on building temporal cnn models apurva pancholi spearheaded initial experiments and was involved in data preprocessing 	umang agarwal led data processing and consolidation 
0	130	8645	adam gotlin collaborated closely with the stanford mobilize center and other project partners to secure data and share knowledge he orchestrated meetings with advisors and experts and helped identify best practices for time series and movement data he led efforts on building temporal cnn models apurva pancholi spearheaded initial experiments and was involved in data preprocessing 	he owned efforts to read and interpret densepose source code in order to generate gdi net model inputs 
0	130	8646	adam gotlin collaborated closely with the stanford mobilize center and other project partners to secure data and share knowledge he orchestrated meetings with advisors and experts and helped identify best practices for time series and movement data he led efforts on building temporal cnn models apurva pancholi spearheaded initial experiments and was involved in data preprocessing 	he led efforts to exploit transfer learning and contributed to tuning neural network models to maximize performance 
0	130	8647	code for this project can be found at https github com agotlin cs229dp	code for this project can be found at https github com agotlin cs229dp
0	131	8648	using words can be limited when communicating across cultures and literacy levels drawing images is a shared communication method that can bridge those divides if successful this model can be applied for a variety of interesting tasks including a new search interface where someone can draw what they need and search for it or an app where a language learner can draw an image and get the translation immediately 	using words can be limited when communicating across cultures and literacy levels 
0	131	8649	using words can be limited when communicating across cultures and literacy levels drawing images is a shared communication method that can bridge those divides if successful this model can be applied for a variety of interesting tasks including a new search interface where someone can draw what they need and search for it or an app where a language learner can draw an image and get the translation immediately 	drawing images is a shared communication method that can bridge those divides 
1	131	8650	using words can be limited when communicating across cultures and literacy levels drawing images is a shared communication method that can bridge those divides if successful this model can be applied for a variety of interesting tasks including a new search interface where someone can draw what they need and search for it or an app where a language learner can draw an image and get the translation immediately 	if successful this model can be applied for a variety of interesting tasks including a new search interface where someone can draw what they need and search for it or an app where a language learner can draw an image and get the translation immediately 
0	131	8651	using words can be limited when communicating across cultures and literacy levels drawing images is a shared communication method that can bridge those divides if successful this model can be applied for a variety of interesting tasks including a new search interface where someone can draw what they need and search for it or an app where a language learner can draw an image and get the translation immediately 	these applications require computers to understand our quick line drawings or doodles 
0	131	8652	our goal is to develop an efficient system to recognize labels of hand drawn images from google s quickdraw dataset the input to our algorithm is an image we use logistic regression support vector machines svms convolutional neural networks cnns and transfer learning to output a predicted class 	our goal is to develop an efficient system to recognize labels of hand drawn images from google s quickdraw dataset 
0	131	8653	our goal is to develop an efficient system to recognize labels of hand drawn images from google s quickdraw dataset the input to our algorithm is an image we use logistic regression support vector machines svms convolutional neural networks cnns and transfer learning to output a predicted class 	the input to our algorithm is an image 
0	131	8654	our goal is to develop an efficient system to recognize labels of hand drawn images from google s quickdraw dataset the input to our algorithm is an image we use logistic regression support vector machines svms convolutional neural networks cnns and transfer learning to output a predicted class 	we use logistic regression support vector machines svms convolutional neural networks cnns and transfer learning to output a predicted class 
0	131	8655	deep learning has proven to be very successful in general image classification past imagenet 2 1 1 transfer learning 	deep learning has proven to be very successful in general image classification 
0	131	8656	deep learning has proven to be very successful in general image classification past imagenet 2 1 1 transfer learning 	past imagenet 2 1 1 
1	131	8657	deep learning has proven to be very successful in general image classification past imagenet 2 1 1 transfer learning 	the idea behind transfer learning is that we can apply knowledge from a generalized area to a novel task while pre trained imagenet models have been widely used in transfer learning for other natural image classification tasks they have not commonly been used for handdrawn images 
0	131	8658	deep learning has proven to be very successful in general image classification past imagenet 2 1 1 transfer learning 	however researchers lagunas and garces have successfully used transfer learning with vgg
0	131	8659	we are working on a relatively new dataset released under two years ago with a focus on efficiency our project could be seen as within the domain of image recognition however doodles only consist of lines and have two colors black and white which could render complex features learned by image recognition models useless 	we are working on a relatively new dataset released under two years ago with a focus on efficiency 
0	131	8660	we are working on a relatively new dataset released under two years ago with a focus on efficiency our project could be seen as within the domain of image recognition however doodles only consist of lines and have two colors black and white which could render complex features learned by image recognition models useless 	our project could be seen as within the domain of image recognition 
0	131	8661	we are working on a relatively new dataset released under two years ago with a focus on efficiency our project could be seen as within the domain of image recognition however doodles only consist of lines and have two colors black and white which could render complex features learned by image recognition models useless 	however doodles only consist of lines and have two colors black and white which could render complex features learned by image recognition models useless 
0	131	8662	we are working on a relatively new dataset released under two years ago with a focus on efficiency our project could be seen as within the domain of image recognition however doodles only consist of lines and have two colors black and white which could render complex features learned by image recognition models useless 	we want to focus on not just accuracy but also efficiency 
1	131	8663	we are working on a relatively new dataset released under two years ago with a focus on efficiency our project could be seen as within the domain of image recognition however doodles only consist of lines and have two colors black and white which could render complex features learned by image recognition models useless 	efficiency i e model size training time is critical to allow deployment of the system in real life applications yet it has not received sufficient attention in research 
0	131	8664	we are working on a relatively new dataset released under two years ago with a focus on efficiency our project could be seen as within the domain of image recognition however doodles only consist of lines and have two colors black and white which could render complex features learned by image recognition models useless 	we used the bitmap version of the data 
0	131	8665	we are working on a relatively new dataset released under two years ago with a focus on efficiency our project could be seen as within the domain of image recognition however doodles only consist of lines and have two colors black and white which could render complex features learned by image recognition models useless 	each drawing consists of 28 by 28 raw pixel inputs with values from 0 to 255 
0	131	8666	we are working on a relatively new dataset released under two years ago with a focus on efficiency our project could be seen as within the domain of image recognition however doodles only consist of lines and have two colors black and white which could render complex features learned by image recognition models useless 	we took advantage of the fact that each image has only two colors black and white to binarize the pixels for a more compact representation 
0	131	8667	to make training more tractable on modest computing resources we elected to work with a subset of the data the classes selected were chosen randomly from the overall pool of classes and were fixed throughout our experimentation the number of examples per class is 20 000 	to make training more tractable on modest computing resources we elected to work with a subset of the data 
0	131	8668	to make training more tractable on modest computing resources we elected to work with a subset of the data the classes selected were chosen randomly from the overall pool of classes and were fixed throughout our experimentation the number of examples per class is 20 000 	the classes selected were chosen randomly from the overall pool of classes and were fixed throughout our experimentation 
0	131	8669	to make training more tractable on modest computing resources we elected to work with a subset of the data the classes selected were chosen randomly from the overall pool of classes and were fixed throughout our experimentation the number of examples per class is 20 000 	the number of examples per class is 20 000 
0	131	8670	to make training more tractable on modest computing resources we elected to work with a subset of the data the classes selected were chosen randomly from the overall pool of classes and were fixed throughout our experimentation the number of examples per class is 20 000 	we picked 3 10 and 50 classes to train 
0	131	8671	to make training more tractable on modest computing resources we elected to work with a subset of the data the classes selected were chosen randomly from the overall pool of classes and were fixed throughout our experimentation the number of examples per class is 20 000 	for each of the classes we split our data into training validation and test sets with the ratio of 80 10 10 
0	131	8672	broadly speaking we looked at two classes of algorithms for our task traditional machine learning approaches and deep learning techniques the link to our github with our code is here https github com jervisfm cs229 project 	broadly speaking we looked at two classes of algorithms for our task traditional machine learning approaches and deep learning techniques 
0	131	8673	broadly speaking we looked at two classes of algorithms for our task traditional machine learning approaches and deep learning techniques the link to our github with our code is here https github com jervisfm cs229 project 	the link to our github with our code is here https github com jervisfm cs229 project 
0	131	8674	4 1 1 logistic regression for our baseline we used logistic regression a simple and fast model to train the log likelihood for our logistic model where h x 1 1 e t x 4 1 2 	4 1 1 
1	131	8675	4 1 1 logistic regression for our baseline we used logistic regression a simple and fast model to train the log likelihood for our logistic model where h x 1 1 e t x 4 1 2 	for our baseline we used logistic regression a simple and fast model to train the log likelihood for our logistic model where h x 1 1 e t x 4 1 2 
0	131	8676	4 1 1 logistic regression for our baseline we used logistic regression a simple and fast model to train the log likelihood for our logistic model where h x 1 1 e t x 4 1 2 	support vector machine 
1	131	8677	4 1 1 logistic regression for our baseline we used logistic regression a simple and fast model to train the log likelihood for our logistic model where h x 1 1 e t x 4 1 2 	support vector machines are optimal margin classifiers and the optimization objective for these models we explored using support vector machines with various kernels to find empirically the kernel most suited for the task of doodle classification 
0	131	8678	4 1 1 logistic regression for our baseline we used logistic regression a simple and fast model to train the log likelihood for our logistic model where h x 1 1 e t x 4 1 2 	the types of kernels we experimented with are linear rbf radial basis function polynomial and sigmoid 
0	131	8679	we started out with a cnn a natural candidate for image recognition given the convolutional layer s ability to capture spatial dependency a key insight is that since a doodle is a simple image some components of the cnn may be unnecessary by identifying and removing these layers we developed a compact model that is both fast to train and still accurate 	we started out with a cnn a natural candidate for image recognition given the convolutional layer s ability to capture spatial dependency 
0	131	8680	we started out with a cnn a natural candidate for image recognition given the convolutional layer s ability to capture spatial dependency a key insight is that since a doodle is a simple image some components of the cnn may be unnecessary by identifying and removing these layers we developed a compact model that is both fast to train and still accurate 	a key insight is that since a doodle is a simple image some components of the cnn may be unnecessary 
0	131	8681	we started out with a cnn a natural candidate for image recognition given the convolutional layer s ability to capture spatial dependency a key insight is that since a doodle is a simple image some components of the cnn may be unnecessary by identifying and removing these layers we developed a compact model that is both fast to train and still accurate 	by identifying and removing these layers we developed a compact model that is both fast to train and still accurate 
0	131	8682	we started out with a cnn a natural candidate for image recognition given the convolutional layer s ability to capture spatial dependency a key insight is that since a doodle is a simple image some components of the cnn may be unnecessary by identifying and removing these layers we developed a compact model that is both fast to train and still accurate 	the cnn architecture is given in
0	131	8683	even with a simple cnn we noticed that training a deep learning model from the ground up can be time and resource intensive since a doodle is also an image we explored if it is feasible to transfer knowledge from winning imagenet architectures to our specific problem of doodle classification via transfer learning we used four different baseline models namely inception v3 vgg mobilenet and resnet50 from the imagenet competition and extended them for doodle classification figure 4 	even with a simple cnn we noticed that training a deep learning model from the ground up can be time and resource intensive 
1	131	8684	even with a simple cnn we noticed that training a deep learning model from the ground up can be time and resource intensive since a doodle is also an image we explored if it is feasible to transfer knowledge from winning imagenet architectures to our specific problem of doodle classification via transfer learning we used four different baseline models namely inception v3 vgg mobilenet and resnet50 from the imagenet competition and extended them for doodle classification figure 4 	since a doodle is also an image we explored if it is feasible to transfer knowledge from winning imagenet architectures to our specific problem of doodle classification via transfer learning 
0	131	8685	even with a simple cnn we noticed that training a deep learning model from the ground up can be time and resource intensive since a doodle is also an image we explored if it is feasible to transfer knowledge from winning imagenet architectures to our specific problem of doodle classification via transfer learning we used four different baseline models namely inception v3 vgg mobilenet and resnet50 from the imagenet competition and extended them for doodle classification figure 4 	we used four different baseline models namely inception v3 vgg mobilenet and resnet50 from the imagenet competition and extended them for doodle classification figure 4 
1	131	8686	even with a simple cnn we noticed that training a deep learning model from the ground up can be time and resource intensive since a doodle is also an image we explored if it is feasible to transfer knowledge from winning imagenet architectures to our specific problem of doodle classification via transfer learning we used four different baseline models namely inception v3 vgg mobilenet and resnet50 from the imagenet competition and extended them for doodle classification figure 4 	more specifically we added a global spatial average pooling layer after the original architecture followed by a dense layer of 128 units with relu activation and finally a softmax layer for classification 
0	131	8687	even with a simple cnn we noticed that training a deep learning model from the ground up can be time and resource intensive since a doodle is also an image we explored if it is feasible to transfer knowledge from winning imagenet architectures to our specific problem of doodle classification via transfer learning we used four different baseline models namely inception v3 vgg mobilenet and resnet50 from the imagenet competition and extended them for doodle classification figure 4 	we used stochastic gradient descent with an adam optimizer to fine tune the added layers 
0	131	8688	even with a simple cnn we noticed that training a deep learning model from the ground up can be time and resource intensive since a doodle is also an image we explored if it is feasible to transfer knowledge from winning imagenet architectures to our specific problem of doodle classification via transfer learning we used four different baseline models namely inception v3 vgg mobilenet and resnet50 from the imagenet competition and extended them for doodle classification figure 4 	due to constraints on computational resources we optionally finetuned the top two layers of the original network 
0	131	8689	for logistic regression and svms we ran these models locally due to our local machines computational power constraints we ran the cnns and transfer learning models on google cloud on a virtual machine with 320gb of local disk 12 cores of cpu 64gb of ram and an nvidia p100 gpu with 16gb of memory we used google cloud deep learning machine image	for logistic regression and svms we ran these models locally 
0	131	8690	for logistic regression and svms we ran these models locally due to our local machines computational power constraints we ran the cnns and transfer learning models on google cloud on a virtual machine with 320gb of local disk 12 cores of cpu 64gb of ram and an nvidia p100 gpu with 16gb of memory we used google cloud deep learning machine image	due to our local machines computational power constraints we ran the cnns and transfer learning models on google cloud on a virtual machine with 320gb of local disk 12 cores of cpu 64gb of ram and an nvidia p100 gpu with 16gb of memory 
0	131	8691	for logistic regression and svms we ran these models locally due to our local machines computational power constraints we ran the cnns and transfer learning models on google cloud on a virtual machine with 320gb of local disk 12 cores of cpu 64gb of ram and an nvidia p100 gpu with 16gb of memory we used google cloud deep learning machine image	we used google cloud deep learning machine image
1	131	8692	logistic regression was implemented using python s scikit learn framework putting these numbers in context for the 50 class dataset a classifier that randomly guesses the class would have expected accuracy of 2 we see that the logistic regression classifier did relatively well that said there was still room for improvement and we performed error analysis next to understand the types of errors that the classifier was making looking at the confusion matrix we can see that logistic regression performs relatively well 	logistic regression was implemented using python s scikit learn framework putting these numbers in context for the 50 class dataset a classifier that randomly guesses the class would have expected accuracy of 2 
1	131	8693	logistic regression was implemented using python s scikit learn framework putting these numbers in context for the 50 class dataset a classifier that randomly guesses the class would have expected accuracy of 2 we see that the logistic regression classifier did relatively well that said there was still room for improvement and we performed error analysis next to understand the types of errors that the classifier was making looking at the confusion matrix we can see that logistic regression performs relatively well 	we see that the logistic regression classifier did relatively well that said there was still room for improvement and we performed error analysis next to understand the types of errors that the classifier was making 
0	131	8694	logistic regression was implemented using python s scikit learn framework putting these numbers in context for the 50 class dataset a classifier that randomly guesses the class would have expected accuracy of 2 we see that the logistic regression classifier did relatively well that said there was still room for improvement and we performed error analysis next to understand the types of errors that the classifier was making looking at the confusion matrix we can see that logistic regression performs relatively well 	looking at the confusion matrix we can see that logistic regression performs relatively well 
0	131	8695	logistic regression was implemented using python s scikit learn framework putting these numbers in context for the 50 class dataset a classifier that randomly guesses the class would have expected accuracy of 2 we see that the logistic regression classifier did relatively well that said there was still room for improvement and we performed error analysis next to understand the types of errors that the classifier was making looking at the confusion matrix we can see that logistic regression performs relatively well 	the diagonal of the confusion matrix carries the most weight indicating it often makes the correct prediction 
0	131	8696	logistic regression was implemented using python s scikit learn framework putting these numbers in context for the 50 class dataset a classifier that randomly guesses the class would have expected accuracy of 2 we see that the logistic regression classifier did relatively well that said there was still room for improvement and we performed error analysis next to understand the types of errors that the classifier was making looking at the confusion matrix we can see that logistic regression performs relatively well 	we notice that in the wrongly classified regions the true label banana is highly misclassified with hockey stick 
0	131	8697	logistic regression was implemented using python s scikit learn framework putting these numbers in context for the 50 class dataset a classifier that randomly guesses the class would have expected accuracy of 2 we see that the logistic regression classifier did relatively well that said there was still room for improvement and we performed error analysis next to understand the types of errors that the classifier was making looking at the confusion matrix we can see that logistic regression performs relatively well 	this is expected as a hand drawn banana is very similar to a hand drawn hockey stick as seen in figures 6 and 7 
1	131	8698	logistic regression was implemented using python s scikit learn framework putting these numbers in context for the 50 class dataset a classifier that randomly guesses the class would have expected accuracy of 2 we see that the logistic regression classifier did relatively well that said there was still room for improvement and we performed error analysis next to understand the types of errors that the classifier was making looking at the confusion matrix we can see that logistic regression performs relatively well 	thus this experiment suggests that in order to predict hand drawn doodles contrary to our initial belief we may need a more sophisticated model instead of a simpler model because the quality of the drawing may not be very good 
0	131	8699	logistic regression was implemented using python s scikit learn framework putting these numbers in context for the 50 class dataset a classifier that randomly guesses the class would have expected accuracy of 2 we see that the logistic regression classifier did relatively well that said there was still room for improvement and we performed error analysis next to understand the types of errors that the classifier was making looking at the confusion matrix we can see that logistic regression performs relatively well 	to our surprise the svms performed worse than linear regression overall 
1	131	8700	logistic regression was implemented using python s scikit learn framework putting these numbers in context for the 50 class dataset a classifier that randomly guesses the class would have expected accuracy of 2 we see that the logistic regression classifier did relatively well that said there was still room for improvement and we performed error analysis next to understand the types of errors that the classifier was making looking at the confusion matrix we can see that logistic regression performs relatively well 	however this could be due to the fact that we have not done extensive hyperparameter tuning for svms among our different choices of kernels the rbf kernel performed the best followed by the polynomial kernel with degree 5 then the linear kernel 
1	131	8701	logistic regression was implemented using python s scikit learn framework putting these numbers in context for the 50 class dataset a classifier that randomly guesses the class would have expected accuracy of 2 we see that the logistic regression classifier did relatively well that said there was still room for improvement and we performed error analysis next to understand the types of errors that the classifier was making looking at the confusion matrix we can see that logistic regression performs relatively well 	the sigmoid kernel performed the worst with an accuracy equitable to assigning a category at random this result is consistent with what we expected 
0	131	8702	logistic regression was implemented using python s scikit learn framework putting these numbers in context for the 50 class dataset a classifier that randomly guesses the class would have expected accuracy of 2 we see that the logistic regression classifier did relatively well that said there was still room for improvement and we performed error analysis next to understand the types of errors that the classifier was making looking at the confusion matrix we can see that logistic regression performs relatively well 	the accuracy corresponds to the complexity of the feature space with rbf corresponding to an infinite feature space and polynomial and linear having fewer features 
0	131	8703	logistic regression was implemented using python s scikit learn framework putting these numbers in context for the 50 class dataset a classifier that randomly guesses the class would have expected accuracy of 2 we see that the logistic regression classifier did relatively well that said there was still room for improvement and we performed error analysis next to understand the types of errors that the classifier was making looking at the confusion matrix we can see that logistic regression performs relatively well 	although the sigmoid corresponds to a higher dimensional feature space its corresponding kernel matrix is not guaranteed to be positive semi definite 
0	131	8704	logistic regression was implemented using python s scikit learn framework putting these numbers in context for the 50 class dataset a classifier that randomly guesses the class would have expected accuracy of 2 we see that the logistic regression classifier did relatively well that said there was still room for improvement and we performed error analysis next to understand the types of errors that the classifier was making looking at the confusion matrix we can see that logistic regression performs relatively well 	therefore if the chosen parameters are not well tuned the algorithm can perform worse than random
0	131	8705	we tested four different variations of a basic cnn using binarized and non binarized data a base v1 model included two convolutions and two max pool layers from which we then iteratively simplified as shown in we find that the best performing cnn on the larger classification task of 50 classes was v2 which attained a validation set accuracy of 81 83 with the confusion matrix below there is a close correlation between the training accuracy and validation accuracy 	we tested four different variations of a basic cnn using binarized and non binarized data 
1	131	8706	we tested four different variations of a basic cnn using binarized and non binarized data a base v1 model included two convolutions and two max pool layers from which we then iteratively simplified as shown in we find that the best performing cnn on the larger classification task of 50 classes was v2 which attained a validation set accuracy of 81 83 with the confusion matrix below there is a close correlation between the training accuracy and validation accuracy 	a base v1 model included two convolutions and two max pool layers from which we then iteratively simplified as shown in we find that the best performing cnn on the larger classification task of 50 classes was v2 which attained a validation set accuracy of 81 83 with the confusion matrix below 
0	131	8707	we tested four different variations of a basic cnn using binarized and non binarized data a base v1 model included two convolutions and two max pool layers from which we then iteratively simplified as shown in we find that the best performing cnn on the larger classification task of 50 classes was v2 which attained a validation set accuracy of 81 83 with the confusion matrix below there is a close correlation between the training accuracy and validation accuracy 	there is a close correlation between the training accuracy and validation accuracy 
0	131	8708	we tested four different variations of a basic cnn using binarized and non binarized data a base v1 model included two convolutions and two max pool layers from which we then iteratively simplified as shown in we find that the best performing cnn on the larger classification task of 50 classes was v2 which attained a validation set accuracy of 81 83 with the confusion matrix below there is a close correlation between the training accuracy and validation accuracy 	testing this v2 model on the test set we obtained a final accuracy score of 81 87 
1	131	8709	we tested four different variations of a basic cnn using binarized and non binarized data a base v1 model included two convolutions and two max pool layers from which we then iteratively simplified as shown in we find that the best performing cnn on the larger classification task of 50 classes was v2 which attained a validation set accuracy of 81 83 with the confusion matrix below there is a close correlation between the training accuracy and validation accuracy 	this shows that our trained model is able to generalize well to unseen data in general simple cnns performed well for our task 
0	131	8710	we tested four different variations of a basic cnn using binarized and non binarized data a base v1 model included two convolutions and two max pool layers from which we then iteratively simplified as shown in we find that the best performing cnn on the larger classification task of 50 classes was v2 which attained a validation set accuracy of 81 83 with the confusion matrix below there is a close correlation between the training accuracy and validation accuracy 	with fewer classes the accuracy is roughly the same when taking away layers but the training time decreases 
1	131	8711	we tested four different variations of a basic cnn using binarized and non binarized data a base v1 model included two convolutions and two max pool layers from which we then iteratively simplified as shown in we find that the best performing cnn on the larger classification task of 50 classes was v2 which attained a validation set accuracy of 81 83 with the confusion matrix below there is a close correlation between the training accuracy and validation accuracy 	with more classes accuracy decreases with fewer layers as expected but the lowest accuracy is still significantly greater than the accuracy found using logistic regression 
1	131	8712	we ran transfer learning with weights pre trained on the imagenet dataset with four different architectures vgg a model proposed in the imagenet 2013 challenge which was widely used due to its simple architecture consisting of only repeated units of convolutional layers followed by max pooling the results of transfer learning experiments are discussed below we noticed however that the best performing model on the quickdraw dataset vgg is also the one with the simplest architecture this suggests that more complex architectures such as the inception module used in inception v3 or the residual block used in resnet50 may not be beneficial for the problem of doodle classification since doodles are simple drawings only using the classic convolutional and max pool layers may be the best 	we ran transfer learning with weights pre trained on the imagenet dataset with four different architectures vgg a model proposed in the imagenet 2013 challenge which was widely used due to its simple architecture consisting of only repeated units of convolutional layers followed by max pooling the results of transfer learning experiments are discussed below we noticed however that the best performing model on the quickdraw dataset vgg is also the one with the simplest architecture 
1	131	8713	we ran transfer learning with weights pre trained on the imagenet dataset with four different architectures vgg a model proposed in the imagenet 2013 challenge which was widely used due to its simple architecture consisting of only repeated units of convolutional layers followed by max pooling the results of transfer learning experiments are discussed below we noticed however that the best performing model on the quickdraw dataset vgg is also the one with the simplest architecture this suggests that more complex architectures such as the inception module used in inception v3 or the residual block used in resnet50 may not be beneficial for the problem of doodle classification since doodles are simple drawings only using the classic convolutional and max pool layers may be the best 	this suggests that more complex architectures such as the inception module used in inception v3 or the residual block used in resnet50 may not be beneficial for the problem of doodle classification 
0	131	8714	we ran transfer learning with weights pre trained on the imagenet dataset with four different architectures vgg a model proposed in the imagenet 2013 challenge which was widely used due to its simple architecture consisting of only repeated units of convolutional layers followed by max pooling the results of transfer learning experiments are discussed below we noticed however that the best performing model on the quickdraw dataset vgg is also the one with the simplest architecture this suggests that more complex architectures such as the inception module used in inception v3 or the residual block used in resnet50 may not be beneficial for the problem of doodle classification since doodles are simple drawings only using the classic convolutional and max pool layers may be the best 	since doodles are simple drawings only using the classic convolutional and max pool layers may be the best 
0	131	8715	we ran transfer learning with weights pre trained on the imagenet dataset with four different architectures vgg a model proposed in the imagenet 2013 challenge which was widely used due to its simple architecture consisting of only repeated units of convolutional layers followed by max pooling the results of transfer learning experiments are discussed below we noticed however that the best performing model on the quickdraw dataset vgg is also the one with the simplest architecture this suggests that more complex architectures such as the inception module used in inception v3 or the residual block used in resnet50 may not be beneficial for the problem of doodle classification since doodles are simple drawings only using the classic convolutional and max pool layers may be the best 	this also corroborates our earlier finding with cnn where our simple cnn built from the ground up did well on the quickdraw dataset 
0	131	8716	we ran transfer learning with weights pre trained on the imagenet dataset with four different architectures vgg a model proposed in the imagenet 2013 challenge which was widely used due to its simple architecture consisting of only repeated units of convolutional layers followed by max pooling the results of transfer learning experiments are discussed below we noticed however that the best performing model on the quickdraw dataset vgg is also the one with the simplest architecture this suggests that more complex architectures such as the inception module used in inception v3 or the residual block used in resnet50 may not be beneficial for the problem of doodle classification since doodles are simple drawings only using the classic convolutional and max pool layers may be the best 	in terms of training time mobilenet performed the best 
0	131	8717	we ran transfer learning with weights pre trained on the imagenet dataset with four different architectures vgg a model proposed in the imagenet 2013 challenge which was widely used due to its simple architecture consisting of only repeated units of convolutional layers followed by max pooling the results of transfer learning experiments are discussed below we noticed however that the best performing model on the quickdraw dataset vgg is also the one with the simplest architecture this suggests that more complex architectures such as the inception module used in inception v3 or the residual block used in resnet50 may not be beneficial for the problem of doodle classification since doodles are simple drawings only using the classic convolutional and max pool layers may be the best 	this is expected since the model is optimized for efficiency and has the smallest number of parameters 
0	131	8718	we ran transfer learning with weights pre trained on the imagenet dataset with four different architectures vgg a model proposed in the imagenet 2013 challenge which was widely used due to its simple architecture consisting of only repeated units of convolutional layers followed by max pooling the results of transfer learning experiments are discussed below we noticed however that the best performing model on the quickdraw dataset vgg is also the one with the simplest architecture this suggests that more complex architectures such as the inception module used in inception v3 or the residual block used in resnet50 may not be beneficial for the problem of doodle classification since doodles are simple drawings only using the classic convolutional and max pool layers may be the best 	overall the trend in training time follows the number of parameters in the base model which is expected 
0	131	8719	we ran transfer learning with weights pre trained on the imagenet dataset with four different architectures vgg a model proposed in the imagenet 2013 challenge which was widely used due to its simple architecture consisting of only repeated units of convolutional layers followed by max pooling the results of transfer learning experiments are discussed below we noticed however that the best performing model on the quickdraw dataset vgg is also the one with the simplest architecture this suggests that more complex architectures such as the inception module used in inception v3 or the residual block used in resnet50 may not be beneficial for the problem of doodle classification since doodles are simple drawings only using the classic convolutional and max pool layers may be the best 	the exception of inception v3 whose training time is the largest despite it having the second largest number of parameters of all the four models 
0	131	8720	we ran transfer learning with weights pre trained on the imagenet dataset with four different architectures vgg a model proposed in the imagenet 2013 challenge which was widely used due to its simple architecture consisting of only repeated units of convolutional layers followed by max pooling the results of transfer learning experiments are discussed below we noticed however that the best performing model on the quickdraw dataset vgg is also the one with the simplest architecture this suggests that more complex architectures such as the inception module used in inception v3 or the residual block used in resnet50 may not be beneficial for the problem of doodle classification since doodles are simple drawings only using the classic convolutional and max pool layers may be the best 	this was due to us additionally fine tuning the parameters of the two top most layers of inception v3 
1	131	8721	we ran transfer learning with weights pre trained on the imagenet dataset with four different architectures vgg a model proposed in the imagenet 2013 challenge which was widely used due to its simple architecture consisting of only repeated units of convolutional layers followed by max pooling the results of transfer learning experiments are discussed below we noticed however that the best performing model on the quickdraw dataset vgg is also the one with the simplest architecture this suggests that more complex architectures such as the inception module used in inception v3 or the residual block used in resnet50 may not be beneficial for the problem of doodle classification since doodles are simple drawings only using the classic convolutional and max pool layers may be the best 	this was because transfer learning with inception v3 was performing poorly in terms of accuracy and we wanted to see if further tuning hyper parameters would help 
1	131	8722	we ran transfer learning with weights pre trained on the imagenet dataset with four different architectures vgg a model proposed in the imagenet 2013 challenge which was widely used due to its simple architecture consisting of only repeated units of convolutional layers followed by max pooling the results of transfer learning experiments are discussed below we noticed however that the best performing model on the quickdraw dataset vgg is also the one with the simplest architecture this suggests that more complex architectures such as the inception module used in inception v3 or the residual block used in resnet50 may not be beneficial for the problem of doodle classification since doodles are simple drawings only using the classic convolutional and max pool layers may be the best 	overall we find that optimizing the model by reducing the number of parameters will help with reducing training time which further supports our initial push for simplifying the models to achieve higher efficiency 
0	131	8723	our project aimed to recognize the meaning of doodles a critical first task in order to build any system that uses hand drawn images for communication we focused on doodle recognition with an emphasis on efficiency in conjunction with accuracy after implementing logistic regression svms cnns and transfer learning and analyzing our results we found that a simplified cnn was best for the task balancing both accuracy and training time 	our project aimed to recognize the meaning of doodles a critical first task in order to build any system that uses hand drawn images for communication 
0	131	8724	our project aimed to recognize the meaning of doodles a critical first task in order to build any system that uses hand drawn images for communication we focused on doodle recognition with an emphasis on efficiency in conjunction with accuracy after implementing logistic regression svms cnns and transfer learning and analyzing our results we found that a simplified cnn was best for the task balancing both accuracy and training time 	we focused on doodle recognition with an emphasis on efficiency in conjunction with accuracy 
1	131	8725	our project aimed to recognize the meaning of doodles a critical first task in order to build any system that uses hand drawn images for communication we focused on doodle recognition with an emphasis on efficiency in conjunction with accuracy after implementing logistic regression svms cnns and transfer learning and analyzing our results we found that a simplified cnn was best for the task balancing both accuracy and training time 	after implementing logistic regression svms cnns and transfer learning and analyzing our results we found that a simplified cnn was best for the task balancing both accuracy and training time 
1	131	8726	our project aimed to recognize the meaning of doodles a critical first task in order to build any system that uses hand drawn images for communication we focused on doodle recognition with an emphasis on efficiency in conjunction with accuracy after implementing logistic regression svms cnns and transfer learning and analyzing our results we found that a simplified cnn was best for the task balancing both accuracy and training time 	we also found that for simpler images such as doodles using classic architectures such as a combination of convolutional and max pool layers can outperform complex architectures for future work we would further develop our most promising approach by performing more extensive experiments to determine the effect of each layer in the cnn 
1	131	8727	we would also explore using transfer learning as a fixed feature extractor for logistic regression our fastest model given more time we would also love to explore working on efficiency in conjunction with smaller datasets each team member contributed equally to this project 	we would also explore using transfer learning as a fixed feature extractor for logistic regression our fastest model 
0	131	8728	we would also explore using transfer learning as a fixed feature extractor for logistic regression our fastest model given more time we would also love to explore working on efficiency in conjunction with smaller datasets each team member contributed equally to this project 	given more time we would also love to explore working on efficiency in conjunction with smaller datasets each team member contributed equally to this project 
0	132	8729	in this paper we are exploring the generation of depthmaps from a sequence of images compared to similar projects in the field we have decided to incorporate both spatial cnn and temporal lstm aspects in our model by creating convlstm cells these are used in a u net encoder decoder architecture 	in this paper we are exploring the generation of depthmaps from a sequence of images 
0	132	8730	in this paper we are exploring the generation of depthmaps from a sequence of images compared to similar projects in the field we have decided to incorporate both spatial cnn and temporal lstm aspects in our model by creating convlstm cells these are used in a u net encoder decoder architecture 	compared to similar projects in the field we have decided to incorporate both spatial cnn and temporal lstm aspects in our model by creating convlstm cells 
0	132	8731	in this paper we are exploring the generation of depthmaps from a sequence of images compared to similar projects in the field we have decided to incorporate both spatial cnn and temporal lstm aspects in our model by creating convlstm cells these are used in a u net encoder decoder architecture 	these are used in a u net encoder decoder architecture 
0	132	8732	in this paper we are exploring the generation of depthmaps from a sequence of images compared to similar projects in the field we have decided to incorporate both spatial cnn and temporal lstm aspects in our model by creating convlstm cells these are used in a u net encoder decoder architecture 	the results indicate some potential in such an approach 
0	132	8733	hardware progress has enabled solutions which were historically computationally intractable this is particularly true in video analysis this technological advance has opened a new frontier of problems 	hardware progress has enabled solutions which were historically computationally intractable 
0	132	8734	hardware progress has enabled solutions which were historically computationally intractable this is particularly true in video analysis this technological advance has opened a new frontier of problems 	this is particularly true in video analysis 
0	132	8735	hardware progress has enabled solutions which were historically computationally intractable this is particularly true in video analysis this technological advance has opened a new frontier of problems 	this technological advance has opened a new frontier of problems 
0	132	8736	hardware progress has enabled solutions which were historically computationally intractable this is particularly true in video analysis this technological advance has opened a new frontier of problems 	within this expanse we have chosen the classic problem of depth inference from images 
1	132	8737	hardware progress has enabled solutions which were historically computationally intractable this is particularly true in video analysis this technological advance has opened a new frontier of problems 	specifically given a sequence of images captured over time we output depth maps corresponding one to one with the input sequence 
1	132	8738	hardware progress has enabled solutions which were historically computationally intractable this is particularly true in video analysis this technological advance has opened a new frontier of problems 	as a spatiotemporal problem we were motivated to model it with convolutions spatial and lstms temporal the input to our algorithm is a sequence of images 
1	132	8739	hardware progress has enabled solutions which were historically computationally intractable this is particularly true in video analysis this technological advance has opened a new frontier of problems 	we then use a neural network u net encoder decoder architecture with bi convlstm cells for encoding and convolutions and transconvolutions to decode to output a predicted depth map sequence 
1	132	8740	hardware progress has enabled solutions which were historically computationally intractable this is particularly true in video analysis this technological advance has opened a new frontier of problems 	as we deal with sequences of images this process is many to many where for each input image we output one depth map 
0	132	8741	hardware progress has enabled solutions which were historically computationally intractable this is particularly true in video analysis this technological advance has opened a new frontier of problems 	solutions to the above problem would enable 3d world generation from simple video input with applications from vr to robotics 
0	132	8742	hardware progress has enabled solutions which were historically computationally intractable this is particularly true in video analysis this technological advance has opened a new frontier of problems 	while there are hardware approaches to depth determination problems such as lidar or multiple lenses software solutions provide flexibility in their application 
0	132	8743	after researching this initial problem in depth we became familiar with literature on depth maps their algorithms and datasets this presented itself as a sensible path forward as it seemed simpler and better scoped this area is a classic one with not only history but ongoing and recent progress 	after researching this initial problem in depth we became familiar with literature on depth maps their algorithms and datasets 
0	132	8744	after researching this initial problem in depth we became familiar with literature on depth maps their algorithms and datasets this presented itself as a sensible path forward as it seemed simpler and better scoped this area is a classic one with not only history but ongoing and recent progress 	this presented itself as a sensible path forward as it seemed simpler and better scoped 
0	132	8745	after researching this initial problem in depth we became familiar with literature on depth maps their algorithms and datasets this presented itself as a sensible path forward as it seemed simpler and better scoped this area is a classic one with not only history but ongoing and recent progress 	this area is a classic one with not only history but ongoing and recent progress 
1	132	8746	after researching this initial problem in depth we became familiar with literature on depth maps their algorithms and datasets this presented itself as a sensible path forward as it seemed simpler and better scoped this area is a classic one with not only history but ongoing and recent progress 	concerning depth maps there are various families of problems single image to depth map depth map alignments from sparse to dense but given the background research we d done on the image depth map sequence we were naturally drawn to the most similar problem from a sequence of images generate a sequence of depth maps there are many reasons to be excited about such a problem especially as the interest for spatiotemporal models is booming 
1	132	8747	after researching this initial problem in depth we became familiar with literature on depth maps their algorithms and datasets this presented itself as a sensible path forward as it seemed simpler and better scoped this area is a classic one with not only history but ongoing and recent progress 	for us however we wanted to learn about rnns and cnns and as space time lends itself to natural conceptions of convolutions and recurrent networks we proceeded down that path quite excited to apply modern rnn and cnn techniques we were both disappointed and relieved to find extremely relevant literature depthnet while there some people praise cnn to the detriment of rnn we wanted to explore this avenue further 
0	132	8748	after researching this initial problem in depth we became familiar with literature on depth maps their algorithms and datasets this presented itself as a sensible path forward as it seemed simpler and better scoped this area is a classic one with not only history but ongoing and recent progress 	in pursuit of this approach we have our own opinion as will be discussed at the end 
1	132	8749	it is fitting to begin with paper that introduced the core unit of our model convolutional lstm a machine learning approach for precipitation nowcasting we chose depthnet there are many great people and great ideas 	it is fitting to begin with paper that introduced the core unit of our model convolutional lstm a machine learning approach for precipitation nowcasting we chose depthnet there are many great people and great ideas 
1	132	8750	in the search for a dataset with both picture and depth map we have decided to use the kitti dataset	in the search for a dataset with both picture and depth map we have decided to use the kitti dataset
0	132	8751	first we organized the data and store image sequences in subfolders as it seems to simplify and speed up the training	first we organized the data and store image sequences in subfolders as it seems to simplify and speed up the training
1	132	8752	convlstms are more than a convolutional layer into an lstm layer they convolve on the hidden state and the input together this functional difference has led to some speculation as to the merits of one over the other where convlstms sometimes prove more effective as in very deep convolutional networks for end to end speech recognition the specific math for a convlstm is where refers to a convolution operation and to the hadamard product 	convlstms are more than a convolutional layer into an lstm layer they convolve on the hidden state and the input together 
1	132	8753	convlstms are more than a convolutional layer into an lstm layer they convolve on the hidden state and the input together this functional difference has led to some speculation as to the merits of one over the other where convlstms sometimes prove more effective as in very deep convolutional networks for end to end speech recognition the specific math for a convlstm is where refers to a convolution operation and to the hadamard product 	this functional difference has led to some speculation as to the merits of one over the other where convlstms sometimes prove more effective as in very deep convolutional networks for end to end speech recognition the specific math for a convlstm is where refers to a convolution operation and to the hadamard product 
0	132	8754	we picked this model to optimize for simplicity of approach while maintain sophistication of the model s capacity the u net structure is symmetrical and conceptually simple and the main complexity is within its subcomponent bi convlstm this subcomponent could be tinkered without alteration to the whole 	we picked this model to optimize for simplicity of approach while maintain sophistication of the model s capacity 
0	132	8755	we picked this model to optimize for simplicity of approach while maintain sophistication of the model s capacity the u net structure is symmetrical and conceptually simple and the main complexity is within its subcomponent bi convlstm this subcomponent could be tinkered without alteration to the whole 	the u net structure is symmetrical and conceptually simple and the main complexity is within its subcomponent bi convlstm 
0	132	8756	we picked this model to optimize for simplicity of approach while maintain sophistication of the model s capacity the u net structure is symmetrical and conceptually simple and the main complexity is within its subcomponent bi convlstm this subcomponent could be tinkered without alteration to the whole 	this subcomponent could be tinkered without alteration to the whole 
0	132	8757	we picked this model to optimize for simplicity of approach while maintain sophistication of the model s capacity the u net structure is symmetrical and conceptually simple and the main complexity is within its subcomponent bi convlstm this subcomponent could be tinkered without alteration to the whole 	the inputs to our network are 5d tensors b n c h w where b refers to batch size n to sequence length c to channels h to height and w to width 
0	132	8758	we picked this model to optimize for simplicity of approach while maintain sophistication of the model s capacity the u net structure is symmetrical and conceptually simple and the main complexity is within its subcomponent bi convlstm this subcomponent could be tinkered without alteration to the whole 	per layer the number of filters and therefore output channels of that layer increase during the encoding phase starting from 3 rgb and decrease during the decoding phase finishing at 1 depth 
0	132	8759	we picked this model to optimize for simplicity of approach while maintain sophistication of the model s capacity the u net structure is symmetrical and conceptually simple and the main complexity is within its subcomponent bi convlstm this subcomponent could be tinkered without alteration to the whole 	we use relu activation functions for each encoding layer and at the last step of the decoding phase 
0	132	8760	we picked this model to optimize for simplicity of approach while maintain sophistication of the model s capacity the u net structure is symmetrical and conceptually simple and the main complexity is within its subcomponent bi convlstm this subcomponent could be tinkered without alteration to the whole 	skip connections in the u net structure pass forward outputs to later layers concatenating with the output of the directly previous layer 
0	132	8761	we picked this model to optimize for simplicity of approach while maintain sophistication of the model s capacity the u net structure is symmetrical and conceptually simple and the main complexity is within its subcomponent bi convlstm this subcomponent could be tinkered without alteration to the whole 	see figures 2 3 for greater details 
0	132	8762	for this project we are using two separate machines with both a recent nvidia gpu 1080ti and p100 our current implementation of the model uses pytorch 0 4 1	for this project we are using two separate machines with both a recent nvidia gpu 1080ti and p100 
0	132	8763	for this project we are using two separate machines with both a recent nvidia gpu 1080ti and p100 our current implementation of the model uses pytorch 0 4 1	our current implementation of the model uses pytorch 0 4 1
0	132	8764	we used multiple metrics for training and evaluation purposes based on properties distinct to each function specifically rmse irmse mae imae and a custom scale invariant loss rmsewhere d i log y i log y i for the i th pixel n the number of pixels and 0 5 succinctly rmse is standard and easy to implement while providing comparison against other models 	we used multiple metrics for training and evaluation purposes based on properties distinct to each function 
0	132	8765	we used multiple metrics for training and evaluation purposes based on properties distinct to each function specifically rmse irmse mae imae and a custom scale invariant loss rmsewhere d i log y i log y i for the i th pixel n the number of pixels and 0 5 succinctly rmse is standard and easy to implement while providing comparison against other models 	specifically rmse irmse mae imae and a custom scale invariant loss 
1	132	8766	we used multiple metrics for training and evaluation purposes based on properties distinct to each function specifically rmse irmse mae imae and a custom scale invariant loss rmsewhere d i log y i log y i for the i th pixel n the number of pixels and 0 5 succinctly rmse is standard and easy to implement while providing comparison against other models 	rmsewhere d i log y i log y i for the i th pixel n the number of pixels and 0 5 succinctly rmse is standard and easy to implement while providing comparison against other models 
1	132	8767	we used multiple metrics for training and evaluation purposes based on properties distinct to each function specifically rmse irmse mae imae and a custom scale invariant loss rmsewhere d i log y i log y i for the i th pixel n the number of pixels and 0 5 succinctly rmse is standard and easy to implement while providing comparison against other models 	it is distinct from mae in that rmse significantly larger than mae indicates large variance of error distribution frequency finally a1 a2 and a3 metrics are accuracies that represent the percent of pixels that fall within a threshold ratio of inferred depth value and ground truth depth value 
0	132	8768	we used multiple metrics for training and evaluation purposes based on properties distinct to each function specifically rmse irmse mae imae and a custom scale invariant loss rmsewhere d i log y i log y i for the i th pixel n the number of pixels and 0 5 succinctly rmse is standard and easy to implement while providing comparison against other models 	a refers to a base and 1 2 3 refer to powers of that base ie a3 is the most lenient and a1 the strictest 
0	132	8769	we used multiple metrics for training and evaluation purposes based on properties distinct to each function specifically rmse irmse mae imae and a custom scale invariant loss rmsewhere d i log y i log y i for the i th pixel n the number of pixels and 0 5 succinctly rmse is standard and easy to implement while providing comparison against other models 	these accuracies are independent of image size and therefore ideal for baseline comparison 
0	132	8770	we used multiple metrics for training and evaluation purposes based on properties distinct to each function specifically rmse irmse mae imae and a custom scale invariant loss rmsewhere d i log y i log y i for the i th pixel n the number of pixels and 0 5 succinctly rmse is standard and easy to implement while providing comparison against other models 	also whereas losses provide an unintuitive metric of goodness and progress accuracy is more comprehensible 
0	132	8771	we used multiple metrics for training and evaluation purposes based on properties distinct to each function specifically rmse irmse mae imae and a custom scale invariant loss rmsewhere d i log y i log y i for the i th pixel n the number of pixels and 0 5 succinctly rmse is standard and easy to implement while providing comparison against other models 	multiple a values indicate the distribution of inferences 
0	132	8772	we are comparing ourselves most directly to depthnet and other kitti competitors with the corresponding loss measures the current two leaders are dl 61 dorn and dl sord sq 	we are comparing ourselves most directly to depthnet and other kitti competitors with the corresponding loss measures 
0	132	8773	we are comparing ourselves most directly to depthnet and other kitti competitors with the corresponding loss measures the current two leaders are dl 61 dorn and dl sord sq 	the current two leaders are dl 61 dorn and dl sord sq 
0	132	8774	comparing sequences of length 1 to 6 we see that the 6 outperformed the 1 on every metric this implies that the lstm does provide utility to analysis and that over time information holds clues to depth this somewhat justifies the theory behind the model and is consistent with the results of previous teams such as depthnet 	comparing sequences of length 1 to 6 we see that the 6 outperformed the 1 on every metric 
0	132	8775	comparing sequences of length 1 to 6 we see that the 6 outperformed the 1 on every metric this implies that the lstm does provide utility to analysis and that over time information holds clues to depth this somewhat justifies the theory behind the model and is consistent with the results of previous teams such as depthnet 	this implies that the lstm does provide utility to analysis and that over time information holds clues to depth 
0	132	8776	comparing sequences of length 1 to 6 we see that the 6 outperformed the 1 on every metric this implies that the lstm does provide utility to analysis and that over time information holds clues to depth this somewhat justifies the theory behind the model and is consistent with the results of previous teams such as depthnet 	this somewhat justifies the theory behind the model and is consistent with the results of previous teams such as depthnet 
0	132	8777	we presented a different approach to image to depthmap implementation using jointly a u net architecture and convlstm cells we tried to incorporate both spacial and temporal elements to insure consistency when generating depth maps for sequences of images i e video there are several areas to continue our work here 	we presented a different approach to image to depthmap implementation 
1	132	8778	we presented a different approach to image to depthmap implementation using jointly a u net architecture and convlstm cells we tried to incorporate both spacial and temporal elements to insure consistency when generating depth maps for sequences of images i e video there are several areas to continue our work here 	using jointly a u net architecture and convlstm cells we tried to incorporate both spacial and temporal elements to insure consistency when generating depth maps for sequences of images i e 
0	132	8779	we presented a different approach to image to depthmap implementation using jointly a u net architecture and convlstm cells we tried to incorporate both spacial and temporal elements to insure consistency when generating depth maps for sequences of images i e video there are several areas to continue our work here 	video there are several areas to continue our work here 
0	132	8780	we presented a different approach to image to depthmap implementation using jointly a u net architecture and convlstm cells we tried to incorporate both spacial and temporal elements to insure consistency when generating depth maps for sequences of images i e video there are several areas to continue our work here 	we d first like to train for an extra week and see if we continue to progress towards convergence 
0	132	8781	we presented a different approach to image to depthmap implementation using jointly a u net architecture and convlstm cells we tried to incorporate both spacial and temporal elements to insure consistency when generating depth maps for sequences of images i e video there are several areas to continue our work here 	first increasing sequence length 
0	132	8782	we presented a different approach to image to depthmap implementation using jointly a u net architecture and convlstm cells we tried to incorporate both spacial and temporal elements to insure consistency when generating depth maps for sequences of images i e video there are several areas to continue our work here 	while we limited ourselves to a length of 6 we are curious as to the impact a sequence of 600 would compare 
0	132	8783	we presented a different approach to image to depthmap implementation using jointly a u net architecture and convlstm cells we tried to incorporate both spacial and temporal elements to insure consistency when generating depth maps for sequences of images i e video there are several areas to continue our work here 	second data processing 
0	132	8784	we presented a different approach to image to depthmap implementation using jointly a u net architecture and convlstm cells we tried to incorporate both spacial and temporal elements to insure consistency when generating depth maps for sequences of images i e video there are several areas to continue our work here 	there are many transformation alternatives to be played with 
0	132	8785	we presented a different approach to image to depthmap implementation using jointly a u net architecture and convlstm cells we tried to incorporate both spacial and temporal elements to insure consistency when generating depth maps for sequences of images i e video there are several areas to continue our work here 	we especially would like to train on bigger image sizes if we had more time and compute 
1	132	8786	we presented a different approach to image to depthmap implementation using jointly a u net architecture and convlstm cells we tried to incorporate both spacial and temporal elements to insure consistency when generating depth maps for sequences of images i e video there are several areas to continue our work here 	there are also multiple ways to iterate over the data as for loss functions we used many of them for evaluation and it d be interesting to explore if any of those is better than our custom loss for guiding the gradient and training 
1	132	8787	we presented a different approach to image to depthmap implementation using jointly a u net architecture and convlstm cells we tried to incorporate both spacial and temporal elements to insure consistency when generating depth maps for sequences of images i e video there are several areas to continue our work here 	beyond that we would like to play with kernel size and the number of filters per layer as there are interesting questions in the optimal number per layer 
0	132	8788	we presented a different approach to image to depthmap implementation using jointly a u net architecture and convlstm cells we tried to incorporate both spacial and temporal elements to insure consistency when generating depth maps for sequences of images i e video there are several areas to continue our work here 	expanding in creativity how would increasing the number of encodingdecoding layers affect performance 
0	132	8789	we presented a different approach to image to depthmap implementation using jointly a u net architecture and convlstm cells we tried to incorporate both spacial and temporal elements to insure consistency when generating depth maps for sequences of images i e video there are several areas to continue our work here 	we did not nearly approach overfitting problems 
0	132	8790	we presented a different approach to image to depthmap implementation using jointly a u net architecture and convlstm cells we tried to incorporate both spacial and temporal elements to insure consistency when generating depth maps for sequences of images i e video there are several areas to continue our work here 	more dramatically what are the effects of u net 
0	132	8791	we presented a different approach to image to depthmap implementation using jointly a u net architecture and convlstm cells we tried to incorporate both spacial and temporal elements to insure consistency when generating depth maps for sequences of images i e video there are several areas to continue our work here 	if we were to remove the skip connections how would we perform 
0	132	8792	we presented a different approach to image to depthmap implementation using jointly a u net architecture and convlstm cells we tried to incorporate both spacial and temporal elements to insure consistency when generating depth maps for sequences of images i e video there are several areas to continue our work here 	other advanced techniques invite exploration 
1	132	8793	we presented a different approach to image to depthmap implementation using jointly a u net architecture and convlstm cells we tried to incorporate both spacial and temporal elements to insure consistency when generating depth maps for sequences of images i e video there are several areas to continue our work here 	pyramids for convolutions attention for lstm drop the rnn and just use a 3d convolution perhaps use a 3d convolution inside a convlstm multiple timesteps to each lstm timestep 
0	132	8794	we presented a different approach to image to depthmap implementation using jointly a u net architecture and convlstm cells we tried to incorporate both spacial and temporal elements to insure consistency when generating depth maps for sequences of images i e video there are several areas to continue our work here 	there are many possibilities 
0	132	8795	while everyone has done some of everything the largest contributions from each to any particulars may be described as follows john has researched the literature designed the models and assisted manuel with several of the models and metrics manuel implemented the final model and metrics and ran multiple training runs 	while everyone has done some of everything the largest contributions from each to any particulars may be described as follows 
0	132	8796	while everyone has done some of everything the largest contributions from each to any particulars may be described as follows john has researched the literature designed the models and assisted manuel with several of the models and metrics manuel implemented the final model and metrics and ran multiple training runs 	john has researched the literature designed the models and assisted manuel with several of the models and metrics 
0	132	8797	while everyone has done some of everything the largest contributions from each to any particulars may be described as follows john has researched the literature designed the models and assisted manuel with several of the models and metrics manuel implemented the final model and metrics and ran multiple training runs 	manuel implemented the final model and metrics and ran multiple training runs 
0	132	8798	while everyone has done some of everything the largest contributions from each to any particulars may be described as follows john has researched the literature designed the models and assisted manuel with several of the models and metrics manuel implemented the final model and metrics and ran multiple training runs 	geoffrey has set up the infrastructure for preprocessing data with various transforms saving and loading models and worked on metrics 
1	133	8799	existing studies have shown that excessive alcohol drinking can impact the normal structural development of brain anatomy during adolescence in this project our goal is to design a classification model to predict if a subject is a heavy drinker based on their resting state fmri data stored as blood oxygen level dependent bold signals after pre processing the inputs to our models are parcellated fmri data as bold signals as well as patient demographic information age sex scanner type we then used each model logistic regression svm deep learning to output a predicted classification of the patient as a heavy drinker versus non heavy drinker 	existing studies have shown that excessive alcohol drinking can impact the normal structural development of brain anatomy during adolescence in this project our goal is to design a classification model to predict if a subject is a heavy drinker based on their resting state fmri data stored as blood oxygen level dependent bold signals 
1	133	8800	existing studies have shown that excessive alcohol drinking can impact the normal structural development of brain anatomy during adolescence in this project our goal is to design a classification model to predict if a subject is a heavy drinker based on their resting state fmri data stored as blood oxygen level dependent bold signals after pre processing the inputs to our models are parcellated fmri data as bold signals as well as patient demographic information age sex scanner type we then used each model logistic regression svm deep learning to output a predicted classification of the patient as a heavy drinker versus non heavy drinker 	after pre processing the inputs to our models are parcellated fmri data as bold signals as well as patient demographic information age sex scanner type 
1	133	8801	existing studies have shown that excessive alcohol drinking can impact the normal structural development of brain anatomy during adolescence in this project our goal is to design a classification model to predict if a subject is a heavy drinker based on their resting state fmri data stored as blood oxygen level dependent bold signals after pre processing the inputs to our models are parcellated fmri data as bold signals as well as patient demographic information age sex scanner type we then used each model logistic regression svm deep learning to output a predicted classification of the patient as a heavy drinker versus non heavy drinker 	we then used each model logistic regression svm deep learning to output a predicted classification of the patient as a heavy drinker versus non heavy drinker 
0	133	8802	as more and more neuroimaging databases become publicly available machine learning models are becoming increasingly useful in functional neuroimaging classification over the past decade there have been several attempts to leverage machine learning on fmri data to classify neurodegenerative diseases or different tasks these fmri classifications are often compared to traditional manual classification methods using clinical behavioral data such as the dsm iv criteria for psychological disorders 	as more and more neuroimaging databases become publicly available machine learning models are becoming increasingly useful in functional neuroimaging classification 
0	133	8803	as more and more neuroimaging databases become publicly available machine learning models are becoming increasingly useful in functional neuroimaging classification over the past decade there have been several attempts to leverage machine learning on fmri data to classify neurodegenerative diseases or different tasks these fmri classifications are often compared to traditional manual classification methods using clinical behavioral data such as the dsm iv criteria for psychological disorders 	over the past decade there have been several attempts to leverage machine learning on fmri data to classify neurodegenerative diseases or different tasks 
1	133	8804	as more and more neuroimaging databases become publicly available machine learning models are becoming increasingly useful in functional neuroimaging classification over the past decade there have been several attempts to leverage machine learning on fmri data to classify neurodegenerative diseases or different tasks these fmri classifications are often compared to traditional manual classification methods using clinical behavioral data such as the dsm iv criteria for psychological disorders 	these fmri classifications are often compared to traditional manual classification methods using clinical behavioral data such as the dsm iv criteria for psychological disorders 
1	133	8805	the earliest experiments we found mostly relied on support vector machines svms or linear classifiers which achieved accuracies between 69 92 chanel et al used svms for classifying austistic spectrum disorder asd from both task based and resting based fmri	the earliest experiments we found mostly relied on support vector machines svms or linear classifiers which achieved accuracies between 69 92 
0	133	8806	the earliest experiments we found mostly relied on support vector machines svms or linear classifiers which achieved accuracies between 69 92 chanel et al used svms for classifying austistic spectrum disorder asd from both task based and resting based fmri	chanel et al 
1	133	8807	the earliest experiments we found mostly relied on support vector machines svms or linear classifiers which achieved accuracies between 69 92 chanel et al used svms for classifying austistic spectrum disorder asd from both task based and resting based fmri	used svms for classifying austistic spectrum disorder asd from both task based and resting based fmri
1	133	8808	recent and current techniques for classifying on fmri data seem to take more advantage of recurrent neural networks rnns which naturally lends itself to time series data chen and hu developed a rnn based model that was able to identify individuals by their fmri functional brain fingerprints with up to 94 accuracy	recent and current techniques for classifying on fmri data seem to take more advantage of recurrent neural networks rnns which naturally lends itself to time series data 
1	133	8809	recent and current techniques for classifying on fmri data seem to take more advantage of recurrent neural networks rnns which naturally lends itself to time series data chen and hu developed a rnn based model that was able to identify individuals by their fmri functional brain fingerprints with up to 94 accuracy	chen and hu developed a rnn based model that was able to identify individuals by their fmri functional brain fingerprints with up to 94 accuracy
1	133	8810	for alcohol abuse classification little work has been done with machine learning models on fmri data with most analyses being done statistically or using basic regression models these papers vary on their feature selection and validation methods based on the results presented by these works a mixed cnn rnn based approach seemed promising due to the nature of our data which consists of resting state fmri only as a result we chose to build a cnn rnn based model to classify heavy drinkers as well as compare the performance against other neural networkbased models as well as regression and svm based models 	for alcohol abuse classification little work has been done with machine learning models on fmri data with most analyses being done statistically or using basic regression models these papers vary on their feature selection and validation methods 
1	133	8811	for alcohol abuse classification little work has been done with machine learning models on fmri data with most analyses being done statistically or using basic regression models these papers vary on their feature selection and validation methods based on the results presented by these works a mixed cnn rnn based approach seemed promising due to the nature of our data which consists of resting state fmri only as a result we chose to build a cnn rnn based model to classify heavy drinkers as well as compare the performance against other neural networkbased models as well as regression and svm based models 	based on the results presented by these works a mixed cnn rnn based approach seemed promising due to the nature of our data which consists of resting state fmri only 
1	133	8812	for alcohol abuse classification little work has been done with machine learning models on fmri data with most analyses being done statistically or using basic regression models these papers vary on their feature selection and validation methods based on the results presented by these works a mixed cnn rnn based approach seemed promising due to the nature of our data which consists of resting state fmri only as a result we chose to build a cnn rnn based model to classify heavy drinkers as well as compare the performance against other neural networkbased models as well as regression and svm based models 	as a result we chose to build a cnn rnn based model to classify heavy drinkers as well as compare the performance against other neural networkbased models as well as regression and svm based models 
0	133	8813	our original dataset consists of fmri scans from 715 adolescents young adults from the ncanda database the scans measure the bold signal from each brain region per second over t 269 timesteps 2 2 seconds frame preliminary visualization of the data such as time series plots of individual patients	our original dataset consists of fmri scans from 715 adolescents young adults from the ncanda database 
0	133	8814	our original dataset consists of fmri scans from 715 adolescents young adults from the ncanda database the scans measure the bold signal from each brain region per second over t 269 timesteps 2 2 seconds frame preliminary visualization of the data such as time series plots of individual patients	the scans measure the bold signal from each brain region per second over t 269 timesteps 2 2 seconds frame 
0	133	8815	our original dataset consists of fmri scans from 715 adolescents young adults from the ncanda database the scans measure the bold signal from each brain region per second over t 269 timesteps 2 2 seconds frame preliminary visualization of the data such as time series plots of individual patients	preliminary visualization of the data such as time series plots of individual patients
0	133	8816	fundamentally bold signals were normalized by z score to reduce variability between patients moreover there was significant class imbalance within the dataset 122 17 heavy drinkers out of 715 after setting class balance 50 50 our dataset size was reduced to 244 patients because we were limited by the size of our dataset it was imperative for us to make smart use of our data 	fundamentally bold signals were normalized by z score to reduce variability between patients 
0	133	8817	fundamentally bold signals were normalized by z score to reduce variability between patients moreover there was significant class imbalance within the dataset 122 17 heavy drinkers out of 715 after setting class balance 50 50 our dataset size was reduced to 244 patients because we were limited by the size of our dataset it was imperative for us to make smart use of our data 	moreover there was significant class imbalance within the dataset 122 17 heavy drinkers out of 715 
0	133	8818	fundamentally bold signals were normalized by z score to reduce variability between patients moreover there was significant class imbalance within the dataset 122 17 heavy drinkers out of 715 after setting class balance 50 50 our dataset size was reduced to 244 patients because we were limited by the size of our dataset it was imperative for us to make smart use of our data 	after setting class balance 50 50 our dataset size was reduced to 244 patients because we were limited by the size of our dataset it was imperative for us to make smart use of our data 
1	133	8819	fundamentally bold signals were normalized by z score to reduce variability between patients moreover there was significant class imbalance within the dataset 122 17 heavy drinkers out of 715 after setting class balance 50 50 our dataset size was reduced to 244 patients because we were limited by the size of our dataset it was imperative for us to make smart use of our data 	because neural networks have a high tendency to overfit we needed a proper dev set in addition to train test 
0	133	8820	fundamentally bold signals were normalized by z score to reduce variability between patients moreover there was significant class imbalance within the dataset 122 17 heavy drinkers out of 715 after setting class balance 50 50 our dataset size was reduced to 244 patients because we were limited by the size of our dataset it was imperative for us to make smart use of our data 	we took a random sample of 5 of our dataset 13 taken 231 leftover and set it aside as our dev set which will not be used during kfold cross validation of the test training data 
1	133	8821	fundamentally bold signals were normalized by z score to reduce variability between patients moreover there was significant class imbalance within the dataset 122 17 heavy drinkers out of 715 after setting class balance 50 50 our dataset size was reduced to 244 patients because we were limited by the size of our dataset it was imperative for us to make smart use of our data 	we decided on 5 of the dataset because we wanted to keep this set size small but still enough to not be overly skewed towards one class based on the cumulative probability of the binomial distribution likelihood of getting 3 or less of a single class is 17 
0	133	8822	fundamentally bold signals were normalized by z score to reduce variability between patients moreover there was significant class imbalance within the dataset 122 17 heavy drinkers out of 715 after setting class balance 50 50 our dataset size was reduced to 244 patients because we were limited by the size of our dataset it was imperative for us to make smart use of our data 	for 10 fold validation the remaining 231 samples were split 90 10 resulting in 208 training and 23 test samples 
0	133	8823	other features included demographic information sex age and scanner type 	other features included demographic information sex age and scanner type 
1	133	8824	to classify heavy drinkers versus non heavy drinkers we implemented a series of models based on related experiments found in the literature for our baseline we utilized logistic regression for our main exploration of the project we experimented with deep learning models 	to classify heavy drinkers versus non heavy drinkers we implemented a series of models based on related experiments found in the literature 
1	133	8825	to classify heavy drinkers versus non heavy drinkers we implemented a series of models based on related experiments found in the literature for our baseline we utilized logistic regression for our main exploration of the project we experimented with deep learning models 	for our baseline we utilized logistic regression 
0	133	8826	to classify heavy drinkers versus non heavy drinkers we implemented a series of models based on related experiments found in the literature for our baseline we utilized logistic regression for our main exploration of the project we experimented with deep learning models 	for our main exploration of the project we experimented with deep learning models 
0	133	8827	to classify heavy drinkers versus non heavy drinkers we implemented a series of models based on related experiments found in the literature for our baseline we utilized logistic regression for our main exploration of the project we experimented with deep learning models 	we briefly attempted support vector machines as a foil for deep learning 
1	133	8828	logistic regression a common binary classification algorithm utilizes the sigmoid function also known as the logistic function incorporated with linear prediction parameters and the features of x the classification prediction is given by the following probability distribution whose log likelihood is given by the following due to our low number of features 25 when using derived features from ica parcellated data we used newton s method for convergence newton s method requires the hessian of the loss with respect to the features to be calculated which is impractical for high dimensional data 	logistic regression a common binary classification algorithm utilizes the sigmoid function also known as the logistic function 
1	133	8829	logistic regression a common binary classification algorithm utilizes the sigmoid function also known as the logistic function incorporated with linear prediction parameters and the features of x the classification prediction is given by the following probability distribution whose log likelihood is given by the following due to our low number of features 25 when using derived features from ica parcellated data we used newton s method for convergence newton s method requires the hessian of the loss with respect to the features to be calculated which is impractical for high dimensional data 	incorporated with linear prediction parameters and the features of x the classification prediction is given by the following probability distribution whose log likelihood is given by the following due to our low number of features 25 when using derived features from ica parcellated data we used newton s method for convergence 
0	133	8830	logistic regression a common binary classification algorithm utilizes the sigmoid function also known as the logistic function incorporated with linear prediction parameters and the features of x the classification prediction is given by the following probability distribution whose log likelihood is given by the following due to our low number of features 25 when using derived features from ica parcellated data we used newton s method for convergence newton s method requires the hessian of the loss with respect to the features to be calculated which is impractical for high dimensional data 	newton s method requires the hessian of the loss with respect to the features to be calculated which is impractical for high dimensional data 
0	133	8831	logistic regression a common binary classification algorithm utilizes the sigmoid function also known as the logistic function incorporated with linear prediction parameters and the features of x the classification prediction is given by the following probability distribution whose log likelihood is given by the following due to our low number of features 25 when using derived features from ica parcellated data we used newton s method for convergence newton s method requires the hessian of the loss with respect to the features to be calculated which is impractical for high dimensional data 	for fewer features newton s method has the benefit of converging quickly which also allows us to use batch gradient ascent 
0	133	8832	logistic regression a common binary classification algorithm utilizes the sigmoid function also known as the logistic function incorporated with linear prediction parameters and the features of x the classification prediction is given by the following probability distribution whose log likelihood is given by the following due to our low number of features 25 when using derived features from ica parcellated data we used newton s method for convergence newton s method requires the hessian of the loss with respect to the features to be calculated which is impractical for high dimensional data 	newton s method update rule is given by the following 
0	133	8833	support vector machines svms map a given set of features to a higher dimensional space so that nonlinear classifications can be made as opposed to logistic regression which minimizes functional margin defined by the following equation support vector machines seek to minimize the geometric margin which is defined by the following equation in doing so the convergence of the algorithm takes the norm of the parameters into account and essentially the parameters become invariant to random meaningless scaling this is important in allowing the parameter change to be small enough for the algorithm to converge appropriately 	support vector machines svms map a given set of features to a higher dimensional space so that nonlinear classifications can be made 
1	133	8834	support vector machines svms map a given set of features to a higher dimensional space so that nonlinear classifications can be made as opposed to logistic regression which minimizes functional margin defined by the following equation support vector machines seek to minimize the geometric margin which is defined by the following equation in doing so the convergence of the algorithm takes the norm of the parameters into account and essentially the parameters become invariant to random meaningless scaling this is important in allowing the parameter change to be small enough for the algorithm to converge appropriately 	as opposed to logistic regression which minimizes functional margin defined by the following equation support vector machines seek to minimize the geometric margin which is defined by the following equation in doing so the convergence of the algorithm takes the norm of the parameters into account and essentially the parameters become invariant to random meaningless scaling 
0	133	8835	support vector machines svms map a given set of features to a higher dimensional space so that nonlinear classifications can be made as opposed to logistic regression which minimizes functional margin defined by the following equation support vector machines seek to minimize the geometric margin which is defined by the following equation in doing so the convergence of the algorithm takes the norm of the parameters into account and essentially the parameters become invariant to random meaningless scaling this is important in allowing the parameter change to be small enough for the algorithm to converge appropriately 	this is important in allowing the parameter change to be small enough for the algorithm to converge appropriately 
1	133	8836	support vector machines svms map a given set of features to a higher dimensional space so that nonlinear classifications can be made as opposed to logistic regression which minimizes functional margin defined by the following equation support vector machines seek to minimize the geometric margin which is defined by the following equation in doing so the convergence of the algorithm takes the norm of the parameters into account and essentially the parameters become invariant to random meaningless scaling this is important in allowing the parameter change to be small enough for the algorithm to converge appropriately 	as various svms have been used on fmri classification models in the literature we ran 4 svms with different kernels 1 linear 2 polynomial degree 2 3 sigmoid and 4 radial basis function rbf 
1	133	8837	finally we chose to primarily use deep learning in our most promising model as we saw an analogy between our data image processing and natural language processing 	finally we chose to primarily use deep learning in our most promising model as we saw an analogy between our data image processing and natural language processing 
1	133	8838	convolutional neural networks cnns are used frequently in image processing to recognize patterns that can be anywhere throughout the picture in our case the fmri data is a time series data for different brain regions we are trying to recognize patterns of brain activity at any time point throughout the time series along multiple channels brain regions we tried using 1 d convolution each region as an input channel time series for convolution formally the equation for computing 1 d convolution looks like the following where o represents the output of the convolution m represents the m th filter output in the convolution i represents the axis for time series b represents the bias term in the convolution w represents the weight matrix associated with a given output filter x represents the input data and n represents the n th input channel brain region in this case 	convolutional neural networks cnns are used frequently in image processing to recognize patterns that can be anywhere throughout the picture in our case the fmri data is a time series data for different brain regions 
1	133	8839	convolutional neural networks cnns are used frequently in image processing to recognize patterns that can be anywhere throughout the picture in our case the fmri data is a time series data for different brain regions we are trying to recognize patterns of brain activity at any time point throughout the time series along multiple channels brain regions we tried using 1 d convolution each region as an input channel time series for convolution formally the equation for computing 1 d convolution looks like the following where o represents the output of the convolution m represents the m th filter output in the convolution i represents the axis for time series b represents the bias term in the convolution w represents the weight matrix associated with a given output filter x represents the input data and n represents the n th input channel brain region in this case 	we are trying to recognize patterns of brain activity at any time point throughout the time series along multiple channels brain regions we tried using 1 d convolution each region as an input channel time series for convolution 
0	133	8840	convolutional neural networks cnns are used frequently in image processing to recognize patterns that can be anywhere throughout the picture in our case the fmri data is a time series data for different brain regions we are trying to recognize patterns of brain activity at any time point throughout the time series along multiple channels brain regions we tried using 1 d convolution each region as an input channel time series for convolution formally the equation for computing 1 d convolution looks like the following where o represents the output of the convolution m represents the m th filter output in the convolution i represents the axis for time series b represents the bias term in the convolution w represents the weight matrix associated with a given output filter x represents the input data and n represents the n th input channel brain region in this case 	formally the equation for computing 1 d convolution looks like the following where o represents the output of the convolution m represents the m th filter output in the convolution i represents the axis for time series b represents the bias term in the convolution w represents the weight matrix associated with a given output filter x represents the input data and n represents the n th input channel brain region in this case 
0	133	8841	convolutional neural networks cnns are used frequently in image processing to recognize patterns that can be anywhere throughout the picture in our case the fmri data is a time series data for different brain regions we are trying to recognize patterns of brain activity at any time point throughout the time series along multiple channels brain regions we tried using 1 d convolution each region as an input channel time series for convolution formally the equation for computing 1 d convolution looks like the following where o represents the output of the convolution m represents the m th filter output in the convolution i represents the axis for time series b represents the bias term in the convolution w represents the weight matrix associated with a given output filter x represents the input data and n represents the n th input channel brain region in this case 	 note convolution above assumes stride 1 
0	133	8842	our most promising model	our most promising model
1	133	8843	in general for all models because we used 10 fold cross validation as described in the data section to get an accurate measurement of the measurement of our model small dataset means cross validation is imperative we had to make sure to set aside a dev set that will not be seen during cross validation but will be used as a means for adjust for the hyperparameters this was the best way to properly adjust hyperparameters without being biased by the very data that we eventually train the model on 	in general for all models because we used 10 fold cross validation as described in the data section to get an accurate measurement of the measurement of our model small dataset means cross validation is imperative we had to make sure to set aside a dev set that will not be seen during cross validation but will be used as a means for adjust for the hyperparameters 
1	133	8844	in general for all models because we used 10 fold cross validation as described in the data section to get an accurate measurement of the measurement of our model small dataset means cross validation is imperative we had to make sure to set aside a dev set that will not be seen during cross validation but will be used as a means for adjust for the hyperparameters this was the best way to properly adjust hyperparameters without being biased by the very data that we eventually train the model on 	this was the best way to properly adjust hyperparameters without being biased by the very data that we eventually train the model on 
1	133	8845	for logistic regression we either used fmri data in addition to all demographic information or with all demographic information excluding age as expected of newton s method the algorithm converged very quickly 1 6 iterations depending on the threshold for convergence batch gradient descent the criteria for convergence c was measured by the magnitude of change in the parameter c d 2 2 where the threshold for convergence was determined manually using the dev set 	for logistic regression we either used fmri data in addition to all demographic information or with all demographic information excluding age 
1	133	8846	for logistic regression we either used fmri data in addition to all demographic information or with all demographic information excluding age as expected of newton s method the algorithm converged very quickly 1 6 iterations depending on the threshold for convergence batch gradient descent the criteria for convergence c was measured by the magnitude of change in the parameter c d 2 2 where the threshold for convergence was determined manually using the dev set 	as expected of newton s method the algorithm converged very quickly 1 6 iterations depending on the threshold for convergence batch gradient descent 
0	133	8847	for logistic regression we either used fmri data in addition to all demographic information or with all demographic information excluding age as expected of newton s method the algorithm converged very quickly 1 6 iterations depending on the threshold for convergence batch gradient descent the criteria for convergence c was measured by the magnitude of change in the parameter c d 2 2 where the threshold for convergence was determined manually using the dev set 	the criteria for convergence c was measured by the magnitude of change in the parameter c d 2 2 where the threshold for convergence was determined manually using the dev set 
1	133	8848	for logistic regression we either used fmri data in addition to all demographic information or with all demographic information excluding age as expected of newton s method the algorithm converged very quickly 1 6 iterations depending on the threshold for convergence batch gradient descent the criteria for convergence c was measured by the magnitude of change in the parameter c d 2 2 where the threshold for convergence was determined manually using the dev set 	we started with a value of 100 for and decreased by a factor of 10 and observed the effect on the average difference in accuracy between the train set and dev set over the k fold validation the dev set never enters the train or test set 
0	133	8849	for logistic regression we either used fmri data in addition to all demographic information or with all demographic information excluding age as expected of newton s method the algorithm converged very quickly 1 6 iterations depending on the threshold for convergence batch gradient descent the criteria for convergence c was measured by the magnitude of change in the parameter c d 2 2 where the threshold for convergence was determined manually using the dev set 	the average difference in accuracy was invariant to the changes in see chart below 
0	133	8850	for logistic regression we either used fmri data in addition to all demographic information or with all demographic information excluding age as expected of newton s method the algorithm converged very quickly 1 6 iterations depending on the threshold for convergence batch gradient descent the criteria for convergence c was measured by the magnitude of change in the parameter c d 2 2 where the threshold for convergence was determined manually using the dev set 	in other words 1 iteration was enough for meaningful convergence of the algorithm and that any additional iterations will not cause significant overfitting 
0	133	8851	for logistic regression we either used fmri data in addition to all demographic information or with all demographic information excluding age as expected of newton s method the algorithm converged very quickly 1 6 iterations depending on the threshold for convergence batch gradient descent the criteria for convergence c was measured by the magnitude of change in the parameter c d 2 2 where the threshold for convergence was determined manually using the dev set 	therefore we left the value of at 1e 5 a value used in class 
0	133	8852	1 1e 1 1e 2 avg train dev accuracy 16 16 14 17	1 1e 1 1e 2 avg train dev accuracy 16 16 14 17
0	133	8853	using sklearn s package for svms	using sklearn s package for svms
1	133	8854	for the deep learning models which was implemented through keras with theano backend to that end we established three conditions for exiting iterations as the model was training the model was to exit iterating if 1 t rainacc devacc t rainacc devacc 20 t rainacc devacc t hreshold the difference in accuracy between train and dev sets is fairly small and they both have reached a threshold value of our interest 0 6 better than random guessing 2 t rainacc devacc t rainacc devacc 20 t rainacc m ax t rainacc devacc the difference in accuracy between train and dev sets is getting too large the train accuracy is too high 0 65 and it is greater than the dev accuracy suggests that the model is beginning to overfit 	for the deep learning models which was implemented through keras with theano backend to that end we established three conditions for exiting iterations 
1	133	8855	for the deep learning models which was implemented through keras with theano backend to that end we established three conditions for exiting iterations as the model was training the model was to exit iterating if 1 t rainacc devacc t rainacc devacc 20 t rainacc devacc t hreshold the difference in accuracy between train and dev sets is fairly small and they both have reached a threshold value of our interest 0 6 better than random guessing 2 t rainacc devacc t rainacc devacc 20 t rainacc m ax t rainacc devacc the difference in accuracy between train and dev sets is getting too large the train accuracy is too high 0 65 and it is greater than the dev accuracy suggests that the model is beginning to overfit 	as the model was training the model was to exit iterating if 1 t rainacc devacc t rainacc devacc 20 t rainacc devacc t hreshold the difference in accuracy between train and dev sets is fairly small and they both have reached a threshold value of our interest 0 6 better than random guessing 2 t rainacc devacc t rainacc devacc 20 t rainacc m ax t rainacc devacc the difference in accuracy between train and dev sets is getting too large the train accuracy is too high 0 65 and it is greater than the dev accuracy 
0	133	8856	for the deep learning models which was implemented through keras with theano backend to that end we established three conditions for exiting iterations as the model was training the model was to exit iterating if 1 t rainacc devacc t rainacc devacc 20 t rainacc devacc t hreshold the difference in accuracy between train and dev sets is fairly small and they both have reached a threshold value of our interest 0 6 better than random guessing 2 t rainacc devacc t rainacc devacc 20 t rainacc m ax t rainacc devacc the difference in accuracy between train and dev sets is getting too large the train accuracy is too high 0 65 and it is greater than the dev accuracy suggests that the model is beginning to overfit 	suggests that the model is beginning to overfit 
0	133	8857	 the train accuracy is high 0 65 while the dev accuracy is low 0 55 suggests that the model is beginning to overfit as for window size stride and output filter dimensions for cnn and output dimensions for rnn we manually judged based on the rate of training and changes in accuracy differences between the train and dev set to have window size 5 stride 1 output filter dimensions 5 and output dimensions for rnn 5 except for when rnn was the only layer present 	 the train accuracy is high 0 65 while the dev accuracy is low 0 55 
1	133	8858	 the train accuracy is high 0 65 while the dev accuracy is low 0 55 suggests that the model is beginning to overfit as for window size stride and output filter dimensions for cnn and output dimensions for rnn we manually judged based on the rate of training and changes in accuracy differences between the train and dev set to have window size 5 stride 1 output filter dimensions 5 and output dimensions for rnn 5 except for when rnn was the only layer present 	suggests that the model is beginning to overfit as for window size stride and output filter dimensions for cnn and output dimensions for rnn we manually judged based on the rate of training and changes in accuracy differences between the train and dev set to have window size 5 stride 1 output filter dimensions 5 and output dimensions for rnn 5 except for when rnn was the only layer present 
1	133	8859	to measure the final performance of all our models we utilized accuracy and f1 scoresince we balanced our classes accuracy gave us a good indications of how the models performed however f1 score was a way to formally take both recall what proportion of correct predictions was for heavy drinkers over non heavy drinkers and precision what proportion of heavy drinker predictions was correct 	to measure the final performance of all our models we utilized accuracy and f1 scoresince we balanced our classes accuracy gave us a good indications of how the models performed 
0	133	8860	to measure the final performance of all our models we utilized accuracy and f1 scoresince we balanced our classes accuracy gave us a good indications of how the models performed however f1 score was a way to formally take both recall what proportion of correct predictions was for heavy drinkers over non heavy drinkers and precision what proportion of heavy drinker predictions was correct 	however f1 score was a way to formally take both recall what proportion of correct predictions was for heavy drinkers over non heavy drinkers 
0	133	8861	to measure the final performance of all our models we utilized accuracy and f1 scoresince we balanced our classes accuracy gave us a good indications of how the models performed however f1 score was a way to formally take both recall what proportion of correct predictions was for heavy drinkers over non heavy drinkers and precision what proportion of heavy drinker predictions was correct 	and precision what proportion of heavy drinker predictions was correct 
1	133	8862	to measure the final performance of all our models we utilized accuracy and f1 scoresince we balanced our classes accuracy gave us a good indications of how the models performed however f1 score was a way to formally take both recall what proportion of correct predictions was for heavy drinkers over non heavy drinkers and precision what proportion of heavy drinker predictions was correct 	logistic regression with our derived features and demographics performed the best
0	133	8863	generally svms were not our major focus purely based on data we obtained while adjusting for the threshold hyperparameter we saw that all svms either underfit or overfit to prevent overfitting we were able to adjust our threshold but we may need to explore other hyperparameters kernels relevant to svms the bulk of our work went into attempting deeplearning models 	generally svms were not our major focus 
1	133	8864	generally svms were not our major focus purely based on data we obtained while adjusting for the threshold hyperparameter we saw that all svms either underfit or overfit to prevent overfitting we were able to adjust our threshold but we may need to explore other hyperparameters kernels relevant to svms the bulk of our work went into attempting deeplearning models 	purely based on data we obtained while adjusting for the threshold hyperparameter we saw that all svms either underfit or overfit 
1	133	8865	generally svms were not our major focus purely based on data we obtained while adjusting for the threshold hyperparameter we saw that all svms either underfit or overfit to prevent overfitting we were able to adjust our threshold but we may need to explore other hyperparameters kernels relevant to svms the bulk of our work went into attempting deeplearning models 	to prevent overfitting we were able to adjust our threshold but we may need to explore other hyperparameters kernels relevant to svms the bulk of our work went into attempting deeplearning models 
0	133	8866	generally svms were not our major focus purely based on data we obtained while adjusting for the threshold hyperparameter we saw that all svms either underfit or overfit to prevent overfitting we were able to adjust our threshold but we may need to explore other hyperparameters kernels relevant to svms the bulk of our work went into attempting deeplearning models 	as we initially experimented with different hyperparameters one of the first things we noticed was the tendency to underfit or overfit 
0	133	8867	generally svms were not our major focus purely based on data we obtained while adjusting for the threshold hyperparameter we saw that all svms either underfit or overfit to prevent overfitting we were able to adjust our threshold but we may need to explore other hyperparameters kernels relevant to svms the bulk of our work went into attempting deeplearning models 	we had to make sure to have a dev set we could test on such that we could determine window size output filter dimension and output dimension for rnn 
0	133	8868	generally svms were not our major focus purely based on data we obtained while adjusting for the threshold hyperparameter we saw that all svms either underfit or overfit to prevent overfitting we were able to adjust our threshold but we may need to explore other hyperparameters kernels relevant to svms the bulk of our work went into attempting deeplearning models 	we also attempted using regularization to no avail as well as drop out 
0	133	8869	generally svms were not our major focus purely based on data we obtained while adjusting for the threshold hyperparameter we saw that all svms either underfit or overfit to prevent overfitting we were able to adjust our threshold but we may need to explore other hyperparameters kernels relevant to svms the bulk of our work went into attempting deeplearning models 	our most important use of the dev set was to know when to drop after an epoch 
0	133	8870	generally svms were not our major focus purely based on data we obtained while adjusting for the threshold hyperparameter we saw that all svms either underfit or overfit to prevent overfitting we were able to adjust our threshold but we may need to explore other hyperparameters kernels relevant to svms the bulk of our work went into attempting deeplearning models 	to ensure the model trained enough but not too much we reasoned that the best place to stop was when both the train and dev set accuracies were fairly close and above a certain threshold or when the two accuracies were getting too far apart 
0	133	8871	generally svms were not our major focus purely based on data we obtained while adjusting for the threshold hyperparameter we saw that all svms either underfit or overfit to prevent overfitting we were able to adjust our threshold but we may need to explore other hyperparameters kernels relevant to svms the bulk of our work went into attempting deeplearning models 	in doing so we were able to avoid overfitting 
0	133	8872	generally svms were not our major focus purely based on data we obtained while adjusting for the threshold hyperparameter we saw that all svms either underfit or overfit to prevent overfitting we were able to adjust our threshold but we may need to explore other hyperparameters kernels relevant to svms the bulk of our work went into attempting deeplearning models 	however all in all we saw that we could not yield any significant improvements in accuracy f1 score 
1	133	8873	overall although we were able to control many of the hyperparameters to prevent under overfitting and adjust the number of layers we found that deep learning models did not perform very well this is either because our overall architecture was fundamentally flawed inclusion of demographics is crucial or we were simply lacking in data to be able to make meaningful distinctions as indicated by aggressive guessing of a single class as for svms they were not our primary focus for developing our models so we weren t able to see their full potential on the data 	overall although we were able to control many of the hyperparameters to prevent under overfitting and adjust the number of layers we found that deep learning models did not perform very well 
1	133	8874	overall although we were able to control many of the hyperparameters to prevent under overfitting and adjust the number of layers we found that deep learning models did not perform very well this is either because our overall architecture was fundamentally flawed inclusion of demographics is crucial or we were simply lacking in data to be able to make meaningful distinctions as indicated by aggressive guessing of a single class as for svms they were not our primary focus for developing our models so we weren t able to see their full potential on the data 	this is either because our overall architecture was fundamentally flawed inclusion of demographics is crucial or we were simply lacking in data to be able to make meaningful distinctions as indicated by aggressive guessing of a single class 
0	133	8875	overall although we were able to control many of the hyperparameters to prevent under overfitting and adjust the number of layers we found that deep learning models did not perform very well this is either because our overall architecture was fundamentally flawed inclusion of demographics is crucial or we were simply lacking in data to be able to make meaningful distinctions as indicated by aggressive guessing of a single class as for svms they were not our primary focus for developing our models so we weren t able to see their full potential on the data 	as for svms they were not our primary focus for developing our models so we weren t able to see their full potential on the data 
0	133	8876	overall although we were able to control many of the hyperparameters to prevent under overfitting and adjust the number of layers we found that deep learning models did not perform very well this is either because our overall architecture was fundamentally flawed inclusion of demographics is crucial or we were simply lacking in data to be able to make meaningful distinctions as indicated by aggressive guessing of a single class as for svms they were not our primary focus for developing our models so we weren t able to see their full potential on the data 	nonetheless our most preliminary work suggests they will have similar issues as our deep learning models 
1	133	8877	overall although we were able to control many of the hyperparameters to prevent under overfitting and adjust the number of layers we found that deep learning models did not perform very well this is either because our overall architecture was fundamentally flawed inclusion of demographics is crucial or we were simply lacking in data to be able to make meaningful distinctions as indicated by aggressive guessing of a single class as for svms they were not our primary focus for developing our models so we weren t able to see their full potential on the data 	although we do have our most successful model baseline logistic regression using derived features we also found that removing age as a factor reduces its efficacy suggesting that even logistic regression with our derived features was not any more successful than our other models moving forward there are multiple things we would like to try for our future directions 
0	133	8878	overall although we were able to control many of the hyperparameters to prevent under overfitting and adjust the number of layers we found that deep learning models did not perform very well this is either because our overall architecture was fundamentally flawed inclusion of demographics is crucial or we were simply lacking in data to be able to make meaningful distinctions as indicated by aggressive guessing of a single class as for svms they were not our primary focus for developing our models so we weren t able to see their full potential on the data 	as mentioned above we simply may not have had enough data to be able to pick up sensitive features 
1	133	8879	overall although we were able to control many of the hyperparameters to prevent under overfitting and adjust the number of layers we found that deep learning models did not perform very well this is either because our overall architecture was fundamentally flawed inclusion of demographics is crucial or we were simply lacking in data to be able to make meaningful distinctions as indicated by aggressive guessing of a single class as for svms they were not our primary focus for developing our models so we weren t able to see their full potential on the data 	to circumvent this issue we can use transfer learning which is commonly used for model development on medical images due to relatively modest sample sizes 
1	133	8880	overall although we were able to control many of the hyperparameters to prevent under overfitting and adjust the number of layers we found that deep learning models did not perform very well this is either because our overall architecture was fundamentally flawed inclusion of demographics is crucial or we were simply lacking in data to be able to make meaningful distinctions as indicated by aggressive guessing of a single class as for svms they were not our primary focus for developing our models so we weren t able to see their full potential on the data 	this involves applying learned parameters from other large datasets ideally those that combine images and sequence data as our fmri dataset does and training our own final few layers to use features detected from larger data but predicting for or own another important direction is to incorporate demographics as features into our deep learning models 
0	133	8881	overall although we were able to control many of the hyperparameters to prevent under overfitting and adjust the number of layers we found that deep learning models did not perform very well this is either because our overall architecture was fundamentally flawed inclusion of demographics is crucial or we were simply lacking in data to be able to make meaningful distinctions as indicated by aggressive guessing of a single class as for svms they were not our primary focus for developing our models so we weren t able to see their full potential on the data 	although we were limited by the fact that all of us had just learned to use keras and didn t have the time to incorporate multiple data inputs we saw how important demographics can be 
0	133	8882	overall although we were able to control many of the hyperparameters to prevent under overfitting and adjust the number of layers we found that deep learning models did not perform very well this is either because our overall architecture was fundamentally flawed inclusion of demographics is crucial or we were simply lacking in data to be able to make meaningful distinctions as indicated by aggressive guessing of a single class as for svms they were not our primary focus for developing our models so we weren t able to see their full potential on the data 	moreover we have not normalized for an natural biological changes in brain function as an individual ages 
1	133	8883	overall although we were able to control many of the hyperparameters to prevent under overfitting and adjust the number of layers we found that deep learning models did not perform very well this is either because our overall architecture was fundamentally flawed inclusion of demographics is crucial or we were simply lacking in data to be able to make meaningful distinctions as indicated by aggressive guessing of a single class as for svms they were not our primary focus for developing our models so we weren t able to see their full potential on the data 	including demographics may potentially drastically change our models performances another idea would be to look into different parcellation methods for pre processing the data craddock with more regions etc 
0	133	8884	overall although we were able to control many of the hyperparameters to prevent under overfitting and adjust the number of layers we found that deep learning models did not perform very well this is either because our overall architecture was fundamentally flawed inclusion of demographics is crucial or we were simply lacking in data to be able to make meaningful distinctions as indicated by aggressive guessing of a single class as for svms they were not our primary focus for developing our models so we weren t able to see their full potential on the data 	 as currently there is no consensus on the best parcellation method of fmri data lastly we can look into making our svm models more robust for our classification problem 
0	133	8885	overall although we were able to control many of the hyperparameters to prevent under overfitting and adjust the number of layers we found that deep learning models did not perform very well this is either because our overall architecture was fundamentally flawed inclusion of demographics is crucial or we were simply lacking in data to be able to make meaningful distinctions as indicated by aggressive guessing of a single class as for svms they were not our primary focus for developing our models so we weren t able to see their full potential on the data 	for this project we kept most of the default scikit learn parameters except for tolerance as exploring svms was not the main focus of our project 
0	133	8886	overall although we were able to control many of the hyperparameters to prevent under overfitting and adjust the number of layers we found that deep learning models did not perform very well this is either because our overall architecture was fundamentally flawed inclusion of demographics is crucial or we were simply lacking in data to be able to make meaningful distinctions as indicated by aggressive guessing of a single class as for svms they were not our primary focus for developing our models so we weren t able to see their full potential on the data 	however fine tuning additional parameters as in poly sigmoid rbf kernels or considering additional custom kernels may help with the issue we had of either extremely underfitting or extremely overfitting 
0	133	8887	all team members contributed equally to writing this project report joseph noh researched models and built the frame work to use through keras theanos he was primarily responsible for pioneering different methods models 	all team members contributed equally to writing this project report 
0	133	8888	all team members contributed equally to writing this project report joseph noh researched models and built the frame work to use through keras theanos he was primarily responsible for pioneering different methods models 	joseph noh researched models and built the frame work to use through keras theanos 
0	133	8889	all team members contributed equally to writing this project report joseph noh researched models and built the frame work to use through keras theanos he was primarily responsible for pioneering different methods models 	he was primarily responsible for pioneering different methods models 
0	133	8890	all team members contributed equally to writing this project report joseph noh researched models and built the frame work to use through keras theanos he was primarily responsible for pioneering different methods models 	yong hun kim tested code for different model combinations and hyperparameters and recorded the resulting data 
0	133	8891	all team members contributed equally to writing this project report joseph noh researched models and built the frame work to use through keras theanos he was primarily responsible for pioneering different methods models 	cindy liu generated a large number of images tables for data and result visualizations implemented the svm models and created the model diagram we would like to acknowledge qingyu zhao for providing the pre processed data and serving as a mentor for the project giving us guidance on our set up and answering our questions about the data 
0	133	8892	all team members contributed equally to writing this project report joseph noh researched models and built the frame work to use through keras theanos he was primarily responsible for pioneering different methods models 	we would also like to thank tas atharva parulekar and raphael townshend for providing guidance and advice during project office hours as well as professors andrew ng and ron dror for teaching cs229 this quarter 
1	134	8893	using a training set provided by the pacific earthquake engineering research peer center we build a classifier to label images of structures as damaged or undamaged using a variety of machine learning techniques k nearest neighbors logistic regression svm and convolutional neural networks cnn we find that when compared to classical machine learning techniques the performance of a cnn is best on our data set we evaluate the mistakes made by our classifiers and we tune our models using information gleaned from learning curves 	using a training set provided by the pacific earthquake engineering research peer center we build a classifier to label images of structures as damaged or undamaged using a variety of machine learning techniques k nearest neighbors logistic regression svm and convolutional neural networks cnn 
1	134	8894	using a training set provided by the pacific earthquake engineering research peer center we build a classifier to label images of structures as damaged or undamaged using a variety of machine learning techniques k nearest neighbors logistic regression svm and convolutional neural networks cnn we find that when compared to classical machine learning techniques the performance of a cnn is best on our data set we evaluate the mistakes made by our classifiers and we tune our models using information gleaned from learning curves 	we find that when compared to classical machine learning techniques the performance of a cnn is best on our data set 
1	134	8895	using a training set provided by the pacific earthquake engineering research peer center we build a classifier to label images of structures as damaged or undamaged using a variety of machine learning techniques k nearest neighbors logistic regression svm and convolutional neural networks cnn we find that when compared to classical machine learning techniques the performance of a cnn is best on our data set we evaluate the mistakes made by our classifiers and we tune our models using information gleaned from learning curves 	we evaluate the mistakes made by our classifiers and we tune our models using information gleaned from learning curves 
1	134	8896	using a training set provided by the pacific earthquake engineering research peer center we build a classifier to label images of structures as damaged or undamaged using a variety of machine learning techniques k nearest neighbors logistic regression svm and convolutional neural networks cnn we find that when compared to classical machine learning techniques the performance of a cnn is best on our data set we evaluate the mistakes made by our classifiers and we tune our models using information gleaned from learning curves 	we find that our best performing model which uses transfer learning using inceptionv3 trained on imagenet with an added fully connected layer and softmax has a test accuracy of 83 
0	134	8897	the pacific earthquake engineering research peer center has provided image datasets that can be used to classify structures in terms of damage the input to this project consisted of 5913 images of this set of images 3186 images were labeled as undamaged or 0 54 and 2727 images were labeled as damaged or 1 46 each image includes 224 by 224 eight bit rgb pixels 	the pacific earthquake engineering research peer center has provided image datasets that can be used to classify structures in terms of damage the input to this project consisted of 5913 images 
0	134	8898	the pacific earthquake engineering research peer center has provided image datasets that can be used to classify structures in terms of damage the input to this project consisted of 5913 images of this set of images 3186 images were labeled as undamaged or 0 54 and 2727 images were labeled as damaged or 1 46 each image includes 224 by 224 eight bit rgb pixels 	of this set of images 3186 images were labeled as undamaged or 0 54 and 2727 images were labeled as damaged or 1 46 
0	134	8899	the pacific earthquake engineering research peer center has provided image datasets that can be used to classify structures in terms of damage the input to this project consisted of 5913 images of this set of images 3186 images were labeled as undamaged or 0 54 and 2727 images were labeled as damaged or 1 46 each image includes 224 by 224 eight bit rgb pixels 	each image includes 224 by 224 eight bit rgb pixels 
0	134	8900	the pacific earthquake engineering research peer center has provided image datasets that can be used to classify structures in terms of damage the input to this project consisted of 5913 images of this set of images 3186 images were labeled as undamaged or 0 54 and 2727 images were labeled as damaged or 1 46 each image includes 224 by 224 eight bit rgb pixels 	we split the images into the following sets 90 for training 2870 undamaged and 2451 damaged 
0	134	8901	the pacific earthquake engineering research peer center has provided image datasets that can be used to classify structures in terms of damage the input to this project consisted of 5913 images of this set of images 3186 images were labeled as undamaged or 0 54 and 2727 images were labeled as damaged or 1 46 each image includes 224 by 224 eight bit rgb pixels 	 46 are damaged 10 for validation 316 undamaged and 276 damaged 
0	134	8902	the pacific earthquake engineering research peer center has provided image datasets that can be used to classify structures in terms of damage the input to this project consisted of 5913 images of this set of images 3186 images were labeled as undamaged or 0 54 and 2727 images were labeled as damaged or 1 46 each image includes 224 by 224 eight bit rgb pixels 	 47 are damaged 
1	134	8903	the pacific earthquake engineering research peer center has provided image datasets that can be used to classify structures in terms of damage the input to this project consisted of 5913 images of this set of images 3186 images were labeled as undamaged or 0 54 and 2727 images were labeled as damaged or 1 46 each image includes 224 by 224 eight bit rgb pixels 	we decided not to set aside images for testing because of the limited number of samples although if we were to eventually submit to an academic journal we would need to be more rigorous in this regard we used several models three classical machine learning models several variations of two deep learning models mobilenet and inceptionv3 convolutional neural networks and one model which combined classical and deep learning techniques the primary output of our classifier models is the accuracy as determined by the number of correctly predicted images over the total number of predicted images 
0	134	8904	there are few references on image classification of damaged buildings one good survey paper on structural image classification is	there are few references on image classification of damaged buildings 
0	134	8905	there are few references on image classification of damaged buildings one good survey paper on structural image classification is	one good survey paper on structural image classification is
0	134	8906	minnie ho intel corporation minnie ho intel com	minnie ho intel corporation minnie ho intel com
0	134	8907	google llc jatron google com a few examples from our dataset are shown in	google llc jatron google com a few examples from our dataset are shown in
0	134	8908	we normalized the images so each of the 224x224 8 bit rgb pixels x was in the range 1 1 this was done by setting each pixel x to 128 1 for k nearest neighbors logistic regression and support vector machine we also scaled and flattened the pictures before feeding them into the models 	we normalized the images so each of the 224x224 8 bit rgb pixels x was in the range 1 1 
0	134	8909	we normalized the images so each of the 224x224 8 bit rgb pixels x was in the range 1 1 this was done by setting each pixel x to 128 1 for k nearest neighbors logistic regression and support vector machine we also scaled and flattened the pictures before feeding them into the models 	this was done by setting each pixel x to 128 1 
1	134	8910	we normalized the images so each of the 224x224 8 bit rgb pixels x was in the range 1 1 this was done by setting each pixel x to 128 1 for k nearest neighbors logistic regression and support vector machine we also scaled and flattened the pictures before feeding them into the models 	for k nearest neighbors logistic regression and support vector machine we also scaled and flattened the pictures before feeding them into the models 
1	134	8911	we built six models three classical machine learning models k nearest neighbors with k 5 logistic regression support vector machine with rbf kernel two deep learning models mobilenetv1 0 and inceptionv3 convolutional neural networks and one model which combined classical and deep learning techniques support vector machine based on activations earlier in the inceptionv3 network the performance of each of these models is summarized in the results section 	we built six models three classical machine learning models k nearest neighbors with k 5 logistic regression support vector machine with rbf kernel two deep learning models mobilenetv1 0 and inceptionv3 convolutional neural networks and one model which combined classical and deep learning techniques support vector machine based on activations earlier in the inceptionv3 network 
0	134	8912	we built six models three classical machine learning models k nearest neighbors with k 5 logistic regression support vector machine with rbf kernel two deep learning models mobilenetv1 0 and inceptionv3 convolutional neural networks and one model which combined classical and deep learning techniques support vector machine based on activations earlier in the inceptionv3 network the performance of each of these models is summarized in the results section 	the performance of each of these models is summarized in the results section 
0	134	8913	in k nearest neighbors an unlabeled vector is classified by assigning the label which is most frequent among the k training samples nearest to that query point 	in k nearest neighbors an unlabeled vector is classified by assigning the label which is most frequent among the k training samples nearest to that query point 
0	134	8914	in logistic regression we use the sigmoid function to estimate the probability that an image belongs to a certain class this sigmoid function is parametrized by a vector which is obtained by maximizing the log likelihood our logistic regression model included l2 regularization with 1 0 	in logistic regression we use the sigmoid function to estimate the probability that an image belongs to a certain class 
0	134	8915	in logistic regression we use the sigmoid function to estimate the probability that an image belongs to a certain class this sigmoid function is parametrized by a vector which is obtained by maximizing the log likelihood our logistic regression model included l2 regularization with 1 0 	this sigmoid function is parametrized by a vector which is obtained by maximizing the log likelihood 
1	134	8916	in logistic regression we use the sigmoid function to estimate the probability that an image belongs to a certain class this sigmoid function is parametrized by a vector which is obtained by maximizing the log likelihood our logistic regression model included l2 regularization with 1 0 	our logistic regression model included l2 regularization with 1 0 
1	134	8917	in logistic regression we use the sigmoid function to estimate the probability that an image belongs to a certain class this sigmoid function is parametrized by a vector which is obtained by maximizing the log likelihood our logistic regression model included l2 regularization with 1 0 	due to the suboptimal results achieved by this model we did not spend additional time tuning the regularization parameters 
1	134	8918	during training support vector machines try to find the maximum margin hyperplane that divides data points with different labels supports vector machines can also efficiently perform non linear classification using what is called the kernel trick implicitly mapping the inputs into high dimensional feature spaces our support vector machine model performed non linear classification using the radial basis function kernel which is defined by the formula below 2 we set the penalty parameter c of the error term to 1 0 and the kernel coefficient for the rbf kernel to 0 001 	during training support vector machines try to find the maximum margin hyperplane that divides data points with different labels 
0	134	8919	during training support vector machines try to find the maximum margin hyperplane that divides data points with different labels supports vector machines can also efficiently perform non linear classification using what is called the kernel trick implicitly mapping the inputs into high dimensional feature spaces our support vector machine model performed non linear classification using the radial basis function kernel which is defined by the formula below 2 we set the penalty parameter c of the error term to 1 0 and the kernel coefficient for the rbf kernel to 0 001 	 supports vector machines can also efficiently perform non linear classification using what is called the kernel trick implicitly mapping the inputs into high dimensional feature spaces our support vector machine model performed non linear classification using the radial basis function kernel which is defined by the formula below 
0	134	8920	during training support vector machines try to find the maximum margin hyperplane that divides data points with different labels supports vector machines can also efficiently perform non linear classification using what is called the kernel trick implicitly mapping the inputs into high dimensional feature spaces our support vector machine model performed non linear classification using the radial basis function kernel which is defined by the formula below 2 we set the penalty parameter c of the error term to 1 0 and the kernel coefficient for the rbf kernel to 0 001 	 2 we set the penalty parameter c of the error term to 1 0 and the kernel coefficient for the rbf kernel to 0 001 
1	134	8921	during training support vector machines try to find the maximum margin hyperplane that divides data points with different labels supports vector machines can also efficiently perform non linear classification using what is called the kernel trick implicitly mapping the inputs into high dimensional feature spaces our support vector machine model performed non linear classification using the radial basis function kernel which is defined by the formula below 2 we set the penalty parameter c of the error term to 1 0 and the kernel coefficient for the rbf kernel to 0 001 	due to the suboptimal results achieved by this first model we did not do further tuning of these parameters 
0	134	8922	mobilenetv1 and inceptionv3 are two convolutional neural network cnn architectures designed for image recognition tasks mobilenetv1 is a lighter lower latency neural network designed for use on mobile devices while inceptionv3 is a heavier architecture which tends to achieve better performance since we only had a few thousand images training these networks from scratch would surely cause overfitting so instead we downloaded pre trained versions of these models using tensorflow with weights optimized to classify images in the imagenet dataset froze the weights of most of the layers of the pre trained networks and trained a new fully connected layer with a sigmoid or softmax activation placed on top of each of the pre trained networks 	mobilenetv1 and inceptionv3 are two convolutional neural network cnn architectures designed for image recognition tasks 
0	134	8923	mobilenetv1 and inceptionv3 are two convolutional neural network cnn architectures designed for image recognition tasks mobilenetv1 is a lighter lower latency neural network designed for use on mobile devices while inceptionv3 is a heavier architecture which tends to achieve better performance since we only had a few thousand images training these networks from scratch would surely cause overfitting so instead we downloaded pre trained versions of these models using tensorflow with weights optimized to classify images in the imagenet dataset froze the weights of most of the layers of the pre trained networks and trained a new fully connected layer with a sigmoid or softmax activation placed on top of each of the pre trained networks 	mobilenetv1 is a lighter lower latency neural network designed for use on mobile devices while inceptionv3 is a heavier architecture which tends to achieve better performance 
1	134	8924	mobilenetv1 and inceptionv3 are two convolutional neural network cnn architectures designed for image recognition tasks mobilenetv1 is a lighter lower latency neural network designed for use on mobile devices while inceptionv3 is a heavier architecture which tends to achieve better performance since we only had a few thousand images training these networks from scratch would surely cause overfitting so instead we downloaded pre trained versions of these models using tensorflow with weights optimized to classify images in the imagenet dataset froze the weights of most of the layers of the pre trained networks and trained a new fully connected layer with a sigmoid or softmax activation placed on top of each of the pre trained networks 	since we only had a few thousand images training these networks from scratch would surely cause overfitting so instead we downloaded pre trained versions of these models using tensorflow with weights optimized to classify images in the imagenet dataset froze the weights of most of the layers of the pre trained networks and trained a new fully connected layer with a sigmoid or softmax activation placed on top of each of the pre trained networks 
0	134	8925	mobilenetv1 and inceptionv3 are two convolutional neural network cnn architectures designed for image recognition tasks mobilenetv1 is a lighter lower latency neural network designed for use on mobile devices while inceptionv3 is a heavier architecture which tends to achieve better performance since we only had a few thousand images training these networks from scratch would surely cause overfitting so instead we downloaded pre trained versions of these models using tensorflow with weights optimized to classify images in the imagenet dataset froze the weights of most of the layers of the pre trained networks and trained a new fully connected layer with a sigmoid or softmax activation placed on top of each of the pre trained networks 	this is a common technique used in machine learning known as transfer learning
1	134	8926	since our dataset was quite different from the imagenet dataset the features extracted at the top of the inceptionv3 network were probably not optimized for our application so we thought we might be able to achieve better performance by building an svm classifier based on activations earlier in the inceptionv3 network which contains more general features this was achieved by feeding the pretrained inceptionv3 network all of our images computing the output of the 288th later for reference the inceptionv3 network has 311 layers and using these outputs as features for an svm classifier here we also implemented model selection to find the optimal kernel coefficient gamma of the rbf kernel as shown in we used a google cloud deep learning vm instance for many of our simulation runs with tensorflow optimized for an nvdia p100 gpu and intel skylake 8 core cpu using intel mkl and nvidia cuda we discovered an instance optimized for nvdia was faster on cnns but an instance optimized for intel was faster for sci py all of the code used in this project including many experiments whose results we did not include in this report due to lack of space is available in our github repository https github com jatron structural damage recognition 	since our dataset was quite different from the imagenet dataset the features extracted at the top of the inceptionv3 network were probably not optimized for our application so we thought we might be able to achieve better performance by building an svm classifier based on activations earlier in the inceptionv3 network which contains more general features this was achieved by feeding the pretrained inceptionv3 network all of our images computing the output of the 288th later for reference the inceptionv3 network has 311 layers and using these outputs as features for an svm classifier 
1	134	8927	since our dataset was quite different from the imagenet dataset the features extracted at the top of the inceptionv3 network were probably not optimized for our application so we thought we might be able to achieve better performance by building an svm classifier based on activations earlier in the inceptionv3 network which contains more general features this was achieved by feeding the pretrained inceptionv3 network all of our images computing the output of the 288th later for reference the inceptionv3 network has 311 layers and using these outputs as features for an svm classifier here we also implemented model selection to find the optimal kernel coefficient gamma of the rbf kernel as shown in we used a google cloud deep learning vm instance for many of our simulation runs with tensorflow optimized for an nvdia p100 gpu and intel skylake 8 core cpu using intel mkl and nvidia cuda we discovered an instance optimized for nvdia was faster on cnns but an instance optimized for intel was faster for sci py all of the code used in this project including many experiments whose results we did not include in this report due to lack of space is available in our github repository https github com jatron structural damage recognition 	here we also implemented model selection to find the optimal kernel coefficient gamma of the rbf kernel as shown in we used a google cloud deep learning vm instance for many of our simulation runs with tensorflow optimized for an nvdia p100 gpu and intel skylake 8 core cpu using intel mkl and nvidia cuda 
1	134	8928	since our dataset was quite different from the imagenet dataset the features extracted at the top of the inceptionv3 network were probably not optimized for our application so we thought we might be able to achieve better performance by building an svm classifier based on activations earlier in the inceptionv3 network which contains more general features this was achieved by feeding the pretrained inceptionv3 network all of our images computing the output of the 288th later for reference the inceptionv3 network has 311 layers and using these outputs as features for an svm classifier here we also implemented model selection to find the optimal kernel coefficient gamma of the rbf kernel as shown in we used a google cloud deep learning vm instance for many of our simulation runs with tensorflow optimized for an nvdia p100 gpu and intel skylake 8 core cpu using intel mkl and nvidia cuda we discovered an instance optimized for nvdia was faster on cnns but an instance optimized for intel was faster for sci py all of the code used in this project including many experiments whose results we did not include in this report due to lack of space is available in our github repository https github com jatron structural damage recognition 	we discovered an instance optimized for nvdia was faster on cnns but an instance optimized for intel was faster for sci py all of the code used in this project including many experiments whose results we did not include in this report due to lack of space is available in our github repository https github com jatron structural damage recognition 
1	134	8929	the performance achieved by each of our models is summarized in the it is not surprising that the models based on cnns performed the best since the parameters could best take advantage of the spatial information in the images we note however that the mixed network svm plus inceptionv3 also did well after tuning the kernel coefficient gamma of the rbf kernel we were able to achieve 75 validation accuracy and 95 training accuracy with this model 	the performance achieved by each of our models is summarized in the it is not surprising that the models based on cnns performed the best since the parameters could best take advantage of the spatial information in the images 
1	134	8930	the performance achieved by each of our models is summarized in the it is not surprising that the models based on cnns performed the best since the parameters could best take advantage of the spatial information in the images we note however that the mixed network svm plus inceptionv3 also did well after tuning the kernel coefficient gamma of the rbf kernel we were able to achieve 75 validation accuracy and 95 training accuracy with this model 	we note however that the mixed network svm plus inceptionv3 also did well after tuning the kernel coefficient gamma of the rbf kernel we were able to achieve 75 validation accuracy and 95 training accuracy with this model 
1	134	8931	as mentioned earlier we had applied transfer learning in tensorflow to baseline inceptionv3 model originally trained using imagenet adding a fully connected and softmax layer similar to	as mentioned earlier we had applied transfer learning in tensorflow to baseline inceptionv3 model originally trained using imagenet adding a fully connected and softmax layer similar to
1	134	8932	we find that by using 4000 images for training 1000 images for testing on the retrained inceptionv3 model discussed in 5 2 we obtain the following test confusion matrix 448 86 117 349 after performing the prediction we checked manually through several hundred images to determine patterns in correctly predicted images false negatives and false positives examples of misclassified images are depicted in	we find that by using 4000 images for training 1000 images for testing on the retrained inceptionv3 model discussed in 5 2 we obtain the following test confusion matrix 448 86 117 349 
1	134	8933	we find that by using 4000 images for training 1000 images for testing on the retrained inceptionv3 model discussed in 5 2 we obtain the following test confusion matrix 448 86 117 349 after performing the prediction we checked manually through several hundred images to determine patterns in correctly predicted images false negatives and false positives examples of misclassified images are depicted in	after performing the prediction we checked manually through several hundred images to determine patterns in correctly predicted images false negatives and false positives 
0	134	8934	we find that by using 4000 images for training 1000 images for testing on the retrained inceptionv3 model discussed in 5 2 we obtain the following test confusion matrix 448 86 117 349 after performing the prediction we checked manually through several hundred images to determine patterns in correctly predicted images false negatives and false positives examples of misclassified images are depicted in	examples of misclassified images are depicted in
1	134	8935	we retrain the model from 5 2 using keras but this time we remove the top layer of the inceptionv3 network flatten the output of the penultimate layer add a fully connected layer and softmax activation we find that we are now overfitting	we retrain the model from 5 2 using keras but this time we remove the top layer of the inceptionv3 network flatten the output of the penultimate layer add a fully connected layer and softmax activation 
0	134	8936	we retrain the model from 5 2 using keras but this time we remove the top layer of the inceptionv3 network flatten the output of the penultimate layer add a fully connected layer and softmax activation we find that we are now overfitting	we find that we are now overfitting
0	134	8937	we conclude that a variation of a convolutional neural network performs best on our dataset furthermore while bias can be managed by training more parameters layers of the cnn we must be careful not to add so many parameters that we overfit however overfitting can be also managed by adding random images to data in terms of future work and next steps more controlled experimentation can be done to manage bias and variance 	we conclude that a variation of a convolutional neural network performs best on our dataset 
1	134	8938	we conclude that a variation of a convolutional neural network performs best on our dataset furthermore while bias can be managed by training more parameters layers of the cnn we must be careful not to add so many parameters that we overfit however overfitting can be also managed by adding random images to data in terms of future work and next steps more controlled experimentation can be done to manage bias and variance 	furthermore while bias can be managed by training more parameters layers of the cnn we must be careful not to add so many parameters that we overfit 
1	134	8939	we conclude that a variation of a convolutional neural network performs best on our dataset furthermore while bias can be managed by training more parameters layers of the cnn we must be careful not to add so many parameters that we overfit however overfitting can be also managed by adding random images to data in terms of future work and next steps more controlled experimentation can be done to manage bias and variance 	however overfitting can be also managed by adding random images to data in terms of future work and next steps more controlled experimentation can be done to manage bias and variance 
1	134	8940	we conclude that a variation of a convolutional neural network performs best on our dataset furthermore while bias can be managed by training more parameters layers of the cnn we must be careful not to add so many parameters that we overfit however overfitting can be also managed by adding random images to data in terms of future work and next steps more controlled experimentation can be done to manage bias and variance 	we could improve validation accuracy by better managing the data correct mislabeled images add images similar to the false positives or negatives cropping irrelevant features understanding differences in texture or pattern vs damage and accommodating wide angle versus close up images 
0	134	8941	we conclude that a variation of a convolutional neural network performs best on our dataset furthermore while bias can be managed by training more parameters layers of the cnn we must be careful not to add so many parameters that we overfit however overfitting can be also managed by adding random images to data in terms of future work and next steps more controlled experimentation can be done to manage bias and variance 	furthermore other techniques such as ensemble averaging could perhaps lead to better performance 
0	134	8942	we acknowledge sanjay govindjee who alerted us to this problem the guidance of fantine huot and mark daoust are also gratefully acknowledged 	we acknowledge sanjay govindjee who alerted us to this problem 
0	134	8943	we acknowledge sanjay govindjee who alerted us to this problem the guidance of fantine huot and mark daoust are also gratefully acknowledged 	the guidance of fantine huot and mark daoust are also gratefully acknowledged 
1	135	8944	autonomous fly by feel vehicles motivated by the supreme flight skills of birds a new concept called fly by feel fbf has been proposed to develop the next generation of intelligent aircrafts to achieve this goal stanford structures and composites lab sacl has developed a smart wing which embeds a multifunctional sensor network on the surface layup of the wing collected from a series of wind tunnel tests with different flight states the dataset explored in this study includes conditions of angle of attack from 0 to 15 degrees incremental step of 5 degrees and conditions of airflow velocity from 0 to 25 m s minimum incremental step of 0 5 m s 60 000 data points are collected from every piezoelectric sensor for each flight state 	autonomous fly by feel vehicles motivated by the supreme flight skills of birds a new concept called fly by feel fbf has been proposed to develop the next generation of intelligent aircrafts 
1	135	8945	autonomous fly by feel vehicles motivated by the supreme flight skills of birds a new concept called fly by feel fbf has been proposed to develop the next generation of intelligent aircrafts to achieve this goal stanford structures and composites lab sacl has developed a smart wing which embeds a multifunctional sensor network on the surface layup of the wing collected from a series of wind tunnel tests with different flight states the dataset explored in this study includes conditions of angle of attack from 0 to 15 degrees incremental step of 5 degrees and conditions of airflow velocity from 0 to 25 m s minimum incremental step of 0 5 m s 60 000 data points are collected from every piezoelectric sensor for each flight state 	to achieve this goal stanford structures and composites lab sacl has developed a smart wing which embeds a multifunctional sensor network on the surface layup of the wing collected from a series of wind tunnel tests with different flight states the dataset explored in this study includes conditions of angle of attack from 0 to 15 degrees incremental step of 5 degrees and conditions of airflow velocity from 0 to 25 m s minimum incremental step of 0 5 m s 
1	135	8946	autonomous fly by feel vehicles motivated by the supreme flight skills of birds a new concept called fly by feel fbf has been proposed to develop the next generation of intelligent aircrafts to achieve this goal stanford structures and composites lab sacl has developed a smart wing which embeds a multifunctional sensor network on the surface layup of the wing collected from a series of wind tunnel tests with different flight states the dataset explored in this study includes conditions of angle of attack from 0 to 15 degrees incremental step of 5 degrees and conditions of airflow velocity from 0 to 25 m s minimum incremental step of 0 5 m s 60 000 data points are collected from every piezoelectric sensor for each flight state 	60 000 data points are collected from every piezoelectric sensor for each flight state 
1	135	8947	autonomous fly by feel vehicles motivated by the supreme flight skills of birds a new concept called fly by feel fbf has been proposed to develop the next generation of intelligent aircrafts to achieve this goal stanford structures and composites lab sacl has developed a smart wing which embeds a multifunctional sensor network on the surface layup of the wing collected from a series of wind tunnel tests with different flight states the dataset explored in this study includes conditions of angle of attack from 0 to 15 degrees incremental step of 5 degrees and conditions of airflow velocity from 0 to 25 m s minimum incremental step of 0 5 m s 60 000 data points are collected from every piezoelectric sensor for each flight state 	we perform data augmentation in the time domain by splitting 60 000 data points into numerous segments as samples 80 samples are used as training data 10 are used as validation data and the 10 else are used as testing data with uniform distribution among each flight state goal minimize misclassification rate 1 1 
0	135	8948	the gini index categorical cross entropy convolutional neural network architecture 1 results of the decision tree algorithm indicate that mean and standard deviation of signal magnitudes and power spectrum are key features when velocity interval becomes smaller features from different sensors are required to guarantee higher classification performance 	the gini index categorical cross entropy convolutional neural network architecture 1 
1	135	8949	the gini index categorical cross entropy convolutional neural network architecture 1 results of the decision tree algorithm indicate that mean and standard deviation of signal magnitudes and power spectrum are key features when velocity interval becomes smaller features from different sensors are required to guarantee higher classification performance 	results of the decision tree algorithm indicate that mean and standard deviation of signal magnitudes and power spectrum are key features 
1	135	8950	the gini index categorical cross entropy convolutional neural network architecture 1 results of the decision tree algorithm indicate that mean and standard deviation of signal magnitudes and power spectrum are key features when velocity interval becomes smaller features from different sensors are required to guarantee higher classification performance 	when velocity interval becomes smaller features from different sensors are required to guarantee higher classification performance 
1	135	8951	the gini index categorical cross entropy convolutional neural network architecture 1 results of the decision tree algorithm indicate that mean and standard deviation of signal magnitudes and power spectrum are key features when velocity interval becomes smaller features from different sensors are required to guarantee higher classification performance 	linear models work well with manually designed features 
1	135	8952	the gini index categorical cross entropy convolutional neural network architecture 1 results of the decision tree algorithm indicate that mean and standard deviation of signal magnitudes and power spectrum are key features when velocity interval becomes smaller features from different sensors are required to guarantee higher classification performance 	feature selection improves linear separability of the data 
1	135	8953	the gini index categorical cross entropy convolutional neural network architecture 1 results of the decision tree algorithm indicate that mean and standard deviation of signal magnitudes and power spectrum are key features when velocity interval becomes smaller features from different sensors are required to guarantee higher classification performance 	the convolutional neural network shows comparable performance by feeding in only standardized signal segments 
1	135	8954	the gini index categorical cross entropy convolutional neural network architecture 1 results of the decision tree algorithm indicate that mean and standard deviation of signal magnitudes and power spectrum are key features when velocity interval becomes smaller features from different sensors are required to guarantee higher classification performance 	it is demonstrated that the convolutional neural network can be trained to capture important features from the original signal directly 
1	135	8955	we are going to develop a regression model in the following 6 months discretized flight state has constrained application if the resolution is not sufficient and high resolution requirement with limited data also poses difficulties for classification we hope to train a regression model to provide an accurate estimate of the flight state for example aoa 9 8 airflow velocity 24 3 m s which would be more of practical use than specifying an approximate range of aoa and velocity 	we are going to develop a regression model in the following 6 months 
1	135	8956	we are going to develop a regression model in the following 6 months discretized flight state has constrained application if the resolution is not sufficient and high resolution requirement with limited data also poses difficulties for classification we hope to train a regression model to provide an accurate estimate of the flight state for example aoa 9 8 airflow velocity 24 3 m s which would be more of practical use than specifying an approximate range of aoa and velocity 	discretized flight state has constrained application if the resolution is not sufficient and high resolution requirement with limited data also poses difficulties for classification 
1	135	8957	we are going to develop a regression model in the following 6 months discretized flight state has constrained application if the resolution is not sufficient and high resolution requirement with limited data also poses difficulties for classification we hope to train a regression model to provide an accurate estimate of the flight state for example aoa 9 8 airflow velocity 24 3 m s which would be more of practical use than specifying an approximate range of aoa and velocity 	we hope to train a regression model to provide an accurate estimate of the flight state for example aoa 9 8 airflow velocity 24 3 m s which would be more of practical use than specifying an approximate range of aoa and velocity 
0	135	8958	we are going to develop a regression model in the following 6 months discretized flight state has constrained application if the resolution is not sufficient and high resolution requirement with limited data also poses difficulties for classification we hope to train a regression model to provide an accurate estimate of the flight state for example aoa 9 8 airflow velocity 24 3 m s which would be more of practical use than specifying an approximate range of aoa and velocity 	 1 f p 
1	135	8959	we are going to develop a regression model in the following 6 months discretized flight state has constrained application if the resolution is not sufficient and high resolution requirement with limited data also poses difficulties for classification we hope to train a regression model to provide an accurate estimate of the flight state for example aoa 9 8 airflow velocity 24 3 m s which would be more of practical use than specifying an approximate range of aoa and velocity 	kopsaftopoulos r nardari y h li p wang b ye f k chang experimental identification of structural dynamics and aeroelastic properties of a self sensing smart composite wing in proceedings of the 10th international workshop on structural health monitoring stanford ca usa 1 3 september 2015 
0	135	8960	we are going to develop a regression model in the following 6 months discretized flight state has constrained application if the resolution is not sufficient and high resolution requirement with limited data also poses difficulties for classification we hope to train a regression model to provide an accurate estimate of the flight state for example aoa 9 8 airflow velocity 24 3 m s which would be more of practical use than specifying an approximate range of aoa and velocity 	 2 x chen f p 
1	135	8961	we are going to develop a regression model in the following 6 months discretized flight state has constrained application if the resolution is not sufficient and high resolution requirement with limited data also poses difficulties for classification we hope to train a regression model to provide an accurate estimate of the flight state for example aoa 9 8 airflow velocity 24 3 m s which would be more of practical use than specifying an approximate range of aoa and velocity 	kopsaftopoulos q wu h ren f in this problem a large feature pool from both the time and frequency domains is created to obtain enough useful information from the raw signal data 
1	135	8962	we are going to develop a regression model in the following 6 months discretized flight state has constrained application if the resolution is not sufficient and high resolution requirement with limited data also poses difficulties for classification we hope to train a regression model to provide an accurate estimate of the flight state for example aoa 9 8 airflow velocity 24 3 m s which would be more of practical use than specifying an approximate range of aoa and velocity 	we split total data into 80 10 and 10 for training validation and test dataset respectively 
1	135	8963	we are going to develop a regression model in the following 6 months discretized flight state has constrained application if the resolution is not sufficient and high resolution requirement with limited data also poses difficulties for classification we hope to train a regression model to provide an accurate estimate of the flight state for example aoa 9 8 airflow velocity 24 3 m s which would be more of practical use than specifying an approximate range of aoa and velocity 	there are 4 743 training samples 522 validation samples and 522 test samples 
0	136	8964	artist identification is the task of identifying the artist of a work given only the image with no other metadata many paintings have unknown or highly contested artists and experts in the field need a long time to learn the styles of various artists before being able to analyze paintings additionally even with significant expertise artist identification can be difficult because one artist s style can vary wildly from painting to painting 	artist identification is the task of identifying the artist of a work given only the image with no other metadata 
1	136	8965	artist identification is the task of identifying the artist of a work given only the image with no other metadata many paintings have unknown or highly contested artists and experts in the field need a long time to learn the styles of various artists before being able to analyze paintings additionally even with significant expertise artist identification can be difficult because one artist s style can vary wildly from painting to painting 	many paintings have unknown or highly contested artists and experts in the field need a long time to learn the styles of various artists before being able to analyze paintings 
1	136	8966	artist identification is the task of identifying the artist of a work given only the image with no other metadata many paintings have unknown or highly contested artists and experts in the field need a long time to learn the styles of various artists before being able to analyze paintings additionally even with significant expertise artist identification can be difficult because one artist s style can vary wildly from painting to painting 	additionally even with significant expertise artist identification can be difficult because one artist s style can vary wildly from painting to painting 
1	136	8967	artist identification is the task of identifying the artist of a work given only the image with no other metadata many paintings have unknown or highly contested artists and experts in the field need a long time to learn the styles of various artists before being able to analyze paintings additionally even with significant expertise artist identification can be difficult because one artist s style can vary wildly from painting to painting 	with machine learning we can provide experts with baseline estimate to reduce necessary time and effort as well as making artist identification more accessible to those with less experience additionally this task was chosen because it is a fairly straightforward image classification task and we want to compare conventional machine learning techniques for image classification to more recent deep learning techniques 
1	136	8968	artist identification is the task of identifying the artist of a work given only the image with no other metadata many paintings have unknown or highly contested artists and experts in the field need a long time to learn the styles of various artists before being able to analyze paintings additionally even with significant expertise artist identification can be difficult because one artist s style can vary wildly from painting to painting 	specifically in this report we will compare the use of feature extraction with an svm to the use of a cnn 
1	136	8969	artist identification is the task of identifying the artist of a work given only the image with no other metadata many paintings have unknown or highly contested artists and experts in the field need a long time to learn the styles of various artists before being able to analyze paintings additionally even with significant expertise artist identification can be difficult because one artist s style can vary wildly from painting to painting 	our dataset contains 256 x 256 x 3 color images of paintings for the svm we extract features from these images as the input for the classifier while we feed in the raw image to the cnn 
0	136	8970	artist identification is the task of identifying the artist of a work given only the image with no other metadata many paintings have unknown or highly contested artists and experts in the field need a long time to learn the styles of various artists before being able to analyze paintings additionally even with significant expertise artist identification can be difficult because one artist s style can vary wildly from painting to painting 	for both models the output is a predicted artist 
0	136	8971	artist identification is the task of identifying the artist of a work given only the image with no other metadata many paintings have unknown or highly contested artists and experts in the field need a long time to learn the styles of various artists before being able to analyze paintings additionally even with significant expertise artist identification can be difficult because one artist s style can vary wildly from painting to painting 	the metrics we chose to compare the two methods are accuracy training and inference times and ease of implementation 
1	136	8972	as previously stated artist identification is often a task done by human experts such as curators in museums art historians and other collectors however recent times has lead to a marked increase in computational methods for artist identification much of previous work on this task involves exploration into feature extraction and the subsequent application of a classifier like an svm blessing and wen uses features including dense sift hog2x2 ssim and texton histograms to classify works using an svm with 85 13 accuracy approaches using deep learning have also been very successful on this task 	as previously stated artist identification is often a task done by human experts such as curators in museums art historians and other collectors 
0	136	8973	as previously stated artist identification is often a task done by human experts such as curators in museums art historians and other collectors however recent times has lead to a marked increase in computational methods for artist identification much of previous work on this task involves exploration into feature extraction and the subsequent application of a classifier like an svm blessing and wen uses features including dense sift hog2x2 ssim and texton histograms to classify works using an svm with 85 13 accuracy approaches using deep learning have also been very successful on this task 	however recent times has lead to a marked increase in computational methods for artist identification much of previous work on this task involves exploration into feature extraction and the subsequent application of a classifier like an svm 
1	136	8974	as previously stated artist identification is often a task done by human experts such as curators in museums art historians and other collectors however recent times has lead to a marked increase in computational methods for artist identification much of previous work on this task involves exploration into feature extraction and the subsequent application of a classifier like an svm blessing and wen uses features including dense sift hog2x2 ssim and texton histograms to classify works using an svm with 85 13 accuracy approaches using deep learning have also been very successful on this task 	blessing and wen uses features including dense sift hog2x2 ssim and texton histograms to classify works using an svm with 85 13 accuracy approaches using deep learning have also been very successful on this task 
1	136	8975	as previously stated artist identification is often a task done by human experts such as curators in museums art historians and other collectors however recent times has lead to a marked increase in computational methods for artist identification much of previous work on this task involves exploration into feature extraction and the subsequent application of a classifier like an svm blessing and wen uses features including dense sift hog2x2 ssim and texton histograms to classify works using an svm with 85 13 accuracy approaches using deep learning have also been very successful on this task 	viswanathan explores the use of three different cnn models demonstrating that features from imagenet are generally applicable to artist identification and thus showing that transfer learning can be very useful for this task most prior work has worked with datasets involving relatively few classes blessing and wen only used work from 7 artists
0	136	8976	our data is obtained from the dataset for the kaggle competition painters by numbers 	our data is obtained from the dataset for the kaggle competition painters by numbers 
1	136	8977	a set of 6 image descriptors were explored as features for the svm gist descriptors hu moments color histograms sift keypoints histogram of oriented gradients and haralick textures each of the 64 possible feature combinations was explored by concatenating feature vectors obtained from each method horizontally after exploring the combinations thoroughly the 3 most useful features were found to be the gist descriptors hu moments and color histograms 	a set of 6 image descriptors were explored as features for the svm gist descriptors hu moments color histograms sift keypoints histogram of oriented gradients and haralick textures 
0	136	8978	a set of 6 image descriptors were explored as features for the svm gist descriptors hu moments color histograms sift keypoints histogram of oriented gradients and haralick textures each of the 64 possible feature combinations was explored by concatenating feature vectors obtained from each method horizontally after exploring the combinations thoroughly the 3 most useful features were found to be the gist descriptors hu moments and color histograms 	each of the 64 possible feature combinations was explored by concatenating feature vectors obtained from each method horizontally 
0	136	8979	a set of 6 image descriptors were explored as features for the svm gist descriptors hu moments color histograms sift keypoints histogram of oriented gradients and haralick textures each of the 64 possible feature combinations was explored by concatenating feature vectors obtained from each method horizontally after exploring the combinations thoroughly the 3 most useful features were found to be the gist descriptors hu moments and color histograms 	after exploring the combinations thoroughly the 3 most useful features were found to be the gist descriptors hu moments and color histograms 
0	136	8980	a set of 6 image descriptors were explored as features for the svm gist descriptors hu moments color histograms sift keypoints histogram of oriented gradients and haralick textures each of the 64 possible feature combinations was explored by concatenating feature vectors obtained from each method horizontally after exploring the combinations thoroughly the 3 most useful features were found to be the gist descriptors hu moments and color histograms 	gist descriptors are a set of 5 attributes that represent intuitive properties of a scene naturalness openness roughness expansion and ruggedness 
0	136	8981	a set of 6 image descriptors were explored as features for the svm gist descriptors hu moments color histograms sift keypoints histogram of oriented gradients and haralick textures each of the 64 possible feature combinations was explored by concatenating feature vectors obtained from each method horizontally after exploring the combinations thoroughly the 3 most useful features were found to be the gist descriptors hu moments and color histograms 	these attributes were determined by aude oliva and antonio torralba hu moments are a set of 7 polynomial combinations of image moments that are defined as to be scale shift and rotation invariant 
0	136	8982	a set of 6 image descriptors were explored as features for the svm gist descriptors hu moments color histograms sift keypoints histogram of oriented gradients and haralick textures each of the 64 possible feature combinations was explored by concatenating feature vectors obtained from each method horizontally after exploring the combinations thoroughly the 3 most useful features were found to be the gist descriptors hu moments and color histograms 	scale and shift invariance are ensured by using the central moments to account for shift and scaling by the 0th moment to account for scale 
0	136	8983	a set of 6 image descriptors were explored as features for the svm gist descriptors hu moments color histograms sift keypoints histogram of oriented gradients and haralick textures each of the 64 possible feature combinations was explored by concatenating feature vectors obtained from each method horizontally after exploring the combinations thoroughly the 3 most useful features were found to be the gist descriptors hu moments and color histograms 	rotation invariance is obtained by the specific polynomial combinations of moments from ming huei hu the color histogram is a simple set of 3 histograms representing the distribution of color appearances in the image 
0	136	8984	a set of 6 image descriptors were explored as features for the svm gist descriptors hu moments color histograms sift keypoints histogram of oriented gradients and haralick textures each of the 64 possible feature combinations was explored by concatenating feature vectors obtained from each method horizontally after exploring the combinations thoroughly the 3 most useful features were found to be the gist descriptors hu moments and color histograms 	a histogram for each color was computed by quantizing the color values for each channel into 8 bins and counting appearances of each quantized color 
1	136	8985	a set of 6 image descriptors were explored as features for the svm gist descriptors hu moments color histograms sift keypoints histogram of oriented gradients and haralick textures each of the 64 possible feature combinations was explored by concatenating feature vectors obtained from each method horizontally after exploring the combinations thoroughly the 3 most useful features were found to be the gist descriptors hu moments and color histograms 	the histogram has no representation of the spatial distribution of the colors only their appearance each feature combination was scaled using min max scaling so that each element was in the rage 0 1 
1	136	8986	a set of 6 image descriptors were explored as features for the svm gist descriptors hu moments color histograms sift keypoints histogram of oriented gradients and haralick textures each of the 64 possible feature combinations was explored by concatenating feature vectors obtained from each method horizontally after exploring the combinations thoroughly the 3 most useful features were found to be the gist descriptors hu moments and color histograms 	for input to the svm pca was applied with 100 components on the training features to determine the top 100 principal components 
0	136	8987	a set of 6 image descriptors were explored as features for the svm gist descriptors hu moments color histograms sift keypoints histogram of oriented gradients and haralick textures each of the 64 possible feature combinations was explored by concatenating feature vectors obtained from each method horizontally after exploring the combinations thoroughly the 3 most useful features were found to be the gist descriptors hu moments and color histograms 	test data was projected onto the same principal components prior to prediction 
1	136	8988	a set of 6 image descriptors were explored as features for the svm gist descriptors hu moments color histograms sift keypoints histogram of oriented gradients and haralick textures each of the 64 possible feature combinations was explored by concatenating feature vectors obtained from each method horizontally after exploring the combinations thoroughly the 3 most useful features were found to be the gist descriptors hu moments and color histograms 	examples of feature extraction can be seen in for each class where a one vs rest classification scheme was used so a classifier for each class was trained to identify that particular class 
0	136	8989	a set of 6 image descriptors were explored as features for the svm gist descriptors hu moments color histograms sift keypoints histogram of oriented gradients and haralick textures each of the 64 possible feature combinations was explored by concatenating feature vectors obtained from each method horizontally after exploring the combinations thoroughly the 3 most useful features were found to be the gist descriptors hu moments and color histograms 	for each class a distinct svm finds the margin that best separates the data in one class from the data in the rest after the rbf kernel is applied while minimizing the allowable misclassification terms i 
0	136	8990	as a deep learning approach to image classification the cnn is a neural network that consists of several layers of different types including convolutional max pooling relu activation dropout dense fully connected and softmax each convolutional layer is a set of learnable 2d filters which are applied to input data by the 2d cross correlation operation in max pooling layers data is downsampled by a set factor n by choosing only the max value in each n n square to propagate forward 	as a deep learning approach to image classification the cnn is a neural network that consists of several layers of different types including convolutional max pooling relu activation dropout dense fully connected and softmax 
0	136	8991	as a deep learning approach to image classification the cnn is a neural network that consists of several layers of different types including convolutional max pooling relu activation dropout dense fully connected and softmax each convolutional layer is a set of learnable 2d filters which are applied to input data by the 2d cross correlation operation in max pooling layers data is downsampled by a set factor n by choosing only the max value in each n n square to propagate forward 	each convolutional layer is a set of learnable 2d filters which are applied to input data by the 2d cross correlation operation 
0	136	8992	as a deep learning approach to image classification the cnn is a neural network that consists of several layers of different types including convolutional max pooling relu activation dropout dense fully connected and softmax each convolutional layer is a set of learnable 2d filters which are applied to input data by the 2d cross correlation operation in max pooling layers data is downsampled by a set factor n by choosing only the max value in each n n square to propagate forward 	in max pooling layers data is downsampled by a set factor n by choosing only the max value in each n n square to propagate forward 
0	136	8993	as a deep learning approach to image classification the cnn is a neural network that consists of several layers of different types including convolutional max pooling relu activation dropout dense fully connected and softmax each convolutional layer is a set of learnable 2d filters which are applied to input data by the 2d cross correlation operation in max pooling layers data is downsampled by a set factor n by choosing only the max value in each n n square to propagate forward 	the relu activation function is a unit ramp function f x max x 0 that allows for nonlinearlity in the network 
0	136	8994	as a deep learning approach to image classification the cnn is a neural network that consists of several layers of different types including convolutional max pooling relu activation dropout dense fully connected and softmax each convolutional layer is a set of learnable 2d filters which are applied to input data by the 2d cross correlation operation in max pooling layers data is downsampled by a set factor n by choosing only the max value in each n n square to propagate forward 	in dense layers input data is flattened into 1d vectors multiplied by a matrix of learnable weights and added with a learnable bias 
0	136	8995	as a deep learning approach to image classification the cnn is a neural network that consists of several layers of different types including convolutional max pooling relu activation dropout dense fully connected and softmax each convolutional layer is a set of learnable 2d filters which are applied to input data by the 2d cross correlation operation in max pooling layers data is downsampled by a set factor n by choosing only the max value in each n n square to propagate forward 	dropout removes a percentage of activations to help prevent overfitting 
1	136	8996	as a deep learning approach to image classification the cnn is a neural network that consists of several layers of different types including convolutional max pooling relu activation dropout dense fully connected and softmax each convolutional layer is a set of learnable 2d filters which are applied to input data by the 2d cross correlation operation in max pooling layers data is downsampled by a set factor n by choosing only the max value in each n n square to propagate forward 	finally the softmax layer computes the class probabilities for the data for our cnn we base our architecture on the winning submission to the kaggle painters by numbers competition j e fj where l i indicates the loss for example i y i is the correct class for example i j is one of the potential classes and f k indicates the score for class k according to the cnn 
0	136	8997	as a deep learning approach to image classification the cnn is a neural network that consists of several layers of different types including convolutional max pooling relu activation dropout dense fully connected and softmax each convolutional layer is a set of learnable 2d filters which are applied to input data by the 2d cross correlation operation in max pooling layers data is downsampled by a set factor n by choosing only the max value in each n n square to propagate forward 	the architecture is shown in
1	136	8998	our svm model is implemented with the scikit learn package to choose the best hyperparameters features and c for our svm we ran a grid search and 3 fold cross validation across 10 5 10 5 and c 10 5 10 5 using the training set for each feature combination the model for each feature combination with best and c was then tested on our validation data and the best was chosen as our best model the cnn was also trained multiple times using different architectures adding and removing convolutional and max pooling layers and the best model was chosen using the final validation accuracy after training 	our svm model is implemented with the scikit learn package to choose the best hyperparameters features and c for our svm we ran a grid search and 3 fold cross validation across 10 5 10 5 and c 10 5 10 5 using the training set for each feature combination 
0	136	8999	our svm model is implemented with the scikit learn package to choose the best hyperparameters features and c for our svm we ran a grid search and 3 fold cross validation across 10 5 10 5 and c 10 5 10 5 using the training set for each feature combination the model for each feature combination with best and c was then tested on our validation data and the best was chosen as our best model the cnn was also trained multiple times using different architectures adding and removing convolutional and max pooling layers and the best model was chosen using the final validation accuracy after training 	the model for each feature combination with best and c was then tested on our validation data and the best was chosen as our best model 
1	136	9000	our svm model is implemented with the scikit learn package to choose the best hyperparameters features and c for our svm we ran a grid search and 3 fold cross validation across 10 5 10 5 and c 10 5 10 5 using the training set for each feature combination the model for each feature combination with best and c was then tested on our validation data and the best was chosen as our best model the cnn was also trained multiple times using different architectures adding and removing convolutional and max pooling layers and the best model was chosen using the final validation accuracy after training 	the cnn was also trained multiple times using different architectures adding and removing convolutional and max pooling layers and the best model was chosen using the final validation accuracy after training 
0	136	9001	the metric we used for judging our models was accuracy on the test set our dataset was overall reasonably balanced so this should be a good measure for how our models perform on these classes we found that the 3 most useful features in classifying artists were the gist descriptors hu moments and color histograms 	the metric we used for judging our models was accuracy on the test set 
0	136	9002	the metric we used for judging our models was accuracy on the test set our dataset was overall reasonably balanced so this should be a good measure for how our models perform on these classes we found that the 3 most useful features in classifying artists were the gist descriptors hu moments and color histograms 	our dataset was overall reasonably balanced so this should be a good measure for how our models perform on these classes 
0	136	9003	the metric we used for judging our models was accuracy on the test set our dataset was overall reasonably balanced so this should be a good measure for how our models perform on these classes we found that the 3 most useful features in classifying artists were the gist descriptors hu moments and color histograms 	we found that the 3 most useful features in classifying artists were the gist descriptors hu moments and color histograms 
0	136	9004	the metric we used for judging our models was accuracy on the test set our dataset was overall reasonably balanced so this should be a good measure for how our models perform on these classes we found that the 3 most useful features in classifying artists were the gist descriptors hu moments and color histograms 	from the results tabulated in we can see that compared to even the best svm results the cnn is superior 
0	136	9005	the metric we used for judging our models was accuracy on the test set our dataset was overall reasonably balanced so this should be a good measure for how our models perform on these classes we found that the 3 most useful features in classifying artists were the gist descriptors hu moments and color histograms 	it achieves higher test accuracy with less noticeable overfitting 
1	136	9006	to compare inference time fairly inference for both models were performed on the ec2 instance the results show that cnn took an order of magnitude less time to run inference while achieving higher accuracy the bulk of the inference time for the svm can also be accounted for by the feature extraction of the test data however so if feature extraction were accelerated the svm might achieve similar time results 	to compare inference time fairly inference for both models were performed on the ec2 instance 
0	136	9007	to compare inference time fairly inference for both models were performed on the ec2 instance the results show that cnn took an order of magnitude less time to run inference while achieving higher accuracy the bulk of the inference time for the svm can also be accounted for by the feature extraction of the test data however so if feature extraction were accelerated the svm might achieve similar time results 	the results show that cnn took an order of magnitude less time to run inference while achieving higher accuracy 
1	136	9008	to compare inference time fairly inference for both models were performed on the ec2 instance the results show that cnn took an order of magnitude less time to run inference while achieving higher accuracy the bulk of the inference time for the svm can also be accounted for by the feature extraction of the test data however so if feature extraction were accelerated the svm might achieve similar time results 	the bulk of the inference time for the svm can also be accounted for by the feature extraction of the test data however so if feature extraction were accelerated the svm might achieve similar time results 
0	136	9009	to compare inference time fairly inference for both models were performed on the ec2 instance the results show that cnn took an order of magnitude less time to run inference while achieving higher accuracy the bulk of the inference time for the svm can also be accounted for by the feature extraction of the test data however so if feature extraction were accelerated the svm might achieve similar time results 	judging from our current results however the cnn is superior in inference each blue data point in
1	136	9010	in terms of ease of implementation the cnn was superior all it required was setting up the initial architecture and feeding in the raw images in contrast implementing the svm required a lot of overhead in researching potential features correctly extracting these features from the images and then tuning different combinations to be used with the model 	in terms of ease of implementation the cnn was superior all it required was setting up the initial architecture and feeding in the raw images 
1	136	9011	in terms of ease of implementation the cnn was superior all it required was setting up the initial architecture and feeding in the raw images in contrast implementing the svm required a lot of overhead in researching potential features correctly extracting these features from the images and then tuning different combinations to be used with the model 	in contrast implementing the svm required a lot of overhead in researching potential features correctly extracting these features from the images and then tuning different combinations to be used with the model 
1	136	9012	we explored the task of artist identification using a dataset of 7462 paintings from 15 artists and compare the performance training and inference time and ease of implementation for both the classical method of feature extraction with an svm classifier and the deep learning method of a cnn our best result came from the cnn with an accuracy of 74 7 in comparison to our best svm using gist features and hu moments with an accuracy of 68 1 for future work we would like to address the overfitting in our svm models despite the tuning of the regularization parameter our svms are overfitting heavily to the training data this may be mitigated by other regularization techniques such as early stopping we would also like to investigate additional features such as classemes given our rather small dataset we believe that transfer learning would have a lot of success on this task 	we explored the task of artist identification using a dataset of 7462 paintings from 15 artists and compare the performance training and inference time and ease of implementation for both the classical method of feature extraction with an svm classifier and the deep learning method of a cnn 
1	136	9013	we explored the task of artist identification using a dataset of 7462 paintings from 15 artists and compare the performance training and inference time and ease of implementation for both the classical method of feature extraction with an svm classifier and the deep learning method of a cnn our best result came from the cnn with an accuracy of 74 7 in comparison to our best svm using gist features and hu moments with an accuracy of 68 1 for future work we would like to address the overfitting in our svm models despite the tuning of the regularization parameter our svms are overfitting heavily to the training data this may be mitigated by other regularization techniques such as early stopping we would also like to investigate additional features such as classemes given our rather small dataset we believe that transfer learning would have a lot of success on this task 	our best result came from the cnn with an accuracy of 74 7 in comparison to our best svm using gist features and hu moments with an accuracy of 68 1 for future work we would like to address the overfitting in our svm models 
1	136	9014	we explored the task of artist identification using a dataset of 7462 paintings from 15 artists and compare the performance training and inference time and ease of implementation for both the classical method of feature extraction with an svm classifier and the deep learning method of a cnn our best result came from the cnn with an accuracy of 74 7 in comparison to our best svm using gist features and hu moments with an accuracy of 68 1 for future work we would like to address the overfitting in our svm models despite the tuning of the regularization parameter our svms are overfitting heavily to the training data this may be mitigated by other regularization techniques such as early stopping we would also like to investigate additional features such as classemes given our rather small dataset we believe that transfer learning would have a lot of success on this task 	despite the tuning of the regularization parameter our svms are overfitting heavily to the training data this may be mitigated by other regularization techniques such as early stopping we would also like to investigate additional features such as classemes given our rather small dataset we believe that transfer learning would have a lot of success on this task 
1	136	9015	we explored the task of artist identification using a dataset of 7462 paintings from 15 artists and compare the performance training and inference time and ease of implementation for both the classical method of feature extraction with an svm classifier and the deep learning method of a cnn our best result came from the cnn with an accuracy of 74 7 in comparison to our best svm using gist features and hu moments with an accuracy of 68 1 for future work we would like to address the overfitting in our svm models despite the tuning of the regularization parameter our svms are overfitting heavily to the training data this may be mitigated by other regularization techniques such as early stopping we would also like to investigate additional features such as classemes given our rather small dataset we believe that transfer learning would have a lot of success on this task 	training our cnn with a large dataset such as the imagenet dataset and then fine tuning for artist classification with our smaller dataset can help our model gain knowledge about object recognition which has already proven useful in features for our svm 
0	136	9016	the initial dataset and implementations of the cnn	the initial dataset and implementations of the cnn
0	136	9017	our code can be found at www github com jchen437 artist classification 	our code can be found at www github com jchen437 artist classification 
1	137	9018	many present day facial recognition systems focus on making verification or identification facial feature invariant while these systems are highly effective for fully automated tasks manual facial recognition is still required in numerous real life scenarios that involve comparing against id photos our goal is to build a system that is capable of transforming faces to include or exclude glasses and beard 	many present day facial recognition systems focus on making verification or identification facial feature invariant 
1	137	9019	many present day facial recognition systems focus on making verification or identification facial feature invariant while these systems are highly effective for fully automated tasks manual facial recognition is still required in numerous real life scenarios that involve comparing against id photos our goal is to build a system that is capable of transforming faces to include or exclude glasses and beard 	while these systems are highly effective for fully automated tasks manual facial recognition is still required in numerous real life scenarios that involve comparing against id photos 
0	137	9020	many present day facial recognition systems focus on making verification or identification facial feature invariant while these systems are highly effective for fully automated tasks manual facial recognition is still required in numerous real life scenarios that involve comparing against id photos our goal is to build a system that is capable of transforming faces to include or exclude glasses and beard 	our goal is to build a system that is capable of transforming faces to include or exclude glasses and beard 
0	137	9021	many present day facial recognition systems focus on making verification or identification facial feature invariant while these systems are highly effective for fully automated tasks manual facial recognition is still required in numerous real life scenarios that involve comparing against id photos our goal is to build a system that is capable of transforming faces to include or exclude glasses and beard 	such a system should be able to handle a wider range of facial features with small modifications 
1	137	9022	many present day facial recognition systems focus on making verification or identification facial feature invariant while these systems are highly effective for fully automated tasks manual facial recognition is still required in numerous real life scenarios that involve comparing against id photos our goal is to build a system that is capable of transforming faces to include or exclude glasses and beard 	a few network structures have been tested for this purpose and we have found that cyclegan 1 is the most capable compared to other vanilla gan systems 
1	137	9023	many present day facial recognition systems focus on making verification or identification facial feature invariant while these systems are highly effective for fully automated tasks manual facial recognition is still required in numerous real life scenarios that involve comparing against id photos our goal is to build a system that is capable of transforming faces to include or exclude glasses and beard 	generated images from test set are presented and their inception scores 2 are analyzed 
0	137	9024	many present day facial recognition systems focus on making verification or identification facial feature invariant while these systems are highly effective for fully automated tasks manual facial recognition is still required in numerous real life scenarios that involve comparing against id photos our goal is to build a system that is capable of transforming faces to include or exclude glasses and beard 	details regarding characteristics of these generated images are also included in our discussion 
1	137	9025	many present day facial recognition systems focus on making verification or identification facial feature invariant while these systems are highly effective for fully automated tasks manual facial recognition is still required in numerous real life scenarios that involve comparing against id photos our goal is to build a system that is capable of transforming faces to include or exclude glasses and beard 	potential future improvements could involve making our system more generic or introducing semi supervised learning to expand usable data sources 
0	137	9026	many present day facial recognition systems focus on making verification or identification facial feature invariant while these systems are highly effective for fully automated tasks manual facial recognition is still required in numerous real life scenarios that involve comparing against id photos our goal is to build a system that is capable of transforming faces to include or exclude glasses and beard 	source code for this project is available on github 
0	137	9027	there have been significant improvement in our capability to identify and verify human faces over the past few years device makers are already taking advantage of such development by equipping their latest phones and tablets with ai co processor and powerful image processing algorithms however the recent trend has mostly focused on making facial identification and verification invariant to facial features 	there have been significant improvement in our capability to identify and verify human faces over the past few years 
0	137	9028	there have been significant improvement in our capability to identify and verify human faces over the past few years device makers are already taking advantage of such development by equipping their latest phones and tablets with ai co processor and powerful image processing algorithms however the recent trend has mostly focused on making facial identification and verification invariant to facial features 	device makers are already taking advantage of such development by equipping their latest phones and tablets with ai co processor and powerful image processing algorithms 
0	137	9029	there have been significant improvement in our capability to identify and verify human faces over the past few years device makers are already taking advantage of such development by equipping their latest phones and tablets with ai co processor and powerful image processing algorithms however the recent trend has mostly focused on making facial identification and verification invariant to facial features 	however the recent trend has mostly focused on making facial identification and verification invariant to facial features 
1	137	9030	there have been significant improvement in our capability to identify and verify human faces over the past few years device makers are already taking advantage of such development by equipping their latest phones and tablets with ai co processor and powerful image processing algorithms however the recent trend has mostly focused on making facial identification and verification invariant to facial features 	these works certainly help machine recognize human faces however most humans are interested in seeing people in the natural state without any facial disguise a system that can recover undisguised faces could be helpful for criminal investigation 
1	137	9031	there have been significant improvement in our capability to identify and verify human faces over the past few years device makers are already taking advantage of such development by equipping their latest phones and tablets with ai co processor and powerful image processing algorithms however the recent trend has mostly focused on making facial identification and verification invariant to facial features 	in particular witnesses should be able to make use of these processed images to identify the criminal among a series of id photos which typically include no disguise or in person among a number of held suspects 
1	137	9032	there have been significant improvement in our capability to identify and verify human faces over the past few years device makers are already taking advantage of such development by equipping their latest phones and tablets with ai co processor and powerful image processing algorithms however the recent trend has mostly focused on making facial identification and verification invariant to facial features 	people utilizing online dating apps could also utilize this system to reveal the real person behind facial disguise a feature that many find useful we build on current work related to gan based style transform methods that are commonly employed for applying facial disguise 
1	137	9033	there have been significant improvement in our capability to identify and verify human faces over the past few years device makers are already taking advantage of such development by equipping their latest phones and tablets with ai co processor and powerful image processing algorithms however the recent trend has mostly focused on making facial identification and verification invariant to facial features 	recent works have demonstrated much success in related areas we train our generative neural network using a facial disguise database from hong kong polytechnic university
1	137	9034	the seminal paper on gans was first published in 2014 where g tries to minimize this function where an adversary tries to maximize it creating the min max optimization problem min g max dy l gan another related work that builds on top of the traditional gan is called the cyclegan	the seminal paper on gans was first published in 2014 where g tries to minimize this function where an adversary tries to maximize it creating the min max optimization problem min g max dy l gan another related work that builds on top of the traditional gan is called the cyclegan
0	137	9035	finding an appropriate dataset is one of the most important task for this work unfortunately due to privacy concerns and inherent difficulties in obtaining ground truth associated with human faces	finding an appropriate dataset is one of the most important task for this work 
0	137	9036	finding an appropriate dataset is one of the most important task for this work unfortunately due to privacy concerns and inherent difficulties in obtaining ground truth associated with human faces	unfortunately due to privacy concerns and inherent difficulties in obtaining ground truth associated with human faces
1	137	9037	we expect neural networks to produce better results when faces are intelligently selected from the images cropping out faces help the neural network select area of interest and reduces input size with a reduced input size the network can spend resources on applying feature transformations and on identifying features 	we expect neural networks to produce better results when faces are intelligently selected from the images 
0	137	9038	we expect neural networks to produce better results when faces are intelligently selected from the images cropping out faces help the neural network select area of interest and reduces input size with a reduced input size the network can spend resources on applying feature transformations and on identifying features 	cropping out faces help the neural network select area of interest and reduces input size 
1	137	9039	we expect neural networks to produce better results when faces are intelligently selected from the images cropping out faces help the neural network select area of interest and reduces input size with a reduced input size the network can spend resources on applying feature transformations and on identifying features 	with a reduced input size the network can spend resources on applying feature transformations and on identifying features 
1	137	9040	we expect neural networks to produce better results when faces are intelligently selected from the images cropping out faces help the neural network select area of interest and reduces input size with a reduced input size the network can spend resources on applying feature transformations and on identifying features 	for this project we used image sizes of 64 64 to decrease demand on gpu memory our dataset from hong kong polytechnic comes with cropped images 
0	137	9041	we expect neural networks to produce better results when faces are intelligently selected from the images cropping out faces help the neural network select area of interest and reduces input size with a reduced input size the network can spend resources on applying feature transformations and on identifying features 	celeba in contrast contains too much background for the dataset to be generic enough for a variety of tasks 
0	137	9042	we expect neural networks to produce better results when faces are intelligently selected from the images cropping out faces help the neural network select area of interest and reduces input size with a reduced input size the network can spend resources on applying feature transformations and on identifying features 	we made use of opencv s haar cascades
1	137	9043	neural network used for our purpose are much more sophisticated than typical generative adversarial networks that deals with mnist datasets multiple network structures have been attempted and their differences will be presented in the results section given the theoretical background of generative adversarial neural network as discussed in section ii a vanila gan can be roughly represented by the project milestone has demonstrated that vanila gans have limited capability in terms of both beard removal and facial feature reconstruction 	neural network used for our purpose are much more sophisticated than typical generative adversarial networks that deals with mnist datasets 
0	137	9044	neural network used for our purpose are much more sophisticated than typical generative adversarial networks that deals with mnist datasets multiple network structures have been attempted and their differences will be presented in the results section given the theoretical background of generative adversarial neural network as discussed in section ii a vanila gan can be roughly represented by the project milestone has demonstrated that vanila gans have limited capability in terms of both beard removal and facial feature reconstruction 	multiple network structures have been attempted and their differences will be presented in the results section 
1	137	9045	neural network used for our purpose are much more sophisticated than typical generative adversarial networks that deals with mnist datasets multiple network structures have been attempted and their differences will be presented in the results section given the theoretical background of generative adversarial neural network as discussed in section ii a vanila gan can be roughly represented by the project milestone has demonstrated that vanila gans have limited capability in terms of both beard removal and facial feature reconstruction 	given the theoretical background of generative adversarial neural network as discussed in section ii a vanila gan can be roughly represented by the project milestone has demonstrated that vanila gans have limited capability in terms of both beard removal and facial feature reconstruction 
1	137	9046	neural network used for our purpose are much more sophisticated than typical generative adversarial networks that deals with mnist datasets multiple network structures have been attempted and their differences will be presented in the results section given the theoretical background of generative adversarial neural network as discussed in section ii a vanila gan can be roughly represented by the project milestone has demonstrated that vanila gans have limited capability in terms of both beard removal and facial feature reconstruction 	this as described in section ii can be tackled by introducing coupling implemented as a cyclegan like structure shown in the forward generator g maps disguised faces to original faces whereas the backward generator f maps original faces back to disguised faces 
1	137	9047	neural network used for our purpose are much more sophisticated than typical generative adversarial networks that deals with mnist datasets multiple network structures have been attempted and their differences will be presented in the results section given the theoretical background of generative adversarial neural network as discussed in section ii a vanila gan can be roughly represented by the project milestone has demonstrated that vanila gans have limited capability in terms of both beard removal and facial feature reconstruction 	we apply adversarial loss functions to both gan s in addition to the adversarial loss functions we have an additional cycle consistency loss to preserve the individual identities through the generation process such that our full objective would be where is a hyperparameter that controls the relative importance of the two objectiv losses we tested cyclegan using relatively simple network structure 
1	137	9048	neural network used for our purpose are much more sophisticated than typical generative adversarial networks that deals with mnist datasets multiple network structures have been attempted and their differences will be presented in the results section given the theoretical background of generative adversarial neural network as discussed in section ii a vanila gan can be roughly represented by the project milestone has demonstrated that vanila gans have limited capability in terms of both beard removal and facial feature reconstruction 	however the cyclegan structure has two sets of generator discriminator pairing effectively doubling the size of the network 
1	137	9049	neural network used for our purpose are much more sophisticated than typical generative adversarial networks that deals with mnist datasets multiple network structures have been attempted and their differences will be presented in the results section given the theoretical background of generative adversarial neural network as discussed in section ii a vanila gan can be roughly represented by the project milestone has demonstrated that vanila gans have limited capability in terms of both beard removal and facial feature reconstruction 	structure of both pairs are the same as presented in taking the exponential makes it easier for us to compare the values 
0	137	9050	our experiments are conducted on google cloud vm instances with nvidia k80 gpus this setup significantly speeds up the training process compared to running on cpu only machines decreasing discriminator training time from over an hour to less than a minute and generator training from 10 to 15 minutes per batch on a simple cnn structure to a few seconds we built the training infrastructure using keras in addition we have developed a generic infrastructure that is capable of handling difference generators and discriminators in a plug and go fashion 	our experiments are conducted on google cloud vm instances with nvidia k80 gpus 
0	137	9051	our experiments are conducted on google cloud vm instances with nvidia k80 gpus this setup significantly speeds up the training process compared to running on cpu only machines decreasing discriminator training time from over an hour to less than a minute and generator training from 10 to 15 minutes per batch on a simple cnn structure to a few seconds we built the training infrastructure using keras in addition we have developed a generic infrastructure that is capable of handling difference generators and discriminators in a plug and go fashion 	this setup significantly speeds up the training process compared to running on cpu only machines decreasing discriminator training time from over an hour to less than a minute and generator training from 10 to 15 minutes per batch on a simple cnn structure to a few seconds we built the training infrastructure using keras 
0	137	9052	our experiments are conducted on google cloud vm instances with nvidia k80 gpus this setup significantly speeds up the training process compared to running on cpu only machines decreasing discriminator training time from over an hour to less than a minute and generator training from 10 to 15 minutes per batch on a simple cnn structure to a few seconds we built the training infrastructure using keras in addition we have developed a generic infrastructure that is capable of handling difference generators and discriminators in a plug and go fashion 	in addition we have developed a generic infrastructure that is capable of handling difference generators and discriminators in a plug and go fashion 
0	137	9053	our experiments are conducted on google cloud vm instances with nvidia k80 gpus this setup significantly speeds up the training process compared to running on cpu only machines decreasing discriminator training time from over an hour to less than a minute and generator training from 10 to 15 minutes per batch on a simple cnn structure to a few seconds we built the training infrastructure using keras in addition we have developed a generic infrastructure that is capable of handling difference generators and discriminators in a plug and go fashion 	this modular infrastructure has significantly lowered overhead associated with experimenting with a wide range of network structures 
0	137	9054	our experiments are conducted on google cloud vm instances with nvidia k80 gpus this setup significantly speeds up the training process compared to running on cpu only machines decreasing discriminator training time from over an hour to less than a minute and generator training from 10 to 15 minutes per batch on a simple cnn structure to a few seconds we built the training infrastructure using keras in addition we have developed a generic infrastructure that is capable of handling difference generators and discriminators in a plug and go fashion 	our custom code referenced vanilla gan implementation from
0	137	9055	we present generated images from different networks that we have experimented with 	we present generated images from different networks that we have experimented with 
0	137	9056	b simple convolutional neural network as shown in fig 5 generated images are a lot smoother than that from multilayer perceptron there is also traces of beard mustache region being modified by the generator network 	simple convolutional neural network as shown in fig 5 generated images are a lot smoother than that from multilayer perceptron 
0	137	9057	b simple convolutional neural network as shown in fig 5 generated images are a lot smoother than that from multilayer perceptron there is also traces of beard mustache region being modified by the generator network 	there is also traces of beard mustache region being modified by the generator network 
0	137	9058	b simple convolutional neural network as shown in fig 5 generated images are a lot smoother than that from multilayer perceptron there is also traces of beard mustache region being modified by the generator network 	also the generator seems to be brightening columns near nose where mustache typically appears 
1	137	9059	c residual convolutional neural network residual networks are supposed to be better in retaining characteristics of the original image since this network also contains more convolutional layers the result shown in fig 6 has slightly higher quality than images generated using simple cnns these images have far less bright dark bars 	c residual convolutional neural network residual networks are supposed to be better in retaining characteristics of the original image 
1	137	9060	c residual convolutional neural network residual networks are supposed to be better in retaining characteristics of the original image since this network also contains more convolutional layers the result shown in fig 6 has slightly higher quality than images generated using simple cnns these images have far less bright dark bars 	since this network also contains more convolutional layers the result shown in fig 6 has slightly higher quality than images generated using simple cnns 
0	137	9061	c residual convolutional neural network residual networks are supposed to be better in retaining characteristics of the original image since this network also contains more convolutional layers the result shown in fig 6 has slightly higher quality than images generated using simple cnns these images have far less bright dark bars 	these images have far less bright dark bars 
0	137	9062	d cyclegan a relatively simple cyclegan structure is implemented for this work this is because cycle gan consumes more than twice the memory compared to its vanilla counterparts expanding our network to support colored images also significantly limits complexity of the network 	d cyclegan a relatively simple cyclegan structure is implemented for this work 
0	137	9063	d cyclegan a relatively simple cyclegan structure is implemented for this work this is because cycle gan consumes more than twice the memory compared to its vanilla counterparts expanding our network to support colored images also significantly limits complexity of the network 	this is because cycle gan consumes more than twice the memory compared to its vanilla counterparts 
0	137	9064	d cyclegan a relatively simple cyclegan structure is implemented for this work this is because cycle gan consumes more than twice the memory compared to its vanilla counterparts expanding our network to support colored images also significantly limits complexity of the network 	expanding our network to support colored images also significantly limits complexity of the network 
0	137	9065	d cyclegan a relatively simple cyclegan structure is implemented for this work this is because cycle gan consumes more than twice the memory compared to its vanilla counterparts expanding our network to support colored images also significantly limits complexity of the network 	nevertheless cyclegan produces high quality images as shown
1	137	9066	clearly with the introduction of reconstruction and identity loss the generated images are of much higher quality not only that irrelevant features are modified our reconstructed images look almost identical to the original verifying that the reconstruction losses are highly effective plot of losses for cyclegan running the beard and glasses modification task is presented	clearly with the introduction of reconstruction and identity loss the generated images are of much higher quality 
1	137	9067	clearly with the introduction of reconstruction and identity loss the generated images are of much higher quality not only that irrelevant features are modified our reconstructed images look almost identical to the original verifying that the reconstruction losses are highly effective plot of losses for cyclegan running the beard and glasses modification task is presented	not only that irrelevant features are modified our reconstructed images look almost identical to the original verifying that the reconstruction losses are highly effective plot of losses for cyclegan running the beard and glasses modification task is presented
0	137	9068	all images presented here are faces of male this is because training the network with female faces introduces makeup to modified faces for example removing beard adds lipstick regardless of gender 	all images presented here are faces of male 
0	137	9069	all images presented here are faces of male this is because training the network with female faces introduces makeup to modified faces for example removing beard adds lipstick regardless of gender 	this is because training the network with female faces introduces makeup to modified faces 
0	137	9070	all images presented here are faces of male this is because training the network with female faces introduces makeup to modified faces for example removing beard adds lipstick regardless of gender 	for example removing beard adds lipstick regardless of gender 
0	137	9071	all images presented here are faces of male this is because training the network with female faces introduces makeup to modified faces for example removing beard adds lipstick regardless of gender 	similarly removing sunglasses frequently adds eyeshadow or eyeline 
0	137	9072	all images presented here are faces of male this is because training the network with female faces introduces makeup to modified faces for example removing beard adds lipstick regardless of gender 	another interesting phenomenon we notices is that old celebrities tend to get clear glasses whereas younger celebrities tend to get sunglasses 
0	137	9073	all images presented here are faces of male this is because training the network with female faces introduces makeup to modified faces for example removing beard adds lipstick regardless of gender 	though the network handles most images reasonably well we have noticed that it is still struggling with removing opaque sunglasses 
0	137	9074	all images presented here are faces of male this is because training the network with female faces introduces makeup to modified faces for example removing beard adds lipstick regardless of gender 	this difficulty is expected because image with opaque sunglasses provides little information about wearers eyes 
0	137	9075	all images presented here are faces of male this is because training the network with female faces introduces makeup to modified faces for example removing beard adds lipstick regardless of gender 	the algorithm has nothing to construct the eyes from 
0	137	9076	all images presented here are faces of male this is because training the network with female faces introduces makeup to modified faces for example removing beard adds lipstick regardless of gender 	instead it puts a generic eye in place of sunglasses which often look out of place 
0	137	9077	all images presented here are faces of male this is because training the network with female faces introduces makeup to modified faces for example removing beard adds lipstick regardless of gender 	this effect is observed among images in which glasses hide significant portion of eye brows 
0	137	9078	all images presented here are faces of male this is because training the network with female faces introduces makeup to modified faces for example removing beard adds lipstick regardless of gender 	reconstructed eye brows in those cases are of dubious quality since this project is generative in nature there is no accuracy to evaluate 
0	137	9079	all images presented here are faces of male this is because training the network with female faces introduces makeup to modified faces for example removing beard adds lipstick regardless of gender 	inception score is perhaps the more appropriate numerical metric to include for the experiment 
0	137	9080	all images presented here are faces of male this is because training the network with female faces introduces makeup to modified faces for example removing beard adds lipstick regardless of gender 	inception score of all tested networks are presented in table vii 
0	137	9081	all images presented here are faces of male this is because training the network with female faces introduces makeup to modified faces for example removing beard adds lipstick regardless of gender 	since inception score for cifar 10 images
0	137	9082	this project successfully identified a neural network structure to perform the task of modifying facial features although results of this work focuses exclusively on beard and glasses the same infrastructure can certainly be used for other features in the future we would like to build a generic infrastructure that is capable of handling any facial feature it would also be helpful to make the training process semisupervised 	this project successfully identified a neural network structure to perform the task of modifying facial features 
1	137	9083	this project successfully identified a neural network structure to perform the task of modifying facial features although results of this work focuses exclusively on beard and glasses the same infrastructure can certainly be used for other features in the future we would like to build a generic infrastructure that is capable of handling any facial feature it would also be helpful to make the training process semisupervised 	although results of this work focuses exclusively on beard and glasses the same infrastructure can certainly be used for other features in the future we would like to build a generic infrastructure that is capable of handling any facial feature 
0	137	9084	this project successfully identified a neural network structure to perform the task of modifying facial features although results of this work focuses exclusively on beard and glasses the same infrastructure can certainly be used for other features in the future we would like to build a generic infrastructure that is capable of handling any facial feature it would also be helpful to make the training process semisupervised 	it would also be helpful to make the training process semisupervised 
0	137	9085	this project successfully identified a neural network structure to perform the task of modifying facial features although results of this work focuses exclusively on beard and glasses the same infrastructure can certainly be used for other features in the future we would like to build a generic infrastructure that is capable of handling any facial feature it would also be helpful to make the training process semisupervised 	this will allow us to include other datasets that do not have relevant tags 
0	137	9086	our team has divided work evenly based on each team member s technical background and course load to be more specific jingbo worked on pre processing and testing neural network models boning worked on building various neural network models and meixian focused on plotting and writing reports poster 	our team has divided work evenly based on each team member s technical background and course load 
0	137	9087	our team has divided work evenly based on each team member s technical background and course load to be more specific jingbo worked on pre processing and testing neural network models boning worked on building various neural network models and meixian focused on plotting and writing reports poster 	to be more specific jingbo worked on pre processing and testing neural network models boning worked on building various neural network models and meixian focused on plotting and writing reports poster 
1	138	9088	traditional off the shelf lossy image compression techniques such as jpeg and webp are not designed specifically for the data being compressed and therefore do not achieve the best possible compression rates for images in this paper we construct a deep neural network based compression architecture using a generative model pretrained with the celeba faces dataset which consists of semantically related images our architecture compresses related images by reversing the generator of a gan and omits the encoder altogether 	traditional off the shelf lossy image compression techniques such as jpeg and webp are not designed specifically for the data being compressed and therefore do not achieve the best possible compression rates for images 
1	138	9089	traditional off the shelf lossy image compression techniques such as jpeg and webp are not designed specifically for the data being compressed and therefore do not achieve the best possible compression rates for images in this paper we construct a deep neural network based compression architecture using a generative model pretrained with the celeba faces dataset which consists of semantically related images our architecture compresses related images by reversing the generator of a gan and omits the encoder altogether 	in this paper we construct a deep neural network based compression architecture using a generative model pretrained with the celeba faces dataset which consists of semantically related images 
1	138	9090	traditional off the shelf lossy image compression techniques such as jpeg and webp are not designed specifically for the data being compressed and therefore do not achieve the best possible compression rates for images in this paper we construct a deep neural network based compression architecture using a generative model pretrained with the celeba faces dataset which consists of semantically related images our architecture compresses related images by reversing the generator of a gan and omits the encoder altogether 	our architecture compresses related images by reversing the generator of a gan and omits the encoder altogether 
1	138	9091	traditional off the shelf lossy image compression techniques such as jpeg and webp are not designed specifically for the data being compressed and therefore do not achieve the best possible compression rates for images in this paper we construct a deep neural network based compression architecture using a generative model pretrained with the celeba faces dataset which consists of semantically related images our architecture compresses related images by reversing the generator of a gan and omits the encoder altogether 	we report orders of magnitude improvements in the compression rate compared to standard methods such as the high quality jpeg and are able to achieve comparable compression magnitudes to the 1 quality jpeg while maintaining a much higher fidelity to the original image and being able to create much more perceptual reconstructions 
0	138	9092	traditional off the shelf lossy image compression techniques such as jpeg and webp are not designed specifically for the data being compressed and therefore do not achieve the best possible compression rates for images in this paper we construct a deep neural network based compression architecture using a generative model pretrained with the celeba faces dataset which consists of semantically related images our architecture compresses related images by reversing the generator of a gan and omits the encoder altogether 	finally we evaluate our reconstructions with mse psnr and ssim measures compare them with jpeg of different qualities and k means compression and report compression magnitudes in bits per pixel bbp and the compression ratio cr 
0	138	9093	standard lossy image compression techniques such as jpeg and webp are not data specific i e they are not designed specifically to handle individual datasets in which the images are semantically related to each other hence these techniques do not make use of the semantic relations among the images in a specific dataset and do not achieve the best possible compression rates 	standard lossy image compression techniques such as jpeg and webp are not data specific i e 
1	138	9094	standard lossy image compression techniques such as jpeg and webp are not data specific i e they are not designed specifically to handle individual datasets in which the images are semantically related to each other hence these techniques do not make use of the semantic relations among the images in a specific dataset and do not achieve the best possible compression rates 	they are not designed specifically to handle individual datasets in which the images are semantically related to each other 
1	138	9095	standard lossy image compression techniques such as jpeg and webp are not data specific i e they are not designed specifically to handle individual datasets in which the images are semantically related to each other hence these techniques do not make use of the semantic relations among the images in a specific dataset and do not achieve the best possible compression rates 	hence these techniques do not make use of the semantic relations among the images in a specific dataset and do not achieve the best possible compression rates 
0	138	9096	standard lossy image compression techniques such as jpeg and webp are not data specific i e they are not designed specifically to handle individual datasets in which the images are semantically related to each other hence these techniques do not make use of the semantic relations among the images in a specific dataset and do not achieve the best possible compression rates 	this has led to a growth in the research towards deep neural network based compression architectures 
0	138	9097	standard lossy image compression techniques such as jpeg and webp are not data specific i e they are not designed specifically to handle individual datasets in which the images are semantically related to each other hence these techniques do not make use of the semantic relations among the images in a specific dataset and do not achieve the best possible compression rates 	these models tend to achieve orders of magnitude better compression rates while still maintaining higher accuracy and fidelity in their reconstructions 
0	138	9098	standard lossy image compression techniques such as jpeg and webp are not data specific i e they are not designed specifically to handle individual datasets in which the images are semantically related to each other hence these techniques do not make use of the semantic relations among the images in a specific dataset and do not achieve the best possible compression rates 	another challenge in constructing a generative deep neural compressor rises from the fact that gans lack the encoder function 
1	138	9099	standard lossy image compression techniques such as jpeg and webp are not data specific i e they are not designed specifically to handle individual datasets in which the images are semantically related to each other hence these techniques do not make use of the semantic relations among the images in a specific dataset and do not achieve the best possible compression rates 	the generator network of a gan can map from the smaller dimensional latent space to the larger dimensional image space but not the other way around 
0	138	9100	standard lossy image compression techniques such as jpeg and webp are not data specific i e they are not designed specifically to handle individual datasets in which the images are semantically related to each other hence these techniques do not make use of the semantic relations among the images in a specific dataset and do not achieve the best possible compression rates 	in compression language this means that a gan can give us a decoder but not an encoder 
0	138	9101	standard lossy image compression techniques such as jpeg and webp are not data specific i e they are not designed specifically to handle individual datasets in which the images are semantically related to each other hence these techniques do not make use of the semantic relations among the images in a specific dataset and do not achieve the best possible compression rates 	addressing this issue is the work of
0	138	9102	our main contribution in this paper is the introduction of this novel method of latent space vector recovery into the compression literature accordingly we construct a compressor using solely a pretrained gan generator omitting the encoder altogether we refer to this method as gan reversal throughout the paper 	our main contribution in this paper is the introduction of this novel method of latent space vector recovery into the compression literature 
0	138	9103	our main contribution in this paper is the introduction of this novel method of latent space vector recovery into the compression literature accordingly we construct a compressor using solely a pretrained gan generator omitting the encoder altogether we refer to this method as gan reversal throughout the paper 	accordingly we construct a compressor using solely a pretrained gan generator omitting the encoder altogether 
0	138	9104	our main contribution in this paper is the introduction of this novel method of latent space vector recovery into the compression literature accordingly we construct a compressor using solely a pretrained gan generator omitting the encoder altogether we refer to this method as gan reversal throughout the paper 	we refer to this method as gan reversal throughout the paper 
1	138	9105	our main contribution in this paper is the introduction of this novel method of latent space vector recovery into the compression literature accordingly we construct a compressor using solely a pretrained gan generator omitting the encoder altogether we refer to this method as gan reversal throughout the paper 	compression is done via training a vector in the latent space which is further compressed with bzip2 a standard lossless compression scheme 
1	138	9106	our main contribution in this paper is the introduction of this novel method of latent space vector recovery into the compression literature accordingly we construct a compressor using solely a pretrained gan generator omitting the encoder altogether we refer to this method as gan reversal throughout the paper 	decompression of images is simply done with a forward propagation of the latent vector through the gan generator 
0	138	9107	our main contribution in this paper is the introduction of this novel method of latent space vector recovery into the compression literature accordingly we construct a compressor using solely a pretrained gan generator omitting the encoder altogether we refer to this method as gan reversal throughout the paper 	to the best of our knowledge we are not familiar with any other literature that uses this gan reversal scheme for image compression furthermore the mentioned paper of
0	138	9108	a gan consists of two networks called the generator and the discriminator during training the network tries to minimize an adversarial loss function to achieve this the generator tries to create images that cannot be differentiated from true images while the discriminator tries to correctly classify images as real or generated 	a gan consists of two networks called the generator and the discriminator 
0	138	9109	a gan consists of two networks called the generator and the discriminator during training the network tries to minimize an adversarial loss function to achieve this the generator tries to create images that cannot be differentiated from true images while the discriminator tries to correctly classify images as real or generated 	during training the network tries to minimize an adversarial loss function 
1	138	9110	a gan consists of two networks called the generator and the discriminator during training the network tries to minimize an adversarial loss function to achieve this the generator tries to create images that cannot be differentiated from true images while the discriminator tries to correctly classify images as real or generated 	to achieve this the generator tries to create images that cannot be differentiated from true images while the discriminator tries to correctly classify images as real or generated 
0	138	9111	a gan consists of two networks called the generator and the discriminator during training the network tries to minimize an adversarial loss function to achieve this the generator tries to create images that cannot be differentiated from true images while the discriminator tries to correctly classify images as real or generated 	the discriminator is constructed just for the training purposes and is discarded after training 
1	138	9112	a gan consists of two networks called the generator and the discriminator during training the network tries to minimize an adversarial loss function to achieve this the generator tries to create images that cannot be differentiated from true images while the discriminator tries to correctly classify images as real or generated 	as stated in the introduction the remaining generator network maps from a low dimensional latent space to a higher dimensional image space 
0	138	9113	a gan consists of two networks called the generator and the discriminator during training the network tries to minimize an adversarial loss function to achieve this the generator tries to create images that cannot be differentiated from true images while the discriminator tries to correctly classify images as real or generated 	for the formulas below we will refer to the latent space as the z space and refer to the image space as the x space 
0	138	9114	a gan consists of two networks called the generator and the discriminator during training the network tries to minimize an adversarial loss function to achieve this the generator tries to create images that cannot be differentiated from true images while the discriminator tries to correctly classify images as real or generated 	it is not an inherent feature of a gan network to perform a mapping from x z 
0	138	9115	a gan consists of two networks called the generator and the discriminator during training the network tries to minimize an adversarial loss function to achieve this the generator tries to create images that cannot be differentiated from true images while the discriminator tries to correctly classify images as real or generated 	however this mapping is exactly the necessary encoding step of compression 
1	138	9116	a gan consists of two networks called the generator and the discriminator during training the network tries to minimize an adversarial loss function to achieve this the generator tries to create images that cannot be differentiated from true images while the discriminator tries to correctly classify images as real or generated 	in this compression architecture we encode images to the latent space based on the work of the specific model construction is as follows first we either train a gan or acquire the pretrained generator of a gan that is capable of generating images of a specific domain such as human faces 
1	138	9117	a gan consists of two networks called the generator and the discriminator during training the network tries to minimize an adversarial loss function to achieve this the generator tries to create images that cannot be differentiated from true images while the discriminator tries to correctly classify images as real or generated 	the weights of this generator network which exactly corresponds to the decoder of our compressor are kept frozen 
1	138	9118	a gan consists of two networks called the generator and the discriminator during training the network tries to minimize an adversarial loss function to achieve this the generator tries to create images that cannot be differentiated from true images while the discriminator tries to correctly classify images as real or generated 	the gans that we use for image generation are from a specific category called dcgans introduced by because these mentioned loss functions are pixel wise distance metrics they have limitations in terms of outputting perceptual images and recovering the edges in the original images 
0	138	9119	a gan consists of two networks called the generator and the discriminator during training the network tries to minimize an adversarial loss function to achieve this the generator tries to create images that cannot be differentiated from true images while the discriminator tries to correctly classify images as real or generated 	this motivates us to use perceptual similarity metrics for our training 
0	138	9120	a gan consists of two networks called the generator and the discriminator during training the network tries to minimize an adversarial loss function to achieve this the generator tries to create images that cannot be differentiated from true images while the discriminator tries to correctly classify images as real or generated 	one of such metrics is the well known structural similarity index ssim 
1	138	9121	a gan consists of two networks called the generator and the discriminator during training the network tries to minimize an adversarial loss function to achieve this the generator tries to create images that cannot be differentiated from true images while the discriminator tries to correctly classify images as real or generated 	for two aligned windows x and y from different images this metric is defined as note that this function takes the neighboring pixels into account and it is a more perceptual metric than pixel wise metrics 
0	138	9122	a gan consists of two networks called the generator and the discriminator during training the network tries to minimize an adversarial loss function to achieve this the generator tries to create images that cannot be differentiated from true images while the discriminator tries to correctly classify images as real or generated 	another remark is that ssim value increases up to 1 as the images become more and more similar so the corresponding loss function to be minimized is we minimize these loss functions with respect to the latent vector z via stochastic gradient descent 
1	138	9123	a gan consists of two networks called the generator and the discriminator during training the network tries to minimize an adversarial loss function to achieve this the generator tries to create images that cannot be differentiated from true images while the discriminator tries to correctly classify images as real or generated 	note that we actually know that the unknown latent vector was sampled from u 1 1 
0	138	9124	a gan consists of two networks called the generator and the discriminator during training the network tries to minimize an adversarial loss function to achieve this the generator tries to create images that cannot be differentiated from true images while the discriminator tries to correctly classify images as real or generated 	thus after each iteration we can clip the vector to stay in this range 
1	138	9125	a gan consists of two networks called the generator and the discriminator during training the network tries to minimize an adversarial loss function to achieve this the generator tries to create images that cannot be differentiated from true images while the discriminator tries to correctly classify images as real or generated 	another important remark is that this gan reversal training is non convex and we cannot guarantee to recover the same latent vector after each training 
1	138	9126	a gan consists of two networks called the generator and the discriminator during training the network tries to minimize an adversarial loss function to achieve this the generator tries to create images that cannot be differentiated from true images while the discriminator tries to correctly classify images as real or generated 	multiple latent vectors can indeed map to the same image but for our compression purposes it does not matter which latent vector we recover as long as its corresponding image is close to the original 
0	138	9127	in our initial experiments we use a gan architecture implemented according to the work of for the test set and for an unbiased evaluation of the models and baselines used we used a test set of 10 images from the celeba dataset face centered and resized to be of size 128 128 pixels the images in the test set are not outputs of the gan generator unlike in	in our initial experiments we use a gan architecture implemented according to the work of for the test set and for an unbiased evaluation of the models and baselines used we used a test set of 10 images from the celeba dataset face centered and resized to be of size 128 128 pixels 
1	138	9128	in our initial experiments we use a gan architecture implemented according to the work of for the test set and for an unbiased evaluation of the models and baselines used we used a test set of 10 images from the celeba dataset face centered and resized to be of size 128 128 pixels the images in the test set are not outputs of the gan generator unlike in	the images in the test set are not outputs of the gan generator unlike in
1	138	9129	in this paper we are aiming to design a better extreme compression scheme with two main objectives the scheme must achieve higher compression rates than other standard lossy image compression techniques and the reconstructed images must still be of high perceptual quality and true to their originals to assess how well our scheme is doing based on these objectives we have to use suitable metrics for image quality the metrics that are relevant to our work and that are most used in the literature include the mean squared error mse peak signal to noise ratio psnr and structural similarity index ssim while mse seems like the easiest metric to use it is actually not as indicative of perceptual or textural quality as the other metrics to measure how much compression our scheme is achieving we use the bits per pixel bpp metric which conveys the magnitude of the compression scheme by indicating the average number of bits needed to represent one pixel 	in this paper we are aiming to design a better extreme compression scheme with two main objectives the scheme must achieve higher compression rates than other standard lossy image compression techniques and the reconstructed images must still be of high perceptual quality and true to their originals 
0	138	9130	in this paper we are aiming to design a better extreme compression scheme with two main objectives the scheme must achieve higher compression rates than other standard lossy image compression techniques and the reconstructed images must still be of high perceptual quality and true to their originals to assess how well our scheme is doing based on these objectives we have to use suitable metrics for image quality the metrics that are relevant to our work and that are most used in the literature include the mean squared error mse peak signal to noise ratio psnr and structural similarity index ssim while mse seems like the easiest metric to use it is actually not as indicative of perceptual or textural quality as the other metrics to measure how much compression our scheme is achieving we use the bits per pixel bpp metric which conveys the magnitude of the compression scheme by indicating the average number of bits needed to represent one pixel 	to assess how well our scheme is doing based on these objectives we have to use suitable metrics for image quality the metrics that are relevant to our work and that are most used in the literature include the mean squared error mse peak signal to noise ratio psnr and structural similarity index ssim 
1	138	9131	in this paper we are aiming to design a better extreme compression scheme with two main objectives the scheme must achieve higher compression rates than other standard lossy image compression techniques and the reconstructed images must still be of high perceptual quality and true to their originals to assess how well our scheme is doing based on these objectives we have to use suitable metrics for image quality the metrics that are relevant to our work and that are most used in the literature include the mean squared error mse peak signal to noise ratio psnr and structural similarity index ssim while mse seems like the easiest metric to use it is actually not as indicative of perceptual or textural quality as the other metrics to measure how much compression our scheme is achieving we use the bits per pixel bpp metric which conveys the magnitude of the compression scheme by indicating the average number of bits needed to represent one pixel 	while mse seems like the easiest metric to use it is actually not as indicative of perceptual or textural quality as the other metrics to measure how much compression our scheme is achieving we use the bits per pixel bpp metric which conveys the magnitude of the compression scheme by indicating the average number of bits needed to represent one pixel 
1	138	9132	in this paper we are aiming to design a better extreme compression scheme with two main objectives the scheme must achieve higher compression rates than other standard lossy image compression techniques and the reconstructed images must still be of high perceptual quality and true to their originals to assess how well our scheme is doing based on these objectives we have to use suitable metrics for image quality the metrics that are relevant to our work and that are most used in the literature include the mean squared error mse peak signal to noise ratio psnr and structural similarity index ssim while mse seems like the easiest metric to use it is actually not as indicative of perceptual or textural quality as the other metrics to measure how much compression our scheme is achieving we use the bits per pixel bpp metric which conveys the magnitude of the compression scheme by indicating the average number of bits needed to represent one pixel 	to put the numbers in context a non compressed image that represents each color channel of the pixel using one byte has a bpp of 24 
1	138	9133	in this paper we are aiming to design a better extreme compression scheme with two main objectives the scheme must achieve higher compression rates than other standard lossy image compression techniques and the reconstructed images must still be of high perceptual quality and true to their originals to assess how well our scheme is doing based on these objectives we have to use suitable metrics for image quality the metrics that are relevant to our work and that are most used in the literature include the mean squared error mse peak signal to noise ratio psnr and structural similarity index ssim while mse seems like the easiest metric to use it is actually not as indicative of perceptual or textural quality as the other metrics to measure how much compression our scheme is achieving we use the bits per pixel bpp metric which conveys the magnitude of the compression scheme by indicating the average number of bits needed to represent one pixel 	however for fairness before reporting the bpp we losslessly compress the images from any scheme similar to what we do with the latent vectors in our gan reversal approach 
1	138	9134	in this paper we are aiming to design a better extreme compression scheme with two main objectives the scheme must achieve higher compression rates than other standard lossy image compression techniques and the reconstructed images must still be of high perceptual quality and true to their originals to assess how well our scheme is doing based on these objectives we have to use suitable metrics for image quality the metrics that are relevant to our work and that are most used in the literature include the mean squared error mse peak signal to noise ratio psnr and structural similarity index ssim while mse seems like the easiest metric to use it is actually not as indicative of perceptual or textural quality as the other metrics to measure how much compression our scheme is achieving we use the bits per pixel bpp metric which conveys the magnitude of the compression scheme by indicating the average number of bits needed to represent one pixel 	therefore the bpp of the uncompressed png the other baseline we investigated was jpeg optimized which is a popular lossy image compression scheme 
0	138	9135	in this paper we are aiming to design a better extreme compression scheme with two main objectives the scheme must achieve higher compression rates than other standard lossy image compression techniques and the reconstructed images must still be of high perceptual quality and true to their originals to assess how well our scheme is doing based on these objectives we have to use suitable metrics for image quality the metrics that are relevant to our work and that are most used in the literature include the mean squared error mse peak signal to noise ratio psnr and structural similarity index ssim while mse seems like the easiest metric to use it is actually not as indicative of perceptual or textural quality as the other metrics to measure how much compression our scheme is achieving we use the bits per pixel bpp metric which conveys the magnitude of the compression scheme by indicating the average number of bits needed to represent one pixel 	we used two values for the quality parameter of the jpeg schem 10 and 1 
1	138	9136	in this paper we are aiming to design a better extreme compression scheme with two main objectives the scheme must achieve higher compression rates than other standard lossy image compression techniques and the reconstructed images must still be of high perceptual quality and true to their originals to assess how well our scheme is doing based on these objectives we have to use suitable metrics for image quality the metrics that are relevant to our work and that are most used in the literature include the mean squared error mse peak signal to noise ratio psnr and structural similarity index ssim while mse seems like the easiest metric to use it is actually not as indicative of perceptual or textural quality as the other metrics to measure how much compression our scheme is achieving we use the bits per pixel bpp metric which conveys the magnitude of the compression scheme by indicating the average number of bits needed to represent one pixel 	note that for the purposes of this paper where we are aiming for extreme compression our main objective is beating the jpeg 1 baseline in the ssim metric which corresponds to better perceptual quality while still having a comparable compression ratio cr 
0	138	9137	the main future goal should be to improve the loss function used for latent vector training even further this will enable us to build compressors that achieve more perceptual reconstructions with higher fidelity the additional challenge will be to build an automated process for the compressor 	the main future goal should be to improve the loss function used for latent vector training even further 
0	138	9138	the main future goal should be to improve the loss function used for latent vector training even further this will enable us to build compressors that achieve more perceptual reconstructions with higher fidelity the additional challenge will be to build an automated process for the compressor 	this will enable us to build compressors that achieve more perceptual reconstructions with higher fidelity 
0	138	9139	the main future goal should be to improve the loss function used for latent vector training even further this will enable us to build compressors that achieve more perceptual reconstructions with higher fidelity the additional challenge will be to build an automated process for the compressor 	the additional challenge will be to build an automated process for the compressor 
1	138	9140	the main future goal should be to improve the loss function used for latent vector training even further this will enable us to build compressors that achieve more perceptual reconstructions with higher fidelity the additional challenge will be to build an automated process for the compressor 	since the compression procedure utilizes a gradient descent based training scheme there are significant parts of compression that rely on human observation such as the hyperparameter tuning running other training sessions with differently initialized random vectors for improvement and picking the best perceptual output among all reconstructions 
0	138	9141	the main future goal should be to improve the loss function used for latent vector training even further this will enable us to build compressors that achieve more perceptual reconstructions with higher fidelity the additional challenge will be to build an automated process for the compressor 	for a practical compressor all these processes must be automated 
0	138	9142	we would like to thank kedar tatwawadi and shubham chandak for their very helpful discussions and comments 	we would like to thank kedar tatwawadi and shubham chandak for their very helpful discussions and comments 
0	139	9143	behavioral cloning the model car is equipped with a mono frontal wide angle camera capturing 120x160 rgb images which are used for the training and testing inputs for the autopilot in addition the model car s steering angle and motor throttling values are used for the classification labels so the autopilot can estimate and output the best steering angle and throttling output given an input image in the testing phase 	behavioral cloning the model car is equipped with a mono frontal wide angle camera capturing 120x160 rgb images which are used for the training and testing inputs for the autopilot 
1	139	9144	behavioral cloning the model car is equipped with a mono frontal wide angle camera capturing 120x160 rgb images which are used for the training and testing inputs for the autopilot in addition the model car s steering angle and motor throttling values are used for the classification labels so the autopilot can estimate and output the best steering angle and throttling output given an input image in the testing phase 	in addition the model car s steering angle and motor throttling values are used for the classification labels so the autopilot can estimate and output the best steering angle and throttling output given an input image in the testing phase 
0	139	9145	bojarski et all 1 have shown that it is possible to use a cnn based supervised model to drive a car the work has used three frontal cameras using the middle camera as the main source for the agent s inputs and using the side cameras to compensate the car s shift and rotation movements 	bojarski et all 
0	139	9146	bojarski et all 1 have shown that it is possible to use a cnn based supervised model to drive a car the work has used three frontal cameras using the middle camera as the main source for the agent s inputs and using the side cameras to compensate the car s shift and rotation movements 	 1 have shown that it is possible to use a cnn based supervised model to drive a car 
0	139	9147	bojarski et all 1 have shown that it is possible to use a cnn based supervised model to drive a car the work has used three frontal cameras using the middle camera as the main source for the agent s inputs and using the side cameras to compensate the car s shift and rotation movements 	the work has used three frontal cameras using the middle camera as the main source for the agent s inputs and using the side cameras to compensate the car s shift and rotation movements 
0	139	9148	bojarski et all 1 have shown that it is possible to use a cnn based supervised model to drive a car the work has used three frontal cameras using the middle camera as the main source for the agent s inputs and using the side cameras to compensate the car s shift and rotation movements 	it basically has relied only on the frontal captured images to classify the right steering angle to keep the car on the track 
1	139	9149	bojarski et all 1 have shown that it is possible to use a cnn based supervised model to drive a car the work has used three frontal cameras using the middle camera as the main source for the agent s inputs and using the side cameras to compensate the car s shift and rotation movements 	this modeling is quite simple to come up with a decent performance if it is trained with sufficient amount of data 
1	139	9150	bojarski et all 1 have shown that it is possible to use a cnn based supervised model to drive a car the work has used three frontal cameras using the middle camera as the main source for the agent s inputs and using the side cameras to compensate the car s shift and rotation movements 	however the biggest problem is that once it encounters a state which it has not seen before it is very easy for the agent to drift away significantly figure 1 behavioral cloning trajectory drifting image from where l s is the 0 1 loss of with respect to in state s as to optimize the bound of this approach the same work has suggested data aggregation which can achieve the cost j to be bounded linear to the trajectory t as shown from the following theorem from the same work letting n to designates the number of iteration to perform the data aggregation section 5 3 if n is o ut then there exists a policy 1 n such that
0	139	9151	our model car has a mono wide angle camera a servo controlled steering and a thrust motor	our model car has a mono wide angle camera a servo controlled steering and a thrust motor
0	139	9152	the primary inputs for the training and testing are the 120x160 rgb images captured from the frontal camera classifying the best matching steering angles and the throttling values the agent has performed about total 200 wraps of running on the indoor track in multiple sessions each wrap equaling to capturing about 520 images their corresponding steering angles and throttling values the training and validation is performed with split ratio of 0 8 0 1 0 1 between the training developing and validation sets 	the primary inputs for the training and testing are the 120x160 rgb images captured from the frontal camera classifying the best matching steering angles and the throttling values 
0	139	9153	the primary inputs for the training and testing are the 120x160 rgb images captured from the frontal camera classifying the best matching steering angles and the throttling values the agent has performed about total 200 wraps of running on the indoor track in multiple sessions each wrap equaling to capturing about 520 images their corresponding steering angles and throttling values the training and validation is performed with split ratio of 0 8 0 1 0 1 between the training developing and validation sets 	the agent has performed about total 200 wraps of running on the indoor track in multiple sessions each wrap equaling to capturing about 520 images their corresponding steering angles and throttling values 
0	139	9154	the primary inputs for the training and testing are the 120x160 rgb images captured from the frontal camera classifying the best matching steering angles and the throttling values the agent has performed about total 200 wraps of running on the indoor track in multiple sessions each wrap equaling to capturing about 520 images their corresponding steering angles and throttling values the training and validation is performed with split ratio of 0 8 0 1 0 1 between the training developing and validation sets 	the training and validation is performed with split ratio of 0 8 0 1 0 1 between the training developing and validation sets 
1	139	9155	the the image on the right in the same figure shows an input image super imposed with a masking to show the image segments activating the cnn most where p p s designating the distribution of states visited by the expert 	the the image on the right in the same figure shows an input image super imposed with a masking to show the image segments activating the cnn most where p p s designating the distribution of states visited by the expert 
1	139	9156	in order to improve the baseline autopilot policy i have employed data aggregation also specific to this project s in the iteration i is achieved by the expert manually modifying the actions by its best estimation for a specific example 	in order to improve the baseline autopilot policy i have employed data aggregation also specific to this project s in the iteration i is achieved by the expert manually modifying the actions by its best estimation 
0	139	9157	in order to improve the baseline autopilot policy i have employed data aggregation also specific to this project s in the iteration i is achieved by the expert manually modifying the actions by its best estimation for a specific example 	for a specific example 
1	139	9158	the table in the first row of the table shows the failure rates of each individual policy i without merging all the datasets from each iterations that is not performing d d d i for the next training dataset the first column shows the failure rate of the human driver 1 is the first policy trained with the training data collected from the human driver 	the table in the first row of the table shows the failure rates of each individual policy i without merging all the datasets from each iterations that is not performing d d d i for the next training dataset 
0	139	9159	the table in the first row of the table shows the failure rates of each individual policy i without merging all the datasets from each iterations that is not performing d d d i for the next training dataset the first column shows the failure rate of the human driver 1 is the first policy trained with the training data collected from the human driver 	the first column shows the failure rate of the human driver 
0	139	9160	the table in the first row of the table shows the failure rates of each individual policy i without merging all the datasets from each iterations that is not performing d d d i for the next training dataset the first column shows the failure rate of the human driver 1 is the first policy trained with the training data collected from the human driver 	 1 is the first policy trained with the training data collected from the human driver 
0	139	9161	the table in the first row of the table shows the failure rates of each individual policy i without merging all the datasets from each iterations that is not performing d d d i for the next training dataset the first column shows the failure rate of the human driver 1 is the first policy trained with the training data collected from the human driver 	 2 is the first policy which is generated by manually modifying the misbehavior using 1 
0	139	9162	the table in the first row of the table shows the failure rates of each individual policy i without merging all the datasets from each iterations that is not performing d d d i for the next training dataset the first column shows the failure rate of the human driver 1 is the first policy trained with the training data collected from the human driver 	 3 is the next iteration policy generated from 2 the second row shows the case of the dataset being merged as d d d i 
1	139	9163	the table in the first row of the table shows the failure rates of each individual policy i without merging all the datasets from each iterations that is not performing d d d i for the next training dataset the first column shows the failure rate of the human driver 1 is the first policy trained with the training data collected from the human driver 	only the first iteration is conducted stopping early as the agent failed to track from the start with the given policy the also if i had more time i would have tried modeling the policy through a reinforcement learning model other than an imitation learning tried here 
1	139	9164	the table in the first row of the table shows the failure rates of each individual policy i without merging all the datasets from each iterations that is not performing d d d i for the next training dataset the first column shows the failure rate of the human driver 1 is the first policy trained with the training data collected from the human driver 	even though i don t have any quantitative data to support after training the autopilot many times it seems like confirming the fact that the dependency of current state to its past states plays very important role for the agent s robustness section 2 
1	139	9165	the table in the first row of the table shows the failure rates of each individual policy i without merging all the datasets from each iterations that is not performing d d d i for the next training dataset the first column shows the failure rate of the human driver 1 is the first policy trained with the training data collected from the human driver 	launching a real world agent using a reinforcement learning taking it out of the simulated environment will pose very interesting challenging problems 8 references
1	140	9166	we introduce a novel technique for data augmentation with the goal of improving robustness of semantic segmentation models standard data augmentation methods rely upon augmenting the existing dataset with various transformations of the training samples but do not utilize other existing datasets we propose a method that draws images from external datasets that are related in content but perhaps stylistically different we perform style normalization on these external datasets to counter differences in style 	we introduce a novel technique for data augmentation with the goal of improving robustness of semantic segmentation models 
1	140	9167	we introduce a novel technique for data augmentation with the goal of improving robustness of semantic segmentation models standard data augmentation methods rely upon augmenting the existing dataset with various transformations of the training samples but do not utilize other existing datasets we propose a method that draws images from external datasets that are related in content but perhaps stylistically different we perform style normalization on these external datasets to counter differences in style 	standard data augmentation methods rely upon augmenting the existing dataset with various transformations of the training samples but do not utilize other existing datasets 
1	140	9168	we introduce a novel technique for data augmentation with the goal of improving robustness of semantic segmentation models standard data augmentation methods rely upon augmenting the existing dataset with various transformations of the training samples but do not utilize other existing datasets we propose a method that draws images from external datasets that are related in content but perhaps stylistically different we perform style normalization on these external datasets to counter differences in style 	we propose a method that draws images from external datasets that are related in content but perhaps stylistically different we perform style normalization on these external datasets to counter differences in style 
1	140	9169	we introduce a novel technique for data augmentation with the goal of improving robustness of semantic segmentation models standard data augmentation methods rely upon augmenting the existing dataset with various transformations of the training samples but do not utilize other existing datasets we propose a method that draws images from external datasets that are related in content but perhaps stylistically different we perform style normalization on these external datasets to counter differences in style 	we apply and benchmark our technique on the semantic segmentation task with the deeplabv3 model architecture and the cityscapes dataset leveraging the gta5 dataset for our data augmentation 
0	140	9170	the task of semantic segmentation is a key topic in the field of computer vision recent advances in deep learning have yielded increasingly successful models intuitively semantic segmentation should depend only the content of an image and not on the style indeed the style of an image captures domain specific properties while the content is domaininvariant 	the task of semantic segmentation is a key topic in the field of computer vision 
1	140	9171	the task of semantic segmentation is a key topic in the field of computer vision recent advances in deep learning have yielded increasingly successful models intuitively semantic segmentation should depend only the content of an image and not on the style indeed the style of an image captures domain specific properties while the content is domaininvariant 	recent advances in deep learning have yielded increasingly successful models intuitively semantic segmentation should depend only the content of an image and not on the style 
1	140	9172	the task of semantic segmentation is a key topic in the field of computer vision recent advances in deep learning have yielded increasingly successful models intuitively semantic segmentation should depend only the content of an image and not on the style indeed the style of an image captures domain specific properties while the content is domaininvariant 	indeed the style of an image captures domain specific properties while the content is domaininvariant 
0	140	9173	the task of semantic segmentation is a key topic in the field of computer vision recent advances in deep learning have yielded increasingly successful models intuitively semantic segmentation should depend only the content of an image and not on the style indeed the style of an image captures domain specific properties while the content is domaininvariant 	we choose to focus on the deeplabv3 model flipping
0	140	9174	the cityscapes dataset collects a diverse set of street view images from 50 cities in germany and surrounding countries some examples can be seen in the gta5 dataset consists of screenshots of the open world sandbox game grand theft auto v the game is a particularly apt content domain as it is set in a large metropolitan city where street view style images are possible further the gta5 images have pixel level image labels that are compatible with those of cityscapes however they required some additional preprocessing 	the cityscapes dataset collects a diverse set of street view images from 50 cities in germany and surrounding countries 
1	140	9175	the cityscapes dataset collects a diverse set of street view images from 50 cities in germany and surrounding countries some examples can be seen in the gta5 dataset consists of screenshots of the open world sandbox game grand theft auto v the game is a particularly apt content domain as it is set in a large metropolitan city where street view style images are possible further the gta5 images have pixel level image labels that are compatible with those of cityscapes however they required some additional preprocessing 	some examples can be seen in the gta5 dataset consists of screenshots of the open world sandbox game grand theft auto v the game is a particularly apt content domain as it is set in a large metropolitan city where street view style images are possible 
1	140	9176	the cityscapes dataset collects a diverse set of street view images from 50 cities in germany and surrounding countries some examples can be seen in the gta5 dataset consists of screenshots of the open world sandbox game grand theft auto v the game is a particularly apt content domain as it is set in a large metropolitan city where street view style images are possible further the gta5 images have pixel level image labels that are compatible with those of cityscapes however they required some additional preprocessing 	further the gta5 images have pixel level image labels that are compatible with those of cityscapes however they required some additional preprocessing 
1	140	9177	the cityscapes dataset collects a diverse set of street view images from 50 cities in germany and surrounding countries some examples can be seen in the gta5 dataset consists of screenshots of the open world sandbox game grand theft auto v the game is a particularly apt content domain as it is set in a large metropolitan city where street view style images are possible further the gta5 images have pixel level image labels that are compatible with those of cityscapes however they required some additional preprocessing 	the cityscapes and gta5 datasets have a difference in their representations of ground truth 
1	140	9178	the cityscapes dataset collects a diverse set of street view images from 50 cities in germany and surrounding countries some examples can be seen in the gta5 dataset consists of screenshots of the open world sandbox game grand theft auto v the game is a particularly apt content domain as it is set in a large metropolitan city where street view style images are possible further the gta5 images have pixel level image labels that are compatible with those of cityscapes however they required some additional preprocessing 	in particular the cityscapes dataset encodes class labels with a grayscale image where each pixel s grayscale value represents the class label 
0	140	9179	the cityscapes dataset collects a diverse set of street view images from 50 cities in germany and surrounding countries some examples can be seen in the gta5 dataset consists of screenshots of the open world sandbox game grand theft auto v the game is a particularly apt content domain as it is set in a large metropolitan city where street view style images are possible further the gta5 images have pixel level image labels that are compatible with those of cityscapes however they required some additional preprocessing 	on the other hand the gta5 dataset encodes class labels with an image where the pixel color represents the class label 
0	140	9180	the cityscapes dataset collects a diverse set of street view images from 50 cities in germany and surrounding countries some examples can be seen in the gta5 dataset consists of screenshots of the open world sandbox game grand theft auto v the game is a particularly apt content domain as it is set in a large metropolitan city where street view style images are possible further the gta5 images have pixel level image labels that are compatible with those of cityscapes however they required some additional preprocessing 	this difference is displayed below in
1	140	9181	we selected the well known deeplabv3 architecture for semantic segmentation and used a popular pytorch implementation https github com jfzhang95 pytorch deeplab xception deeplabv3 uses a pre trained resnet 101 model as its backbone but adds two additional modules an atrous spacial pyramid pooling module and decoder module designed specifically for the task of semantic segmentation it utilizes cross entropy loss 	we selected the well known deeplabv3 architecture for semantic segmentation and used a popular pytorch implementation https github com jfzhang95 pytorch deeplab xception 
1	140	9182	we selected the well known deeplabv3 architecture for semantic segmentation and used a popular pytorch implementation https github com jfzhang95 pytorch deeplab xception deeplabv3 uses a pre trained resnet 101 model as its backbone but adds two additional modules an atrous spacial pyramid pooling module and decoder module designed specifically for the task of semantic segmentation it utilizes cross entropy loss 	deeplabv3 uses a pre trained resnet 101 model as its backbone but adds two additional modules an atrous spacial pyramid pooling module and decoder module designed specifically for the task of semantic segmentation 
0	140	9183	we selected the well known deeplabv3 architecture for semantic segmentation and used a popular pytorch implementation https github com jfzhang95 pytorch deeplab xception deeplabv3 uses a pre trained resnet 101 model as its backbone but adds two additional modules an atrous spacial pyramid pooling module and decoder module designed specifically for the task of semantic segmentation it utilizes cross entropy loss 	it utilizes cross entropy loss 
1	140	9184	we selected the well known deeplabv3 architecture for semantic segmentation and used a popular pytorch implementation https github com jfzhang95 pytorch deeplab xception deeplabv3 uses a pre trained resnet 101 model as its backbone but adds two additional modules an atrous spacial pyramid pooling module and decoder module designed specifically for the task of semantic segmentation it utilizes cross entropy loss 	cross entropy loss is defined as follows for a set of classes c and an image i if y i c indicates whether the true label of pixel i is c and i c is the probability computed by our model that pixel i is of class c thenfor style normalization we utilized a recent state of the art image to image translation model called unit introduced in
1	140	9185	to quantify the effects of our data augmentation technique on the robustness of semantic segmentation models we ran two primary experiments for the first experiment we took a deeplabv3 network pretrained on the pascal voc and semantic boundaries dataset and applied transfer learning with two additional datasets the first dataset consisted of the 587 cityscapes training images and 302 additional gta5 images and the second dataset consisted of the 587 cityscapes training images and the 302 gta5 images mapped to the cityscapes style domain with unit for the second experiment we trained deeplabv3 from scratch with two datasets the first dataset consisted of the 587 cityscapes training images and 587 additional gta5 images and the second dataset consisted of the 587 cityscapes images and 587 additional gta5 images mapped to the cityscapes style domain with unit the standard benchmark statistic for semantic segmentation is mean intersection over union score miou 	to quantify the effects of our data augmentation technique on the robustness of semantic segmentation models we ran two primary experiments 
1	140	9186	to quantify the effects of our data augmentation technique on the robustness of semantic segmentation models we ran two primary experiments for the first experiment we took a deeplabv3 network pretrained on the pascal voc and semantic boundaries dataset and applied transfer learning with two additional datasets the first dataset consisted of the 587 cityscapes training images and 302 additional gta5 images and the second dataset consisted of the 587 cityscapes training images and the 302 gta5 images mapped to the cityscapes style domain with unit for the second experiment we trained deeplabv3 from scratch with two datasets the first dataset consisted of the 587 cityscapes training images and 587 additional gta5 images and the second dataset consisted of the 587 cityscapes images and 587 additional gta5 images mapped to the cityscapes style domain with unit the standard benchmark statistic for semantic segmentation is mean intersection over union score miou 	for the first experiment we took a deeplabv3 network pretrained on the pascal voc and semantic boundaries dataset and applied transfer learning with two additional datasets the first dataset consisted of the 587 cityscapes training images and 302 additional gta5 images and the second dataset consisted of the 587 cityscapes training images and the 302 gta5 images mapped to the cityscapes style domain with unit 
1	140	9187	to quantify the effects of our data augmentation technique on the robustness of semantic segmentation models we ran two primary experiments for the first experiment we took a deeplabv3 network pretrained on the pascal voc and semantic boundaries dataset and applied transfer learning with two additional datasets the first dataset consisted of the 587 cityscapes training images and 302 additional gta5 images and the second dataset consisted of the 587 cityscapes training images and the 302 gta5 images mapped to the cityscapes style domain with unit for the second experiment we trained deeplabv3 from scratch with two datasets the first dataset consisted of the 587 cityscapes training images and 587 additional gta5 images and the second dataset consisted of the 587 cityscapes images and 587 additional gta5 images mapped to the cityscapes style domain with unit the standard benchmark statistic for semantic segmentation is mean intersection over union score miou 	for the second experiment we trained deeplabv3 from scratch with two datasets the first dataset consisted of the 587 cityscapes training images and 587 additional gta5 images and the second dataset consisted of the 587 cityscapes images and 587 additional gta5 images mapped to the cityscapes style domain with unit the standard benchmark statistic for semantic segmentation is mean intersection over union score miou 
1	140	9188	to quantify the effects of our data augmentation technique on the robustness of semantic segmentation models we ran two primary experiments for the first experiment we took a deeplabv3 network pretrained on the pascal voc and semantic boundaries dataset and applied transfer learning with two additional datasets the first dataset consisted of the 587 cityscapes training images and 302 additional gta5 images and the second dataset consisted of the 587 cityscapes training images and the 302 gta5 images mapped to the cityscapes style domain with unit for the second experiment we trained deeplabv3 from scratch with two datasets the first dataset consisted of the 587 cityscapes training images and 587 additional gta5 images and the second dataset consisted of the 587 cityscapes images and 587 additional gta5 images mapped to the cityscapes style domain with unit the standard benchmark statistic for semantic segmentation is mean intersection over union score miou 	intuitively the intersection over union quantifies how accurately a particular model estimates the location of an object relative to a ground truth image by computing the ratio of the number of pixels the model correctly identifies intersection to the total number of pixels representing either the ground truth or object or the model s prediction of the object union 
0	140	9189	to quantify the effects of our data augmentation technique on the robustness of semantic segmentation models we ran two primary experiments for the first experiment we took a deeplabv3 network pretrained on the pascal voc and semantic boundaries dataset and applied transfer learning with two additional datasets the first dataset consisted of the 587 cityscapes training images and 302 additional gta5 images and the second dataset consisted of the 587 cityscapes training images and the 302 gta5 images mapped to the cityscapes style domain with unit for the second experiment we trained deeplabv3 from scratch with two datasets the first dataset consisted of the 587 cityscapes training images and 587 additional gta5 images and the second dataset consisted of the 587 cityscapes images and 587 additional gta5 images mapped to the cityscapes style domain with unit the standard benchmark statistic for semantic segmentation is mean intersection over union score miou 	to extend this notion beyond binary classification we introduce the notion of a confusion matrix 
1	140	9190	to quantify the effects of our data augmentation technique on the robustness of semantic segmentation models we ran two primary experiments for the first experiment we took a deeplabv3 network pretrained on the pascal voc and semantic boundaries dataset and applied transfer learning with two additional datasets the first dataset consisted of the 587 cityscapes training images and 302 additional gta5 images and the second dataset consisted of the 587 cityscapes training images and the 302 gta5 images mapped to the cityscapes style domain with unit for the second experiment we trained deeplabv3 from scratch with two datasets the first dataset consisted of the 587 cityscapes training images and 587 additional gta5 images and the second dataset consisted of the 587 cityscapes images and 587 additional gta5 images mapped to the cityscapes style domain with unit the standard benchmark statistic for semantic segmentation is mean intersection over union score miou 	a confusion matrix m is defined such that m ij is the number of pixels whose ground truth label is i that the model classifies as j notice that the diagonal elements m ii represent correctly classified pixels 
1	140	9191	to quantify the effects of our data augmentation technique on the robustness of semantic segmentation models we ran two primary experiments for the first experiment we took a deeplabv3 network pretrained on the pascal voc and semantic boundaries dataset and applied transfer learning with two additional datasets the first dataset consisted of the 587 cityscapes training images and 302 additional gta5 images and the second dataset consisted of the 587 cityscapes training images and the 302 gta5 images mapped to the cityscapes style domain with unit for the second experiment we trained deeplabv3 from scratch with two datasets the first dataset consisted of the 587 cityscapes training images and 587 additional gta5 images and the second dataset consisted of the 587 cityscapes images and 587 additional gta5 images mapped to the cityscapes style domain with unit the standard benchmark statistic for semantic segmentation is mean intersection over union score miou 	suppose we have a set of class labels c we can then defineas stated above we first used a pretrained deeplabv3 model and applied transfer learning in two ways 
1	140	9192	to quantify the effects of our data augmentation technique on the robustness of semantic segmentation models we ran two primary experiments for the first experiment we took a deeplabv3 network pretrained on the pascal voc and semantic boundaries dataset and applied transfer learning with two additional datasets the first dataset consisted of the 587 cityscapes training images and 302 additional gta5 images and the second dataset consisted of the 587 cityscapes training images and the 302 gta5 images mapped to the cityscapes style domain with unit for the second experiment we trained deeplabv3 from scratch with two datasets the first dataset consisted of the 587 cityscapes training images and 587 additional gta5 images and the second dataset consisted of the 587 cityscapes images and 587 additional gta5 images mapped to the cityscapes style domain with unit the standard benchmark statistic for semantic segmentation is mean intersection over union score miou 	for both models we trained on the first combined dataset of cityscapes and gta5 587 images of each 
0	140	9193	to quantify the effects of our data augmentation technique on the robustness of semantic segmentation models we ran two primary experiments for the first experiment we took a deeplabv3 network pretrained on the pascal voc and semantic boundaries dataset and applied transfer learning with two additional datasets the first dataset consisted of the 587 cityscapes training images and 302 additional gta5 images and the second dataset consisted of the 587 cityscapes training images and the 302 gta5 images mapped to the cityscapes style domain with unit for the second experiment we trained deeplabv3 from scratch with two datasets the first dataset consisted of the 587 cityscapes training images and 587 additional gta5 images and the second dataset consisted of the 587 cityscapes images and 587 additional gta5 images mapped to the cityscapes style domain with unit the standard benchmark statistic for semantic segmentation is mean intersection over union score miou 	for the baseline model deeplabv3 was trained on this dataset to produce semantic segmentation predictions 
1	140	9194	to quantify the effects of our data augmentation technique on the robustness of semantic segmentation models we ran two primary experiments for the first experiment we took a deeplabv3 network pretrained on the pascal voc and semantic boundaries dataset and applied transfer learning with two additional datasets the first dataset consisted of the 587 cityscapes training images and 302 additional gta5 images and the second dataset consisted of the 587 cityscapes training images and the 302 gta5 images mapped to the cityscapes style domain with unit for the second experiment we trained deeplabv3 from scratch with two datasets the first dataset consisted of the 587 cityscapes training images and 587 additional gta5 images and the second dataset consisted of the 587 cityscapes images and 587 additional gta5 images mapped to the cityscapes style domain with unit the standard benchmark statistic for semantic segmentation is mean intersection over union score miou 	for the unit mapped model we first mapped the gta images in our training dataset to the cityscapes domain using the pretrained unit model 
0	140	9195	to quantify the effects of our data augmentation technique on the robustness of semantic segmentation models we ran two primary experiments for the first experiment we took a deeplabv3 network pretrained on the pascal voc and semantic boundaries dataset and applied transfer learning with two additional datasets the first dataset consisted of the 587 cityscapes training images and 302 additional gta5 images and the second dataset consisted of the 587 cityscapes training images and the 302 gta5 images mapped to the cityscapes style domain with unit for the second experiment we trained deeplabv3 from scratch with two datasets the first dataset consisted of the 587 cityscapes training images and 587 additional gta5 images and the second dataset consisted of the 587 cityscapes images and 587 additional gta5 images mapped to the cityscapes style domain with unit the standard benchmark statistic for semantic segmentation is mean intersection over union score miou 	we then trained deeplabv3 on the cityscapes images and these unit mapped gta images 
0	140	9196	to quantify the effects of our data augmentation technique on the robustness of semantic segmentation models we ran two primary experiments for the first experiment we took a deeplabv3 network pretrained on the pascal voc and semantic boundaries dataset and applied transfer learning with two additional datasets the first dataset consisted of the 587 cityscapes training images and 302 additional gta5 images and the second dataset consisted of the 587 cityscapes training images and the 302 gta5 images mapped to the cityscapes style domain with unit for the second experiment we trained deeplabv3 from scratch with two datasets the first dataset consisted of the 587 cityscapes training images and 587 additional gta5 images and the second dataset consisted of the 587 cityscapes images and 587 additional gta5 images mapped to the cityscapes style domain with unit the standard benchmark statistic for semantic segmentation is mean intersection over union score miou 	we also trained deeplab3 from scratch on the second combined dataset of cityscape and gta5 images 
1	140	9197	to quantify the effects of our data augmentation technique on the robustness of semantic segmentation models we ran two primary experiments for the first experiment we took a deeplabv3 network pretrained on the pascal voc and semantic boundaries dataset and applied transfer learning with two additional datasets the first dataset consisted of the 587 cityscapes training images and 302 additional gta5 images and the second dataset consisted of the 587 cityscapes training images and the 302 gta5 images mapped to the cityscapes style domain with unit for the second experiment we trained deeplabv3 from scratch with two datasets the first dataset consisted of the 587 cityscapes training images and 587 additional gta5 images and the second dataset consisted of the 587 cityscapes images and 587 additional gta5 images mapped to the cityscapes style domain with unit the standard benchmark statistic for semantic segmentation is mean intersection over union score miou 	our baseline and unit mapped followed the pipelines in miou scores training from scratch baseline 0 48 unit mapped 0 51 our code is available at the following link https bit ly 2pxnla9 
0	140	9198	to quantify the effects of our data augmentation technique on the robustness of semantic segmentation models we ran two primary experiments for the first experiment we took a deeplabv3 network pretrained on the pascal voc and semantic boundaries dataset and applied transfer learning with two additional datasets the first dataset consisted of the 587 cityscapes training images and 302 additional gta5 images and the second dataset consisted of the 587 cityscapes training images and the 302 gta5 images mapped to the cityscapes style domain with unit for the second experiment we trained deeplabv3 from scratch with two datasets the first dataset consisted of the 587 cityscapes training images and 587 additional gta5 images and the second dataset consisted of the 587 cityscapes images and 587 additional gta5 images mapped to the cityscapes style domain with unit the standard benchmark statistic for semantic segmentation is mean intersection over union score miou 	the downloadable zip file includes our codebase for both the unit and deeplab models 
0	140	9199	we find similar performance between baseline and unit mapped for our models trained using transfer learning we hypothesize that the pretrained model may not be the best fit for the street city scenes of the cityscapes and gta datasets the pascal visual object classes voc dataset and the semantic boundaries dataset sbd are different tasks than the semantic segmentation of classes in street scenes we also performed qualitative analyses of our results on this experiment 	we find similar performance between baseline and unit mapped for our models trained using transfer learning 
0	140	9200	we find similar performance between baseline and unit mapped for our models trained using transfer learning we hypothesize that the pretrained model may not be the best fit for the street city scenes of the cityscapes and gta datasets the pascal visual object classes voc dataset and the semantic boundaries dataset sbd are different tasks than the semantic segmentation of classes in street scenes we also performed qualitative analyses of our results on this experiment 	we hypothesize that the pretrained model may not be the best fit for the street city scenes of the cityscapes and gta datasets 
1	140	9201	we find similar performance between baseline and unit mapped for our models trained using transfer learning we hypothesize that the pretrained model may not be the best fit for the street city scenes of the cityscapes and gta datasets the pascal visual object classes voc dataset and the semantic boundaries dataset sbd are different tasks than the semantic segmentation of classes in street scenes we also performed qualitative analyses of our results on this experiment 	the pascal visual object classes voc dataset and the semantic boundaries dataset sbd are different tasks than the semantic segmentation of classes in street scenes we also performed qualitative analyses of our results on this experiment 
1	140	9202	we find similar performance between baseline and unit mapped for our models trained using transfer learning we hypothesize that the pretrained model may not be the best fit for the street city scenes of the cityscapes and gta datasets the pascal visual object classes voc dataset and the semantic boundaries dataset sbd are different tasks than the semantic segmentation of classes in street scenes we also performed qualitative analyses of our results on this experiment 	we compared the predicted semantic segmentations of our baseline model and the unit mapped model and find that the segmentations are similar with no noticeable differences 
0	140	9203	we find similar performance between baseline and unit mapped for our models trained using transfer learning we hypothesize that the pretrained model may not be the best fit for the street city scenes of the cityscapes and gta datasets the pascal visual object classes voc dataset and the semantic boundaries dataset sbd are different tasks than the semantic segmentation of classes in street scenes we also performed qualitative analyses of our results on this experiment 	we hypothesize that training on a larger dataset would yield higher miou scores for our unit mapped model as well as more clear visual differences 
0	140	9204	we find similar performance between baseline and unit mapped for our models trained using transfer learning we hypothesize that the pretrained model may not be the best fit for the street city scenes of the cityscapes and gta datasets the pascal visual object classes voc dataset and the semantic boundaries dataset sbd are different tasks than the semantic segmentation of classes in street scenes we also performed qualitative analyses of our results on this experiment 	given our constrained resources and limited compute power we were restricted to a small dataset 
0	140	9205	we find similar performance between baseline and unit mapped for our models trained using transfer learning we hypothesize that the pretrained model may not be the best fit for the street city scenes of the cityscapes and gta datasets the pascal visual object classes voc dataset and the semantic boundaries dataset sbd are different tasks than the semantic segmentation of classes in street scenes we also performed qualitative analyses of our results on this experiment 	our observed results in the pre trained deeplabv3 experiments reinforce the fact that more data is necessary 
0	140	9206	we find similar performance between baseline and unit mapped for our models trained using transfer learning we hypothesize that the pretrained model may not be the best fit for the street city scenes of the cityscapes and gta datasets the pascal visual object classes voc dataset and the semantic boundaries dataset sbd are different tasks than the semantic segmentation of classes in street scenes we also performed qualitative analyses of our results on this experiment 	the comparable performance on the task suggests that neither training regimen could shift the model s weights particularly far from the voc sbd optimum in the parameter space 
0	140	9207	we find similar performance between baseline and unit mapped for our models trained using transfer learning we hypothesize that the pretrained model may not be the best fit for the street city scenes of the cityscapes and gta datasets the pascal visual object classes voc dataset and the semantic boundaries dataset sbd are different tasks than the semantic segmentation of classes in street scenes we also performed qualitative analyses of our results on this experiment 	the learned features for the voc task effectively drowned out any subtleties of the street view segmentation task and the effect of our additional images 
0	140	9208	we find similar performance between baseline and unit mapped for our models trained using transfer learning we hypothesize that the pretrained model may not be the best fit for the street city scenes of the cityscapes and gta datasets the pascal visual object classes voc dataset and the semantic boundaries dataset sbd are different tasks than the semantic segmentation of classes in street scenes we also performed qualitative analyses of our results on this experiment 	we believe that we would find more significant improvements provided more unit mapped gta images our results from our second round of experiments where we trained deeplabv3 from scratch show some potential 
0	140	9209	we find similar performance between baseline and unit mapped for our models trained using transfer learning we hypothesize that the pretrained model may not be the best fit for the street city scenes of the cityscapes and gta datasets the pascal visual object classes voc dataset and the semantic boundaries dataset sbd are different tasks than the semantic segmentation of classes in street scenes we also performed qualitative analyses of our results on this experiment 	here we find that unit mapped slightly outperformed baseline 
1	140	9210	we find similar performance between baseline and unit mapped for our models trained using transfer learning we hypothesize that the pretrained model may not be the best fit for the street city scenes of the cityscapes and gta datasets the pascal visual object classes voc dataset and the semantic boundaries dataset sbd are different tasks than the semantic segmentation of classes in street scenes we also performed qualitative analyses of our results on this experiment 	the improved miou scores suggest that mapping synthetic data onto the real world domain could potentially improve the robustness of a real world classifier 
0	140	9211	our results as discussed above do not yield any significant conclusions regarding our novel technique for data augmentation we note again that compute and time restrictions did not allow us to train deeplabv3 with sufficiently many training samples to achieve baseline results as reported in other papers nonetheless our results yield various promising avenues for future research 	our results as discussed above do not yield any significant conclusions regarding our novel technique for data augmentation 
0	140	9212	our results as discussed above do not yield any significant conclusions regarding our novel technique for data augmentation we note again that compute and time restrictions did not allow us to train deeplabv3 with sufficiently many training samples to achieve baseline results as reported in other papers nonetheless our results yield various promising avenues for future research 	we note again that compute and time restrictions did not allow us to train deeplabv3 with sufficiently many training samples to achieve baseline results as reported in other papers 
0	140	9213	our results as discussed above do not yield any significant conclusions regarding our novel technique for data augmentation we note again that compute and time restrictions did not allow us to train deeplabv3 with sufficiently many training samples to achieve baseline results as reported in other papers nonetheless our results yield various promising avenues for future research 	nonetheless our results yield various promising avenues for future research 
1	140	9214	our results as discussed above do not yield any significant conclusions regarding our novel technique for data augmentation we note again that compute and time restrictions did not allow us to train deeplabv3 with sufficiently many training samples to achieve baseline results as reported in other papers nonetheless our results yield various promising avenues for future research 	in particular the superior performance of the deeplabv3 model with our novel technique for data augmentation when trained from scratch in comparison to the model with simply a combined dataset suggests that our technique could be successful if we used more training samples 
0	140	9215	our results as discussed above do not yield any significant conclusions regarding our novel technique for data augmentation we note again that compute and time restrictions did not allow us to train deeplabv3 with sufficiently many training samples to achieve baseline results as reported in other papers nonetheless our results yield various promising avenues for future research 	another area for future work is exploring the efficacy of our data augmentation approach across other tasks in computer vision 
0	140	9216	our results as discussed above do not yield any significant conclusions regarding our novel technique for data augmentation we note again that compute and time restrictions did not allow us to train deeplabv3 with sufficiently many training samples to achieve baseline results as reported in other papers nonetheless our results yield various promising avenues for future research 	for instance we would like to test our methodology on object detection and localization 
0	140	9217	there were two main tasks over the course of this project data preprocessing and training deeplabv3 evani worked on training the deeplabv3 model using transfer learning for our initial results andrew handled parts of the data preprocessing such as converting gta5 images to the cityscapes style domain with the unit model and contributed to training deeplabv3 for the initial results 	there were two main tasks over the course of this project data preprocessing and training deeplabv3 
0	140	9218	there were two main tasks over the course of this project data preprocessing and training deeplabv3 evani worked on training the deeplabv3 model using transfer learning for our initial results andrew handled parts of the data preprocessing such as converting gta5 images to the cityscapes style domain with the unit model and contributed to training deeplabv3 for the initial results 	evani worked on training the deeplabv3 model using transfer learning for our initial results 
1	140	9219	there were two main tasks over the course of this project data preprocessing and training deeplabv3 evani worked on training the deeplabv3 model using transfer learning for our initial results andrew handled parts of the data preprocessing such as converting gta5 images to the cityscapes style domain with the unit model and contributed to training deeplabv3 for the initial results 	andrew handled parts of the data preprocessing such as converting gta5 images to the cityscapes style domain with the unit model and contributed to training deeplabv3 for the initial results 
0	140	9220	there were two main tasks over the course of this project data preprocessing and training deeplabv3 evani worked on training the deeplabv3 model using transfer learning for our initial results andrew handled parts of the data preprocessing such as converting gta5 images to the cityscapes style domain with the unit model and contributed to training deeplabv3 for the initial results 	felix worked on training the deeplabv3 codebase from scratch and some of the data preprocessing such as making the gta5 and cityscapes labels compatible 
1	141	9221	in this work we implemented and trained an end to end deep neural network songnet to perform real time music genre classification music can be represented in various forms time series decimals spectrum in frequency domain and spectrograms etc the spectrogram stands out as the most popular choice since it incorporates time and frequency information 	in this work we implemented and trained an end to end deep neural network songnet to perform real time music genre classification 
0	141	9222	in this work we implemented and trained an end to end deep neural network songnet to perform real time music genre classification music can be represented in various forms time series decimals spectrum in frequency domain and spectrograms etc the spectrogram stands out as the most popular choice since it incorporates time and frequency information 	music can be represented in various forms time series decimals spectrum in frequency domain and spectrograms etc 
0	141	9223	in this work we implemented and trained an end to end deep neural network songnet to perform real time music genre classification music can be represented in various forms time series decimals spectrum in frequency domain and spectrograms etc the spectrogram stands out as the most popular choice since it incorporates time and frequency information 	the spectrogram stands out as the most popular choice since it incorporates time and frequency information 
0	141	9224	in this work we implemented and trained an end to end deep neural network songnet to perform real time music genre classification music can be represented in various forms time series decimals spectrum in frequency domain and spectrograms etc the spectrogram stands out as the most popular choice since it incorporates time and frequency information 	in this project we used the convolutional recurrent neural network c rnn to classify music 
1	141	9225	in this work we implemented and trained an end to end deep neural network songnet to perform real time music genre classification music can be represented in various forms time series decimals spectrum in frequency domain and spectrograms etc the spectrogram stands out as the most popular choice since it incorporates time and frequency information 	the convolutional network extracts features of spectrogram before feeding them into recurrent network which then performs classification considering both transient and overall characteristics of music 
0	141	9226	in this work we implemented and trained an end to end deep neural network songnet to perform real time music genre classification music can be represented in various forms time series decimals spectrum in frequency domain and spectrograms etc the spectrogram stands out as the most popular choice since it incorporates time and frequency information 	taking only raw audio as input the c rnn achieved 65 23 accuracy on fma small dataset beating the best baseline by 41 
0	141	9227	with the enormous growth of music released online managing music library manually has become more and more challenging not only for users but also audio streaming service companies such as spotify and itunes fast and accurate music classification is in high demand while it is non trivial for machines to perform the task automatically at human level besides music genre classification is an essential backbone for music recommendation and unknown soundtrack recognition which will benefit music service platforms a lot building a robust music classifier using machine learning techniques is essential to automate tagging unlabled music and improve users experience of media players and music libraries in recent years convolutional neural networks cnns have brought revolutionary changes to computer vision community	with the enormous growth of music released online managing music library manually has become more and more challenging not only for users but also audio streaming service companies such as spotify and itunes 
0	141	9228	with the enormous growth of music released online managing music library manually has become more and more challenging not only for users but also audio streaming service companies such as spotify and itunes fast and accurate music classification is in high demand while it is non trivial for machines to perform the task automatically at human level besides music genre classification is an essential backbone for music recommendation and unknown soundtrack recognition which will benefit music service platforms a lot building a robust music classifier using machine learning techniques is essential to automate tagging unlabled music and improve users experience of media players and music libraries in recent years convolutional neural networks cnns have brought revolutionary changes to computer vision community	fast and accurate music classification is in high demand while it is non trivial for machines to perform the task automatically at human level besides music genre classification is an essential backbone for music recommendation and unknown soundtrack recognition which will benefit music service platforms a lot 
1	141	9229	with the enormous growth of music released online managing music library manually has become more and more challenging not only for users but also audio streaming service companies such as spotify and itunes fast and accurate music classification is in high demand while it is non trivial for machines to perform the task automatically at human level besides music genre classification is an essential backbone for music recommendation and unknown soundtrack recognition which will benefit music service platforms a lot building a robust music classifier using machine learning techniques is essential to automate tagging unlabled music and improve users experience of media players and music libraries in recent years convolutional neural networks cnns have brought revolutionary changes to computer vision community	building a robust music classifier using machine learning techniques is essential to automate tagging unlabled music and improve users experience of media players and music libraries in recent years convolutional neural networks cnns have brought revolutionary changes to computer vision community
0	141	9230	music genre classification has been actively studied since the early days of the internet tzanetakis and cook in recent years using audio spectrogram has become mainstream for music genre classification spectrogram encodes time and frequency information of a given music as a whole 	music genre classification has been actively studied since the early days of the internet 
0	141	9231	music genre classification has been actively studied since the early days of the internet tzanetakis and cook in recent years using audio spectrogram has become mainstream for music genre classification spectrogram encodes time and frequency information of a given music as a whole 	tzanetakis and cook in recent years using audio spectrogram has become mainstream for music genre classification 
0	141	9232	music genre classification has been actively studied since the early days of the internet tzanetakis and cook in recent years using audio spectrogram has become mainstream for music genre classification spectrogram encodes time and frequency information of a given music as a whole 	spectrogram encodes time and frequency information of a given music as a whole 
1	141	9233	music genre classification has been actively studied since the early days of the internet tzanetakis and cook in recent years using audio spectrogram has become mainstream for music genre classification spectrogram encodes time and frequency information of a given music as a whole 	wyse this work aims to train a c rnn model with melspectrogram as the only feature and compare this model with the traditional machine learning classifiers that need to be trained with hand crafted features and metadata 
0	141	9234	the dataset used for this project is the free music archive fma an interactive library of high quality legal audio downloads direct by wfmu furthermore it provides music s associated information including precomputed features user level metadata etc to ensure data is balanced among different genres we only use a small subset fma small for the scope of this project 	the dataset used for this project is the free music archive fma an interactive library of high quality legal audio downloads direct by wfmu 
0	141	9235	the dataset used for this project is the free music archive fma an interactive library of high quality legal audio downloads direct by wfmu furthermore it provides music s associated information including precomputed features user level metadata etc to ensure data is balanced among different genres we only use a small subset fma small for the scope of this project 	furthermore it provides music s associated information including precomputed features user level metadata etc 
0	141	9236	the dataset used for this project is the free music archive fma an interactive library of high quality legal audio downloads direct by wfmu furthermore it provides music s associated information including precomputed features user level metadata etc to ensure data is balanced among different genres we only use a small subset fma small for the scope of this project 	to ensure data is balanced among different genres we only use a small subset fma small for the scope of this project 
0	141	9237	the dataset used for this project is the free music archive fma an interactive library of high quality legal audio downloads direct by wfmu furthermore it provides music s associated information including precomputed features user level metadata etc to ensure data is balanced among different genres we only use a small subset fma small for the scope of this project 	it con the fma provided fine genre information for each track with built in genre hierarchy which is claimed by the artists themselves 
0	141	9238	the dataset used for this project is the free music archive fma an interactive library of high quality legal audio downloads direct by wfmu furthermore it provides music s associated information including precomputed features user level metadata etc to ensure data is balanced among different genres we only use a small subset fma small for the scope of this project 	in each of the track table the ids of all the genres indicated by artists are included and the root genres are provided in genre top column the preprocessed dataset is split into 70 training 20 validation 10 test sets respectively 
0	141	9239	a popular representation of sound is the spectrogram which captures both time and frequency information in this study we used mel spectrogram as the only input to train our nerual network a mel spectrogram is a spectrogram transformed to have frequencies in mel scale which basically is a logarithmic scale more naturally representing how human actually senses different sound frequencies 	a popular representation of sound is the spectrogram which captures both time and frequency information 
0	141	9240	a popular representation of sound is the spectrogram which captures both time and frequency information in this study we used mel spectrogram as the only input to train our nerual network a mel spectrogram is a spectrogram transformed to have frequencies in mel scale which basically is a logarithmic scale more naturally representing how human actually senses different sound frequencies 	in this study we used mel spectrogram as the only input to train our nerual network 
0	141	9241	a popular representation of sound is the spectrogram which captures both time and frequency information in this study we used mel spectrogram as the only input to train our nerual network a mel spectrogram is a spectrogram transformed to have frequencies in mel scale which basically is a logarithmic scale more naturally representing how human actually senses different sound frequencies 	a mel spectrogram is a spectrogram transformed to have frequencies in mel scale which basically is a logarithmic scale more naturally representing how human actually senses different sound frequencies 
1	141	9242	a popular representation of sound is the spectrogram which captures both time and frequency information in this study we used mel spectrogram as the only input to train our nerual network a mel spectrogram is a spectrogram transformed to have frequencies in mel scale which basically is a logarithmic scale more naturally representing how human actually senses different sound frequencies 	it is simple to implement thanks to librosa aside from the music features extracted by librosa fma also provides music metadata such as release year number of listens composers durations etc 
0	141	9243	a popular representation of sound is the spectrogram which captures both time and frequency information in this study we used mel spectrogram as the only input to train our nerual network a mel spectrogram is a spectrogram transformed to have frequencies in mel scale which basically is a logarithmic scale more naturally representing how human actually senses different sound frequencies 	there are 140 features in total that could be used for training 
0	141	9244	we trained four traditional classification models on the dataset as baseline classifiers including k nearest neighbors logistic regression multilayer perception and linear support vector machine it was found that baseline models could achieve no higher than 50 accuracy since these models were merely used for comparison we adopted the default implementation and parameters in scikit learn library 	we trained four traditional classification models on the dataset as baseline classifiers including k nearest neighbors logistic regression multilayer perception and linear support vector machine 
0	141	9245	we trained four traditional classification models on the dataset as baseline classifiers including k nearest neighbors logistic regression multilayer perception and linear support vector machine it was found that baseline models could achieve no higher than 50 accuracy since these models were merely used for comparison we adopted the default implementation and parameters in scikit learn library 	it was found that baseline models could achieve no higher than 50 accuracy 
0	141	9246	we trained four traditional classification models on the dataset as baseline classifiers including k nearest neighbors logistic regression multilayer perception and linear support vector machine it was found that baseline models could achieve no higher than 50 accuracy since these models were merely used for comparison we adopted the default implementation and parameters in scikit learn library 	since these models were merely used for comparison we adopted the default implementation and parameters in scikit learn library 
0	141	9247	we trained four traditional classification models on the dataset as baseline classifiers including k nearest neighbors logistic regression multilayer perception and linear support vector machine it was found that baseline models could achieve no higher than 50 accuracy since these models were merely used for comparison we adopted the default implementation and parameters in scikit learn library 	the input features include all 140 features provided by fma 
0	141	9248	as shown in to start features are extracted from the spectrograms using convolutional layers it is important to point out that the features are translation invariant only in time domain frequencies do matter and needs to be distinguished thus 2 d convolution seems unsuitable in this case we are interested in changes across time every convolutional layer should look at a small period of time as a whole extract the most valuable information and create a feature map that is still a sequence over time 	as shown in to start features are extracted from the spectrograms using convolutional layers 
0	141	9249	as shown in to start features are extracted from the spectrograms using convolutional layers it is important to point out that the features are translation invariant only in time domain frequencies do matter and needs to be distinguished thus 2 d convolution seems unsuitable in this case we are interested in changes across time every convolutional layer should look at a small period of time as a whole extract the most valuable information and create a feature map that is still a sequence over time 	it is important to point out that the features are translation invariant only in time domain frequencies do matter and needs to be distinguished 
0	141	9250	as shown in to start features are extracted from the spectrograms using convolutional layers it is important to point out that the features are translation invariant only in time domain frequencies do matter and needs to be distinguished thus 2 d convolution seems unsuitable in this case we are interested in changes across time every convolutional layer should look at a small period of time as a whole extract the most valuable information and create a feature map that is still a sequence over time 	thus 2 d convolution seems unsuitable in this case we are interested in changes across time every convolutional layer should look at a small period of time as a whole extract the most valuable information and create a feature map that is still a sequence over time 
0	141	9251	as shown in to start features are extracted from the spectrograms using convolutional layers it is important to point out that the features are translation invariant only in time domain frequencies do matter and needs to be distinguished thus 2 d convolution seems unsuitable in this case we are interested in changes across time every convolutional layer should look at a small period of time as a whole extract the most valuable information and create a feature map that is still a sequence over time 	then one dimensional convolutions across the time axis were adopted 
0	141	9252	as shown in to start features are extracted from the spectrograms using convolutional layers it is important to point out that the features are translation invariant only in time domain frequencies do matter and needs to be distinguished thus 2 d convolution seems unsuitable in this case we are interested in changes across time every convolutional layer should look at a small period of time as a whole extract the most valuable information and create a feature map that is still a sequence over time 	each convolution is followed by relu activation and 1 d max pooling 
0	141	9253	as shown in to start features are extracted from the spectrograms using convolutional layers it is important to point out that the features are translation invariant only in time domain frequencies do matter and needs to be distinguished thus 2 d convolution seems unsuitable in this case we are interested in changes across time every convolutional layer should look at a small period of time as a whole extract the most valuable information and create a feature map that is still a sequence over time 	to regularize the model we added dropout to every convolutional layers the cnn outputs a sequence of features and it is then fed to rnn represented by a time distributed fully connected layer with softmax activation essentially giving us a sequence of 8 dimensional vectors 8 is the number of genres in fma small at each timestep 
0	141	9254	as shown in to start features are extracted from the spectrograms using convolutional layers it is important to point out that the features are translation invariant only in time domain frequencies do matter and needs to be distinguished thus 2 d convolution seems unsuitable in this case we are interested in changes across time every convolutional layer should look at a small period of time as a whole extract the most valuable information and create a feature map that is still a sequence over time 	the rnn part is designed to find both dependencies across short period of time and a long term structure of a song 
0	141	9255	as shown in to start features are extracted from the spectrograms using convolutional layers it is important to point out that the features are translation invariant only in time domain frequencies do matter and needs to be distinguished thus 2 d convolution seems unsuitable in this case we are interested in changes across time every convolutional layer should look at a small period of time as a whole extract the most valuable information and create a feature map that is still a sequence over time 	these vectors are interpreted as the networks belief of the music genre at the particular point of time i e 
0	141	9256	as shown in to start features are extracted from the spectrograms using convolutional layers it is important to point out that the features are translation invariant only in time domain frequencies do matter and needs to be distinguished thus 2 d convolution seems unsuitable in this case we are interested in changes across time every convolutional layer should look at a small period of time as a whole extract the most valuable information and create a feature map that is still a sequence over time 	to reduce the time series of 8 d probability vectors into a single one genre probability distribution we simply take the mean 
0	141	9257	as shown in to start features are extracted from the spectrograms using convolutional layers it is important to point out that the features are translation invariant only in time domain frequencies do matter and needs to be distinguished thus 2 d convolution seems unsuitable in this case we are interested in changes across time every convolutional layer should look at a small period of time as a whole extract the most valuable information and create a feature map that is still a sequence over time 	it is the most intuitive way to tackle the disproportion problem of inferring music genre per timestep versus just one label for the whole song but it turns out to very effective 
0	141	9258	the accuracies of baseline classifiers and songnet are reported in the table below it can be observed that our c rnn model outperforms the best baseline by 41 the validation set was used to help us tune hyperparameters of songnet 	the accuracies of baseline classifiers and songnet are reported in the table below 
0	141	9259	the accuracies of baseline classifiers and songnet are reported in the table below it can be observed that our c rnn model outperforms the best baseline by 41 the validation set was used to help us tune hyperparameters of songnet 	it can be observed that our c rnn model outperforms the best baseline by 41 
0	141	9260	the accuracies of baseline classifiers and songnet are reported in the table below it can be observed that our c rnn model outperforms the best baseline by 41 the validation set was used to help us tune hyperparameters of songnet 	the validation set was used to help us tune hyperparameters of songnet 
0	141	9261	the accuracies of baseline classifiers and songnet are reported in the table below it can be observed that our c rnn model outperforms the best baseline by 41 the validation set was used to help us tune hyperparameters of songnet 	during training the learning rate was initially set to 0 001 and further decayed subject to reducelronplateau scheduler 
0	141	9262	the accuracies of baseline classifiers and songnet are reported in the table below it can be observed that our c rnn model outperforms the best baseline by 41 the validation set was used to help us tune hyperparameters of songnet 	the reported numbers are accuracies with respect to the test set 
0	141	9263	it is worth mentioning that all of our baseline models were trained and tested with rich features including music metadata year artist etc however in the current c rnn model setting we decided not to incorporate metadata for simpler training setup the fact that c rnn model still beats the best baseline by a significant amount even without metadata demonstrates the power of deep learning models on classification tasks 	it is worth mentioning that all of our baseline models were trained and tested with rich features including music metadata year artist etc 
0	141	9264	it is worth mentioning that all of our baseline models were trained and tested with rich features including music metadata year artist etc however in the current c rnn model setting we decided not to incorporate metadata for simpler training setup the fact that c rnn model still beats the best baseline by a significant amount even without metadata demonstrates the power of deep learning models on classification tasks 	however in the current c rnn model setting we decided not to incorporate metadata for simpler training setup 
0	141	9265	it is worth mentioning that all of our baseline models were trained and tested with rich features including music metadata year artist etc however in the current c rnn model setting we decided not to incorporate metadata for simpler training setup the fact that c rnn model still beats the best baseline by a significant amount even without metadata demonstrates the power of deep learning models on classification tasks 	the fact that c rnn model still beats the best baseline by a significant amount even without metadata demonstrates the power of deep learning models on classification tasks 
0	141	9266	to further interpret the results and guide future work we plotted the confusion matrix	to further interpret the results and guide future work we plotted the confusion matrix
0	141	9267	in computer vision convolutional layers are used to extract features from images low level kernels can detect edges or corners and higher level kernels can capture more sophisticated structures in our setting we also expect convolutional layers to do similar things 	in computer vision convolutional layers are used to extract features from images 
0	141	9268	in computer vision convolutional layers are used to extract features from images low level kernels can detect edges or corners and higher level kernels can capture more sophisticated structures in our setting we also expect convolutional layers to do similar things 	low level kernels can detect edges or corners and higher level kernels can capture more sophisticated structures 
0	141	9269	in computer vision convolutional layers are used to extract features from images low level kernels can detect edges or corners and higher level kernels can capture more sophisticated structures in our setting we also expect convolutional layers to do similar things 	in our setting we also expect convolutional layers to do similar things 
1	141	9270	in computer vision convolutional layers are used to extract features from images low level kernels can detect edges or corners and higher level kernels can capture more sophisticated structures in our setting we also expect convolutional layers to do similar things 	songnet has 3 convolutional layers so we expect kernels to extract different levels of music genre kernels 
1	141	9271	in computer vision convolutional layers are used to extract features from images low level kernels can detect edges or corners and higher level kernels can capture more sophisticated structures in our setting we also expect convolutional layers to do similar things 	it would be straightforward and much clearer if kernel numbers are converted to music clips 
0	141	9272	in computer vision convolutional layers are used to extract features from images low level kernels can detect edges or corners and higher level kernels can capture more sophisticated structures in our setting we also expect convolutional layers to do similar things 	after listening to some of kernels we found that kernel clips in the first convolutional layer are mainly basic beats and elements of music 
0	141	9273	in computer vision convolutional layers are used to extract features from images low level kernels can detect edges or corners and higher level kernels can capture more sophisticated structures in our setting we also expect convolutional layers to do similar things 	the clips from the last convolutional layer however are already human listenable syn thesized music clips of certain genres 
0	141	9274	in computer vision convolutional layers are used to extract features from images low level kernels can detect edges or corners and higher level kernels can capture more sophisticated structures in our setting we also expect convolutional layers to do similar things 	we demonstrated the kernel clips during the poster session and uploaded them to google drive link for grading purposes 
0	141	9275	the ultimate goal of songnet is real time genre classification as the soundtrack plays this is the reason why we combined recurrent network with convolutional neural network in our architecture as discussed in the architecture section for each timestep the model outputs a probability distribution vector among 8 different genres so it enables real time classification 	the ultimate goal of songnet is real time genre classification as the soundtrack plays 
0	141	9276	the ultimate goal of songnet is real time genre classification as the soundtrack plays this is the reason why we combined recurrent network with convolutional neural network in our architecture as discussed in the architecture section for each timestep the model outputs a probability distribution vector among 8 different genres so it enables real time classification 	this is the reason why we combined recurrent network with convolutional neural network in our architecture 
1	141	9277	the ultimate goal of songnet is real time genre classification as the soundtrack plays this is the reason why we combined recurrent network with convolutional neural network in our architecture as discussed in the architecture section for each timestep the model outputs a probability distribution vector among 8 different genres so it enables real time classification 	as discussed in the architecture section for each timestep the model outputs a probability distribution vector among 8 different genres so it enables real time classification 
0	141	9278	the ultimate goal of songnet is real time genre classification as the soundtrack plays this is the reason why we combined recurrent network with convolutional neural network in our architecture as discussed in the architecture section for each timestep the model outputs a probability distribution vector among 8 different genres so it enables real time classification 	it is better to show this functionality with a gui 
0	141	9279	the ultimate goal of songnet is real time genre classification as the soundtrack plays this is the reason why we combined recurrent network with convolutional neural network in our architecture as discussed in the architecture section for each timestep the model outputs a probability distribution vector among 8 different genres so it enables real time classification 	due to limited timeline we did not implement it yet but it could be an interesting extension in future work 
0	141	9280	following our discussion above we conclude two possible extensions of current work 	following our discussion above we conclude two possible extensions of current work 
0	141	9281	to solve the experimental genre issue because it contributes a lot to the loss it is worth trying to incorporate music metadata we expect the metadata to help increase the performance because even though this music itself shares some features with other genres such as rock and electronic additional information such as artists and album years will be able to help the model better classify this genre 2 	to solve the experimental genre issue because it contributes a lot to the loss 
0	141	9282	to solve the experimental genre issue because it contributes a lot to the loss it is worth trying to incorporate music metadata we expect the metadata to help increase the performance because even though this music itself shares some features with other genres such as rock and electronic additional information such as artists and album years will be able to help the model better classify this genre 2 	it is worth trying to incorporate music metadata 
1	141	9283	to solve the experimental genre issue because it contributes a lot to the loss it is worth trying to incorporate music metadata we expect the metadata to help increase the performance because even though this music itself shares some features with other genres such as rock and electronic additional information such as artists and album years will be able to help the model better classify this genre 2 	we expect the metadata to help increase the performance because even though this music itself shares some features with other genres such as rock and electronic additional information such as artists and album years will be able to help the model better classify this genre 2 
1	141	9284	to solve the experimental genre issue because it contributes a lot to the loss it is worth trying to incorporate music metadata we expect the metadata to help increase the performance because even though this music itself shares some features with other genres such as rock and electronic additional information such as artists and album years will be able to help the model better classify this genre 2 	build a graphical user interface to allow users upload a music clip and then visualize the real time classification 
0	141	9285	to solve the experimental genre issue because it contributes a lot to the loss it is worth trying to incorporate music metadata we expect the metadata to help increase the performance because even though this music itself shares some features with other genres such as rock and electronic additional information such as artists and album years will be able to help the model better classify this genre 2 	this is fun as well as beneficial for further tuning the model 
0	141	9286	to solve the experimental genre issue because it contributes a lot to the loss it is worth trying to incorporate music metadata we expect the metadata to help increase the performance because even though this music itself shares some features with other genres such as rock and electronic additional information such as artists and album years will be able to help the model better classify this genre 2 	it s fun because users can have a better way of interaction with the model 
0	141	9287	to solve the experimental genre issue because it contributes a lot to the loss it is worth trying to incorporate music metadata we expect the metadata to help increase the performance because even though this music itself shares some features with other genres such as rock and electronic additional information such as artists and album years will be able to help the model better classify this genre 2 	as users upload more songs we could collect more data to improve the model 
1	142	9288	we have developed a weakly supervised method for localizing pneumonia on chest x rays our model includes two parts 1 a 10 layer convolutional neural network cnn that predicts the presence of pneumonia and 2 a class activation map cam that localizes the pneumonia manifestation without requiring bounding box labels by having our weakly supervised approach achieve slightly better performance than a supervised method r cnn we believe that this brings tremendous value in labeling diseases in images that are often unannotated in medical records 	we have developed a weakly supervised method for localizing pneumonia on chest x rays 
1	142	9289	we have developed a weakly supervised method for localizing pneumonia on chest x rays our model includes two parts 1 a 10 layer convolutional neural network cnn that predicts the presence of pneumonia and 2 a class activation map cam that localizes the pneumonia manifestation without requiring bounding box labels by having our weakly supervised approach achieve slightly better performance than a supervised method r cnn we believe that this brings tremendous value in labeling diseases in images that are often unannotated in medical records 	our model includes two parts 1 a 10 layer convolutional neural network cnn that predicts the presence of pneumonia and 2 a class activation map cam that localizes the pneumonia manifestation without requiring bounding box labels 
1	142	9290	we have developed a weakly supervised method for localizing pneumonia on chest x rays our model includes two parts 1 a 10 layer convolutional neural network cnn that predicts the presence of pneumonia and 2 a class activation map cam that localizes the pneumonia manifestation without requiring bounding box labels by having our weakly supervised approach achieve slightly better performance than a supervised method r cnn we believe that this brings tremendous value in labeling diseases in images that are often unannotated in medical records 	by having our weakly supervised approach achieve slightly better performance than a supervised method r cnn we believe that this brings tremendous value in labeling diseases in images that are often unannotated in medical records 
1	142	9291	we have developed a weakly supervised method for localizing pneumonia on chest x rays our model includes two parts 1 a 10 layer convolutional neural network cnn that predicts the presence of pneumonia and 2 a class activation map cam that localizes the pneumonia manifestation without requiring bounding box labels by having our weakly supervised approach achieve slightly better performance than a supervised method r cnn we believe that this brings tremendous value in labeling diseases in images that are often unannotated in medical records 	thus our method has the potential to provide care to populations with inadequate access to imaging diagnostic specialists while automate other medical image data sets 
1	142	9292	pneumonia is an inflammatory condition of the lung that is responsible for more than 1 million hospitalizations and 50 000 deaths every year in the united states globally pneumonia is responsible for over 15 of all deaths of children under the age of 5 however chest x ray images generated from hospitals do not specify the precise location of the pneumonia which is a significant challenge when training a machine learning model for this purpose at most doctors will keep a brief description such as aggregation of lung opacity on the patient s lower right lung 	pneumonia is an inflammatory condition of the lung that is responsible for more than 1 million hospitalizations and 50 000 deaths every year in the united states 
1	142	9293	pneumonia is an inflammatory condition of the lung that is responsible for more than 1 million hospitalizations and 50 000 deaths every year in the united states globally pneumonia is responsible for over 15 of all deaths of children under the age of 5 however chest x ray images generated from hospitals do not specify the precise location of the pneumonia which is a significant challenge when training a machine learning model for this purpose at most doctors will keep a brief description such as aggregation of lung opacity on the patient s lower right lung 	globally pneumonia is responsible for over 15 of all deaths of children under the age of 5 however chest x ray images generated from hospitals do not specify the precise location of the pneumonia which is a significant challenge when training a machine learning model for this purpose 
1	142	9294	pneumonia is an inflammatory condition of the lung that is responsible for more than 1 million hospitalizations and 50 000 deaths every year in the united states globally pneumonia is responsible for over 15 of all deaths of children under the age of 5 however chest x ray images generated from hospitals do not specify the precise location of the pneumonia which is a significant challenge when training a machine learning model for this purpose at most doctors will keep a brief description such as aggregation of lung opacity on the patient s lower right lung 	at most doctors will keep a brief description such as aggregation of lung opacity on the patient s lower right lung 
1	142	9295	pneumonia is an inflammatory condition of the lung that is responsible for more than 1 million hospitalizations and 50 000 deaths every year in the united states globally pneumonia is responsible for over 15 of all deaths of children under the age of 5 however chest x ray images generated from hospitals do not specify the precise location of the pneumonia which is a significant challenge when training a machine learning model for this purpose at most doctors will keep a brief description such as aggregation of lung opacity on the patient s lower right lung 	this is because the precise pixel location of lung opacity on the x ray image is only part of the equation for diagnosing and only the final conclusion is recorded in the electrical health record ehr 
1	142	9296	pneumonia is an inflammatory condition of the lung that is responsible for more than 1 million hospitalizations and 50 000 deaths every year in the united states globally pneumonia is responsible for over 15 of all deaths of children under the age of 5 however chest x ray images generated from hospitals do not specify the precise location of the pneumonia which is a significant challenge when training a machine learning model for this purpose at most doctors will keep a brief description such as aggregation of lung opacity on the patient s lower right lung 	to developed a machine learning algorithm that predicts pneumonia location using traditional supervised methods requires the 0 precise x y coordinate labels that datasets lack 
1	142	9297	pneumonia is an inflammatory condition of the lung that is responsible for more than 1 million hospitalizations and 50 000 deaths every year in the united states globally pneumonia is responsible for over 15 of all deaths of children under the age of 5 however chest x ray images generated from hospitals do not specify the precise location of the pneumonia which is a significant challenge when training a machine learning model for this purpose at most doctors will keep a brief description such as aggregation of lung opacity on the patient s lower right lung 	hence this deficiency in labelled datasets commonly observed in the medical imaging field is the motivation behind our work in this work we tackle this challenge in a novel approach we use a weakly supervised approach to automate localizing pneumonia in chest x rays 
1	142	9298	pneumonia is an inflammatory condition of the lung that is responsible for more than 1 million hospitalizations and 50 000 deaths every year in the united states globally pneumonia is responsible for over 15 of all deaths of children under the age of 5 however chest x ray images generated from hospitals do not specify the precise location of the pneumonia which is a significant challenge when training a machine learning model for this purpose at most doctors will keep a brief description such as aggregation of lung opacity on the patient s lower right lung 	our model is considered weakly supervised because it only needs the binary labels pneumonia vs no pneumonia during training to estimate a bounding box around the region of the lung opacity 
1	142	9299	pneumonia is an inflammatory condition of the lung that is responsible for more than 1 million hospitalizations and 50 000 deaths every year in the united states globally pneumonia is responsible for over 15 of all deaths of children under the age of 5 however chest x ray images generated from hospitals do not specify the precise location of the pneumonia which is a significant challenge when training a machine learning model for this purpose at most doctors will keep a brief description such as aggregation of lung opacity on the patient s lower right lung 	at a high level our weakly supervised algorithm works as follows 1 input an x ray image in u net architecture for data augmentation 2 input augmented image and original image in a 10 layer cnn architecture to classify if given image is pneumonia positive and 3 if image is pneumonia positive apply cam to precisely localize the pneumonia aggregation 
1	142	9300	there have been recent efforts in detecting pneumonia using x ray images for instance the chexnet 10 team modified chestx ray14 s 13 algorithm to increase the accuracy in detecting 14 diseases including pneumonia however neither effort localizes using bounding boxes and both use imagenet to pretrain their models 	there have been recent efforts in detecting pneumonia using x ray images 
1	142	9301	there have been recent efforts in detecting pneumonia using x ray images for instance the chexnet 10 team modified chestx ray14 s 13 algorithm to increase the accuracy in detecting 14 diseases including pneumonia however neither effort localizes using bounding boxes and both use imagenet to pretrain their models 	for instance the chexnet 10 team modified chestx ray14 s 13 algorithm to increase the accuracy in detecting 14 diseases including pneumonia 
0	142	9302	there have been recent efforts in detecting pneumonia using x ray images for instance the chexnet 10 team modified chestx ray14 s 13 algorithm to increase the accuracy in detecting 14 diseases including pneumonia however neither effort localizes using bounding boxes and both use imagenet to pretrain their models 	however neither effort localizes using bounding boxes and both use imagenet to pretrain their models 
1	142	9303	there have been recent efforts in detecting pneumonia using x ray images for instance the chexnet 10 team modified chestx ray14 s 13 algorithm to increase the accuracy in detecting 14 diseases including pneumonia however neither effort localizes using bounding boxes and both use imagenet to pretrain their models 	despite the fact that both works achieve high accuracy neither solves the problem of clearly annotating the pneumonia manifestation using bounding boxes in the x ray images 
1	142	9304	there have been recent efforts in detecting pneumonia using x ray images for instance the chexnet 10 team modified chestx ray14 s 13 algorithm to increase the accuracy in detecting 14 diseases including pneumonia however neither effort localizes using bounding boxes and both use imagenet to pretrain their models 	as such we leverage the work of four algorithms in our approach 1 r cnn 6 2 cam 14 3 vgg architecture
0	142	9305	3 0 1 dataset we acquired our dataset from kaggles rsna pneumonia detection competition	3 0 1 
0	142	9306	3 0 1 dataset we acquired our dataset from kaggles rsna pneumonia detection competition	dataset we acquired our dataset from kaggles rsna pneumonia detection competition
1	142	9307	each pixel of the images was normalized by subtracting the mean and dividing by the standard deviation at that location we also compressed the original image from 1024 1024 pixels down to 128 128 pixels allowing us to expedite the training of our neural network a u net neural network was used to predict the confidence of each pixel belonging to the lung 	each pixel of the images was normalized by subtracting the mean and dividing by the standard deviation at that location 
1	142	9308	each pixel of the images was normalized by subtracting the mean and dividing by the standard deviation at that location we also compressed the original image from 1024 1024 pixels down to 128 128 pixels allowing us to expedite the training of our neural network a u net neural network was used to predict the confidence of each pixel belonging to the lung 	we also compressed the original image from 1024 1024 pixels down to 128 128 pixels allowing us to expedite the training of our neural network 
1	142	9309	each pixel of the images was normalized by subtracting the mean and dividing by the standard deviation at that location we also compressed the original image from 1024 1024 pixels down to 128 128 pixels allowing us to expedite the training of our neural network a u net neural network was used to predict the confidence of each pixel belonging to the lung 	a u net neural network was used to predict the confidence of each pixel belonging to the lung 
1	142	9310	each pixel of the images was normalized by subtracting the mean and dividing by the standard deviation at that location we also compressed the original image from 1024 1024 pixels down to 128 128 pixels allowing us to expedite the training of our neural network a u net neural network was used to predict the confidence of each pixel belonging to the lung 	then we segmented the lung by multiplying the original image matrix with the localization matrix figure 2 
1	142	9311	each pixel of the images was normalized by subtracting the mean and dividing by the standard deviation at that location we also compressed the original image from 1024 1024 pixels down to 128 128 pixels allowing us to expedite the training of our neural network a u net neural network was used to predict the confidence of each pixel belonging to the lung 	both the original and the segmented images were fed into our model as inputs to provide our model with a hypothesis of lung location 
1	142	9312	each pixel of the images was normalized by subtracting the mean and dividing by the standard deviation at that location we also compressed the original image from 1024 1024 pixels down to 128 128 pixels allowing us to expedite the training of our neural network a u net neural network was used to predict the confidence of each pixel belonging to the lung 	the first part of our work is to build a cnn model that can accurately classify whether a given image is labeled as pneumonia or not 
1	142	9313	each pixel of the images was normalized by subtracting the mean and dividing by the standard deviation at that location we also compressed the original image from 1024 1024 pixels down to 128 128 pixels allowing us to expedite the training of our neural network a u net neural network was used to predict the confidence of each pixel belonging to the lung 	the images that were predicted as pneumonia positive are then fed into our localization model 
1	142	9314	since the first part of this project is a supervised classification task svm random forest and logistic regression were used to baseline our classification model these models could not take a matrix of pixels as an input so we flatten the images into one dimensional vectors 	since the first part of this project is a supervised classification task svm random forest and logistic regression were used to baseline our classification model 
1	142	9315	since the first part of this project is a supervised classification task svm random forest and logistic regression were used to baseline our classification model these models could not take a matrix of pixels as an input so we flatten the images into one dimensional vectors 	these models could not take a matrix of pixels as an input so we flatten the images into one dimensional vectors 
1	142	9316	our best model contains 10 convolutional layers each with zero padding to keep the size of the original image and we used relu as the activation function the convolution filters are matrix of weights that slides through the original image to pick up patterns for prediction the cam requires our model to only have one fully connected fc layer and a global average pool gap layer before that 	our best model contains 10 convolutional layers each with zero padding to keep the size of the original image and we used relu as the activation function 
1	142	9317	our best model contains 10 convolutional layers each with zero padding to keep the size of the original image and we used relu as the activation function the convolution filters are matrix of weights that slides through the original image to pick up patterns for prediction the cam requires our model to only have one fully connected fc layer and a global average pool gap layer before that 	the convolution filters are matrix of weights that slides through the original image to pick up patterns for prediction 
0	142	9318	our best model contains 10 convolutional layers each with zero padding to keep the size of the original image and we used relu as the activation function the convolution filters are matrix of weights that slides through the original image to pick up patterns for prediction the cam requires our model to only have one fully connected fc layer and a global average pool gap layer before that 	the cam requires our model to only have one fully connected fc layer and a global average pool gap layer before that 
0	142	9319	our best model contains 10 convolutional layers each with zero padding to keep the size of the original image and we used relu as the activation function the convolution filters are matrix of weights that slides through the original image to pick up patterns for prediction the cam requires our model to only have one fully connected fc layer and a global average pool gap layer before that 	the gap layer takes the average of the output for each of the convolution filters 
0	142	9320	our best model contains 10 convolutional layers each with zero padding to keep the size of the original image and we used relu as the activation function the convolution filters are matrix of weights that slides through the original image to pick up patterns for prediction the cam requires our model to only have one fully connected fc layer and a global average pool gap layer before that 	the fc layer connects the flattened averages to the two classes 
1	142	9321	our best model contains 10 convolutional layers each with zero padding to keep the size of the original image and we used relu as the activation function the convolution filters are matrix of weights that slides through the original image to pick up patterns for prediction the cam requires our model to only have one fully connected fc layer and a global average pool gap layer before that 	the model was trained with an adam optimizer with 0 0001 learning rate on 20 epochs 
1	142	9322	the second part of our project is to build a weakly supervised model that can predict the localization of pneumonia on the positively classified images without the training labels of the locations 	the second part of our project is to build a weakly supervised model that can predict the localization of pneumonia on the positively classified images without the training labels of the locations 
1	142	9323	to generate region proposals we slide a small network over the convolutional feature map output by the last shared convolutional layer this layer comes from the classifier that is trained on predicting pneumonia this small network takes in a small spatial window from the cnn feature map and predicts whether or not these windows contain pneumonia or not pneumonia 	to generate region proposals we slide a small network over the convolutional feature map output by the last shared convolutional layer 
0	142	9324	to generate region proposals we slide a small network over the convolutional feature map output by the last shared convolutional layer this layer comes from the classifier that is trained on predicting pneumonia this small network takes in a small spatial window from the cnn feature map and predicts whether or not these windows contain pneumonia or not pneumonia 	this layer comes from the classifier that is trained on predicting pneumonia 
1	142	9325	to generate region proposals we slide a small network over the convolutional feature map output by the last shared convolutional layer this layer comes from the classifier that is trained on predicting pneumonia this small network takes in a small spatial window from the cnn feature map and predicts whether or not these windows contain pneumonia or not pneumonia 	this small network takes in a small spatial window from the cnn feature map and predicts whether or not these windows contain pneumonia or not pneumonia 
0	142	9326	to generate region proposals we slide a small network over the convolutional feature map output by the last shared convolutional layer this layer comes from the classifier that is trained on predicting pneumonia this small network takes in a small spatial window from the cnn feature map and predicts whether or not these windows contain pneumonia or not pneumonia 	a window is defined as having four coordinates x1 y1 x2 y2 
1	142	9327	to generate region proposals we slide a small network over the convolutional feature map output by the last shared convolutional layer this layer comes from the classifier that is trained on predicting pneumonia this small network takes in a small spatial window from the cnn feature map and predicts whether or not these windows contain pneumonia or not pneumonia 	we only keep the windows that are classified as having pneumonia and by how much these spatial windows overlap with the ground truth labels 
1	142	9328	to generate region proposals we slide a small network over the convolutional feature map output by the last shared convolutional layer this layer comes from the classifier that is trained on predicting pneumonia this small network takes in a small spatial window from the cnn feature map and predicts whether or not these windows contain pneumonia or not pneumonia 	then for each spatial window the features from the original cnn feature map is mapped back to the cnn feature map from the classifier and these windows are pooled to the same size and are feed to two networks one network to predict class background or pneumonia and another network to predict the coordinates figure 3 b 
0	142	9329	our weakly supervised portion of the model consists of the following components figure 3c 	our weakly supervised portion of the model consists of the following components figure 3c 
1	142	9330	a cam that takes in the output of the final cnn model and the fc layer weights for the pneumonia class neuron and sums up the weighted outputs using the following formula where x is the input image features f k give the output from the last convolution layer given x and w c k is the fully connect weight for the k th filter output to class c in our case class c is the pneumonia class 	a cam that takes in the output of the final cnn model and the fc layer weights for the pneumonia class neuron and sums up the weighted outputs using the following formula where x is the input image features f k give the output from the last convolution layer given x and w c k is the fully connect weight for the k th filter output to class c in our case class c is the pneumonia class 
1	142	9331	the output from cam was then scaled into a 3 channel rgb heatmap 	the output from cam was then scaled into a 3 channel rgb heatmap 
1	142	9332	to find individual clusters of predictions on the heatmap we applied a depth first search clustering algorithm algorithm 1 on a random non zero pixel on the heatmap and repeated until all non zero pixels are clustered algorithm 1 dfs cluster algorithm class index 0 while still exist non zero pixel without class label do pick a random non zero pixel without class label assign pixel to class index for each neighbor pixel do if if neighbor is also a non zero pixel without class then recursively apply dfs end end end class index 1	to find individual clusters of predictions on the heatmap we applied a depth first search clustering algorithm algorithm 1 on a random non zero pixel on the heatmap and repeated until all non zero pixels are clustered algorithm 1 dfs cluster algorithm class index 0 while still exist non zero pixel without class label do pick a random non zero pixel without class label assign pixel to class index for each neighbor pixel do if if neighbor is also a non zero pixel without class then recursively apply dfs end end end class index 1
1	142	9333	lastly we drew a bounding box around each clusters by finding the minimum and maximum x y coordinates of the clusters and only kept boxes that are within 2 standard deviations of all predictions 	lastly we drew a bounding box around each clusters by finding the minimum and maximum x y coordinates of the clusters and only kept boxes that are within 2 standard deviations of all predictions 
1	142	9334	input features running any localizing model on the original x rays yielded low iou scores we discovered that often time the model will localize matter denser than the lung such as muscle tissue and equipment as pneumonia positive figure 4 right and since a high iou score correspond to a more accurate and tightly fitted localization we experimented with a number of approaches to increase our iou score we started by only feeding in the segmented lungs from u net as the input for our model 	input features running any localizing model on the original x rays yielded low iou scores 
1	142	9335	input features running any localizing model on the original x rays yielded low iou scores we discovered that often time the model will localize matter denser than the lung such as muscle tissue and equipment as pneumonia positive figure 4 right and since a high iou score correspond to a more accurate and tightly fitted localization we experimented with a number of approaches to increase our iou score we started by only feeding in the segmented lungs from u net as the input for our model 	we discovered that often time the model will localize matter denser than the lung such as muscle tissue and equipment as pneumonia positive figure 4 right 
1	142	9336	input features running any localizing model on the original x rays yielded low iou scores we discovered that often time the model will localize matter denser than the lung such as muscle tissue and equipment as pneumonia positive figure 4 right and since a high iou score correspond to a more accurate and tightly fitted localization we experimented with a number of approaches to increase our iou score we started by only feeding in the segmented lungs from u net as the input for our model 	and since a high iou score correspond to a more accurate and tightly fitted localization we experimented with a number of approaches to increase our iou score we started by only feeding in the segmented lungs from u net as the input for our model 
0	142	9337	input features running any localizing model on the original x rays yielded low iou scores we discovered that often time the model will localize matter denser than the lung such as muscle tissue and equipment as pneumonia positive figure 4 right and since a high iou score correspond to a more accurate and tightly fitted localization we experimented with a number of approaches to increase our iou score we started by only feeding in the segmented lungs from u net as the input for our model 	though initially the classification results were promising the iou score did not improve significantly 
1	142	9338	input features running any localizing model on the original x rays yielded low iou scores we discovered that often time the model will localize matter denser than the lung such as muscle tissue and equipment as pneumonia positive figure 4 right and since a high iou score correspond to a more accurate and tightly fitted localization we experimented with a number of approaches to increase our iou score we started by only feeding in the segmented lungs from u net as the input for our model 	we soon discovered that in instances of sever pneumonia infection where the density of that part of the lung and surrounding tissue were almost identical the u net algorithm segmented out that part of the lung 
1	142	9339	input features running any localizing model on the original x rays yielded low iou scores we discovered that often time the model will localize matter denser than the lung such as muscle tissue and equipment as pneumonia positive figure 4 right and since a high iou score correspond to a more accurate and tightly fitted localization we experimented with a number of approaches to increase our iou score we started by only feeding in the segmented lungs from u net as the input for our model 	this in turn yielded inaccurate localization results where the algorithm localizes on the healthy part of the lung figure 4 left mid after testing out different combination we found the best results can be achieved when we use both the original and segmented image simultaneously by running them through two channels of the network 
1	142	9340	input features running any localizing model on the original x rays yielded low iou scores we discovered that often time the model will localize matter denser than the lung such as muscle tissue and equipment as pneumonia positive figure 4 right and since a high iou score correspond to a more accurate and tightly fitted localization we experimented with a number of approaches to increase our iou score we started by only feeding in the segmented lungs from u net as the input for our model 	we hypothesize that including the segmented healthy part of the lung provides the model with extra information on where the likely locations of the lung opacity 
0	142	9341	we started our classification model using the vgg16 model which includes 26 layers and 138 357 544 weights it was clear that the large number of weights caused our model to quickly overfit with training accuracy as high as 96 but 72 for validation we then modified the architecture by removing layer by layer until bias of the model dropped but still were able to make above 90 accuracy on the validation set then we tested different filter sizes of the convolutional layers to improve the classification 	we started our classification model using the vgg16 model which includes 26 layers and 138 357 544 weights 
0	142	9342	we started our classification model using the vgg16 model which includes 26 layers and 138 357 544 weights it was clear that the large number of weights caused our model to quickly overfit with training accuracy as high as 96 but 72 for validation we then modified the architecture by removing layer by layer until bias of the model dropped but still were able to make above 90 accuracy on the validation set then we tested different filter sizes of the convolutional layers to improve the classification 	it was clear that the large number of weights caused our model to quickly overfit with training accuracy as high as 96 but 72 for validation 
1	142	9343	we started our classification model using the vgg16 model which includes 26 layers and 138 357 544 weights it was clear that the large number of weights caused our model to quickly overfit with training accuracy as high as 96 but 72 for validation we then modified the architecture by removing layer by layer until bias of the model dropped but still were able to make above 90 accuracy on the validation set then we tested different filter sizes of the convolutional layers to improve the classification 	we then modified the architecture by removing layer by layer until bias of the model dropped but still were able to make above 90 accuracy on the validation set then we tested different filter sizes of the convolutional layers to improve the classification 
1	142	9344	we started our classification model using the vgg16 model which includes 26 layers and 138 357 544 weights it was clear that the large number of weights caused our model to quickly overfit with training accuracy as high as 96 but 72 for validation we then modified the architecture by removing layer by layer until bias of the model dropped but still were able to make above 90 accuracy on the validation set then we tested different filter sizes of the convolutional layers to improve the classification 	our highest validation accuracy was achieved starting with small 3x3 filters and gradually increase the filter size to 16x16 at the last layer 
1	142	9345	we started our classification model using the vgg16 model which includes 26 layers and 138 357 544 weights it was clear that the large number of weights caused our model to quickly overfit with training accuracy as high as 96 but 72 for validation we then modified the architecture by removing layer by layer until bias of the model dropped but still were able to make above 90 accuracy on the validation set then we tested different filter sizes of the convolutional layers to improve the classification 	however a big filter size at the last convolution layer gave us imprecise prediction of the the bounding boxes and caused a sharp decrease in the iou score 
1	142	9346	we started our classification model using the vgg16 model which includes 26 layers and 138 357 544 weights it was clear that the large number of weights caused our model to quickly overfit with training accuracy as high as 96 but 72 for validation we then modified the architecture by removing layer by layer until bias of the model dropped but still were able to make above 90 accuracy on the validation set then we tested different filter sizes of the convolutional layers to improve the classification 	therefore we decided to sacrifice some classification accuracy for an increased iou score by keeping all filters to 3x3 finally different optimizers were tested to train our model 
0	142	9347	we started our classification model using the vgg16 model which includes 26 layers and 138 357 544 weights it was clear that the large number of weights caused our model to quickly overfit with training accuracy as high as 96 but 72 for validation we then modified the architecture by removing layer by layer until bias of the model dropped but still were able to make above 90 accuracy on the validation set then we tested different filter sizes of the convolutional layers to improve the classification 	the standard stochastic gradient descent algorithm trains very slowly and does not converge to above 85 accuracy 
0	142	9348	we started our classification model using the vgg16 model which includes 26 layers and 138 357 544 weights it was clear that the large number of weights caused our model to quickly overfit with training accuracy as high as 96 but 72 for validation we then modified the architecture by removing layer by layer until bias of the model dropped but still were able to make above 90 accuracy on the validation set then we tested different filter sizes of the convolutional layers to improve the classification 	adam and adagrad converge faster but adam achieved a higher validation accuracy 
0	142	9349	we started our classification model using the vgg16 model which includes 26 layers and 138 357 544 weights it was clear that the large number of weights caused our model to quickly overfit with training accuracy as high as 96 but 72 for validation we then modified the architecture by removing layer by layer until bias of the model dropped but still were able to make above 90 accuracy on the validation set then we tested different filter sizes of the convolutional layers to improve the classification 	we also tested out different learning rates 0 001 0001 0 0001 for each optimizer 
0	142	9350	we started our classification model using the vgg16 model which includes 26 layers and 138 357 544 weights it was clear that the large number of weights caused our model to quickly overfit with training accuracy as high as 96 but 72 for validation we then modified the architecture by removing layer by layer until bias of the model dropped but still were able to make above 90 accuracy on the validation set then we tested different filter sizes of the convolutional layers to improve the classification 	learning rate of 0 001 caused the weights to blow up and achieve lower than 50 accuracy 
1	142	9351	we started our classification model using the vgg16 model which includes 26 layers and 138 357 544 weights it was clear that the large number of weights caused our model to quickly overfit with training accuracy as high as 96 but 72 for validation we then modified the architecture by removing layer by layer until bias of the model dropped but still were able to make above 90 accuracy on the validation set then we tested different filter sizes of the convolutional layers to improve the classification 	however a learning rate of 0 0001 with adam optimizer gave us our higher accuracy in the shortest period of time 
0	142	9352	we started our classification model using the vgg16 model which includes 26 layers and 138 357 544 weights it was clear that the large number of weights caused our model to quickly overfit with training accuracy as high as 96 but 72 for validation we then modified the architecture by removing layer by layer until bias of the model dropped but still were able to make above 90 accuracy on the validation set then we tested different filter sizes of the convolutional layers to improve the classification 	formula 2 the iou formula
0	142	9353	our cnn significantly outperformed traditional classifiers without over fitting	our cnn significantly outperformed traditional classifiers without over fitting
1	142	9354	based on our result we have shown that our weakly supervised method is able to localize pneumonia slightly better than a supervised method we predict that our model can perform even better if we have the computing power to train on the full images as a lot of information are lost during compression we also expect improvements by including more training data or transferring learned models from similar works such as chestxnet 	based on our result we have shown that our weakly supervised method is able to localize pneumonia slightly better than a supervised method 
1	142	9355	based on our result we have shown that our weakly supervised method is able to localize pneumonia slightly better than a supervised method we predict that our model can perform even better if we have the computing power to train on the full images as a lot of information are lost during compression we also expect improvements by including more training data or transferring learned models from similar works such as chestxnet 	we predict that our model can perform even better if we have the computing power to train on the full images as a lot of information are lost during compression 
1	142	9356	based on our result we have shown that our weakly supervised method is able to localize pneumonia slightly better than a supervised method we predict that our model can perform even better if we have the computing power to train on the full images as a lot of information are lost during compression we also expect improvements by including more training data or transferring learned models from similar works such as chestxnet 	we also expect improvements by including more training data or transferring learned models from similar works such as chestxnet 
1	142	9357	based on our result we have shown that our weakly supervised method is able to localize pneumonia slightly better than a supervised method we predict that our model can perform even better if we have the computing power to train on the full images as a lot of information are lost during compression we also expect improvements by including more training data or transferring learned models from similar works such as chestxnet 	if improved to human level performance our weakly supervised model not only can automate pneumonia location annotation and classification tasks but also can be used to automate other medical image datasets 
0	142	9358	mars huang came up with project idea and methodology build the cnn classifier and tested different architectures modified the classifier to fit in to class actiation mapping 	mars huang came up with project idea and methodology 
0	142	9359	mars huang came up with project idea and methodology build the cnn classifier and tested different architectures modified the classifier to fit in to class actiation mapping 	build the cnn classifier and tested different architectures 
0	142	9360	mars huang came up with project idea and methodology build the cnn classifier and tested different architectures modified the classifier to fit in to class actiation mapping 	modified the classifier to fit in to class actiation mapping 
0	142	9361	mars huang came up with project idea and methodology build the cnn classifier and tested different architectures modified the classifier to fit in to class actiation mapping 	implemented cam dfs clustering algorithm 
0	142	9362	mars huang came up with project idea and methodology build the cnn classifier and tested different architectures modified the classifier to fit in to class actiation mapping 	made functions to draw bounding box calculate iou and feature engineering 
0	142	9363	mars huang came up with project idea and methodology build the cnn classifier and tested different architectures modified the classifier to fit in to class actiation mapping 	tried to implement em and kmeans to cluster heatmap regions 
1	142	9364	mars huang came up with project idea and methodology build the cnn classifier and tested different architectures modified the classifier to fit in to class actiation mapping 	attempted to reduce dimentions of the data for baseline by using factor analysis 
0	142	9365	mars huang came up with project idea and methodology build the cnn classifier and tested different architectures modified the classifier to fit in to class actiation mapping 	tested all baselines for classification portion of the project and experimentation in the classification and weakly supervised localization 
0	142	9366	mars huang came up with project idea and methodology build the cnn classifier and tested different architectures modified the classifier to fit in to class actiation mapping 	created mltoolkit for baselines 
0	142	9367	mars huang came up with project idea and methodology build the cnn classifier and tested different architectures modified the classifier to fit in to class actiation mapping 	generated all figures major contributed to the paper and poster 
0	142	9368	mars huang came up with project idea and methodology build the cnn classifier and tested different architectures modified the classifier to fit in to class actiation mapping 	set up google cloud medi monam lead in reading literature to gather knowledge in the field 
0	142	9369	mars huang came up with project idea and methodology build the cnn classifier and tested different architectures modified the classifier to fit in to class actiation mapping 	unet segmentation feature engineering 
0	142	9370	mars huang came up with project idea and methodology build the cnn classifier and tested different architectures modified the classifier to fit in to class actiation mapping 	experimented with methods to cluster heatmap islands 
0	142	9371	mars huang came up with project idea and methodology build the cnn classifier and tested different architectures modified the classifier to fit in to class actiation mapping 	experimented with implementation of vgg16 
0	142	9372	mars huang came up with project idea and methodology build the cnn classifier and tested different architectures modified the classifier to fit in to class actiation mapping 	contributed to poster and paper 
0	142	9373	mars huang came up with project idea and methodology build the cnn classifier and tested different architectures modified the classifier to fit in to class actiation mapping 	printed poster emanuel cortes built the supervised model for classification and localization 
1	142	9374	mars huang came up with project idea and methodology build the cnn classifier and tested different architectures modified the classifier to fit in to class actiation mapping 	implemented a resnet backbone classifier custom region proposal layer roi pooling and a bounding box regressor that is pretrained on the coco dataset and finetuned on kaggles rsna pneumonia detection competition dataset 
0	142	9375	mars huang came up with project idea and methodology build the cnn classifier and tested different architectures modified the classifier to fit in to class actiation mapping 	experimented with feeding other cnn based backbone classifier architectures whose out 
1	143	9376	we compared traditional machine learning methods naive bayes linear discriminant analysis knearest neighbors random forest support vector machine and deep learning methods convolutional neural networks in ship detection of satellite images we found that among all traditional methods we have tried random forest gave the best performance 93 accuracy among deep learning approaches the simple train from scratch cnn model achieve 94 accuracy which outperforms the pre trained cnn model using transfer learning 	we compared traditional machine learning methods naive bayes linear discriminant analysis knearest neighbors random forest support vector machine and deep learning methods convolutional neural networks in ship detection of satellite images 
0	143	9377	we compared traditional machine learning methods naive bayes linear discriminant analysis knearest neighbors random forest support vector machine and deep learning methods convolutional neural networks in ship detection of satellite images we found that among all traditional methods we have tried random forest gave the best performance 93 accuracy among deep learning approaches the simple train from scratch cnn model achieve 94 accuracy which outperforms the pre trained cnn model using transfer learning 	we found that among all traditional methods we have tried random forest gave the best performance 93 accuracy 
1	143	9378	we compared traditional machine learning methods naive bayes linear discriminant analysis knearest neighbors random forest support vector machine and deep learning methods convolutional neural networks in ship detection of satellite images we found that among all traditional methods we have tried random forest gave the best performance 93 accuracy among deep learning approaches the simple train from scratch cnn model achieve 94 accuracy which outperforms the pre trained cnn model using transfer learning 	among deep learning approaches the simple train from scratch cnn model achieve 94 accuracy which outperforms the pre trained cnn model using transfer learning 
1	143	9379	the fast growing shipping traffic increases the chances of infractions at sea such as environmentally devastating ship accidents piracy illegal fishing drug trafficking and illegal cargo movement comprehensive maritime monitoring services helps support the maritime industry to increase knowledge anticipate threats trigger alerts and improve efficiency at sea this challenge origins partly from the airbus ship detection challenge on kaggle 	the fast growing shipping traffic increases the chances of infractions at sea such as environmentally devastating ship accidents piracy illegal fishing drug trafficking and illegal cargo movement 
1	143	9380	the fast growing shipping traffic increases the chances of infractions at sea such as environmentally devastating ship accidents piracy illegal fishing drug trafficking and illegal cargo movement comprehensive maritime monitoring services helps support the maritime industry to increase knowledge anticipate threats trigger alerts and improve efficiency at sea this challenge origins partly from the airbus ship detection challenge on kaggle 	comprehensive maritime monitoring services helps support the maritime industry to increase knowledge anticipate threats trigger alerts and improve efficiency at sea 
1	143	9381	the fast growing shipping traffic increases the chances of infractions at sea such as environmentally devastating ship accidents piracy illegal fishing drug trafficking and illegal cargo movement comprehensive maritime monitoring services helps support the maritime industry to increase knowledge anticipate threats trigger alerts and improve efficiency at sea this challenge origins partly from the airbus ship detection challenge on kaggle 	this challenge origins partly from the airbus ship detection challenge on kaggle 
0	143	9382	the fast growing shipping traffic increases the chances of infractions at sea such as environmentally devastating ship accidents piracy illegal fishing drug trafficking and illegal cargo movement comprehensive maritime monitoring services helps support the maritime industry to increase knowledge anticipate threats trigger alerts and improve efficiency at sea this challenge origins partly from the airbus ship detection challenge on kaggle 	we plan to come up with a solution to efficiently detect ship in satellite images 
0	143	9383	the fast growing shipping traffic increases the chances of infractions at sea such as environmentally devastating ship accidents piracy illegal fishing drug trafficking and illegal cargo movement comprehensive maritime monitoring services helps support the maritime industry to increase knowledge anticipate threats trigger alerts and improve efficiency at sea this challenge origins partly from the airbus ship detection challenge on kaggle 	this classification is challenging because boats are really small in the satellite images 
1	143	9384	the fast growing shipping traffic increases the chances of infractions at sea such as environmentally devastating ship accidents piracy illegal fishing drug trafficking and illegal cargo movement comprehensive maritime monitoring services helps support the maritime industry to increase knowledge anticipate threats trigger alerts and improve efficiency at sea this challenge origins partly from the airbus ship detection challenge on kaggle 	various scenes including open water wharf buildings and clouds appear in the dataset in this paper we compared traditional methods and deep learning methods in solving the classification problem 
1	143	9385	the fast growing shipping traffic increases the chances of infractions at sea such as environmentally devastating ship accidents piracy illegal fishing drug trafficking and illegal cargo movement comprehensive maritime monitoring services helps support the maritime industry to increase knowledge anticipate threats trigger alerts and improve efficiency at sea this challenge origins partly from the airbus ship detection challenge on kaggle 	for traditional methods we experimented with naive bayes linear discriminant analysis k nearest neighbors random forest and support vector machine model 
1	143	9386	the fast growing shipping traffic increases the chances of infractions at sea such as environmentally devastating ship accidents piracy illegal fishing drug trafficking and illegal cargo movement comprehensive maritime monitoring services helps support the maritime industry to increase knowledge anticipate threats trigger alerts and improve efficiency at sea this challenge origins partly from the airbus ship detection challenge on kaggle 	before training these models we did image features extraction by finding global feature descriptors of every image which includes color histogram hu moments haralick texture and histogram of oriented gradients hog 
0	143	9387	the fast growing shipping traffic increases the chances of infractions at sea such as environmentally devastating ship accidents piracy illegal fishing drug trafficking and illegal cargo movement comprehensive maritime monitoring services helps support the maritime industry to increase knowledge anticipate threats trigger alerts and improve efficiency at sea this challenge origins partly from the airbus ship detection challenge on kaggle 	we found that feature engineering significantly improved the performance of traditional models 
1	143	9388	the fast growing shipping traffic increases the chances of infractions at sea such as environmentally devastating ship accidents piracy illegal fishing drug trafficking and illegal cargo movement comprehensive maritime monitoring services helps support the maritime industry to increase knowledge anticipate threats trigger alerts and improve efficiency at sea this challenge origins partly from the airbus ship detection challenge on kaggle 	we also noticed that for some model certain com for deep learning method we used a pre trained network densenet 169 of imagenet architecture as the baseline network referred as tl cnn 
1	143	9389	the fast growing shipping traffic increases the chances of infractions at sea such as environmentally devastating ship accidents piracy illegal fishing drug trafficking and illegal cargo movement comprehensive maritime monitoring services helps support the maritime industry to increase knowledge anticipate threats trigger alerts and improve efficiency at sea this challenge origins partly from the airbus ship detection challenge on kaggle 	we then designed a simple cnn model consisting of 4 convolutional layers and 4 maxpooling layers referred as sim cnn without using transfer learning approach 
1	143	9390	the fast growing shipping traffic increases the chances of infractions at sea such as environmentally devastating ship accidents piracy illegal fishing drug trafficking and illegal cargo movement comprehensive maritime monitoring services helps support the maritime industry to increase knowledge anticipate threats trigger alerts and improve efficiency at sea this challenge origins partly from the airbus ship detection challenge on kaggle 	we observed that the simple train from scratch cnn model worked better than the pre trained cnn model 
0	143	9391	recently machine learning and artificial intelligence have attracted increasing attention and achieved great success in different areas including computer vision when narrow down to the problem of ship detection there are no research papers studying this problem since it is a recent kaggle challenge problem however there are several attempts made by some kaggle users for example kevin mader	recently machine learning and artificial intelligence have attracted increasing attention and achieved great success in different areas including computer vision when narrow down to the problem of ship detection there are no research papers studying this problem since it is a recent kaggle challenge problem 
0	143	9392	recently machine learning and artificial intelligence have attracted increasing attention and achieved great success in different areas including computer vision when narrow down to the problem of ship detection there are no research papers studying this problem since it is a recent kaggle challenge problem however there are several attempts made by some kaggle users for example kevin mader	however there are several attempts made by some kaggle users 
0	143	9393	recently machine learning and artificial intelligence have attracted increasing attention and achieved great success in different areas including computer vision when narrow down to the problem of ship detection there are no research papers studying this problem since it is a recent kaggle challenge problem however there are several attempts made by some kaggle users for example kevin mader	for example kevin mader
1	143	9394	we obtained a public dataset provided on the airbus ship detection challenge website	we obtained a public dataset provided on the airbus ship detection challenge website
1	143	9395	first of all we re sized the original image to the size of 256 256 3 then we applied different data preprocessing techniques for traditional machine learning algorithms and deep learning algorithms 	first of all we re sized the original image to the size of 256 256 3 then we applied different data preprocessing techniques for traditional machine learning algorithms and deep learning algorithms 
1	143	9396	we used hand engineering features extraction methods gogul09 2017 to obtain three different global features for traditional ml algorithms the images were converted to grayscale for hu and ha and to hsv color space for his before extraction as shown in hu moments hu features are used to captured the general shape information hu moment or image moment is a certain particular weighted average moment of the image pixels intensities 	we used hand engineering features extraction methods gogul09 2017 to obtain three different global features for traditional ml algorithms 
1	143	9397	we used hand engineering features extraction methods gogul09 2017 to obtain three different global features for traditional ml algorithms the images were converted to grayscale for hu and ha and to hsv color space for his before extraction as shown in hu moments hu features are used to captured the general shape information hu moment or image moment is a certain particular weighted average moment of the image pixels intensities 	the images were converted to grayscale for hu and ha and to hsv color space for his before extraction as shown in hu moments hu features are used to captured the general shape information 
0	143	9398	we used hand engineering features extraction methods gogul09 2017 to obtain three different global features for traditional ml algorithms the images were converted to grayscale for hu and ha and to hsv color space for his before extraction as shown in hu moments hu features are used to captured the general shape information hu moment or image moment is a certain particular weighted average moment of the image pixels intensities 	hu moment or image moment is a certain particular weighted average moment of the image pixels intensities 
1	143	9399	we used hand engineering features extraction methods gogul09 2017 to obtain three different global features for traditional ml algorithms the images were converted to grayscale for hu and ha and to hsv color space for his before extraction as shown in hu moments hu features are used to captured the general shape information hu moment or image moment is a certain particular weighted average moment of the image pixels intensities 	simple properties of the image which are found via image moments include area or total intensity centroid and information about its orientation 
1	143	9400	to enhance the robustness of our cnn model for all the images in the training set we implemented data augmentation method such as rotation shifting adjusting brightness shearing intensity zooming and flipping data augmentation can improve the models ability to generalize and correctly label images with some sort of distortion which can be regarded as adding noise to the data to reduce variance 	to enhance the robustness of our cnn model for all the images in the training set we implemented data augmentation method such as rotation shifting adjusting brightness shearing intensity zooming and flipping 
1	143	9401	to enhance the robustness of our cnn model for all the images in the training set we implemented data augmentation method such as rotation shifting adjusting brightness shearing intensity zooming and flipping data augmentation can improve the models ability to generalize and correctly label images with some sort of distortion which can be regarded as adding noise to the data to reduce variance 	data augmentation can improve the models ability to generalize and correctly label images with some sort of distortion which can be regarded as adding noise to the data to reduce variance 
1	143	9402	to classify whether an image contains ships or not several standard machine learning algorithms and deep learning algorithms were implemented we compared different approaches to evaluate how different model performed for this specific task for all the algorithms the feature vector is x x 1 	to classify whether an image contains ships or not several standard machine learning algorithms and deep learning algorithms were implemented 
0	143	9403	to classify whether an image contains ships or not several standard machine learning algorithms and deep learning algorithms were implemented we compared different approaches to evaluate how different model performed for this specific task for all the algorithms the feature vector is x x 1 	we compared different approaches to evaluate how different model performed for this specific task 
0	143	9404	to classify whether an image contains ships or not several standard machine learning algorithms and deep learning algorithms were implemented we compared different approaches to evaluate how different model performed for this specific task for all the algorithms the feature vector is x x 1 	for all the algorithms the feature vector is x x 1 
0	143	9405	to classify whether an image contains ships or not several standard machine learning algorithms and deep learning algorithms were implemented we compared different approaches to evaluate how different model performed for this specific task for all the algorithms the feature vector is x x 1 	 x n representing n features from the flattened original image or from feature engineering 
1	143	9406	the algorithm finds a linear combination of features that characterizes and separates two classes it assumes that the conditional probability density functions p x y 0 and p x y 0 are both normally distributed with mean and co variance parameters 0 0 and 1 1 lda makes simplifying assumption that the co variances are identical 0 1 and the variances have full rank 	the algorithm finds a linear combination of features that characterizes and separates two classes 
0	143	9407	the algorithm finds a linear combination of features that characterizes and separates two classes it assumes that the conditional probability density functions p x y 0 and p x y 0 are both normally distributed with mean and co variance parameters 0 0 and 1 1 lda makes simplifying assumption that the co variances are identical 0 1 and the variances have full rank 	it assumes that the conditional probability density functions p x y 0 and p x y 0 are both normally distributed with mean and co variance parameters 0 0 and 1 1 
1	143	9408	the algorithm finds a linear combination of features that characterizes and separates two classes it assumes that the conditional probability density functions p x y 0 and p x y 0 are both normally distributed with mean and co variance parameters 0 0 and 1 1 lda makes simplifying assumption that the co variances are identical 0 1 and the variances have full rank 	lda makes simplifying assumption that the co variances are identical 0 1 and the variances have full rank 
0	143	9409	the algorithm finds a linear combination of features that characterizes and separates two classes it assumes that the conditional probability density functions p x y 0 and p x y 0 are both normally distributed with mean and co variance parameters 0 0 and 1 1 lda makes simplifying assumption that the co variances are identical 0 1 and the variances have full rank 	after training on data to estimate mean and co variance bayes theorem is applied to predict the probabilities 
1	143	9410	the algorithm classifies an object by a majority vote of its neighbors with the object being assigned to the class that is most common among its 5 nearest neighbors the distance metric uses standard euclidean distance as	the algorithm classifies an object by a majority vote of its neighbors with the object being assigned to the class that is most common among its 5 nearest neighbors 
0	143	9411	the algorithm classifies an object by a majority vote of its neighbors with the object being assigned to the class that is most common among its 5 nearest neighbors the distance metric uses standard euclidean distance as	the distance metric uses standard euclidean distance as
0	143	9412	naive bayes is a conditional probability model to determine whether there is a ship in the image given a feature vector x x 1 	naive bayes is a conditional probability model 
0	143	9413	naive bayes is a conditional probability model to determine whether there is a ship in the image given a feature vector x x 1 	to determine whether there is a ship in the image given a feature vector x x 1 
0	143	9414	naive bayes is a conditional probability model to determine whether there is a ship in the image given a feature vector x x 1 	 x n the probability of c 0 no ships and c 1 has ships is p c k x 1 
1	143	9415	naive bayes is a conditional probability model to determine whether there is a ship in the image given a feature vector x x 1 	with the naive conditional independent assumption the joint model can be expressedthe algorithm is an ensemble learning method 
1	143	9416	naive bayes is a conditional probability model to determine whether there is a ship in the image given a feature vector x x 1 	bootstrap samples are selected from the training data and then the model learns classification trees using only some subset of the features at random instead of examining all possible feature splits 
1	143	9417	naive bayes is a conditional probability model to determine whether there is a ship in the image given a feature vector x x 1 	after training prediction is made by taking the majority vote of the learned classification trees 
0	143	9418	naive bayes is a conditional probability model to determine whether there is a ship in the image given a feature vector x x 1 	the depth of tree is limited by 5 and the number of trees is 10 
1	143	9419	the algorithm finds the maximum margin between different classes by determining the weights and bias of the separating hyperplane the soft margin svm classifier minimizes the loss asthe fit time complexity is more than quadratic with the number of samples 	the algorithm finds the maximum margin between different classes by determining the weights and bias of the separating hyperplane 
1	143	9420	the algorithm finds the maximum margin between different classes by determining the weights and bias of the separating hyperplane the soft margin svm classifier minimizes the loss asthe fit time complexity is more than quadratic with the number of samples 	the soft margin svm classifier minimizes the loss asthe fit time complexity is more than quadratic with the number of samples 
0	143	9421	convolutional neural network cnn which is prevailing in the area of computer vision is proved to be extremely powerful in learning effective feature representations from a large number of data it is capable of extracting the underlying structure features of the data which produce better representation than hand crafted features since the learned features adapt better to the tasks at hand in our project we experimented two different cnn models 	convolutional neural network cnn which is prevailing in the area of computer vision is proved to be extremely powerful in learning effective feature representations from a large number of data 
1	143	9422	convolutional neural network cnn which is prevailing in the area of computer vision is proved to be extremely powerful in learning effective feature representations from a large number of data it is capable of extracting the underlying structure features of the data which produce better representation than hand crafted features since the learned features adapt better to the tasks at hand in our project we experimented two different cnn models 	it is capable of extracting the underlying structure features of the data which produce better representation than hand crafted features since the learned features adapt better to the tasks at hand 
0	143	9423	convolutional neural network cnn which is prevailing in the area of computer vision is proved to be extremely powerful in learning effective feature representations from a large number of data it is capable of extracting the underlying structure features of the data which produce better representation than hand crafted features since the learned features adapt better to the tasks at hand in our project we experimented two different cnn models 	in our project we experimented two different cnn models 
1	143	9424	the motivation of using a transfer learning technique is because cnns are very good feature extractors this means that you can extract useful attributes from an already trained cnn with its trained weights hence a cnn with pretrained network can provide a reasonable baseline result as mentioned in section 2 we reproduced the so called transfer learning cnn tl cnn as our baseline model we transferred the dense169 	the motivation of using a transfer learning technique is because cnns are very good feature extractors this means that you can extract useful attributes from an already trained cnn with its trained weights 
0	143	9425	the motivation of using a transfer learning technique is because cnns are very good feature extractors this means that you can extract useful attributes from an already trained cnn with its trained weights hence a cnn with pretrained network can provide a reasonable baseline result as mentioned in section 2 we reproduced the so called transfer learning cnn tl cnn as our baseline model we transferred the dense169 	hence a cnn with pretrained network can provide a reasonable baseline result as mentioned in section 2 we reproduced the so called transfer learning cnn tl cnn as our baseline model 
0	143	9426	the motivation of using a transfer learning technique is because cnns are very good feature extractors this means that you can extract useful attributes from an already trained cnn with its trained weights hence a cnn with pretrained network can provide a reasonable baseline result as mentioned in section 2 we reproduced the so called transfer learning cnn tl cnn as our baseline model we transferred the dense169 	we transferred the dense169 
1	143	9427	instead of using a pre trained cnn model we decided to construct a simple cnn model named sim cnn with a few layers and trained it from scratch one might hope that this could improve the performance of tl cnn since its cnn weights were trained specifically by our dataset the	instead of using a pre trained cnn model we decided to construct a simple cnn model named sim cnn with a few layers and trained it from scratch 
1	143	9428	instead of using a pre trained cnn model we decided to construct a simple cnn model named sim cnn with a few layers and trained it from scratch one might hope that this could improve the performance of tl cnn since its cnn weights were trained specifically by our dataset the	one might hope that this could improve the performance of tl cnn since its cnn weights were trained specifically by our dataset the
1	143	9429	for the traditional machine learning algorithms discussed in section we applied sklearn package in python to realize each algorithms for the deep learning approach the two cnn models were trained in microsoft azure cloud with gpus cross entropy loss was used we trained the model for 30 epochs using mini batch size 64 with batch normalization 	for the traditional machine learning algorithms discussed in section we applied sklearn package in python to realize each algorithms for the deep learning approach the two cnn models were trained in microsoft azure cloud with gpus 
0	143	9430	for the traditional machine learning algorithms discussed in section we applied sklearn package in python to realize each algorithms for the deep learning approach the two cnn models were trained in microsoft azure cloud with gpus cross entropy loss was used we trained the model for 30 epochs using mini batch size 64 with batch normalization 	cross entropy loss was used 
1	143	9431	for the traditional machine learning algorithms discussed in section we applied sklearn package in python to realize each algorithms for the deep learning approach the two cnn models were trained in microsoft azure cloud with gpus cross entropy loss was used we trained the model for 30 epochs using mini batch size 64 with batch normalization 	we trained the model for 30 epochs using mini batch size 64 with batch normalization 
1	143	9432	for the traditional machine learning algorithms discussed in section we applied sklearn package in python to realize each algorithms for the deep learning approach the two cnn models were trained in microsoft azure cloud with gpus cross entropy loss was used we trained the model for 30 epochs using mini batch size 64 with batch normalization 	we applied adam optimizer as well as decaying learning rate to facilitate model training 
0	143	9433	for the traditional machine learning algorithms discussed in section we applied sklearn package in python to realize each algorithms for the deep learning approach the two cnn models were trained in microsoft azure cloud with gpus cross entropy loss was used we trained the model for 30 epochs using mini batch size 64 with batch normalization 	the weights of model would not be updated if the loss of development set did not improve after training an epoch 
0	143	9434	for the traditional machine learning algorithms discussed in section we applied sklearn package in python to realize each algorithms for the deep learning approach the two cnn models were trained in microsoft azure cloud with gpus cross entropy loss was used we trained the model for 30 epochs using mini batch size 64 with batch normalization 	comparing performance with and without feature engineering we found that feature engineering significantly improved performance in general 
1	143	9435	for the traditional machine learning algorithms discussed in section we applied sklearn package in python to realize each algorithms for the deep learning approach the two cnn models were trained in microsoft azure cloud with gpus cross entropy loss was used we trained the model for 30 epochs using mini batch size 64 with batch normalization 	instead of directly training on image pixels information the information extracted from feature engineering amplified the signal of whether an image containing ships especially for nb and svm approaches 
1	143	9436	for the traditional machine learning algorithms discussed in section we applied sklearn package in python to realize each algorithms for the deep learning approach the two cnn models were trained in microsoft azure cloud with gpus cross entropy loss was used we trained the model for 30 epochs using mini batch size 64 with batch normalization 	however since 0 84 of the images in test set are labeled as no ships among those traditional machine learning method only lda and rf outperformed the accuracy of blindly guesting no ships 
1	143	9437	for the traditional machine learning algorithms discussed in section we applied sklearn package in python to realize each algorithms for the deep learning approach the two cnn models were trained in microsoft azure cloud with gpus cross entropy loss was used we trained the model for 30 epochs using mini batch size 64 with batch normalization 	in addition we learned that some algorithms give much better performance when working with only certain combination of features 
0	143	9438	for the traditional machine learning algorithms discussed in section we applied sklearn package in python to realize each algorithms for the deep learning approach the two cnn models were trained in microsoft azure cloud with gpus cross entropy loss was used we trained the model for 30 epochs using mini batch size 64 with batch normalization 	it is suggested that haralick textures information of image is of great importance for nb method improving from 0 42 to 0 75 
1	143	9439	we applied 10 fold cross validation to each machine learning algorithms and create the corresponding box plot in	we applied 10 fold cross validation to each machine learning algorithms and create the corresponding box plot in
0	143	9440	consider our dataset being imbalanced instead of using a threshold of 0 5 we wanted to find one that would take imbalance into consideration as suggested by	consider our dataset being imbalanced instead of using a threshold of 0 5 we wanted to find one that would take imbalance into consideration as suggested by
0	143	9441	among all traditional methods random forest gave the best result 0 93 accuracy with feature engineering as for the cnn model our train from scratch sim cnn model outperforms the baseline tl cnn model based on pre trained densenet 169 network in the future for traditional machine learning algorithms we plan to improve feature engineering by extracting global features along with local features such as sift surf or dense which could be used along with bag of visual words bovw technique 	among all traditional methods random forest gave the best result 0 93 accuracy with feature engineering 
1	143	9442	among all traditional methods random forest gave the best result 0 93 accuracy with feature engineering as for the cnn model our train from scratch sim cnn model outperforms the baseline tl cnn model based on pre trained densenet 169 network in the future for traditional machine learning algorithms we plan to improve feature engineering by extracting global features along with local features such as sift surf or dense which could be used along with bag of visual words bovw technique 	as for the cnn model our train from scratch sim cnn model outperforms the baseline tl cnn model based on pre trained densenet 169 network 
1	143	9443	among all traditional methods random forest gave the best result 0 93 accuracy with feature engineering as for the cnn model our train from scratch sim cnn model outperforms the baseline tl cnn model based on pre trained densenet 169 network in the future for traditional machine learning algorithms we plan to improve feature engineering by extracting global features along with local features such as sift surf or dense which could be used along with bag of visual words bovw technique 	in the future for traditional machine learning algorithms we plan to improve feature engineering by extracting global features along with local features such as sift surf or dense which could be used along with bag of visual words bovw technique 
1	143	9444	among all traditional methods random forest gave the best result 0 93 accuracy with feature engineering as for the cnn model our train from scratch sim cnn model outperforms the baseline tl cnn model based on pre trained densenet 169 network in the future for traditional machine learning algorithms we plan to improve feature engineering by extracting global features along with local features such as sift surf or dense which could be used along with bag of visual words bovw technique 	for deep learning algorithms to achieve better performance we will try implementing different networks e g deeper network to train the classifier 
1	143	9445	among all traditional methods random forest gave the best result 0 93 accuracy with feature engineering as for the cnn model our train from scratch sim cnn model outperforms the baseline tl cnn model based on pre trained densenet 169 network in the future for traditional machine learning algorithms we plan to improve feature engineering by extracting global features along with local features such as sift surf or dense which could be used along with bag of visual words bovw technique 	last but not least it is more challenging but also more interesting to try applying segmentation technique to identify the locations of all ships in a image 
1	143	9446	all three members of this team work together and contribute equally to this project in data prepossessing algorithm designing model designing model training and report writing please follow project code or https github com cs229shipdetection cs229project airbus ship detection for the project code 	all three members of this team work together and contribute equally to this project in data prepossessing algorithm designing model designing model training and report writing please follow project code or https github com cs229shipdetection cs229project airbus ship detection for the project code 
0	144	9447	in the modern era an enormous amount of digital pictures from personal photos to medical images is produced and stored every day it is more and more common to have thousands of photos sitting in our smart phones however what comes with the convenience of recording unforgettable moments is the pain of searching for a specific picture or frame how nice it would be to be able to find the desired image just by typing one or few words to describe it 	in the modern era an enormous amount of digital pictures from personal photos to medical images is produced and stored every day 
0	144	9448	in the modern era an enormous amount of digital pictures from personal photos to medical images is produced and stored every day it is more and more common to have thousands of photos sitting in our smart phones however what comes with the convenience of recording unforgettable moments is the pain of searching for a specific picture or frame how nice it would be to be able to find the desired image just by typing one or few words to describe it 	it is more and more common to have thousands of photos sitting in our smart phones however what comes with the convenience of recording unforgettable moments is the pain of searching for a specific picture or frame 
0	144	9449	in the modern era an enormous amount of digital pictures from personal photos to medical images is produced and stored every day it is more and more common to have thousands of photos sitting in our smart phones however what comes with the convenience of recording unforgettable moments is the pain of searching for a specific picture or frame how nice it would be to be able to find the desired image just by typing one or few words to describe it 	how nice it would be to be able to find the desired image just by typing one or few words to describe it 
0	144	9450	in the modern era an enormous amount of digital pictures from personal photos to medical images is produced and stored every day it is more and more common to have thousands of photos sitting in our smart phones however what comes with the convenience of recording unforgettable moments is the pain of searching for a specific picture or frame how nice it would be to be able to find the desired image just by typing one or few words to describe it 	in this context automated captionimage retrieval is becoming an increasingly attracting feature comparable to text search in this project we consider the task of content based image retrieval and propose effective neural network based solutions for that 
0	144	9451	in the modern era an enormous amount of digital pictures from personal photos to medical images is produced and stored every day it is more and more common to have thousands of photos sitting in our smart phones however what comes with the convenience of recording unforgettable moments is the pain of searching for a specific picture or frame how nice it would be to be able to find the desired image just by typing one or few words to describe it 	specifically the input to our algorithm is a collection of raw images in which the user would like to search and a query sentence meant to describe the desired image 
0	144	9452	in the modern era an enormous amount of digital pictures from personal photos to medical images is produced and stored every day it is more and more common to have thousands of photos sitting in our smart phones however what comes with the convenience of recording unforgettable moments is the pain of searching for a specific picture or frame how nice it would be to be able to find the desired image just by typing one or few words to describe it 	the output of the algorithm would be a list of top images that we think are relevant to the query sentence 
0	144	9453	in the modern era an enormous amount of digital pictures from personal photos to medical images is produced and stored every day it is more and more common to have thousands of photos sitting in our smart phones however what comes with the convenience of recording unforgettable moments is the pain of searching for a specific picture or frame how nice it would be to be able to find the desired image just by typing one or few words to describe it 	in particular we train a recurrent neural network to obtain a representation of the sentence that will be properly aligned with the corresponding image features in a shared highdimensional space 
1	144	9454	in the modern era an enormous amount of digital pictures from personal photos to medical images is produced and stored every day it is more and more common to have thousands of photos sitting in our smart phones however what comes with the convenience of recording unforgettable moments is the pain of searching for a specific picture or frame how nice it would be to be able to find the desired image just by typing one or few words to describe it 	the images are found based on nearest neighborhood search in that shared space the paper is organized as follows first we briefly summarize the most relevant work related to our task then we describe the dataset employed for training and the features of our problem 
0	144	9455	in the modern era an enormous amount of digital pictures from personal photos to medical images is produced and stored every day it is more and more common to have thousands of photos sitting in our smart phones however what comes with the convenience of recording unforgettable moments is the pain of searching for a specific picture or frame how nice it would be to be able to find the desired image just by typing one or few words to describe it 	subsequently we introduce our models namely a multi response linear regression model and a deep learning method inspired by
0	144	9456	under the umbrella of multimodal machine learning caption image retrieval has received much attention in recent years one main class of strategies is to learn separate representations for each of the modalities and then coordinate them via some constraint a natural choice of constraint is similarity either in the sense of cosine distance a different class of constraint considered in more recently there is another line of work that tries to improve retrieval performance with the use of generative models 	under the umbrella of multimodal machine learning caption image retrieval has received much attention in recent years 
0	144	9457	under the umbrella of multimodal machine learning caption image retrieval has received much attention in recent years one main class of strategies is to learn separate representations for each of the modalities and then coordinate them via some constraint a natural choice of constraint is similarity either in the sense of cosine distance a different class of constraint considered in more recently there is another line of work that tries to improve retrieval performance with the use of generative models 	one main class of strategies is to learn separate representations for each of the modalities and then coordinate them via some constraint 
0	144	9458	under the umbrella of multimodal machine learning caption image retrieval has received much attention in recent years one main class of strategies is to learn separate representations for each of the modalities and then coordinate them via some constraint a natural choice of constraint is similarity either in the sense of cosine distance a different class of constraint considered in more recently there is another line of work that tries to improve retrieval performance with the use of generative models 	a natural choice of constraint is similarity either in the sense of cosine distance a different class of constraint considered in more recently there is another line of work that tries to improve retrieval performance with the use of generative models 
0	144	9459	under the umbrella of multimodal machine learning caption image retrieval has received much attention in recent years one main class of strategies is to learn separate representations for each of the modalities and then coordinate them via some constraint a natural choice of constraint is similarity either in the sense of cosine distance a different class of constraint considered in more recently there is another line of work that tries to improve retrieval performance with the use of generative models 	in under the hood of most state of the art models the choice of pretrained features embeddings plays an important role 
0	144	9460	under the umbrella of multimodal machine learning caption image retrieval has received much attention in recent years one main class of strategies is to learn separate representations for each of the modalities and then coordinate them via some constraint a natural choice of constraint is similarity either in the sense of cosine distance a different class of constraint considered in more recently there is another line of work that tries to improve retrieval performance with the use of generative models 	we use vgg 19
1	144	9461	we train our models using the microsoft coco dataset three teddy bears laying in bed under the covers a group of stuffed animals sitting next to each other in bed a white beige and brown baby bear under a beige white comforter a trio of teddy bears bundled up on a bed three stuffed animals lay in a bed cuddled together to represent images a common choice is to use a pretrained image model as a feature extractor and use the last layer of the forward pass as the representation in the present work we employ the f c7 features of the 19 layer vgg network	we train our models using the microsoft coco dataset three teddy bears laying in bed under the covers a group of stuffed animals sitting next to each other in bed a white beige and brown baby bear under a beige white comforter a trio of teddy bears bundled up on a bed three stuffed animals lay in a bed cuddled together to represent images a common choice is to use a pretrained image model as a feature extractor and use the last layer of the forward pass as the representation 
0	144	9462	we train our models using the microsoft coco dataset three teddy bears laying in bed under the covers a group of stuffed animals sitting next to each other in bed a white beige and brown baby bear under a beige white comforter a trio of teddy bears bundled up on a bed three stuffed animals lay in a bed cuddled together to represent images a common choice is to use a pretrained image model as a feature extractor and use the last layer of the forward pass as the representation in the present work we employ the f c7 features of the 19 layer vgg network	in the present work we employ the f c7 features of the 19 layer vgg network
0	144	9463	in this section we describe the methods that we use for this task they include a traditional supervised method based on multiple response linear regression and methods based on neural networks 	in this section we describe the methods that we use for this task 
0	144	9464	in this section we describe the methods that we use for this task they include a traditional supervised method based on multiple response linear regression and methods based on neural networks 	they include a traditional supervised method based on multiple response linear regression and methods based on neural networks 
0	144	9465	in multimodal machine learning a common approach is coordinating the representations of different modalities so that certain similarity among their respective spaces are enforced our task involves texts and images to represent the captions we simply average the glove vectors relative to the words of the sentence though more sophisticated methods exist 	in multimodal machine learning a common approach is coordinating the representations of different modalities so that certain similarity among their respective spaces are enforced 
0	144	9466	in multimodal machine learning a common approach is coordinating the representations of different modalities so that certain similarity among their respective spaces are enforced our task involves texts and images to represent the captions we simply average the glove vectors relative to the words of the sentence though more sophisticated methods exist 	our task involves texts and images 
0	144	9467	in multimodal machine learning a common approach is coordinating the representations of different modalities so that certain similarity among their respective spaces are enforced our task involves texts and images to represent the captions we simply average the glove vectors relative to the words of the sentence though more sophisticated methods exist 	to represent the captions we simply average the glove vectors relative to the words of the sentence though more sophisticated methods exist 
0	144	9468	in multimodal machine learning a common approach is coordinating the representations of different modalities so that certain similarity among their respective spaces are enforced our task involves texts and images to represent the captions we simply average the glove vectors relative to the words of the sentence though more sophisticated methods exist 	let f glove be the sentence features and f vgg be the image features coming from the vgg network 
0	144	9469	in multimodal machine learning a common approach is coordinating the representations of different modalities so that certain similarity among their respective spaces are enforced our task involves texts and images to represent the captions we simply average the glove vectors relative to the words of the sentence though more sophisticated methods exist 	in order to encourage similarity between these two different types of representation we would like to find a weight matrix such that this is known as multi response linear regression 
0	144	9470	in multimodal machine learning a common approach is coordinating the representations of different modalities so that certain similarity among their respective spaces are enforced our task involves texts and images to represent the captions we simply average the glove vectors relative to the words of the sentence though more sophisticated methods exist 	as a generalization of linear regression it has closed form solution or can be solve by stochastic gradient descent when we have a large dataset 
0	144	9471	in multimodal machine learning a common approach is coordinating the representations of different modalities so that certain similarity among their respective spaces are enforced our task involves texts and images to represent the captions we simply average the glove vectors relative to the words of the sentence though more sophisticated methods exist 	at test time when we are given a caption c t we compute the caption feature vector f glove c t and find the image s closest to that 
0	144	9472	our method is inspired by where f i and f c are embedding functions for images and captions respectively there is a negative sign since we would like larger s to indicate more similarity for the purpose of contrasting correct and incorrect matches we introduce negative examples that comes handy in the same training batch 	our method is inspired by where f i and f c are embedding functions for images and captions respectively 
0	144	9473	our method is inspired by where f i and f c are embedding functions for images and captions respectively there is a negative sign since we would like larger s to indicate more similarity for the purpose of contrasting correct and incorrect matches we introduce negative examples that comes handy in the same training batch 	there is a negative sign since we would like larger s to indicate more similarity 
0	144	9474	our method is inspired by where f i and f c are embedding functions for images and captions respectively there is a negative sign since we would like larger s to indicate more similarity for the purpose of contrasting correct and incorrect matches we introduce negative examples that comes handy in the same training batch 	for the purpose of contrasting correct and incorrect matches we introduce negative examples that comes handy in the same training batch 
0	144	9475	our method is inspired by where f i and f c are embedding functions for images and captions respectively there is a negative sign since we would like larger s to indicate more similarity for the purpose of contrasting correct and incorrect matches we introduce negative examples that comes handy in the same training batch 	the cost can thus be expressed aswhere c i is the true caption image pair c and i refer to incorrect captions and images for the selected pair 
0	144	9476	our method is inspired by where f i and f c are embedding functions for images and captions respectively there is a negative sign since we would like larger s to indicate more similarity for the purpose of contrasting correct and incorrect matches we introduce negative examples that comes handy in the same training batch 	therefore the cost function enforces positive i e 
0	144	9477	our method is inspired by where f i and f c are embedding functions for images and captions respectively there is a negative sign since we would like larger s to indicate more similarity for the purpose of contrasting correct and incorrect matches we introduce negative examples that comes handy in the same training batch 	correct examples to have zeropenalty and negative i e 
0	144	9478	our method is inspired by where f i and f c are embedding functions for images and captions respectively there is a negative sign since we would like larger s to indicate more similarity for the purpose of contrasting correct and incorrect matches we introduce negative examples that comes handy in the same training batch 	incorrect examples to have penalty greater than a margin feature extraction for both modalities is similar to the baseline 
0	144	9479	our method is inspired by where f i and f c are embedding functions for images and captions respectively there is a negative sign since we would like larger s to indicate more similarity for the purpose of contrasting correct and incorrect matches we introduce negative examples that comes handy in the same training batch 	the embedding function f i is obtained by opportunely weighting the outcome before the output layer of the vgg network and f c takes the last state of a recurrent neural network rnn with gated recurrent unit gru activation functions where w i is a n 4096 matrix of weights to be trained and n is the number of features of the embedding space 
1	144	9480	our method is inspired by where f i and f c are embedding functions for images and captions respectively there is a negative sign since we would like larger s to indicate more similarity for the purpose of contrasting correct and incorrect matches we introduce negative examples that comes handy in the same training batch 	the embedding function f c is now the outcome of a recurrent neural network rnn with gated recurrent unit gru activation functions figure 2 recurrent neural network to process captions we observe that in in addition we explore the usage of a different modules in the rnn namely long short term memory lstm and different rnn architectures such as stacked bidirectional rnns 
0	144	9481	metric the metric we use in this project is recall k r k given a list of predicted rankings r i 1 i m for m images based on their corresponding input captions we definewe should notice that this metric also depends on the size of the image database for example searching over a one million image database is clearly harder than a one hundred database 	metric the metric we use in this project is recall k r k 
0	144	9482	metric the metric we use in this project is recall k r k given a list of predicted rankings r i 1 i m for m images based on their corresponding input captions we definewe should notice that this metric also depends on the size of the image database for example searching over a one million image database is clearly harder than a one hundred database 	given a list of predicted rankings r i 1 i m for m images based on their corresponding input captions we definewe should notice that this metric also depends on the size of the image database 
0	144	9483	metric the metric we use in this project is recall k r k given a list of predicted rankings r i 1 i m for m images based on their corresponding input captions we definewe should notice that this metric also depends on the size of the image database for example searching over a one million image database is clearly harder than a one hundred database 	for example searching over a one million image database is clearly harder than a one hundred database 
0	144	9484	metric the metric we use in this project is recall k r k given a list of predicted rankings r i 1 i m for m images based on their corresponding input captions we definewe should notice that this metric also depends on the size of the image database for example searching over a one million image database is clearly harder than a one hundred database 	in this project we focus on the size of 1k images 
1	144	9485	metric the metric we use in this project is recall k r k given a list of predicted rankings r i 1 i m for m images based on their corresponding input captions we definewe should notice that this metric also depends on the size of the image database for example searching over a one million image database is clearly harder than a one hundred database 	in addition we will also look at some conditional metrics such as the length of the caption to better understand the results hyperparameters we started with a set of hyperparameters suggested in results in we also compare the baseline method and the neural network solution through some real examples 
0	144	9486	we see that the baseline model works well for simple queries like single word object names however for longer captions as in	we see that the baseline model works well for simple queries like single word object names 
0	144	9487	we see that the baseline model works well for simple queries like single word object names however for longer captions as in	however for longer captions as in
0	144	9488	in this project our emphasis is more on language models because as a first step we would like to accurately identify the semantics implied by the query on the image side we only represent each by its features extracted from a pretrained network although we see the image feature is able to capture small details in the image it can still be the bottleneck as our language model becomes more sophisticated 	in this project our emphasis is more on language models because as a first step we would like to accurately identify the semantics implied by the query 
0	144	9489	in this project our emphasis is more on language models because as a first step we would like to accurately identify the semantics implied by the query on the image side we only represent each by its features extracted from a pretrained network although we see the image feature is able to capture small details in the image it can still be the bottleneck as our language model becomes more sophisticated 	on the image side we only represent each by its features extracted from a pretrained network 
0	144	9490	in this project our emphasis is more on language models because as a first step we would like to accurately identify the semantics implied by the query on the image side we only represent each by its features extracted from a pretrained network although we see the image feature is able to capture small details in the image it can still be the bottleneck as our language model becomes more sophisticated 	although we see the image feature is able to capture small details in the image it can still be the bottleneck as our language model becomes more sophisticated 
0	144	9491	in this project our emphasis is more on language models because as a first step we would like to accurately identify the semantics implied by the query on the image side we only represent each by its features extracted from a pretrained network although we see the image feature is able to capture small details in the image it can still be the bottleneck as our language model becomes more sophisticated 	in the future we would like to endow a dynamic attention mechanism so that the model will be able to choose adaptively the region s to focus on in the image 
0	144	9492	in this project our emphasis is more on language models because as a first step we would like to accurately identify the semantics implied by the query on the image side we only represent each by its features extracted from a pretrained network although we see the image feature is able to capture small details in the image it can still be the bottleneck as our language model becomes more sophisticated 	this might be done either by including some pretrained features in the lower layers or by computing features on sub regions of the image 
0	144	9493	in this project our emphasis is more on language models because as a first step we would like to accurately identify the semantics implied by the query on the image side we only represent each by its features extracted from a pretrained network although we see the image feature is able to capture small details in the image it can still be the bottleneck as our language model becomes more sophisticated 	there are some initial attemps in this direction such as link to the code https github com giacomolamberti90 cs229 project
1	145	9494	activity recognition is an important task in several healthcare applications by continuously monitoring and analyzing user activity it is possible to provide automated recommendations to both patients and doctors	activity recognition is an important task in several healthcare applications 
0	145	9495	activity recognition is an important task in several healthcare applications by continuously monitoring and analyzing user activity it is possible to provide automated recommendations to both patients and doctors	by continuously monitoring and analyzing user activity it is possible to provide automated recommendations to both patients and doctors
1	145	9496	because of its many applications supervised human activity classification using sensor data is a relatively popular research area through our research we have found that related articles and their approaches can generally be divided into three categories naive bayes classification svm decision trees and neural networks we consider the use of naive bayes as a classifier for human activity as clever and interesting since it is usually used for text classification 	because of its many applications supervised human activity classification using sensor data is a relatively popular research area 
1	145	9497	because of its many applications supervised human activity classification using sensor data is a relatively popular research area through our research we have found that related articles and their approaches can generally be divided into three categories naive bayes classification svm decision trees and neural networks we consider the use of naive bayes as a classifier for human activity as clever and interesting since it is usually used for text classification 	through our research we have found that related articles and their approaches can generally be divided into three categories naive bayes classification svm decision trees and neural networks 
1	145	9498	because of its many applications supervised human activity classification using sensor data is a relatively popular research area through our research we have found that related articles and their approaches can generally be divided into three categories naive bayes classification svm decision trees and neural networks we consider the use of naive bayes as a classifier for human activity as clever and interesting since it is usually used for text classification 	we consider the use of naive bayes as a classifier for human activity as clever and interesting since it is usually used for text classification 
0	145	9499	because of its many applications supervised human activity classification using sensor data is a relatively popular research area through our research we have found that related articles and their approaches can generally be divided into three categories naive bayes classification svm decision trees and neural networks we consider the use of naive bayes as a classifier for human activity as clever and interesting since it is usually used for text classification 	one such article that uses the naive bayes classifier is long yin and aarts 2009
