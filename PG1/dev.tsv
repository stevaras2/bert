Label	id1	id2	features	sentence
1	108001	8001	similarly note that while the training error is fairly low for all models in the case of solar energy each of the models ends up having a much higher test error relatively .this indicates that the neural network is overfitting to the training data in the case of wind energy .this indicates that the models are once again overfitting in the case of solar energy .results and discussion	similarly note that while the training error is fairly low for all models in the case of solar energy each of the models ends up having a much higher test error relatively 
1	108002	8002	this indicates that the models are once again overfitting in the case of solar energy .similarly note that while the training error is fairly low for all models in the case of solar energy each of the models ends up having a much higher test error relatively .this could be so since the training set is so large while the validation and test sets are relatively small .results and discussion	this indicates that the models are once again overfitting in the case of solar energy 
1	108003	8003	this could be so since the training set is so large while the validation and test sets are relatively small .this indicates that the models are once again overfitting in the case of solar energy .for this purpose we introduced regularization into our code but it did not have a significant impact most interesting perhaps is the performance of the generalized additive model gam .results and discussion	this could be so since the training set is so large while the validation and test sets are relatively small 
1	108004	8004	for this purpose we introduced regularization into our code but it did not have a significant impact most interesting perhaps is the performance of the generalized additive model gam .this could be so since the training set is so large while the validation and test sets are relatively small .from partial dependency plots allow us to gain a visual intuition on how features affect the output value .results and discussion	for this purpose we introduced regularization into our code but it did not have a significant impact most interesting perhaps is the performance of the generalized additive model gam 
1	108005	8005	from partial dependency plots allow us to gain a visual intuition on how features affect the output value .for this purpose we introduced regularization into our code but it did not have a significant impact most interesting perhaps is the performance of the generalized additive model gam .for instance by looking at the solar output vs relative humidity graph in.results and discussion	from partial dependency plots allow us to gain a visual intuition on how features affect the output value 
1	108006	8006	for instance by looking at the solar output vs relative humidity graph in.from partial dependency plots allow us to gain a visual intuition on how features affect the output value .last sentence.results and discussion	for instance by looking at the solar output vs relative humidity graph in
1	108007	8007	over the course of this report we have described the set up and utilization of four distinct supervised learning models in order to predict renewable energy outputs per unit area given the weather data for a particular location .first sentence.during the training and testing process gams produced the best performance for both solar and wind energy albeit with close competition from neural networks in the former category and svms in the latter the success of gams might very well stem from the fact that they allow us to model arbitrarily complex relationships between individual features and the output .conclusion and future work	over the course of this report we have described the set up and utilization of four distinct supervised learning models in order to predict renewable energy outputs per unit area given the weather data for a particular location 
1	108008	8008	during the training and testing process gams produced the best performance for both solar and wind energy albeit with close competition from neural networks in the former category and svms in the latter the success of gams might very well stem from the fact that they allow us to model arbitrarily complex relationships between individual features and the output .over the course of this report we have described the set up and utilization of four distinct supervised learning models in order to predict renewable energy outputs per unit area given the weather data for a particular location .this might also serve as justification for the neural network s performances as it also allows us to glean and model increasingly complex interactions between the input features .conclusion and future work	during the training and testing process gams produced the best performance for both solar and wind energy albeit with close competition from neural networks in the former category and svms in the latter the success of gams might very well stem from the fact that they allow us to model arbitrarily complex relationships between individual features and the output 
1	108009	8009	this might also serve as justification for the neural network s performances as it also allows us to glean and model increasingly complex interactions between the input features .during the training and testing process gams produced the best performance for both solar and wind energy albeit with close competition from neural networks in the former category and svms in the latter the success of gams might very well stem from the fact that they allow us to model arbitrarily complex relationships between individual features and the output .meanwhile the fact that the svm with its gaussian kernel did not produce stellar results for solar energy suggests that the selected input space is reasonably representative of the features that affect solar energy output .conclusion and future work	this might also serve as justification for the neural network s performances as it also allows us to glean and model increasingly complex interactions between the input features 
1	108010	8010	meanwhile the fact that the svm with its gaussian kernel did not produce stellar results for solar energy suggests that the selected input space is reasonably representative of the features that affect solar energy output .this might also serve as justification for the neural network s performances as it also allows us to glean and model increasingly complex interactions between the input features .at the same time close competition from the svm and its derived features in the case of wind energy leads us to believe that we may need to utilize additional features in the case of wind energy since the relationship between the chosen features and the output definitely does not seem linear given more time and resource we would like to develop more complicated models that perhaps combine the advantages of the models we have discussed in this paper .conclusion and future work	meanwhile the fact that the svm with its gaussian kernel did not produce stellar results for solar energy suggests that the selected input space is reasonably representative of the features that affect solar energy output 
1	108011	8011	at the same time close competition from the svm and its derived features in the case of wind energy leads us to believe that we may need to utilize additional features in the case of wind energy since the relationship between the chosen features and the output definitely does not seem linear given more time and resource we would like to develop more complicated models that perhaps combine the advantages of the models we have discussed in this paper .meanwhile the fact that the svm with its gaussian kernel did not produce stellar results for solar energy suggests that the selected input space is reasonably representative of the features that affect solar energy output .at a higher level we would also like to extend our application to even more renewable energy sources .conclusion and future work	at the same time close competition from the svm and its derived features in the case of wind energy leads us to believe that we may need to utilize additional features in the case of wind energy since the relationship between the chosen features and the output definitely does not seem linear given more time and resource we would like to develop more complicated models that perhaps combine the advantages of the models we have discussed in this paper 
0	108012	8012	at a higher level we would also like to extend our application to even more renewable energy sources .at the same time close competition from the svm and its derived features in the case of wind energy leads us to believe that we may need to utilize additional features in the case of wind energy since the relationship between the chosen features and the output definitely does not seem linear given more time and resource we would like to develop more complicated models that perhaps combine the advantages of the models we have discussed in this paper .the end goal here is to determine the most efficient renewable energy source for any given location by utilizing not just weather data but also information like proximity to resources water bodies for hydro power generation for instance .conclusion and future work	at a higher level we would also like to extend our application to even more renewable energy sources 
1	108013	8013	the end goal here is to determine the most efficient renewable energy source for any given location by utilizing not just weather data but also information like proximity to resources water bodies for hydro power generation for instance .at a higher level we would also like to extend our application to even more renewable energy sources .last sentence.conclusion and future work	the end goal here is to determine the most efficient renewable energy source for any given location by utilizing not just weather data but also information like proximity to resources water bodies for hydro power generation for instance 
1	108014	8014	we introduce a novel approach to solving pde constrained optimization problems specifically related to aircraft design .first sentence.these optimization problems require running expensive computational fluid dynamics cfd simulations which have previously been approximated with a reduced order model rom to lower the computational cost .abstract	we introduce a novel approach to solving pde constrained optimization problems specifically related to aircraft design 
1	108015	8015	these optimization problems require running expensive computational fluid dynamics cfd simulations which have previously been approximated with a reduced order model rom to lower the computational cost .we introduce a novel approach to solving pde constrained optimization problems specifically related to aircraft design .instead of using a single global rom as is traditionally done we propose using multiple piecewise roms constructed and used with the aid of machine learning techniques .abstract	these optimization problems require running expensive computational fluid dynamics cfd simulations which have previously been approximated with a reduced order model rom to lower the computational cost 
1	108016	8016	instead of using a single global rom as is traditionally done we propose using multiple piecewise roms constructed and used with the aid of machine learning techniques .these optimization problems require running expensive computational fluid dynamics cfd simulations which have previously been approximated with a reduced order model rom to lower the computational cost .our approach consists of clustering a set of precomputed non linear partial differential equations pde solutions from which we build our piecewise roms .abstract	instead of using a single global rom as is traditionally done we propose using multiple piecewise roms constructed and used with the aid of machine learning techniques 
1	108017	8017	our approach consists of clustering a set of precomputed non linear partial differential equations pde solutions from which we build our piecewise roms .instead of using a single global rom as is traditionally done we propose using multiple piecewise roms constructed and used with the aid of machine learning techniques .then during the optimization problem when we need to run a simulation for a given optimization parameter we select the optimal piecewise rom to use .abstract	our approach consists of clustering a set of precomputed non linear partial differential equations pde solutions from which we build our piecewise roms 
1	108018	8018	then during the optimization problem when we need to run a simulation for a given optimization parameter we select the optimal piecewise rom to use .our approach consists of clustering a set of precomputed non linear partial differential equations pde solutions from which we build our piecewise roms .initial results on our test dataset are promising .abstract	then during the optimization problem when we need to run a simulation for a given optimization parameter we select the optimal piecewise rom to use 
0	108019	8019	initial results on our test dataset are promising .then during the optimization problem when we need to run a simulation for a given optimization parameter we select the optimal piecewise rom to use .we were able to achieve the same or better accuracy by using piecewise roms rather than a global rom while further reducing the computational cost associated with running a simulation .abstract	initial results on our test dataset are promising 
1	108020	8020	we were able to achieve the same or better accuracy by using piecewise roms rather than a global rom while further reducing the computational cost associated with running a simulation .initial results on our test dataset are promising .last sentence.abstract	we were able to achieve the same or better accuracy by using piecewise roms rather than a global rom while further reducing the computational cost associated with running a simulation 
1	108021	8021	improving the design of aircrafts often requires solving pdeconstrained optimization problems such as maximizing the lift drag with respect to some parameters .first sentence.here is an optimization vector containing parameters that we want to optimize .introduction	improving the design of aircrafts often requires solving pdeconstrained optimization problems such as maximizing the lift drag with respect to some parameters 
1	108022	8022	here is an optimization vector containing parameters that we want to optimize .improving the design of aircrafts often requires solving pdeconstrained optimization problems such as maximizing the lift drag with respect to some parameters .it is also common practice to have a lower bound lb and upper bound ub on this vector to find the optimal we must update it iteratively running a computational fluid dynamics cfd simulation at each optimization step .introduction	here is an optimization vector containing parameters that we want to optimize 
1	108023	8023	it is also common practice to have a lower bound lb and upper bound ub on this vector to find the optimal we must update it iteratively running a computational fluid dynamics cfd simulation at each optimization step .here is an optimization vector containing parameters that we want to optimize .in figure.introduction	it is also common practice to have a lower bound lb and upper bound ub on this vector to find the optimal we must update it iteratively running a computational fluid dynamics cfd simulation at each optimization step 
0	108024	8024	in figure.it is also common practice to have a lower bound lb and upper bound ub on this vector to find the optimal we must update it iteratively running a computational fluid dynamics cfd simulation at each optimization step .last sentence.introduction	in figure
0	108025	8025	fluid flow problems are governed by nonlinear partial differential equations pde .first sentence.solving these equations using cfd techniques such as finite volume method is equivalent to solving a set of nonlinear equations where is the set of parameters for our simulation and w is the unknown vector of dimension n w p r n called the state vector .high dimensional model hdm 	fluid flow problems are governed by nonlinear partial differential equations pde 
1	108026	8026	solving these equations using cfd techniques such as finite volume method is equivalent to solving a set of nonlinear equations where is the set of parameters for our simulation and w is the unknown vector of dimension n w p r n called the state vector .fluid flow problems are governed by nonlinear partial differential equations pde .specifically a row of the state wris represents a property of the fluid flow such as pressure at point i of the cfd mesh .high dimensional model hdm 	solving these equations using cfd techniques such as finite volume method is equivalent to solving a set of nonlinear equations where is the set of parameters for our simulation and w is the unknown vector of dimension n w p r n called the state vector 
0	108027	8027	specifically a row of the state wris represents a property of the fluid flow such as pressure at point i of the cfd mesh .solving these equations using cfd techniques such as finite volume method is equivalent to solving a set of nonlinear equations where is the set of parameters for our simulation and w is the unknown vector of dimension n w p r n called the state vector .thus the cfd mesh has n points .high dimensional model hdm 	specifically a row of the state wris represents a property of the fluid flow such as pressure at point i of the cfd mesh 
0	108028	8028	thus the cfd mesh has n points .specifically a row of the state wris represents a property of the fluid flow such as pressure at point i of the cfd mesh .in unfortunately this problem is very expensive to solve when n is large as it is in the case of solving cfd problems where n is in the order of thousands or millions .high dimensional model hdm 	thus the cfd mesh has n points 
0	108029	8029	in unfortunately this problem is very expensive to solve when n is large as it is in the case of solving cfd problems where n is in the order of thousands or millions .thus the cfd mesh has n points .last sentence.high dimensional model hdm 	in unfortunately this problem is very expensive to solve when n is large as it is in the case of solving cfd problems where n is in the order of thousands or millions 
1	108030	8030	in order to solve cfd problems faster a reduced order model rom can be used in order to approximate the hdm where v gl p r n n denotes the global reduce order basis rob and w r p r n denotes the new vector of unknowns called the reduced state .first sentence.substituting eq .reduced order model rom 	in order to solve cfd problems faster a reduced order model rom can be used in order to approximate the hdm where v gl p r n n denotes the global reduce order basis rob and w r p r n denotes the new vector of unknowns called the reduced state 
0	108031	8031	substituting eq .in order to solve cfd problems faster a reduced order model rom can be used in order to approximate the hdm where v gl p r n n denotes the global reduce order basis rob and w r p r n denotes the new vector of unknowns called the reduced state .4 into eq .reduced order model rom 	substituting eq 
0	108032	8032	4 into eq .substituting eq .now the least squares problem to solve is min wrpr n rpv gl w r p q q 2 2 6 .reduced order model rom 	4 into eq 
0	108033	8033	now the least squares problem to solve is min wrpr n rpv gl w r p q q 2 2 6 .4 into eq .last sentence.reduced order model rom 	now the least squares problem to solve is min wrpr n rpv gl w r p q q 2 2 6 
1	108034	8034	to build a rom we first need to find the global reduced order basis rob v gl .first sentence.this is done by solving the non linear equation 2 for many optimization vectors .global reduce order basis rob v gl	to build a rom we first need to find the global reduced order basis rob v gl 
1	108035	8035	this is done by solving the non linear equation 2 for many optimization vectors .to build a rom we first need to find the global reduced order basis rob v gl .thus given a specific vector i we can define the solution state vector therefore for a set of k optimization vectors t u k 1 we solve 3 and we get a set of state vectors twp i qu k finally we perform a singular value decomposition svd on the matrix m to compute the global rob v gl here v gl is computed by only selecting the first n columns of the matrix u and therefore v g l p r n n .global reduce order basis rob v gl	this is done by solving the non linear equation 2 for many optimization vectors 
1	108036	8036	thus given a specific vector i we can define the solution state vector therefore for a set of k optimization vectors t u k 1 we solve 3 and we get a set of state vectors twp i qu k finally we perform a singular value decomposition svd on the matrix m to compute the global rob v gl here v gl is computed by only selecting the first n columns of the matrix u and therefore v g l p r n n .this is done by solving the non linear equation 2 for many optimization vectors .thus the global rom has dimension n and can be used in the entire domain d wp q v gl w r p q p d 10 .global reduce order basis rob v gl	thus given a specific vector i we can define the solution state vector therefore for a set of k optimization vectors t u k 1 we solve 3 and we get a set of state vectors twp i qu k finally we perform a singular value decomposition svd on the matrix m to compute the global rob v gl here v gl is computed by only selecting the first n columns of the matrix u and therefore v g l p r n n 
0	108037	8037	thus the global rom has dimension n and can be used in the entire domain d wp q v gl w r p q p d 10 .thus given a specific vector i we can define the solution state vector therefore for a set of k optimization vectors t u k 1 we solve 3 and we get a set of state vectors twp i qu k finally we perform a singular value decomposition svd on the matrix m to compute the global rob v gl here v gl is computed by only selecting the first n columns of the matrix u and therefore v g l p r n n .last sentence.global reduce order basis rob v gl	thus the global rom has dimension n and can be used in the entire domain d wp q v gl w r p q p d 10 
1	108038	8038	in this project we propose creating multiple piecewise roms in the domain d each having smaller dimensions than a global rom would .first sentence.these piecewise roms do not need to be accurate in the entire domain d but only in limited region of the design space d by using machine learning techniques we hypothesize that we can improve quality of the reduced order basis rob within a given design space d then using these piecewise roms will allow us to solve even cheaper least squares problems than 6 whilst maintaining a similar or better level of accuracy relative to the hdm to do this we must group the precomputed solutions twp i qu k 1 into multiple clusters and then create multiple piecewise reduced order basis tv i u c 1 where c is the number of clusters .piecewise roms in the design space	in this project we propose creating multiple piecewise roms in the domain d each having smaller dimensions than a global rom would 
1	108039	8039	these piecewise roms do not need to be accurate in the entire domain d but only in limited region of the design space d by using machine learning techniques we hypothesize that we can improve quality of the reduced order basis rob within a given design space d then using these piecewise roms will allow us to solve even cheaper least squares problems than 6 whilst maintaining a similar or better level of accuracy relative to the hdm to do this we must group the precomputed solutions twp i qu k 1 into multiple clusters and then create multiple piecewise reduced order basis tv i u c 1 where c is the number of clusters .in this project we propose creating multiple piecewise roms in the domain d each having smaller dimensions than a global rom would .for instance choosing two clusters in figure 5 we can see a schematic comparison of a global rom versus 2 piecewise roms built by clustering twp i u k 1 into 2 clusters .piecewise roms in the design space	these piecewise roms do not need to be accurate in the entire domain d but only in limited region of the design space d by using machine learning techniques we hypothesize that we can improve quality of the reduced order basis rob within a given design space d then using these piecewise roms will allow us to solve even cheaper least squares problems than 6 whilst maintaining a similar or better level of accuracy relative to the hdm to do this we must group the precomputed solutions twp i qu k 1 into multiple clusters and then create multiple piecewise reduced order basis tv i u c 1 where c is the number of clusters 
1	108040	8040	for instance choosing two clusters in figure 5 we can see a schematic comparison of a global rom versus 2 piecewise roms built by clustering twp i u k 1 into 2 clusters .these piecewise roms do not need to be accurate in the entire domain d but only in limited region of the design space d by using machine learning techniques we hypothesize that we can improve quality of the reduced order basis rob within a given design space d then using these piecewise roms will allow us to solve even cheaper least squares problems than 6 whilst maintaining a similar or better level of accuracy relative to the hdm to do this we must group the precomputed solutions twp i qu k 1 into multiple clusters and then create multiple piecewise reduced order basis tv i u c 1 where c is the number of clusters .on the left all the training solutions tw i u 10 1 computed solving 2 using t i u 10 1 are used to create v gl and therefore a global rom .piecewise roms in the design space	for instance choosing two clusters in figure 5 we can see a schematic comparison of a global rom versus 2 piecewise roms built by clustering twp i u k 1 into 2 clusters 
1	108041	8041	on the left all the training solutions tw i u 10 1 computed solving 2 using t i u 10 1 are used to create v gl and therefore a global rom .for instance choosing two clusters in figure 5 we can see a schematic comparison of a global rom versus 2 piecewise roms built by clustering twp i u k 1 into 2 clusters .on the right we first cluster the training solutions tw i u 10 1 into 2 clusters and then we construct 2 reduced order basis v 1 and v 2 .piecewise roms in the design space	on the left all the training solutions tw i u 10 1 computed solving 2 using t i u 10 1 are used to create v gl and therefore a global rom 
1	108042	8042	on the right we first cluster the training solutions tw i u 10 1 into 2 clusters and then we construct 2 reduced order basis v 1 and v 2 .on the left all the training solutions tw i u 10 1 computed solving 2 using t i u 10 1 are used to create v gl and therefore a global rom .v 1 is built using the solutions computed using the parameters t 1 5 6 7 10 u and v 2 using t 2 3 4 8 9 u .piecewise roms in the design space	on the right we first cluster the training solutions tw i u 10 1 into 2 clusters and then we construct 2 reduced order basis v 1 and v 2 
0	108043	8043	v 1 is built using the solutions computed using the parameters t 1 5 6 7 10 u and v 2 using t 2 3 4 8 9 u .on the right we first cluster the training solutions tw i u 10 1 into 2 clusters and then we construct 2 reduced order basis v 1 and v 2 .as such the global rom uses v gl p r n n .piecewise roms in the design space	v 1 is built using the solutions computed using the parameters t 1 5 6 7 10 u and v 2 using t 2 3 4 8 9 u 
0	108044	8044	as such the global rom uses v gl p r n n .v 1 is built using the solutions computed using the parameters t 1 5 6 7 10 u and v 2 using t 2 3 4 8 9 u .the two piecewise roms instead use v 1 p r n n1 and v 2 p r n n2 respectively where by construction n 1 n and n 2 n therefore the first piecewise rom makes the following approximation using v 1 wp q v 1 w r p q 11 and the second piecewise rom makes another approximation usingtherefore by using either 11 or 12 we can solve min wrpr n rpv i w r p q q 2 2where i indicates which piecewise rom i is used .piecewise roms in the design space	as such the global rom uses v gl p r n n 
0	108045	8045	the two piecewise roms instead use v 1 p r n n1 and v 2 p r n n2 respectively where by construction n 1 n and n 2 n therefore the first piecewise rom makes the following approximation using v 1 wp q v 1 w r p q 11 and the second piecewise rom makes another approximation usingtherefore by using either 11 or 12 we can solve min wrpr n rpv i w r p q q 2 2where i indicates which piecewise rom i is used .as such the global rom uses v gl p r n n .using this method with piecewise roms gives rise to two machine learning problems that we must solve 1 .piecewise roms in the design space	the two piecewise roms instead use v 1 p r n n1 and v 2 p r n n2 respectively where by construction n 1 n and n 2 n therefore the first piecewise rom makes the following approximation using v 1 wp q v 1 w r p q 11 and the second piecewise rom makes another approximation usingtherefore by using either 11 or 12 we can solve min wrpr n rpv i w r p q q 2 2where i indicates which piecewise rom i is used 
1	108046	8046	using this method with piecewise roms gives rise to two machine learning problems that we must solve 1 .the two piecewise roms instead use v 1 p r n n1 and v 2 p r n n2 respectively where by construction n 1 n and n 2 n therefore the first piecewise rom makes the following approximation using v 1 wp q v 1 w r p q 11 and the second piecewise rom makes another approximation usingtherefore by using either 11 or 12 we can solve min wrpr n rpv i w r p q q 2 2where i indicates which piecewise rom i is used .given multiple precomputed solutions tw i u k 1 how do we cluster them most effectively into tv i u c 1 2 .piecewise roms in the design space	using this method with piecewise roms gives rise to two machine learning problems that we must solve 1 
0	108047	8047	given multiple precomputed solutions tw i u k 1 how do we cluster them most effectively into tv i u c 1 2 .using this method with piecewise roms gives rise to two machine learning problems that we must solve 1 .given an arbitrary which piecewise rom tv i u c 1 should we use to best represent the hdm in the next section we describe the methods we have implemented for addressing the above problems .piecewise roms in the design space	given multiple precomputed solutions tw i u k 1 how do we cluster them most effectively into tv i u c 1 2 
1	108048	8048	given an arbitrary which piecewise rom tv i u c 1 should we use to best represent the hdm in the next section we describe the methods we have implemented for addressing the above problems .given multiple precomputed solutions tw i u k 1 how do we cluster them most effectively into tv i u c 1 2 .last sentence.piecewise roms in the design space	given an arbitrary which piecewise rom tv i u c 1 should we use to best represent the hdm in the next section we describe the methods we have implemented for addressing the above problems 
1	108049	8049	our proposed methodology to solve a pde constrained optimization problem operates in two phases an offline phase and an online phase .first sentence.in the offline phase we cluster precomputed training solutions from which we build our piecewise roms that are used in the online phase .overview	our proposed methodology to solve a pde constrained optimization problem operates in two phases an offline phase and an online phase 
1	108050	8050	in the offline phase we cluster precomputed training solutions from which we build our piecewise roms that are used in the online phase .our proposed methodology to solve a pde constrained optimization problem operates in two phases an offline phase and an online phase .in the online phase we query multiple during the optimization process .overview	in the offline phase we cluster precomputed training solutions from which we build our piecewise roms that are used in the online phase 
1	108051	8051	in the online phase we query multiple during the optimization process .in the offline phase we cluster precomputed training solutions from which we build our piecewise roms that are used in the online phase .for each queried i we need select which piecewise rom v i to use .overview	in the online phase we query multiple during the optimization process 
1	108052	8052	for each queried i we need select which piecewise rom v i to use .in the online phase we query multiple during the optimization process .then we run the simulation to compute wp i q and lift drag p i q .overview	for each queried i we need select which piecewise rom v i to use 
0	108053	8053	then we run the simulation to compute wp i q and lift drag p i q .for each queried i we need select which piecewise rom v i to use .last sentence.overview	then we run the simulation to compute wp i q and lift drag p i q 
1	108054	8054	since our goal is to break our domain d into smaller sub domains we believe clustering the training points based on the euclidean distance between features will be most effective .first sentence.we have applied three algorithms in order to implement this k means expectation maximization to fit a gaussian mixture model and agglomerative clustering in terms of clustering features we have considered using b a features as they provide more information on the physics problem we are trying to solve and obviously lift and drag are both related to the fluid state .clustering	since our goal is to break our domain d into smaller sub domains we believe clustering the training points based on the euclidean distance between features will be most effective 
1	108055	8055	we have applied three algorithms in order to implement this k means expectation maximization to fit a gaussian mixture model and agglomerative clustering in terms of clustering features we have considered using b a features as they provide more information on the physics problem we are trying to solve and obviously lift and drag are both related to the fluid state .since our goal is to break our domain d into smaller sub domains we believe clustering the training points based on the euclidean distance between features will be most effective .last sentence.clustering	we have applied three algorithms in order to implement this k means expectation maximization to fit a gaussian mixture model and agglomerative clustering in terms of clustering features we have considered using b a features as they provide more information on the physics problem we are trying to solve and obviously lift and drag are both related to the fluid state 
1	108056	8056	the number of training points used when constructing roms are relatively low when compared with other machine learning problems .first sentence.additionally we do not have ground truth values for our classifications and only are able to determine how well our algorithms performs after doing a rom simulation .classification	the number of training points used when constructing roms are relatively low when compared with other machine learning problems 
1	108057	8057	additionally we do not have ground truth values for our classifications and only are able to determine how well our algorithms performs after doing a rom simulation .the number of training points used when constructing roms are relatively low when compared with other machine learning problems .therefore we have chosen to evaluate two simple methods nearest centroid and multinomial logistic regression as our algorithms for performing classifications .classification	additionally we do not have ground truth values for our classifications and only are able to determine how well our algorithms performs after doing a rom simulation 
1	108058	8058	therefore we have chosen to evaluate two simple methods nearest centroid and multinomial logistic regression as our algorithms for performing classifications .additionally we do not have ground truth values for our classifications and only are able to determine how well our algorithms performs after doing a rom simulation .for nearest centroid we simply select the piecewise rom whose clustered points have the closest centroid to the queried point while multinomial logistic regression is trained using a cross entropy loss and the labels output from clustering during the offline phase .classification	therefore we have chosen to evaluate two simple methods nearest centroid and multinomial logistic regression as our algorithms for performing classifications 
1	108059	8059	for nearest centroid we simply select the piecewise rom whose clustered points have the closest centroid to the queried point while multinomial logistic regression is trained using a cross entropy loss and the labels output from clustering during the offline phase .therefore we have chosen to evaluate two simple methods nearest centroid and multinomial logistic regression as our algorithms for performing classifications .since during the online phase we will not have access to the lift or drag for a given query point we are only able to use as a feature for classification .classification	for nearest centroid we simply select the piecewise rom whose clustered points have the closest centroid to the queried point while multinomial logistic regression is trained using a cross entropy loss and the labels output from clustering during the offline phase 
1	108060	8060	since during the online phase we will not have access to the lift or drag for a given query point we are only able to use as a feature for classification .for nearest centroid we simply select the piecewise rom whose clustered points have the closest centroid to the queried point while multinomial logistic regression is trained using a cross entropy loss and the labels output from clustering during the offline phase .last sentence.classification	since during the online phase we will not have access to the lift or drag for a given query point we are only able to use as a feature for classification 
1	108061	8061	in order to to create a train validation test set we sampled the parameter domain d r 3 to compute 90 solutions twp i qu.first sentence.last sentence.design of experiment	in order to to create a train validation test set we sampled the parameter domain d r 3 to compute 90 solutions twp i qu
1	108062	8062	for all of the following experiments we define our error to be the difference in lift drag calculated with a rom and lift drag calculated with the hdm .first sentence.we refer to mse as the mean squared error across our test points and max error as the highest percentage deviation from the lift drag calculated with the hdm .experiments results	for all of the following experiments we define our error to be the difference in lift drag calculated with a rom and lift drag calculated with the hdm 
1	108063	8063	we refer to mse as the mean squared error across our test points and max error as the highest percentage deviation from the lift drag calculated with the hdm .for all of the following experiments we define our error to be the difference in lift drag calculated with a rom and lift drag calculated with the hdm .last sentence.experiments results	we refer to mse as the mean squared error across our test points and max error as the highest percentage deviation from the lift drag calculated with the hdm 
0	108064	8064	for our first set of experiments we determine the best parameters for our methodology given our design space .first sentence.specifically we use the validation set to determine the clustering algorithm.model parameter experiments	for our first set of experiments we determine the best parameters for our methodology given our design space 
1	108065	8065	specifically we use the validation set to determine the clustering algorithm.for our first set of experiments we determine the best parameters for our methodology given our design space .last sentence.model parameter experiments	specifically we use the validation set to determine the clustering algorithm
1	108066	8066	clustering features number of clusters for each experiment we ran three tests each with 20 training points folded from our total 50 training points and test the error on our validation set .first sentence.in subsection 4 3 we investigate using a predictor to automatically determine our parameters without having to run any simulations on a validation set first we tested for the best clustering algorithms fixing our classification algorithm to nearest centroid the number of clusters to 4 and clustering features to t lift drag u .classification algorithm	clustering features number of clusters for each experiment we ran three tests each with 20 training points folded from our total 50 training points and test the error on our validation set 
1	108067	8067	in subsection 4 3 we investigate using a predictor to automatically determine our parameters without having to run any simulations on a validation set first we tested for the best clustering algorithms fixing our classification algorithm to nearest centroid the number of clusters to 4 and clustering features to t lift drag u .clustering features number of clusters for each experiment we ran three tests each with 20 training points folded from our total 50 training points and test the error on our validation set .finally we tested the effect of using different numbers of clusters .classification algorithm	in subsection 4 3 we investigate using a predictor to automatically determine our parameters without having to run any simulations on a validation set first we tested for the best clustering algorithms fixing our classification algorithm to nearest centroid the number of clusters to 4 and clustering features to t lift drag u 
0	108068	8068	finally we tested the effect of using different numbers of clusters .in subsection 4 3 we investigate using a predictor to automatically determine our parameters without having to run any simulations on a validation set first we tested for the best clustering algorithms fixing our classification algorithm to nearest centroid the number of clusters to 4 and clustering features to t lift drag u .we used k means for clustering nearest centroid for classification and t lift drag u as the clustering features .classification algorithm	finally we tested the effect of using different numbers of clusters 
0	108069	8069	we used k means for clustering nearest centroid for classification and t lift drag u as the clustering features .finally we tested the effect of using different numbers of clusters .last sentence.classification algorithm	we used k means for clustering nearest centroid for classification and t lift drag u as the clustering features 
1	108070	8070	with our optimal clustering classification algorithms and features derived from subsection 4 1 and clusters of size 2 and 4 we tested the accuracy of our methodology versus a global rom approach for calculating the lift drag the objective function of the optimization problem .first sentence.for the test with two clusters we show the offline clustering phase in figure 12 .global rom comparison	with our optimal clustering classification algorithms and features derived from subsection 4 1 and clusters of size 2 and 4 we tested the accuracy of our methodology versus a global rom approach for calculating the lift drag the objective function of the optimization problem 
1	108071	8071	for the test with two clusters we show the offline clustering phase in figure 12 .with our optimal clustering classification algorithms and features derived from subsection 4 1 and clusters of size 2 and 4 we tested the accuracy of our methodology versus a global rom approach for calculating the lift drag the objective function of the optimization problem .in.global rom comparison	for the test with two clusters we show the offline clustering phase in figure 12 
0	108072	8072	in.for the test with two clusters we show the offline clustering phase in figure 12 .last sentence.global rom comparison	in
1	108073	8073	in practice users would not want to have to use a validation set to determine the best clustering parameters as the time required to do this may outweigh any efficiency savings from using piecewise roms .first sentence.therefore a predictor for a reliable set of clustering parameters is necessary for real world applications .predictor for rom accuracy	in practice users would not want to have to use a validation set to determine the best clustering parameters as the time required to do this may outweigh any efficiency savings from using piecewise roms 
1	108074	8074	therefore a predictor for a reliable set of clustering parameters is necessary for real world applications .in practice users would not want to have to use a validation set to determine the best clustering parameters as the time required to do this may outweigh any efficiency savings from using piecewise roms .we tested different cluster scoring methods including silhouette score.predictor for rom accuracy	therefore a predictor for a reliable set of clustering parameters is necessary for real world applications 
0	108075	8075	we tested different cluster scoring methods including silhouette score.therefore a predictor for a reliable set of clustering parameters is necessary for real world applications .last sentence.predictor for rom accuracy	we tested different cluster scoring methods including silhouette score
0	108076	8076	from our results we see that the k means and agglomerative clustering perform somewhat similarly compared to the gaussian mixture model .first sentence.this makes sense as the points used in the offline phase are not necessarily gaussian distributed while k means and agglomerative clustering less strong of an assumption .discussion	from our results we see that the k means and agglomerative clustering perform somewhat similarly compared to the gaussian mixture model 
0	108077	8077	this makes sense as the points used in the offline phase are not necessarily gaussian distributed while k means and agglomerative clustering less strong of an assumption .from our results we see that the k means and agglomerative clustering perform somewhat similarly compared to the gaussian mixture model .as for clustering features the difference between feature sets is relatively small .discussion	this makes sense as the points used in the offline phase are not necessarily gaussian distributed while k means and agglomerative clustering less strong of an assumption 
0	108078	8078	as for clustering features the difference between feature sets is relatively small .this makes sense as the points used in the offline phase are not necessarily gaussian distributed while k means and agglomerative clustering less strong of an assumption .this makes sense as for many points in the design space our optimization vector will be highly correlated with the lift and drag we are also able to see an interesting trade off when it comes to the number of clusters used .discussion	as for clustering features the difference between feature sets is relatively small 
1	108079	8079	this makes sense as for many points in the design space our optimization vector will be highly correlated with the lift and drag we are also able to see an interesting trade off when it comes to the number of clusters used .as for clustering features the difference between feature sets is relatively small .from the results we can clearly see that the error decreases with the number of cluster sizes .discussion	this makes sense as for many points in the design space our optimization vector will be highly correlated with the lift and drag we are also able to see an interesting trade off when it comes to the number of clusters used 
0	108080	8080	from the results we can clearly see that the error decreases with the number of cluster sizes .this makes sense as for many points in the design space our optimization vector will be highly correlated with the lift and drag we are also able to see an interesting trade off when it comes to the number of clusters used .this is sensible because as we increase the number of clusters the number of points are assigned points used to create each rob decreases decreasing the accuracy of the rom approximation .discussion	from the results we can clearly see that the error decreases with the number of cluster sizes 
1	108081	8081	this is sensible because as we increase the number of clusters the number of points are assigned points used to create each rob decreases decreasing the accuracy of the rom approximation .from the results we can clearly see that the error decreases with the number of cluster sizes .however as the number of points used to build the rob decreases so does the computation cost of running a simulation with the corresponding rom .discussion	this is sensible because as we increase the number of clusters the number of points are assigned points used to create each rob decreases decreasing the accuracy of the rom approximation 
1	108082	8082	however as the number of points used to build the rob decreases so does the computation cost of running a simulation with the corresponding rom .this is sensible because as we increase the number of clusters the number of points are assigned points used to create each rob decreases decreasing the accuracy of the rom approximation .therefore the number of clusters used should be chosen on a per application basis where the user would select the number of clusters corresponding to the acceptable error overall we can see that the our proposed methodology is superior when compared with using a global rom .discussion	however as the number of points used to build the rob decreases so does the computation cost of running a simulation with the corresponding rom 
1	108083	8083	therefore the number of clusters used should be chosen on a per application basis where the user would select the number of clusters corresponding to the acceptable error overall we can see that the our proposed methodology is superior when compared with using a global rom .however as the number of points used to build the rob decreases so does the computation cost of running a simulation with the corresponding rom .we can see that we are either able to get a much higher accuracy than the global rom with a similar computational cost related to the rom size or we are able to achieve a similar accuracy with half the computation cost of the global rom with regards to predictors for parameter selection we can see that all three cluster scoring methods show some indication that they could be used as a predictor for cluster rom accuracy at least for our design space .discussion	therefore the number of clusters used should be chosen on a per application basis where the user would select the number of clusters corresponding to the acceptable error overall we can see that the our proposed methodology is superior when compared with using a global rom 
1	108084	8084	we can see that we are either able to get a much higher accuracy than the global rom with a similar computational cost related to the rom size or we are able to achieve a similar accuracy with half the computation cost of the global rom with regards to predictors for parameter selection we can see that all three cluster scoring methods show some indication that they could be used as a predictor for cluster rom accuracy at least for our design space .therefore the number of clusters used should be chosen on a per application basis where the user would select the number of clusters corresponding to the acceptable error overall we can see that the our proposed methodology is superior when compared with using a global rom .silhouette score and the calinski harabaz index may be slightly more correlated than the davies bouldin as the distance between points on the edges of clusters are reflected in their scores rather then only accounting for the distances between cluster centroids .discussion	we can see that we are either able to get a much higher accuracy than the global rom with a similar computational cost related to the rom size or we are able to achieve a similar accuracy with half the computation cost of the global rom with regards to predictors for parameter selection we can see that all three cluster scoring methods show some indication that they could be used as a predictor for cluster rom accuracy at least for our design space 
1	108085	8085	silhouette score and the calinski harabaz index may be slightly more correlated than the davies bouldin as the distance between points on the edges of clusters are reflected in their scores rather then only accounting for the distances between cluster centroids .we can see that we are either able to get a much higher accuracy than the global rom with a similar computational cost related to the rom size or we are able to achieve a similar accuracy with half the computation cost of the global rom with regards to predictors for parameter selection we can see that all three cluster scoring methods show some indication that they could be used as a predictor for cluster rom accuracy at least for our design space .however more rigorous testing is needed especially we do not know if it will generalize to other pde constrained optimization problems .discussion	silhouette score and the calinski harabaz index may be slightly more correlated than the davies bouldin as the distance between points on the edges of clusters are reflected in their scores rather then only accounting for the distances between cluster centroids 
1	108086	8086	however more rigorous testing is needed especially we do not know if it will generalize to other pde constrained optimization problems .silhouette score and the calinski harabaz index may be slightly more correlated than the davies bouldin as the distance between points on the edges of clusters are reflected in their scores rather then only accounting for the distances between cluster centroids .last sentence.discussion	however more rigorous testing is needed especially we do not know if it will generalize to other pde constrained optimization problems 
1	108087	8087	in conclusion we present a novel approach to solving pdeconstrained optimization problems by utilizing multiple piecewise roms .first sentence.this approach has proven to be both more accurate and more computationally efficient than using a single global rom .conclusion future work	in conclusion we present a novel approach to solving pdeconstrained optimization problems by utilizing multiple piecewise roms 
0	108088	8088	this approach has proven to be both more accurate and more computationally efficient than using a single global rom .in conclusion we present a novel approach to solving pdeconstrained optimization problems by utilizing multiple piecewise roms .it shows particularly strong promise for time constrained applications with high dimensional design spaces .conclusion future work	this approach has proven to be both more accurate and more computationally efficient than using a single global rom 
0	108089	8089	it shows particularly strong promise for time constrained applications with high dimensional design spaces .this approach has proven to be both more accurate and more computationally efficient than using a single global rom .in these scenarios the global rom would need to be very large in order to be accurate across the whole design space and thus it might not be able to meet real time deadlines .conclusion future work	it shows particularly strong promise for time constrained applications with high dimensional design spaces 
1	108090	8090	in these scenarios the global rom would need to be very large in order to be accurate across the whole design space and thus it might not be able to meet real time deadlines .it shows particularly strong promise for time constrained applications with high dimensional design spaces .piecewise roms on the other hand can be more efficient and thus able to meet the timing constraints we would like to continue testing the performance of our approach in more realistic higher dimensional design spaces 50 60 parameters .conclusion future work	in these scenarios the global rom would need to be very large in order to be accurate across the whole design space and thus it might not be able to meet real time deadlines 
1	108091	8091	piecewise roms on the other hand can be more efficient and thus able to meet the timing constraints we would like to continue testing the performance of our approach in more realistic higher dimensional design spaces 50 60 parameters .in these scenarios the global rom would need to be very large in order to be accurate across the whole design space and thus it might not be able to meet real time deadlines .for this project we chose a limited design space due to time constraints as running tests in higher dimensional design spaces is naturally more computationally expensive and takes more time .conclusion future work	piecewise roms on the other hand can be more efficient and thus able to meet the timing constraints we would like to continue testing the performance of our approach in more realistic higher dimensional design spaces 50 60 parameters 
1	108092	8092	for this project we chose a limited design space due to time constraints as running tests in higher dimensional design spaces is naturally more computationally expensive and takes more time .piecewise roms on the other hand can be more efficient and thus able to meet the timing constraints we would like to continue testing the performance of our approach in more realistic higher dimensional design spaces 50 60 parameters .we would also like to continue research on predictors for clustering effectiveness as this is a key component for this approach to be practical in real world problems .conclusion future work	for this project we chose a limited design space due to time constraints as running tests in higher dimensional design spaces is naturally more computationally expensive and takes more time 
1	108093	8093	we would also like to continue research on predictors for clustering effectiveness as this is a key component for this approach to be practical in real world problems .for this project we chose a limited design space due to time constraints as running tests in higher dimensional design spaces is naturally more computationally expensive and takes more time .last sentence.conclusion future work	we would also like to continue research on predictors for clustering effectiveness as this is a key component for this approach to be practical in real world problems 
0	108094	8094	the first part of this project was discussing and creating a new methodology for solving pde constrained optimization problem .first sentence.this was a significant part of the project where both forest and gabriele discussed on the optimal approach to take .code	the first part of this project was discussing and creating a new methodology for solving pde constrained optimization problem 
0	108095	8095	this was a significant part of the project where both forest and gabriele discussed on the optimal approach to take .the first part of this project was discussing and creating a new methodology for solving pde constrained optimization problem .to implement this methodology gabriele wrote code to build and run roms from a set of training points in addition to writing code to generate the data to start the experiments .code	this was a significant part of the project where both forest and gabriele discussed on the optimal approach to take 
1	108096	8096	to implement this methodology gabriele wrote code to build and run roms from a set of training points in addition to writing code to generate the data to start the experiments .this was a significant part of the project where both forest and gabriele discussed on the optimal approach to take .forest was responsible for implementing the machine learning algorithms from external libraries as well as automating testing .code	to implement this methodology gabriele wrote code to build and run roms from a set of training points in addition to writing code to generate the data to start the experiments 
0	108097	8097	forest was responsible for implementing the machine learning algorithms from external libraries as well as automating testing .to implement this methodology gabriele wrote code to build and run roms from a set of training points in addition to writing code to generate the data to start the experiments .both gabriele and forest contributed towards research and decision making for the use of machine learning techniques in this project in addition writing routines to output and post process results for analysis unfortunately the code must be run on a super computer with many external libraries from the stanford aeronautics astronautics department .code	forest was responsible for implementing the machine learning algorithms from external libraries as well as automating testing 
1	108098	8098	both gabriele and forest contributed towards research and decision making for the use of machine learning techniques in this project in addition writing routines to output and post process results for analysis unfortunately the code must be run on a super computer with many external libraries from the stanford aeronautics astronautics department .forest was responsible for implementing the machine learning algorithms from external libraries as well as automating testing .we have included a zip file containing the only the code written for this project available at https drive google com file d 1bp4iw6rir cn3hxwl58cf pi5xppsi4w view usp sharing.code	both gabriele and forest contributed towards research and decision making for the use of machine learning techniques in this project in addition writing routines to output and post process results for analysis unfortunately the code must be run on a super computer with many external libraries from the stanford aeronautics astronautics department 
0	108099	8099	we have included a zip file containing the only the code written for this project available at https drive google com file d 1bp4iw6rir cn3hxwl58cf pi5xppsi4w view usp sharing.both gabriele and forest contributed towards research and decision making for the use of machine learning techniques in this project in addition writing routines to output and post process results for analysis unfortunately the code must be run on a super computer with many external libraries from the stanford aeronautics astronautics department .last sentence.code	we have included a zip file containing the only the code written for this project available at https drive google com file d 1bp4iw6rir cn3hxwl58cf pi5xppsi4w view usp sharing
0	108100	8100	humans are generally good at categorizing and organizing music .first sentence.we may create novel playlists containing songs that may seem very dissimilar according to any baseline similarity measures .introduction	humans are generally good at categorizing and organizing music 
1	108101	8101	we may create novel playlists containing songs that may seem very dissimilar according to any baseline similarity measures .humans are generally good at categorizing and organizing music .it is this novelty and deeper level of understanding that we attempt to learn in this work how do users put together novel playlists .introduction	we may create novel playlists containing songs that may seem very dissimilar according to any baseline similarity measures 
1	108102	8102	it is this novelty and deeper level of understanding that we attempt to learn in this work how do users put together novel playlists .we may create novel playlists containing songs that may seem very dissimilar according to any baseline similarity measures .music listeners create playlists based on a multitude of strategies and intents .introduction	it is this novelty and deeper level of understanding that we attempt to learn in this work how do users put together novel playlists 
0	108103	8103	music listeners create playlists based on a multitude of strategies and intents .it is this novelty and deeper level of understanding that we attempt to learn in this work how do users put together novel playlists .many users put similar sounding songs together some make playlists solely from one artist others generate heterogeneous mixes spanning multiple genres eras and soundscapes .introduction	music listeners create playlists based on a multitude of strategies and intents 
1	108104	8104	many users put similar sounding songs together some make playlists solely from one artist others generate heterogeneous mixes spanning multiple genres eras and soundscapes .music listeners create playlists based on a multitude of strategies and intents .clearly the problem of playlist classification increases in difficulty as playlist moods become less concrete and or separable .introduction	many users put similar sounding songs together some make playlists solely from one artist others generate heterogeneous mixes spanning multiple genres eras and soundscapes 
1	108105	8105	clearly the problem of playlist classification increases in difficulty as playlist moods become less concrete and or separable .many users put similar sounding songs together some make playlists solely from one artist others generate heterogeneous mixes spanning multiple genres eras and soundscapes .to tackle this problem we will be using spotify s api to gather audio artist and genre data for selected playlists .introduction	clearly the problem of playlist classification increases in difficulty as playlist moods become less concrete and or separable 
1	108106	8106	to tackle this problem we will be using spotify s api to gather audio artist and genre data for selected playlists .clearly the problem of playlist classification increases in difficulty as playlist moods become less concrete and or separable .with a more carefully curated and specialized fingerprint of each playlist we hope to teach an algorithm to understand what makes that user and their playlists special .introduction	to tackle this problem we will be using spotify s api to gather audio artist and genre data for selected playlists 
1	108107	8107	with a more carefully curated and specialized fingerprint of each playlist we hope to teach an algorithm to understand what makes that user and their playlists special .to tackle this problem we will be using spotify s api to gather audio artist and genre data for selected playlists .the playlist lengths in the dataset .introduction	with a more carefully curated and specialized fingerprint of each playlist we hope to teach an algorithm to understand what makes that user and their playlists special 
0	108108	8108	the playlist lengths in the dataset .with a more carefully curated and specialized fingerprint of each playlist we hope to teach an algorithm to understand what makes that user and their playlists special .for example in the toy dataset section 3 m 1044 .introduction	the playlist lengths in the dataset 
0	108109	8109	for example in the toy dataset section 3 m 1044 .the playlist lengths in the dataset .each song is labeled with an integer y i c representing one of the c output classes playlists .introduction	for example in the toy dataset section 3 m 1044 
0	108110	8110	each song is labeled with an integer y i c representing one of the c output classes playlists .for example in the toy dataset section 3 m 1044 .in the toy case c 13 .introduction	each song is labeled with an integer y i c representing one of the c output classes playlists 
0	108111	8111	in the toy case c 13 .each song is labeled with an integer y i c representing one of the c output classes playlists .last sentence.introduction	in the toy case c 13 
1	108112	8112	in general we would describe our problem space as non radio playlist continuation to distinguish from real time radio based solutions .first sentence.there are a few core concepts in our approach .related works	in general we would describe our problem space as non radio playlist continuation to distinguish from real time radio based solutions 
0	108113	8113	there are a few core concepts in our approach .in general we would describe our problem space as non radio playlist continuation to distinguish from real time radio based solutions .first we attempt to model the theme of a playlist with minimal assumptions about the user .related works	there are a few core concepts in our approach 
1	108114	8114	first we attempt to model the theme of a playlist with minimal assumptions about the user .there are a few core concepts in our approach .second the model should be scalable and generalizable to any user and any song on a service we use spotify .related works	first we attempt to model the theme of a playlist with minimal assumptions about the user 
1	108115	8115	second the model should be scalable and generalizable to any user and any song on a service we use spotify .first we attempt to model the theme of a playlist with minimal assumptions about the user .third the model should be novel and specific to each user .related works	second the model should be scalable and generalizable to any user and any song on a service we use spotify 
0	108116	8116	third the model should be novel and specific to each user .second the model should be scalable and generalizable to any user and any song on a service we use spotify .these concepts are informed by three fundamental pieces of literature in the space which conclude the following individuals apply a wide variety of methods and reasoning for how and why their playlists are created model playlists using random walks on a hypergraph of song nodes especially due to large data these implementations would not be fully appropriate for our application however they introduce important core concepts and algorithms that we have borrowed such as hybrid feature sets both collaborative filtering and contentbased features .related works	third the model should be novel and specific to each user 
1	108117	8117	these concepts are informed by three fundamental pieces of literature in the space which conclude the following individuals apply a wide variety of methods and reasoning for how and why their playlists are created model playlists using random walks on a hypergraph of song nodes especially due to large data these implementations would not be fully appropriate for our application however they introduce important core concepts and algorithms that we have borrowed such as hybrid feature sets both collaborative filtering and contentbased features .third the model should be novel and specific to each user .last sentence.related works	these concepts are informed by three fundamental pieces of literature in the space which conclude the following individuals apply a wide variety of methods and reasoning for how and why their playlists are created model playlists using random walks on a hypergraph of song nodes especially due to large data these implementations would not be fully appropriate for our application however they introduce important core concepts and algorithms that we have borrowed such as hybrid feature sets both collaborative filtering and contentbased features 
1	108118	8118	for our first study we started with data we perceived to be most separable a toy set of c 1 13 spotify curated playlists country by the grace of god spread the gospel for a total of m 1 1044 tracks .first sentence.the first experiment is an idealized fully supervised setting .dataset features	for our first study we started with data we perceived to be most separable a toy set of c 1 13 spotify curated playlists country by the grace of god spread the gospel for a total of m 1 1044 tracks 
0	108119	8119	the first experiment is an idealized fully supervised setting .for our first study we started with data we perceived to be most separable a toy set of c 1 13 spotify curated playlists country by the grace of god spread the gospel for a total of m 1 1044 tracks .we also conduct a second study generalizing the most successful model on real user data to truly challenge the models for each playlist we used spotify s api to pull tracks audio features per track and genre tags per artist in the dataset spotify has developed 2095 genre tags spanning from broad ex .dataset features	the first experiment is an idealized fully supervised setting 
1	108120	8120	we also conduct a second study generalizing the most successful model on real user data to truly challenge the models for each playlist we used spotify s api to pull tracks audio features per track and genre tags per artist in the dataset spotify has developed 2095 genre tags spanning from broad ex .the first experiment is an idealized fully supervised setting .pop or r b to extremely granular ex .dataset features	we also conduct a second study generalizing the most successful model on real user data to truly challenge the models for each playlist we used spotify s api to pull tracks audio features per track and genre tags per artist in the dataset spotify has developed 2095 genre tags spanning from broad ex 
0	108121	8121	pop or r b to extremely granular ex .we also conduct a second study generalizing the most successful model on real user data to truly challenge the models for each playlist we used spotify s api to pull tracks audio features per track and genre tags per artist in the dataset spotify has developed 2095 genre tags spanning from broad ex .australian dance and big room .dataset features	pop or r b to extremely granular ex 
0	108122	8122	australian dance and big room .pop or r b to extremely granular ex .these audio features and genre tags are created with spotify s internal machine listening and analysis capabilities acquired mostly from echonest for both experiments we split the dataset 80 20 80 train and 20 test .dataset features	australian dance and big room 
1	108123	8123	these audio features and genre tags are created with spotify s internal machine listening and analysis capabilities acquired mostly from echonest for both experiments we split the dataset 80 20 80 train and 20 test .australian dance and big room .in the future we may explore data augmentation techniques to expand the size of these datasets most of the audio features ex .dataset features	these audio features and genre tags are created with spotify s internal machine listening and analysis capabilities acquired mostly from echonest for both experiments we split the dataset 80 20 80 train and 20 test 
0	108124	8124	in the future we may explore data augmentation techniques to expand the size of these datasets most of the audio features ex .these audio features and genre tags are created with spotify s internal machine listening and analysis capabilities acquired mostly from echonest for both experiments we split the dataset 80 20 80 train and 20 test .danceability energy and speechiness are measures between 0 1 .dataset features	in the future we may explore data augmentation techniques to expand the size of these datasets most of the audio features ex 
0	108125	8125	danceability energy and speechiness are measures between 0 1 .in the future we may explore data augmentation techniques to expand the size of these datasets most of the audio features ex .however key takes on an integer value 0 11 loudness is a float 60 0 measuring average decibel db reading across a track and tempo is a float representing beats per minute bpm .dataset features	danceability energy and speechiness are measures between 0 1 
0	108126	8126	however key takes on an integer value 0 11 loudness is a float 60 0 measuring average decibel db reading across a track and tempo is a float representing beats per minute bpm .danceability energy and speechiness are measures between 0 1 .as such we explore the effects of applying the standard preprocessing step of subtracting the mean and scaling by the variance of each feature .dataset features	however key takes on an integer value 0 11 loudness is a float 60 0 measuring average decibel db reading across a track and tempo is a float representing beats per minute bpm 
0	108127	8127	as such we explore the effects of applying the standard preprocessing step of subtracting the mean and scaling by the variance of each feature .however key takes on an integer value 0 11 loudness is a float 60 0 measuring average decibel db reading across a track and tempo is a float representing beats per minute bpm .this ensures that the relative scale of these features do not negatively skew the results .dataset features	as such we explore the effects of applying the standard preprocessing step of subtracting the mean and scaling by the variance of each feature 
0	108128	8128	this ensures that the relative scale of these features do not negatively skew the results .as such we explore the effects of applying the standard preprocessing step of subtracting the mean and scaling by the variance of each feature .a summary of our results is in section 5 .dataset features	this ensures that the relative scale of these features do not negatively skew the results 
0	108129	8129	a summary of our results is in section 5 .this ensures that the relative scale of these features do not negatively skew the results .last sentence.dataset features	a summary of our results is in section 5 
1	108130	8130	the models we elected to compare are perceptron regression with one versus all binary classification for each class various svms polynomial sigmoid and rbf kernels and two neural network architectures the perceptron update rule given a sample prediction h x i is given bythis update rule comes a generalization of the gradient descent update on the negative log likelihood for logistic regression .first sentence.svms attempt to find a boundary which maximizes the geometric margin between classes .methods	the models we elected to compare are perceptron regression with one versus all binary classification for each class various svms polynomial sigmoid and rbf kernels and two neural network architectures the perceptron update rule given a sample prediction h x i is given bythis update rule comes a generalization of the gradient descent update on the negative log likelihood for logistic regression 
0	108131	8131	svms attempt to find a boundary which maximizes the geometric margin between classes .the models we elected to compare are perceptron regression with one versus all binary classification for each class various svms polynomial sigmoid and rbf kernels and two neural network architectures the perceptron update rule given a sample prediction h x i is given bythis update rule comes a generalization of the gradient descent update on the negative log likelihood for logistic regression .specifically denoting the geometric margin we look to solve the following constrained maximization problem w 1 i e find the separating boundary given by w and b such that all samples are a distance of at least away from the boundary .methods	svms attempt to find a boundary which maximizes the geometric margin between classes 
0	108132	8132	specifically denoting the geometric margin we look to solve the following constrained maximization problem w 1 i e find the separating boundary given by w and b such that all samples are a distance of at least away from the boundary .svms attempt to find a boundary which maximizes the geometric margin between classes .the true algorithm solves the dual form of this primal problem as the problem as stated above is non convex .methods	specifically denoting the geometric margin we look to solve the following constrained maximization problem w 1 i e find the separating boundary given by w and b such that all samples are a distance of at least away from the boundary 
0	108133	8133	the true algorithm solves the dual form of this primal problem as the problem as stated above is non convex .specifically denoting the geometric margin we look to solve the following constrained maximization problem w 1 i e find the separating boundary given by w and b such that all samples are a distance of at least away from the boundary .the library used for the regression and svms is scikit learn python s machine learning platform .methods	the true algorithm solves the dual form of this primal problem as the problem as stated above is non convex 
1	108134	8134	the library used for the regression and svms is scikit learn python s machine learning platform .the true algorithm solves the dual form of this primal problem as the problem as stated above is non convex .lastly a neural network can be thought of as a sequence of linear transformations non linear functions applied to an input set of samples x r m n to obtain a prediction vector y r n .methods	the library used for the regression and svms is scikit learn python s machine learning platform 
0	108135	8135	lastly a neural network can be thought of as a sequence of linear transformations non linear functions applied to an input set of samples x r m n to obtain a prediction vector y r n .the library used for the regression and svms is scikit learn python s machine learning platform .y a h a 2 where w i is a linear transformation a i is a non linear activation function applied element wise to its argument and h denotes the number of hidden layers .methods	lastly a neural network can be thought of as a sequence of linear transformations non linear functions applied to an input set of samples x r m n to obtain a prediction vector y r n 
0	108136	8136	y a h a 2 where w i is a linear transformation a i is a non linear activation function applied element wise to its argument and h denotes the number of hidden layers .lastly a neural network can be thought of as a sequence of linear transformations non linear functions applied to an input set of samples x r m n to obtain a prediction vector y r n .the neural network attempts to learn a weighting of the input features w i s that best classifies the output .methods	y a h a 2 where w i is a linear transformation a i is a non linear activation function applied element wise to its argument and h denotes the number of hidden layers 
1	108137	8137	the neural network attempts to learn a weighting of the input features w i s that best classifies the output .y a h a 2 where w i is a linear transformation a i is a non linear activation function applied element wise to its argument and h denotes the number of hidden layers .the neural networks were implemented in pytorch though we test 6 models architectures in this work we have elected to focus most of our efforts on the neural networks to solve the problem .methods	the neural network attempts to learn a weighting of the input features w i s that best classifies the output 
1	108138	8138	the neural networks were implemented in pytorch though we test 6 models architectures in this work we have elected to focus most of our efforts on the neural networks to solve the problem .the neural network attempts to learn a weighting of the input features w i s that best classifies the output .it is difficult sometimes even for humans to discern exactly what holds a playlist together .methods	the neural networks were implemented in pytorch though we test 6 models architectures in this work we have elected to focus most of our efforts on the neural networks to solve the problem 
0	108139	8139	it is difficult sometimes even for humans to discern exactly what holds a playlist together .the neural networks were implemented in pytorch though we test 6 models architectures in this work we have elected to focus most of our efforts on the neural networks to solve the problem .furthermore the relationship between song features and playlists will not lend itself well to geometric separation in a feature space further complicated by the fact that one data point song may have multiple labels belong to multiple playlists .methods	it is difficult sometimes even for humans to discern exactly what holds a playlist together 
1	108140	8140	furthermore the relationship between song features and playlists will not lend itself well to geometric separation in a feature space further complicated by the fact that one data point song may have multiple labels belong to multiple playlists .it is difficult sometimes even for humans to discern exactly what holds a playlist together .the complexity of this problem indicates that less complex algorithms may not be suited to solving the problem effectively .methods	furthermore the relationship between song features and playlists will not lend itself well to geometric separation in a feature space further complicated by the fact that one data point song may have multiple labels belong to multiple playlists 
0	108141	8141	the complexity of this problem indicates that less complex algorithms may not be suited to solving the problem effectively .furthermore the relationship between song features and playlists will not lend itself well to geometric separation in a feature space further complicated by the fact that one data point song may have multiple labels belong to multiple playlists .last sentence.methods	the complexity of this problem indicates that less complex algorithms may not be suited to solving the problem effectively 
0	108142	8142	for all tests below we took test accuracy to be our primary metric of success .first sentence.ta correctly classified total of samples many models achieved high training accuracy but unless the test accuracy matched this level the model was likely overfitted .results	for all tests below we took test accuracy to be our primary metric of success 
0	108143	8143	ta correctly classified total of samples many models achieved high training accuracy but unless the test accuracy matched this level the model was likely overfitted .for all tests below we took test accuracy to be our primary metric of success .last sentence.results	ta correctly classified total of samples many models achieved high training accuracy but unless the test accuracy matched this level the model was likely overfitted 
1	108144	8144	we compared perceptron with polynomial sigmoid and rbf kerneled svms with and without the preprocessing step of rescaling the features on the toy dataset 13 spotify curated playlists .first sentence.we performed k fold cross validation with k 5 .regression svms	we compared perceptron with polynomial sigmoid and rbf kerneled svms with and without the preprocessing step of rescaling the features on the toy dataset 13 spotify curated playlists 
0	108145	8145	we performed k fold cross validation with k 5 .we compared perceptron with polynomial sigmoid and rbf kerneled svms with and without the preprocessing step of rescaling the features on the toy dataset 13 spotify curated playlists .the results are summarized in without performing the preprocessing step the accuracy of the results takes a significant hit .regression svms	we performed k fold cross validation with k 5 
0	108146	8146	the results are summarized in without performing the preprocessing step the accuracy of the results takes a significant hit .we performed k fold cross validation with k 5 .on preprocessed data rbf kerneled svm reliably performed the best achieved the highest test accuracy .regression svms	the results are summarized in without performing the preprocessing step the accuracy of the results takes a significant hit 
0	108147	8147	on preprocessed data rbf kerneled svm reliably performed the best achieved the highest test accuracy .the results are summarized in without performing the preprocessing step the accuracy of the results takes a significant hit .tuning the penalty parameter on the error term we obtain a final test accuracy of 0 80 0 05 .regression svms	on preprocessed data rbf kerneled svm reliably performed the best achieved the highest test accuracy 
0	108148	8148	tuning the penalty parameter on the error term we obtain a final test accuracy of 0 80 0 05 .on preprocessed data rbf kerneled svm reliably performed the best achieved the highest test accuracy .the precision recall f score and support results are tabulated in note that the playlists celtic punk and spread the gospel achieved 100 precision by the tuned svm all corresponding tracks in the test set were correctly classified into these playlists .regression svms	tuning the penalty parameter on the error term we obtain a final test accuracy of 0 80 0 05 
1	108149	8149	the precision recall f score and support results are tabulated in note that the playlists celtic punk and spread the gospel achieved 100 precision by the tuned svm all corresponding tracks in the test set were correctly classified into these playlists .tuning the penalty parameter on the error term we obtain a final test accuracy of 0 80 0 05 .this demonstrates the efficacy of these methods on more unique genre specific lists .regression svms	the precision recall f score and support results are tabulated in note that the playlists celtic punk and spread the gospel achieved 100 precision by the tuned svm all corresponding tracks in the test set were correctly classified into these playlists 
0	108150	8150	this demonstrates the efficacy of these methods on more unique genre specific lists .the precision recall f score and support results are tabulated in note that the playlists celtic punk and spread the gospel achieved 100 precision by the tuned svm all corresponding tracks in the test set were correctly classified into these playlists .kitchen swagger on the other hand is a harder playlist to classify as it does not match any one genre .regression svms	this demonstrates the efficacy of these methods on more unique genre specific lists 
0	108151	8151	kitchen swagger on the other hand is a harder playlist to classify as it does not match any one genre .this demonstrates the efficacy of these methods on more unique genre specific lists .last sentence.regression svms	kitchen swagger on the other hand is a harder playlist to classify as it does not match any one genre 
1	108152	8152	we considered many parameters of our network number of hidden layers activation functions sigmoid relu identity softmax logsoftmax with vs without l 2 regulation number of iterations stochastic batch full gradient descent loss function mse vs nll etc .first sentence.we ultimately found that a neural network minimizing nll loss via full gradient descent with one hidden layer of c neurons and a logsoftmax output layer performs best on the toy set .neural network	we considered many parameters of our network number of hidden layers activation functions sigmoid relu identity softmax logsoftmax with vs without l 2 regulation number of iterations stochastic batch full gradient descent loss function mse vs nll etc 
1	108153	8153	we ultimately found that a neural network minimizing nll loss via full gradient descent with one hidden layer of c neurons and a logsoftmax output layer performs best on the toy set .we considered many parameters of our network number of hidden layers activation functions sigmoid relu identity softmax logsoftmax with vs without l 2 regulation number of iterations stochastic batch full gradient descent loss function mse vs nll etc .last sentence.neural network	we ultimately found that a neural network minimizing nll loss via full gradient descent with one hidden layer of c neurons and a logsoftmax output layer performs best on the toy set 
1	108154	8154	train test 2 identity sigmoid 0 91 0 77 1 sigmoid 0 89 0 82 with the architecture finalized we tested on a handful of real users again training on 80 of their playlists and testing on the remaining 20 .first sentence.the results are summarized in.hidden layers	train test 2 identity sigmoid 0 91 0 77 1 sigmoid 0 89 0 82 with the architecture finalized we tested on a handful of real users again training on 80 of their playlists and testing on the remaining 20 
0	108155	8155	the results are summarized in.train test 2 identity sigmoid 0 91 0 77 1 sigmoid 0 89 0 82 with the architecture finalized we tested on a handful of real users again training on 80 of their playlists and testing on the remaining 20 .last sentence.hidden layers	the results are summarized in
1	108156	8156	train test a jacob s playlists b myles playlists we see that certain users are more challenging to classify than others ex .first sentence.myles vs jacob .user	train test a jacob s playlists b myles playlists we see that certain users are more challenging to classify than others ex 
0	108157	8157	myles vs jacob .train test a jacob s playlists b myles playlists we see that certain users are more challenging to classify than others ex .last sentence.user	myles vs jacob 
0	108158	8158	in some ways the observed results are not surprising .first sentence.we have observed that a relatively minimal neural network does a better job at classifying playlists than perceptron and svms do .discussion	in some ways the observed results are not surprising 
1	108159	8159	we have observed that a relatively minimal neural network does a better job at classifying playlists than perceptron and svms do .in some ways the observed results are not surprising .this is understandable as the nature of curating a playlist is rather subjective and playlists are not always separable .discussion	we have observed that a relatively minimal neural network does a better job at classifying playlists than perceptron and svms do 
0	108160	8160	this is understandable as the nature of curating a playlist is rather subjective and playlists are not always separable .we have observed that a relatively minimal neural network does a better job at classifying playlists than perceptron and svms do .even so the neural network struggled against a real user set .discussion	this is understandable as the nature of curating a playlist is rather subjective and playlists are not always separable 
0	108161	8161	even so the neural network struggled against a real user set .this is understandable as the nature of curating a playlist is rather subjective and playlists are not always separable .there are multiple factors that play into this larger labeled datasets .discussion	even so the neural network struggled against a real user set 
0	108162	8162	there are multiple factors that play into this larger labeled datasets .even so the neural network struggled against a real user set .in many instances the user data we were training testing on consisted of only hundreds sometimes tens of samples .discussion	there are multiple factors that play into this larger labeled datasets 
0	108163	8163	in many instances the user data we were training testing on consisted of only hundreds sometimes tens of samples .there are multiple factors that play into this larger labeled datasets .training on only 80 of this dataset decreases this number further .discussion	in many instances the user data we were training testing on consisted of only hundreds sometimes tens of samples 
0	108164	8164	training on only 80 of this dataset decreases this number further .in many instances the user data we were training testing on consisted of only hundreds sometimes tens of samples .this is not enough information to train a neural network classifier on nor to gather meaningful test results on given our limited features .discussion	training on only 80 of this dataset decreases this number further 
1	108165	8165	this is not enough information to train a neural network classifier on nor to gather meaningful test results on given our limited features .training on only 80 of this dataset decreases this number further .in the future we may either select larger user playlists or consider applying data augmentation techniques to increase the size of our set segmentation .discussion	this is not enough information to train a neural network classifier on nor to gather meaningful test results on given our limited features 
1	108166	8166	in the future we may either select larger user playlists or consider applying data augmentation techniques to increase the size of our set segmentation .this is not enough information to train a neural network classifier on nor to gather meaningful test results on given our limited features .it is easier to classify a song into a playlist when the number of output classes c is small .discussion	in the future we may either select larger user playlists or consider applying data augmentation techniques to increase the size of our set segmentation 
1	108167	8167	it is easier to classify a song into a playlist when the number of output classes c is small .in the future we may either select larger user playlists or consider applying data augmentation techniques to increase the size of our set segmentation .this concept is known as segmentation different algorithm .discussion	it is easier to classify a song into a playlist when the number of output classes c is small 
0	108168	8168	this concept is known as segmentation different algorithm .it is easier to classify a song into a playlist when the number of output classes c is small .it may be the case that even neural networks are not the best option for this problem though literature in the field suggests otherwise more track features .discussion	this concept is known as segmentation different algorithm 
0	108169	8169	it may be the case that even neural networks are not the best option for this problem though literature in the field suggests otherwise more track features .this concept is known as segmentation different algorithm .we have built a classifier with only audio features and one hot vectors of genre tags for each track .discussion	it may be the case that even neural networks are not the best option for this problem though literature in the field suggests otherwise more track features 
1	108170	8170	we have built a classifier with only audio features and one hot vectors of genre tags for each track .it may be the case that even neural networks are not the best option for this problem though literature in the field suggests otherwise more track features .we are missing vital data such as artist info release date etc section 7 .discussion	we have built a classifier with only audio features and one hot vectors of genre tags for each track 
0	108171	8171	we are missing vital data such as artist info release date etc section 7 .we have built a classifier with only audio features and one hot vectors of genre tags for each track .last sentence.discussion	we are missing vital data such as artist info release date etc section 7 
0	108172	8172	playlist curation and music recommendation is an art that trained music analysts and disc jockeys have perfected over decades .first sentence.with the rise of massive listener data and the wealth of statistical and algorithmic developments we have begun to see a rise in computergenerated playlists and mixes .conclusion future works	playlist curation and music recommendation is an art that trained music analysts and disc jockeys have perfected over decades 
0	108173	8173	with the rise of massive listener data and the wealth of statistical and algorithmic developments we have begun to see a rise in computergenerated playlists and mixes .playlist curation and music recommendation is an art that trained music analysts and disc jockeys have perfected over decades .at this stage however there is massive room for improvement .conclusion future works	with the rise of massive listener data and the wealth of statistical and algorithmic developments we have begun to see a rise in computergenerated playlists and mixes 
0	108174	8174	at this stage however there is massive room for improvement .with the rise of massive listener data and the wealth of statistical and algorithmic developments we have begun to see a rise in computergenerated playlists and mixes .we have attempted to build a playlist classifier using audio features and genre tag data from spotify s public api node2vec .conclusion future works	at this stage however there is massive room for improvement 
1	108175	8175	we have attempted to build a playlist classifier using audio features and genre tag data from spotify s public api node2vec .at this stage however there is massive room for improvement .through spotify s api we have access to relatedartists data each artist comes with a list of 20 similar artists .conclusion future works	we have attempted to build a playlist classifier using audio features and genre tag data from spotify s public api node2vec 
1	108176	8176	through spotify s api we have access to relatedartists data each artist comes with a list of 20 similar artists .we have attempted to build a playlist classifier using audio features and genre tag data from spotify s public api node2vec .we have gathered this information and built a related artists graph where each node represents an artist and edges link like artists .conclusion future works	through spotify s api we have access to relatedartists data each artist comes with a list of 20 similar artists 
1	108177	8177	we have gathered this information and built a related artists graph where each node represents an artist and edges link like artists .through spotify s api we have access to relatedartists data each artist comes with a list of 20 similar artists .using snap.conclusion future works	we have gathered this information and built a related artists graph where each node represents an artist and edges link like artists 
0	108178	8178	using snap.we have gathered this information and built a related artists graph where each node represents an artist and edges link like artists .last sentence.conclusion future works	using snap
1	108179	8179	in the future we hope to integrate this into our framework taking advantage of large free corpuses such as wikipedia and music website scrapes .google has published an open source library for computing vector embeddings of words .results from echonest show that nlp understanding of the text around music are highly effective in music classification for example the stevie wonder vector may be near the words soul michael jackson piano or motown in the vector space .word2vec	in the future we hope to integrate this into our framework taking advantage of large free corpuses such as wikipedia and music website scrapes 
1	108180	8180	results from echonest show that nlp understanding of the text around music are highly effective in music classification for example the stevie wonder vector may be near the words soul michael jackson piano or motown in the vector space .in the future we hope to integrate this into our framework taking advantage of large free corpuses such as wikipedia and music website scrapes .candidates for word2vec representations in our data include artist names user generated tags both from our users and from open sources like acousticbrainz genre tags playlist titles song names etc fasttext .word2vec	results from echonest show that nlp understanding of the text around music are highly effective in music classification for example the stevie wonder vector may be near the words soul michael jackson piano or motown in the vector space 
1	108181	8181	candidates for word2vec representations in our data include artist names user generated tags both from our users and from open sources like acousticbrainz genre tags playlist titles song names etc fasttext .results from echonest show that nlp understanding of the text around music are highly effective in music classification for example the stevie wonder vector may be near the words soul michael jackson piano or motown in the vector space .facebook has developed the fasttext library for document classification .word2vec	candidates for word2vec representations in our data include artist names user generated tags both from our users and from open sources like acousticbrainz genre tags playlist titles song names etc fasttext 
0	108182	8182	facebook has developed the fasttext library for document classification .candidates for word2vec representations in our data include artist names user generated tags both from our users and from open sources like acousticbrainz genre tags playlist titles song names etc fasttext .we hope to try using it with the variety of texts that we hope to collect including the wikipedia pages for our artists user generated tags genre tags and a variety of other textual metadata from track information artist information and more .word2vec	facebook has developed the fasttext library for document classification 
1	108183	8183	we hope to try using it with the variety of texts that we hope to collect including the wikipedia pages for our artists user generated tags genre tags and a variety of other textual metadata from track information artist information and more .facebook has developed the fasttext library for document classification .in this case a playlist can be considered a document category and we will attempt to classify song documents as belonging to the document category .word2vec	we hope to try using it with the variety of texts that we hope to collect including the wikipedia pages for our artists user generated tags genre tags and a variety of other textual metadata from track information artist information and more 
0	108184	8184	in this case a playlist can be considered a document category and we will attempt to classify song documents as belonging to the document category .we hope to try using it with the variety of texts that we hope to collect including the wikipedia pages for our artists user generated tags genre tags and a variety of other textual metadata from track information artist information and more .last sentence.word2vec	in this case a playlist can be considered a document category and we will attempt to classify song documents as belonging to the document category 
0	108185	8185	the data used for this problem comes from spotify s web api .first sentence.kevin obtained the necessary authorization with spotify automated the request process for all playlists tracks and feature data and built a postgres database from scratch via sequelize an object relational manager orm for node js .member contributions	the data used for this problem comes from spotify s web api 
0	108186	8186	kevin obtained the necessary authorization with spotify automated the request process for all playlists tracks and feature data and built a postgres database from scratch via sequelize an object relational manager orm for node js .the data used for this problem comes from spotify s web api .once all the data for the toy set was in the database the duo implemented the baseline perceptron svm tests on the toy set using scikit learn .member contributions	kevin obtained the necessary authorization with spotify automated the request process for all playlists tracks and feature data and built a postgres database from scratch via sequelize an object relational manager orm for node js 
1	108187	8187	once all the data for the toy set was in the database the duo implemented the baseline perceptron svm tests on the toy set using scikit learn .kevin obtained the necessary authorization with spotify automated the request process for all playlists tracks and feature data and built a postgres database from scratch via sequelize an object relational manager orm for node js .amel designed and tested our neural networks using pytorch wrote script to get pca visualizations .member contributions	once all the data for the toy set was in the database the duo implemented the baseline perceptron svm tests on the toy set using scikit learn 
0	108188	8188	amel designed and tested our neural networks using pytorch wrote script to get pca visualizations .once all the data for the toy set was in the database the duo implemented the baseline perceptron svm tests on the toy set using scikit learn .kevin took on advanced data collection building a related artists graph and computing vector embeddings of each node using node2vec .member contributions	amel designed and tested our neural networks using pytorch wrote script to get pca visualizations 
1	108189	8189	kevin took on advanced data collection building a related artists graph and computing vector embeddings of each node using node2vec .amel designed and tested our neural networks using pytorch wrote script to get pca visualizations .last sentence.member contributions	kevin took on advanced data collection building a related artists graph and computing vector embeddings of each node using node2vec 
1	108190	8190	understanding fluid flow in porous media at the microscale is relevant to many fields such as oil and gas recovery geothermal energy and geological co 2 storage .first sentence.properties such as porosity and permeability are often calculated from laboratory measurements or direct imaging of the microstructure .i introduction	understanding fluid flow in porous media at the microscale is relevant to many fields such as oil and gas recovery geothermal energy and geological co 2 storage 
1	108191	8191	properties such as porosity and permeability are often calculated from laboratory measurements or direct imaging of the microstructure .understanding fluid flow in porous media at the microscale is relevant to many fields such as oil and gas recovery geothermal energy and geological co 2 storage .however due to acquisition times and experimental costs it is difficult to evaluate the variability due to rock heterogeneity .i introduction	properties such as porosity and permeability are often calculated from laboratory measurements or direct imaging of the microstructure 
1	108192	8192	however due to acquisition times and experimental costs it is difficult to evaluate the variability due to rock heterogeneity .properties such as porosity and permeability are often calculated from laboratory measurements or direct imaging of the microstructure .instead researchers often use statistical methods to reconstruct porous media based on two point or multi point statistics recent advances in deep learning have shown promising use of generative adversarial networks gans for rapid generation of 3d images with no a priori model.i introduction	however due to acquisition times and experimental costs it is difficult to evaluate the variability due to rock heterogeneity 
1	108193	8193	instead researchers often use statistical methods to reconstruct porous media based on two point or multi point statistics recent advances in deep learning have shown promising use of generative adversarial networks gans for rapid generation of 3d images with no a priori model.however due to acquisition times and experimental costs it is difficult to evaluate the variability due to rock heterogeneity .last sentence.i introduction	instead researchers often use statistical methods to reconstruct porous media based on two point or multi point statistics recent advances in deep learning have shown promising use of generative adversarial networks gans for rapid generation of 3d images with no a priori model
0	108194	8194	gans are made of two components 1 a generator g that creates a synthetic training image and 2 a discriminator d that tries to differentiate between the synthetic and real training image .first sentence.as the gan is trained the generator tries to create more realistic training images to fool the discriminator while the discriminator tries to become better at classifying real label 1 vs fake label 0 images .a generative adversarial networks	gans are made of two components 1 a generator g that creates a synthetic training image and 2 a discriminator d that tries to differentiate between the synthetic and real training image 
0	108195	8195	as the gan is trained the generator tries to create more realistic training images to fool the discriminator while the discriminator tries to become better at classifying real label 1 vs fake label 0 images .gans are made of two components 1 a generator g that creates a synthetic training image and 2 a discriminator d that tries to differentiate between the synthetic and real training image .z is the latent space vector sampled from a normal distribution so g z maps the latent vector to the image space .a generative adversarial networks	as the gan is trained the generator tries to create more realistic training images to fool the discriminator while the discriminator tries to become better at classifying real label 1 vs fake label 0 images 
0	108196	8196	z is the latent space vector sampled from a normal distribution so g z maps the latent vector to the image space .as the gan is trained the generator tries to create more realistic training images to fool the discriminator while the discriminator tries to become better at classifying real label 1 vs fake label 0 images .x is the data from an image real or fake and d g z is the probability that the generated image is real .a generative adversarial networks	z is the latent space vector sampled from a normal distribution so g z maps the latent vector to the image space 
0	108197	8197	x is the data from an image real or fake and d g z is the probability that the generated image is real .z is the latent space vector sampled from a normal distribution so g z maps the latent vector to the image space .the two competing objectives result in the following value function researchers have shown that using a deep convolutional network in the generator and discriminator models can improve synthetic image generation and training of the network .a generative adversarial networks	x is the data from an image real or fake and d g z is the probability that the generated image is real 
0	108198	8198	the two competing objectives result in the following value function researchers have shown that using a deep convolutional network in the generator and discriminator models can improve synthetic image generation and training of the network .x is the data from an image real or fake and d g z is the probability that the generated image is real .the overall guidelines for training a dcgan usually involes 1 using strided convolutions instead of pooling so that energy resources engineering stanford university the network can learn its own pooling functions 2 using batch normalization to improve gradient flow 3 removing fully connected hidden layers and 4 using a specific set of activation functions in the generator and discriminator explained further in the methods .a generative adversarial networks	the two competing objectives result in the following value function researchers have shown that using a deep convolutional network in the generator and discriminator models can improve synthetic image generation and training of the network 
1	108199	8199	the overall guidelines for training a dcgan usually involes 1 using strided convolutions instead of pooling so that energy resources engineering stanford university the network can learn its own pooling functions 2 using batch normalization to improve gradient flow 3 removing fully connected hidden layers and 4 using a specific set of activation functions in the generator and discriminator explained further in the methods .the two competing objectives result in the following value function researchers have shown that using a deep convolutional network in the generator and discriminator models can improve synthetic image generation and training of the network .last sentence.a generative adversarial networks	the overall guidelines for training a dcgan usually involes 1 using strided convolutions instead of pooling so that energy resources engineering stanford university the network can learn its own pooling functions 2 using batch normalization to improve gradient flow 3 removing fully connected hidden layers and 4 using a specific set of activation functions in the generator and discriminator explained further in the methods 
0	108200	8200	the main objective of this project is to investigate the accuracy and feasibility of generating 2d 3d sandstone images through training a dcgan model .first sentence.while this has been already studied in the literature it is a relatively new field with many ongoing areas of interest such as ways to improve training stability image quality and incorporating grayscale and multiscale images this project first aims to successfully create and train a 2d gan before eventually training a 3d gan .ii objective and scope	the main objective of this project is to investigate the accuracy and feasibility of generating 2d 3d sandstone images through training a dcgan model 
0	108201	8201	while this has been already studied in the literature it is a relatively new field with many ongoing areas of interest such as ways to improve training stability image quality and incorporating grayscale and multiscale images this project first aims to successfully create and train a 2d gan before eventually training a 3d gan .the main objective of this project is to investigate the accuracy and feasibility of generating 2d 3d sandstone images through training a dcgan model .we can then evaluate how modifying the gan architecture affects the loss and accuracy of the generated images .ii objective and scope	while this has been already studied in the literature it is a relatively new field with many ongoing areas of interest such as ways to improve training stability image quality and incorporating grayscale and multiscale images this project first aims to successfully create and train a 2d gan before eventually training a 3d gan 
0	108202	8202	we can then evaluate how modifying the gan architecture affects the loss and accuracy of the generated images .while this has been already studied in the literature it is a relatively new field with many ongoing areas of interest such as ways to improve training stability image quality and incorporating grayscale and multiscale images this project first aims to successfully create and train a 2d gan before eventually training a 3d gan .once trained these images would then be able to be used as inputs into digital rock physics calculations of properties such as permeability and capillary pressure .ii objective and scope	we can then evaluate how modifying the gan architecture affects the loss and accuracy of the generated images 
0	108203	8203	once trained these images would then be able to be used as inputs into digital rock physics calculations of properties such as permeability and capillary pressure .we can then evaluate how modifying the gan architecture affects the loss and accuracy of the generated images .understanding how permeability is affected by variations in porosity and connectivity is necessary in many research areas involving fluid flow through porous media .ii objective and scope	once trained these images would then be able to be used as inputs into digital rock physics calculations of properties such as permeability and capillary pressure 
1	108204	8204	understanding how permeability is affected by variations in porosity and connectivity is necessary in many research areas involving fluid flow through porous media .once trained these images would then be able to be used as inputs into digital rock physics calculations of properties such as permeability and capillary pressure .last sentence.ii objective and scope	understanding how permeability is affected by variations in porosity and connectivity is necessary in many research areas involving fluid flow through porous media 
0	108205	8205	the dataset was obtained from micro x ray tomography scans of a bentheimer sandstone .first sentence.the original data is a greyscale representation of the 3d solid and has been processed and binarized into the pore and solid phases .iii dataset	the dataset was obtained from micro x ray tomography scans of a bentheimer sandstone 
0	108206	8206	the original data is a greyscale representation of the 3d solid and has been processed and binarized into the pore and solid phases .the dataset was obtained from micro x ray tomography scans of a bentheimer sandstone .the full dataset is 512 3 voxels large with a voxel size of 3 06 m .iii dataset	the original data is a greyscale representation of the 3d solid and has been processed and binarized into the pore and solid phases 
0	108207	8207	the full dataset is 512 3 voxels large with a voxel size of 3 06 m .the original data is a greyscale representation of the 3d solid and has been processed and binarized into the pore and solid phases .in order for the training image 64 x 64 voxels to capture an adequate area the image was downsampled to 256 3 voxels with a voxel size of 6 12 m .iii dataset	the full dataset is 512 3 voxels large with a voxel size of 3 06 m 
0	108208	8208	in order for the training image 64 x 64 voxels to capture an adequate area the image was downsampled to 256 3 voxels with a voxel size of 6 12 m .the full dataset is 512 3 voxels large with a voxel size of 3 06 m .for data augmentation subvolumes were extracted every 16 voxels to yield 36 864 training images .iii dataset	in order for the training image 64 x 64 voxels to capture an adequate area the image was downsampled to 256 3 voxels with a voxel size of 6 12 m 
0	108209	8209	for data augmentation subvolumes were extracted every 16 voxels to yield 36 864 training images .in order for the training image 64 x 64 voxels to capture an adequate area the image was downsampled to 256 3 voxels with a voxel size of 6 12 m .initial tests were also done on a larger dataset of 72 728 images and yielded comparable results .iii dataset	for data augmentation subvolumes were extracted every 16 voxels to yield 36 864 training images 
0	108210	8210	initial tests were also done on a larger dataset of 72 728 images and yielded comparable results .for data augmentation subvolumes were extracted every 16 voxels to yield 36 864 training images .to reduce training time we used the smaller training set for the majority of our tests .iii dataset	initial tests were also done on a larger dataset of 72 728 images and yielded comparable results 
0	108211	8211	to reduce training time we used the smaller training set for the majority of our tests .initial tests were also done on a larger dataset of 72 728 images and yielded comparable results .last sentence.iii dataset	to reduce training time we used the smaller training set for the majority of our tests 
0	108212	8212	we used a deep convolutional gan dcgan which uses convolutional transpose layers in the generator and convolutional layers in the discriminator as our network model.first sentence.last sentence.iv network architecture	we used a deep convolutional gan dcgan which uses convolutional transpose layers in the generator and convolutional layers in the discriminator as our network model
0	108213	8213	to train d and g we used two different loss functions .first sentence.we first use the binary cross entropy loss function model dcgan 1 training is performed in two steps 1 train the discriminator to maximizewhile keeping the generator fixed and 2 train the generator to minimizewhile keeping the discriminator fixed .v training loss function	to train d and g we used two different loss functions 
0	108214	8214	we first use the binary cross entropy loss function model dcgan 1 training is performed in two steps 1 train the discriminator to maximizewhile keeping the generator fixed and 2 train the generator to minimizewhile keeping the discriminator fixed .to train d and g we used two different loss functions .in practice due to the effect of vanishing gradients it is easier to maximizeconvergence is theoretically reached when the generator can generate a distribution p g x that equal to p data x which corresponds to a discriminator output of 0 5 .v training loss function	we first use the binary cross entropy loss function model dcgan 1 training is performed in two steps 1 train the discriminator to maximizewhile keeping the generator fixed and 2 train the generator to minimizewhile keeping the discriminator fixed 
0	108215	8215	in practice due to the effect of vanishing gradients it is easier to maximizeconvergence is theoretically reached when the generator can generate a distribution p g x that equal to p data x which corresponds to a discriminator output of 0 5 .we first use the binary cross entropy loss function model dcgan 1 training is performed in two steps 1 train the discriminator to maximizewhile keeping the generator fixed and 2 train the generator to minimizewhile keeping the discriminator fixed .further details about the training steps can be found in we also investigated the effect of using the wasserstein distance as the loss function instead model dcgan 2 the primary advantages of using the wasserstein loss are that it can prevent mode collapse in the generator and allow for better convergence .v training loss function	in practice due to the effect of vanishing gradients it is easier to maximizeconvergence is theoretically reached when the generator can generate a distribution p g x that equal to p data x which corresponds to a discriminator output of 0 5 
0	108216	8216	further details about the training steps can be found in we also investigated the effect of using the wasserstein distance as the loss function instead model dcgan 2 the primary advantages of using the wasserstein loss are that it can prevent mode collapse in the generator and allow for better convergence .in practice due to the effect of vanishing gradients it is easier to maximizeconvergence is theoretically reached when the generator can generate a distribution p g x that equal to p data x which corresponds to a discriminator output of 0 5 .the wasserstein distance measures the distance between two probability functions and the discriminator now becomes a critic that evaluates the wasserstein distance between the real and synthetic images .v training loss function	further details about the training steps can be found in we also investigated the effect of using the wasserstein distance as the loss function instead model dcgan 2 the primary advantages of using the wasserstein loss are that it can prevent mode collapse in the generator and allow for better convergence 
0	108217	8217	the wasserstein distance measures the distance between two probability functions and the discriminator now becomes a critic that evaluates the wasserstein distance between the real and synthetic images .further details about the training steps can be found in we also investigated the effect of using the wasserstein distance as the loss function instead model dcgan 2 the primary advantages of using the wasserstein loss are that it can prevent mode collapse in the generator and allow for better convergence .the distance is calculated by enforcing a lipschitz constraint on the critic s model either through weight clipping or a gradient penalty where is the gradient penalty coefficient and is set to 10 for our model .v training loss function	the wasserstein distance measures the distance between two probability functions and the discriminator now becomes a critic that evaluates the wasserstein distance between the real and synthetic images 
0	108218	8218	the distance is calculated by enforcing a lipschitz constraint on the critic s model either through weight clipping or a gradient penalty where is the gradient penalty coefficient and is set to 10 for our model .the wasserstein distance measures the distance between two probability functions and the discriminator now becomes a critic that evaluates the wasserstein distance between the real and synthetic images .last sentence.v training loss function	the distance is calculated by enforcing a lipschitz constraint on the critic s model either through weight clipping or a gradient penalty where is the gradient penalty coefficient and is set to 10 for our model 
0	108219	8219	the objective of using a dcgan with our dataset is to recreate a pore network image with similar morphological properties as the original porous media .first sentence.to evaluate the accuracy of our model we use a set of morphological descriptors known as minkowski functionals .vi morphological evaluation metrics	the objective of using a dcgan with our dataset is to recreate a pore network image with similar morphological properties as the original porous media 
0	108220	8220	to evaluate the accuracy of our model we use a set of morphological descriptors known as minkowski functionals .the objective of using a dcgan with our dataset is to recreate a pore network image with similar morphological properties as the original porous media .in 2d there are 3 minkowski functionals that describe the surface area perimeter and euler characteristic .vi morphological evaluation metrics	to evaluate the accuracy of our model we use a set of morphological descriptors known as minkowski functionals 
0	108221	8221	in 2d there are 3 minkowski functionals that describe the surface area perimeter and euler characteristic .to evaluate the accuracy of our model we use a set of morphological descriptors known as minkowski functionals .the area is simply the percent of pixels that are labeled as pore n pore divided by the total number of pixels n total area n pore n total finally the euler characteristic in 2d describes the connectivity of the surface and is defined as the difference between the number of connected regions solid and the number of holes pores a region with a negative euler characteristic will have more holes than connected regions which indicates low connectivity across the area .vi morphological evaluation metrics	in 2d there are 3 minkowski functionals that describe the surface area perimeter and euler characteristic 
1	108222	8222	the area is simply the percent of pixels that are labeled as pore n pore divided by the total number of pixels n total area n pore n total finally the euler characteristic in 2d describes the connectivity of the surface and is defined as the difference between the number of connected regions solid and the number of holes pores a region with a negative euler characteristic will have more holes than connected regions which indicates low connectivity across the area .in 2d there are 3 minkowski functionals that describe the surface area perimeter and euler characteristic .a region with a positive euler characteristic will have more connected regions and therefore a high connectivity across the area which can allow for fluid flow for evaluation after the model is fully trained we create 64 realizations of 100 2 pixel images using the generator and randomly select the same number of images from our training set .vi morphological evaluation metrics	the area is simply the percent of pixels that are labeled as pore n pore divided by the total number of pixels n total area n pore n total finally the euler characteristic in 2d describes the connectivity of the surface and is defined as the difference between the number of connected regions solid and the number of holes pores a region with a negative euler characteristic will have more holes than connected regions which indicates low connectivity across the area 
0	108223	8223	a region with a positive euler characteristic will have more connected regions and therefore a high connectivity across the area which can allow for fluid flow for evaluation after the model is fully trained we create 64 realizations of 100 2 pixel images using the generator and randomly select the same number of images from our training set .the area is simply the percent of pixels that are labeled as pore n pore divided by the total number of pixels n total area n pore n total finally the euler characteristic in 2d describes the connectivity of the surface and is defined as the difference between the number of connected regions solid and the number of holes pores a region with a negative euler characteristic will have more holes than connected regions which indicates low connectivity across the area .the generator outputs images with values between 1 1 .vi morphological evaluation metrics	a region with a positive euler characteristic will have more connected regions and therefore a high connectivity across the area which can allow for fluid flow for evaluation after the model is fully trained we create 64 realizations of 100 2 pixel images using the generator and randomly select the same number of images from our training set 
0	108224	8224	the generator outputs images with values between 1 1 .a region with a positive euler characteristic will have more connected regions and therefore a high connectivity across the area which can allow for fluid flow for evaluation after the model is fully trained we create 64 realizations of 100 2 pixel images using the generator and randomly select the same number of images from our training set .the images are normalized filtered using a median filter with a neighborhood of 2 pixels and binarized using otsu s method .vi morphological evaluation metrics	the generator outputs images with values between 1 1 
0	108225	8225	the images are normalized filtered using a median filter with a neighborhood of 2 pixels and binarized using otsu s method .the generator outputs images with values between 1 1 .an example of the final synthetic image used for evaluation is shown in we next show the effect of varying different model parameters during the training process .vi morphological evaluation metrics	the images are normalized filtered using a median filter with a neighborhood of 2 pixels and binarized using otsu s method 
0	108226	8226	an example of the final synthetic image used for evaluation is shown in we next show the effect of varying different model parameters during the training process .the images are normalized filtered using a median filter with a neighborhood of 2 pixels and binarized using otsu s method .last sentence.vi morphological evaluation metrics	an example of the final synthetic image used for evaluation is shown in we next show the effect of varying different model parameters during the training process 
0	108227	8227	we were able to successfully implement and train a 2d dcgan model to generate reasonable images with similar morphological properties to the original rock sample .first sentence.however it is still unclear if the generator is accurately capturing the underlying probability distribution of the real data .viii conclusion and future work	we were able to successfully implement and train a 2d dcgan model to generate reasonable images with similar morphological properties to the original rock sample 
0	108228	8228	however it is still unclear if the generator is accurately capturing the underlying probability distribution of the real data .we were able to successfully implement and train a 2d dcgan model to generate reasonable images with similar morphological properties to the original rock sample .further investigation could involve using the wasserstein loss as it is a measurement of the distance between two probability distributions .viii conclusion and future work	however it is still unclear if the generator is accurately capturing the underlying probability distribution of the real data 
0	108229	8229	further investigation could involve using the wasserstein loss as it is a measurement of the distance between two probability distributions .however it is still unclear if the generator is accurately capturing the underlying probability distribution of the real data .while our model using the wasserstein loss did not perform as well there have been extensive studies on ways to improve gans and dcgans and only some of the suggestions have been implemented here the ultimate goal of this work is to create a 3d gan to create 3d pore networks for use in digital rock physics .viii conclusion and future work	further investigation could involve using the wasserstein loss as it is a measurement of the distance between two probability distributions 
0	108230	8230	while our model using the wasserstein loss did not perform as well there have been extensive studies on ways to improve gans and dcgans and only some of the suggestions have been implemented here the ultimate goal of this work is to create a 3d gan to create 3d pore networks for use in digital rock physics .further investigation could involve using the wasserstein loss as it is a measurement of the distance between two probability distributions .the major challenge when scaling from 2d to 3d is expected to be in the computational train required to train the 3d network .viii conclusion and future work	while our model using the wasserstein loss did not perform as well there have been extensive studies on ways to improve gans and dcgans and only some of the suggestions have been implemented here the ultimate goal of this work is to create a 3d gan to create 3d pore networks for use in digital rock physics 
0	108231	8231	the major challenge when scaling from 2d to 3d is expected to be in the computational train required to train the 3d network .while our model using the wasserstein loss did not perform as well there have been extensive studies on ways to improve gans and dcgans and only some of the suggestions have been implemented here the ultimate goal of this work is to create a 3d gan to create 3d pore networks for use in digital rock physics .therefore it is still important to understand the underlying architecture in 2d and knowledge gained from this project will be invaluable when constructing the 3d model .viii conclusion and future work	the major challenge when scaling from 2d to 3d is expected to be in the computational train required to train the 3d network 
0	108232	8232	therefore it is still important to understand the underlying architecture in 2d and knowledge gained from this project will be invaluable when constructing the 3d model .the major challenge when scaling from 2d to 3d is expected to be in the computational train required to train the 3d network .last sentence.viii conclusion and future work	therefore it is still important to understand the underlying architecture in 2d and knowledge gained from this project will be invaluable when constructing the 3d model 
0	108233	8233	thanks to tim anderson and prof anthony kovscek for their guidance on this project .first sentence.part of this work was performed at the stanford nano shared facilities snsf supported by the national science foundation under award eccs 1542152 .ix acknowledgements	thanks to tim anderson and prof anthony kovscek for their guidance on this project 
0	108234	8234	part of this work was performed at the stanford nano shared facilities snsf supported by the national science foundation under award eccs 1542152 .thanks to tim anderson and prof anthony kovscek for their guidance on this project .last sentence.ix acknowledgements	part of this work was performed at the stanford nano shared facilities snsf supported by the national science foundation under award eccs 1542152 
0	108235	8235	project code can be downloaded by clicking here stanford google drive .first sentence.last sentence.x project code	project code can be downloaded by clicking here stanford google drive 
1	108236	8236	to provide a spacecraft with such pareto optimal autonomous planning capabilities is a huge challenge .first sentence.in this project the solution approach to the problem combines machine learning both reinforcement and supervised and numerical multi objective optimization .solution approach using machine learning	to provide a spacecraft with such pareto optimal autonomous planning capabilities is a huge challenge 
1	108237	8237	in this project the solution approach to the problem combines machine learning both reinforcement and supervised and numerical multi objective optimization .to provide a spacecraft with such pareto optimal autonomous planning capabilities is a huge challenge .in particular assuming a value for the nea dynamics parameters an heuristic multi objective optimization algorithm is used to generate a pareto front describing the trade off offered by various motion plans according to two conflicting cost functions .solution approach using machine learning	in this project the solution approach to the problem combines machine learning both reinforcement and supervised and numerical multi objective optimization 
1	108238	8238	in particular assuming a value for the nea dynamics parameters an heuristic multi objective optimization algorithm is used to generate a pareto front describing the trade off offered by various motion plans according to two conflicting cost functions .in this project the solution approach to the problem combines machine learning both reinforcement and supervised and numerical multi objective optimization .the two cost functions to be minimized provide metric of 1 the control effort required to realize the motion plan j v 2 the inverse of the quality quantity of scientific output perceivable through realization of the motion plan j science .solution approach using machine learning	in particular assuming a value for the nea dynamics parameters an heuristic multi objective optimization algorithm is used to generate a pareto front describing the trade off offered by various motion plans according to two conflicting cost functions 
1	108239	8239	the two cost functions to be minimized provide metric of 1 the control effort required to realize the motion plan j v 2 the inverse of the quality quantity of scientific output perceivable through realization of the motion plan j science .in particular assuming a value for the nea dynamics parameters an heuristic multi objective optimization algorithm is used to generate a pareto front describing the trade off offered by various motion plans according to two conflicting cost functions .the pareto front obtained relies on the assumption of the nea dynamics parameters when these parameters are changed different pareto fronts are obtained .solution approach using machine learning	the two cost functions to be minimized provide metric of 1 the control effort required to realize the motion plan j v 2 the inverse of the quality quantity of scientific output perceivable through realization of the motion plan j science 
1	108240	8240	the pareto front obtained relies on the assumption of the nea dynamics parameters when these parameters are changed different pareto fronts are obtained .the two cost functions to be minimized provide metric of 1 the control effort required to realize the motion plan j v 2 the inverse of the quality quantity of scientific output perceivable through realization of the motion plan j science .to identify a specific point on a pareto front which corresponds to a specific trade off between the two conflicting costs the multi objective problem can be scalarized.solution approach using machine learning	the pareto front obtained relies on the assumption of the nea dynamics parameters when these parameters are changed different pareto fronts are obtained 
1	108241	8241	to identify a specific point on a pareto front which corresponds to a specific trade off between the two conflicting costs the multi objective problem can be scalarized.the pareto front obtained relies on the assumption of the nea dynamics parameters when these parameters are changed different pareto fronts are obtained .last sentence.solution approach using machine learning	to identify a specific point on a pareto front which corresponds to a specific trade off between the two conflicting costs the multi objective problem can be scalarized
1	108242	8242	a spacecraft sc capable to autonomously plan its motion while accounting for conflicting mission objectives in a pareto optimal way would permit to accomplish complex mission tasks around highly uncharacterized celestial bodies such as near earth asteroids neas .first sentence.the two most common conflicting objectives of a space exploration mission are the maximization of the scientific output and the minimization of the control effort i e .problem statement	a spacecraft sc capable to autonomously plan its motion while accounting for conflicting mission objectives in a pareto optimal way would permit to accomplish complex mission tasks around highly uncharacterized celestial bodies such as near earth asteroids neas 
1	108243	8243	the two most common conflicting objectives of a space exploration mission are the maximization of the scientific output and the minimization of the control effort i e .a spacecraft sc capable to autonomously plan its motion while accounting for conflicting mission objectives in a pareto optimal way would permit to accomplish complex mission tasks around highly uncharacterized celestial bodies such as near earth asteroids neas .the propellant required on board .problem statement	the two most common conflicting objectives of a space exploration mission are the maximization of the scientific output and the minimization of the control effort i e 
1	108244	8244	the propellant required on board .the two most common conflicting objectives of a space exploration mission are the maximization of the scientific output and the minimization of the control effort i e .if the targeted celestial body is sufficiently known and has a predictable orbital environment both numerical and analytical tools can be leveraged on ground to design spacecraft motion plans that account for the trade off .problem statement	the propellant required on board 
1	108245	8245	if the targeted celestial body is sufficiently known and has a predictable orbital environment both numerical and analytical tools can be leveraged on ground to design spacecraft motion plans that account for the trade off .the propellant required on board .on the contrary if the celestial body and its orbit environment are largely uncertain all plans elaborated on ground may fail dramatically when implemented in space .problem statement	if the targeted celestial body is sufficiently known and has a predictable orbital environment both numerical and analytical tools can be leveraged on ground to design spacecraft motion plans that account for the trade off 
1	108246	8246	on the contrary if the celestial body and its orbit environment are largely uncertain all plans elaborated on ground may fail dramatically when implemented in space .if the targeted celestial body is sufficiently known and has a predictable orbital environment both numerical and analytical tools can be leveraged on ground to design spacecraft motion plans that account for the trade off .a clear example are missions around neas having relevant dynamics parameters i e .problem statement	on the contrary if the celestial body and its orbit environment are largely uncertain all plans elaborated on ground may fail dramatically when implemented in space 
1	108247	8247	a clear example are missions around neas having relevant dynamics parameters i e .on the contrary if the celestial body and its orbit environment are largely uncertain all plans elaborated on ground may fail dramatically when implemented in space .mass shape rotation axis orientation and gravity coefficients largely uncertain .problem statement	a clear example are missions around neas having relevant dynamics parameters i e 
1	108248	8248	mass shape rotation axis orientation and gravity coefficients largely uncertain .a clear example are missions around neas having relevant dynamics parameters i e .in these missions a spacecraft should be capable of autonomously plan its motion when an updated knowledge of the nea environment is provided by the sensors and the navigation filter .problem statement	mass shape rotation axis orientation and gravity coefficients largely uncertain 
1	108249	8249	in these missions a spacecraft should be capable of autonomously plan its motion when an updated knowledge of the nea environment is provided by the sensors and the navigation filter .mass shape rotation axis orientation and gravity coefficients largely uncertain .in addition the generated motion plan should account for the trade off science output vs control effort in an optimal sense .problem statement	in these missions a spacecraft should be capable of autonomously plan its motion when an updated knowledge of the nea environment is provided by the sensors and the navigation filter 
1	108250	8250	in addition the generated motion plan should account for the trade off science output vs control effort in an optimal sense .in these missions a spacecraft should be capable of autonomously plan its motion when an updated knowledge of the nea environment is provided by the sensors and the navigation filter .last sentence.problem statement	in addition the generated motion plan should account for the trade off science output vs control effort in an optimal sense 
1	108251	8251	the multi objective optimizer used is the multi objective particle swarm optimization mopso which is shown in literature to provide high level performances in terms of time of convergence and full reconstruction of the global pareto front.first sentence.last sentence.dataset generation	the multi objective optimizer used is the multi objective particle swarm optimization mopso which is shown in literature to provide high level performances in terms of time of convergence and full reconstruction of the global pareto front
1	108252	8252	the neural network nn weights have been trained on the training set using a mean square error mse loss function .first sentence.the nn hyperparameters have been tuned and optimized according to the performances provided on the development set.results	the neural network nn weights have been trained on the training set using a mean square error mse loss function 
1	108253	8253	the nn hyperparameters have been tuned and optimized according to the performances provided on the development set.the neural network nn weights have been trained on the training set using a mean square error mse loss function .last sentence.results	the nn hyperparameters have been tuned and optimized according to the performances provided on the development set
1	108254	8254	this project explores the use of neural networks nn to provide a spacecraft with autonomous multiobjective motion planning capabilities around a near earth asteroid nea .first sentence.the trained nn has been shown to provide interesting but still moderate accuracy results .conclusions and way forward	this project explores the use of neural networks nn to provide a spacecraft with autonomous multiobjective motion planning capabilities around a near earth asteroid nea 
1	108255	8255	the trained nn has been shown to provide interesting but still moderate accuracy results .this project explores the use of neural networks nn to provide a spacecraft with autonomous multiobjective motion planning capabilities around a near earth asteroid nea .to improve the performances the first way to follow is to enlarge the dataset .conclusions and way forward	the trained nn has been shown to provide interesting but still moderate accuracy results 
1	108256	8256	to improve the performances the first way to follow is to enlarge the dataset .the trained nn has been shown to provide interesting but still moderate accuracy results .in addition future work will explore ways to leverage information about the covariance of the estimated state that the navigation filter outputs .conclusions and way forward	to improve the performances the first way to follow is to enlarge the dataset 
1	108257	8257	in addition future work will explore ways to leverage information about the covariance of the estimated state that the navigation filter outputs .to improve the performances the first way to follow is to enlarge the dataset .in this sense a possible way to go is to reformulate the problem as a stochastic markov decision process .conclusions and way forward	in addition future work will explore ways to leverage information about the covariance of the estimated state that the navigation filter outputs 
1	108258	8258	in this sense a possible way to go is to reformulate the problem as a stochastic markov decision process .in addition future work will explore ways to leverage information about the covariance of the estimated state that the navigation filter outputs .last sentence.conclusions and way forward	in this sense a possible way to go is to reformulate the problem as a stochastic markov decision process 
0	108259	8259	i have developed this project advised by prof simone d amico .first sentence.the topic of this study has been motivated by the on going research project autonomous nanosatellite swarming using radio frequency and optical navigation developed at stanford s space rendezvous laboratory in collaboration with nasa ames research center and sponsored by nasa sstp small spacecraft technology program .acknowledgements	i have developed this project advised by prof simone d amico 
1	108260	8260	the topic of this study has been motivated by the on going research project autonomous nanosatellite swarming using radio frequency and optical navigation developed at stanford s space rendezvous laboratory in collaboration with nasa ames research center and sponsored by nasa sstp small spacecraft technology program .i have developed this project advised by prof simone d amico .last sentence.acknowledgements	the topic of this study has been motivated by the on going research project autonomous nanosatellite swarming using radio frequency and optical navigation developed at stanford s space rendezvous laboratory in collaboration with nasa ames research center and sponsored by nasa sstp small spacecraft technology program 
1	108261	8261	the increasing deployment of distributed energy resources e g .first sentence.solar electric vehicles in power distribution systems will result in greater uncertainty in power demand .i introduction	the increasing deployment of distributed energy resources e g 
0	108262	8262	solar electric vehicles in power distribution systems will result in greater uncertainty in power demand .the increasing deployment of distributed energy resources e g .one method to mitigate this uncertainty and maintain grid reliability is to enable more control of demand side resources through demand response programs .i introduction	solar electric vehicles in power distribution systems will result in greater uncertainty in power demand 
0	108263	8263	one method to mitigate this uncertainty and maintain grid reliability is to enable more control of demand side resources through demand response programs .solar electric vehicles in power distribution systems will result in greater uncertainty in power demand .demand response dr is a reduction or shift in power consumption relative to baseline behavior during peak loads or high prices .i introduction	one method to mitigate this uncertainty and maintain grid reliability is to enable more control of demand side resources through demand response programs 
1	108264	8264	demand response dr is a reduction or shift in power consumption relative to baseline behavior during peak loads or high prices .one method to mitigate this uncertainty and maintain grid reliability is to enable more control of demand side resources through demand response programs .while dr programs have historically focused on the industrial and commercial customers residential dr programs are expanding .i introduction	demand response dr is a reduction or shift in power consumption relative to baseline behavior during peak loads or high prices 
0	108265	8265	while dr programs have historically focused on the industrial and commercial customers residential dr programs are expanding .demand response dr is a reduction or shift in power consumption relative to baseline behavior during peak loads or high prices .these programs are run by utility companies or third party aggregators and generally focus on control of specific types of residential appliances such as air conditioners or pool pumps the effectiveness of a dr program depends on the power consumption patterns of consumers and their responsiveness to prices or incentives .i introduction	while dr programs have historically focused on the industrial and commercial customers residential dr programs are expanding 
1	108266	8266	these programs are run by utility companies or third party aggregators and generally focus on control of specific types of residential appliances such as air conditioners or pool pumps the effectiveness of a dr program depends on the power consumption patterns of consumers and their responsiveness to prices or incentives .while dr programs have historically focused on the industrial and commercial customers residential dr programs are expanding .power consumption at the household level is extremely volatile and can vary significantly from one household to another given heterogeneity in consumer behavior and the stochastic nature of exogenous variables e g .i introduction	these programs are run by utility companies or third party aggregators and generally focus on control of specific types of residential appliances such as air conditioners or pool pumps the effectiveness of a dr program depends on the power consumption patterns of consumers and their responsiveness to prices or incentives 
0	108267	8267	power consumption at the household level is extremely volatile and can vary significantly from one household to another given heterogeneity in consumer behavior and the stochastic nature of exogenous variables e g .these programs are run by utility companies or third party aggregators and generally focus on control of specific types of residential appliances such as air conditioners or pool pumps the effectiveness of a dr program depends on the power consumption patterns of consumers and their responsiveness to prices or incentives .weather patterns .i introduction	power consumption at the household level is extremely volatile and can vary significantly from one household to another given heterogeneity in consumer behavior and the stochastic nature of exogenous variables e g 
0	108268	8268	weather patterns .power consumption at the household level is extremely volatile and can vary significantly from one household to another given heterogeneity in consumer behavior and the stochastic nature of exogenous variables e g .direct targeting of consumers with behavior patterns well suited for dr would be highly beneficial and cost effective for utility companies .i introduction	weather patterns 
0	108269	8269	direct targeting of consumers with behavior patterns well suited for dr would be highly beneficial and cost effective for utility companies .weather patterns .last sentence.i introduction	direct targeting of consumers with behavior patterns well suited for dr would be highly beneficial and cost effective for utility companies 
0	108270	8270	past research has focused on using smart meter data for consumer segmentation to identify households with similar power consumption patterns using unsupervised learning kwac et al .first sentence.utilized a combination of adaptive k means and hierarchical clustering to develop a load profile dictionary from a dataset of 220 000 consumers in ca.ii related work	past research has focused on using smart meter data for consumer segmentation to identify households with similar power consumption patterns using unsupervised learning kwac et al 
0	108271	8271	utilized a combination of adaptive k means and hierarchical clustering to develop a load profile dictionary from a dataset of 220 000 consumers in ca.past research has focused on using smart meter data for consumer segmentation to identify households with similar power consumption patterns using unsupervised learning kwac et al .last sentence.ii related work	utilized a combination of adaptive k means and hierarchical clustering to develop a load profile dictionary from a dataset of 220 000 consumers in ca
1	108272	8272	the objective of this project is to segment consumers based on their appliance level power consumption with respect to three factors consumer 1 availability 2 variability and 3 flexibility .first sentence.in contrast with previous studies we focus on appliance level power consumption .iii project objective	the objective of this project is to segment consumers based on their appliance level power consumption with respect to three factors consumer 1 availability 2 variability and 3 flexibility 
0	108273	8273	in contrast with previous studies we focus on appliance level power consumption .the objective of this project is to segment consumers based on their appliance level power consumption with respect to three factors consumer 1 availability 2 variability and 3 flexibility .availability refers to the tendency of a consumer group to consume power during periods of peak system demand when a utility would be most interested in curtailing power consumption .iii project objective	in contrast with previous studies we focus on appliance level power consumption 
0	108274	8274	availability refers to the tendency of a consumer group to consume power during periods of peak system demand when a utility would be most interested in curtailing power consumption .in contrast with previous studies we focus on appliance level power consumption .we evaluate consumer availability using unsupervised learning to cluster consumers into groups with similar temporal use patterns for each appliance .iii project objective	availability refers to the tendency of a consumer group to consume power during periods of peak system demand when a utility would be most interested in curtailing power consumption 
0	108275	8275	we evaluate consumer availability using unsupervised learning to cluster consumers into groups with similar temporal use patterns for each appliance .availability refers to the tendency of a consumer group to consume power during periods of peak system demand when a utility would be most interested in curtailing power consumption .variability is associated with the consistency of power consumption patterns .iii project objective	we evaluate consumer availability using unsupervised learning to cluster consumers into groups with similar temporal use patterns for each appliance 
0	108276	8276	variability is associated with the consistency of power consumption patterns .we evaluate consumer availability using unsupervised learning to cluster consumers into groups with similar temporal use patterns for each appliance .consistent use patterns typically result in more accurate power demand forecasts which improve the effectiveness of a dr program .iii project objective	variability is associated with the consistency of power consumption patterns 
0	108277	8277	consistent use patterns typically result in more accurate power demand forecasts which improve the effectiveness of a dr program .variability is associated with the consistency of power consumption patterns .we evaluate variability by using unsupervised learning to cluster load profiles into discrete groups and analyze the entropy of the distribution of the load profile assignments for each consumer .iii project objective	consistent use patterns typically result in more accurate power demand forecasts which improve the effectiveness of a dr program 
0	108278	8278	we evaluate variability by using unsupervised learning to cluster load profiles into discrete groups and analyze the entropy of the distribution of the load profile assignments for each consumer .consistent use patterns typically result in more accurate power demand forecasts which improve the effectiveness of a dr program .flexibility refers to the willingness of a consumer to shift power consumption from a peak load period to an off peak period .iii project objective	we evaluate variability by using unsupervised learning to cluster load profiles into discrete groups and analyze the entropy of the distribution of the load profile assignments for each consumer 
0	108279	8279	flexibility refers to the willingness of a consumer to shift power consumption from a peak load period to an off peak period .we evaluate variability by using unsupervised learning to cluster load profiles into discrete groups and analyze the entropy of the distribution of the load profile assignments for each consumer .we use supervised learning to predict the responsiveness of consumers to changes in price based on household characteristics and features extracted from power consumption profiles .iii project objective	flexibility refers to the willingness of a consumer to shift power consumption from a peak load period to an off peak period 
0	108280	8280	we use supervised learning to predict the responsiveness of consumers to changes in price based on household characteristics and features extracted from power consumption profiles .flexibility refers to the willingness of a consumer to shift power consumption from a peak load period to an off peak period .last sentence.iii project objective	we use supervised learning to predict the responsiveness of consumers to changes in price based on household characteristics and features extracted from power consumption profiles 
1	108281	8281	appliance and meter level real power consumption data was obtained from the open source pecan street database for the availability and variability analyses twelve months of minute level data from 2014 2015 and 2016 were used for the training validation and tests sets respectively .first sentence.availability analysis was applied only to deferrable loads which are appliances that are user initiated .iv dataset and features	appliance and meter level real power consumption data was obtained from the open source pecan street database for the availability and variability analyses twelve months of minute level data from 2014 2015 and 2016 were used for the training validation and tests sets respectively 
0	108282	8282	availability analysis was applied only to deferrable loads which are appliances that are user initiated .appliance and meter level real power consumption data was obtained from the open source pecan street database for the availability and variability analyses twelve months of minute level data from 2014 2015 and 2016 were used for the training validation and tests sets respectively .the power consumption of these appliances is primarily dependent on consumer use patterns .iv dataset and features	availability analysis was applied only to deferrable loads which are appliances that are user initiated 
0	108283	8283	the power consumption of these appliances is primarily dependent on consumer use patterns .availability analysis was applied only to deferrable loads which are appliances that are user initiated .we extracted the start times of each appliance use event from the raw data using thresholding heuristics based on changes in the moving average of power consumption .iv dataset and features	the power consumption of these appliances is primarily dependent on consumer use patterns 
1	108284	8284	we extracted the start times of each appliance use event from the raw data using thresholding heuristics based on changes in the moving average of power consumption .the power consumption of these appliances is primarily dependent on consumer use patterns .a multinomial distribution of the start time over the hours of the day was fit for each home and appliance using maximum likelihood estimation with laplace smoothing .iv dataset and features	we extracted the start times of each appliance use event from the raw data using thresholding heuristics based on changes in the moving average of power consumption 
0	108285	8285	a multinomial distribution of the start time over the hours of the day was fit for each home and appliance using maximum likelihood estimation with laplace smoothing .we extracted the start times of each appliance use event from the raw data using thresholding heuristics based on changes in the moving average of power consumption .given an extracted set of appliance start times s s 1 .iv dataset and features	a multinomial distribution of the start time over the hours of the day was fit for each home and appliance using maximum likelihood estimation with laplace smoothing 
0	108286	8286	for hierarchical clustering we used agglomerative methods with the ward variance minimization algorithm the gmm was modeled as a mixture of 24 dimensional multivariate gaussians each with tied covariance matrices .23 such that equation 1 is always defined .this was required since the 24 features are elements of a discrete probability distribution and are thus not independent of each other .a availability	for hierarchical clustering we used agglomerative methods with the ward variance minimization algorithm the gmm was modeled as a mixture of 24 dimensional multivariate gaussians each with tied covariance matrices 
0	108287	8287	this was required since the 24 features are elements of a discrete probability distribution and are thus not independent of each other .for hierarchical clustering we used agglomerative methods with the ward variance minimization algorithm the gmm was modeled as a mixture of 24 dimensional multivariate gaussians each with tied covariance matrices .the model was trained using expectation maximization both probabilistic models were each run 10 times for 2 14 clusters and the assignments with the lowest intra cluster variation as measured by symmetrized kl divergence were selected for each of the analyzed cluster sizes all four unsupervised learning methods were implemented using the sci kit learn three different metrics were used to evaluate the performance of each method .a availability	this was required since the 24 features are elements of a discrete probability distribution and are thus not independent of each other 
1	108288	8288	the model was trained using expectation maximization both probabilistic models were each run 10 times for 2 14 clusters and the assignments with the lowest intra cluster variation as measured by symmetrized kl divergence were selected for each of the analyzed cluster sizes all four unsupervised learning methods were implemented using the sci kit learn three different metrics were used to evaluate the performance of each method .this was required since the 24 features are elements of a discrete probability distribution and are thus not independent of each other .suppose the training and validation set both contain n consumers and k clusters are obtained from both sets such that cluster c t 0 .a availability	the model was trained using expectation maximization both probabilistic models were each run 10 times for 2 14 clusters and the assignments with the lowest intra cluster variation as measured by symmetrized kl divergence were selected for each of the analyzed cluster sizes all four unsupervised learning methods were implemented using the sci kit learn three different metrics were used to evaluate the performance of each method 
0	108289	8289	supposec j is the cluster assignment associated with consumer j .k from the validation set contains n c d consumers .we define the availability of a cluster of appliances during hour h as the mean power consumption during hour h for the entire cluster .a availability	supposec j is the cluster assignment associated with consumer j 
0	108290	8290	we define the availability of a cluster of appliances during hour h as the mean power consumption during hour h for the entire cluster .supposec j is the cluster assignment associated with consumer j .the increase in availability from consumer segmentation is the maximum availability over all clusters divided by the availability of all of the appliances in the entire datasetwhere is the set of all time indices associated with hour h and p j t is the power consumption of consumer j at time t in the test set .a availability	we define the availability of a cluster of appliances during hour h as the mean power consumption during hour h for the entire cluster 
0	108291	8291	the increase in availability from consumer segmentation is the maximum availability over all clusters divided by the availability of all of the appliances in the entire datasetwhere is the set of all time indices associated with hour h and p j t is the power consumption of consumer j at time t in the test set .we define the availability of a cluster of appliances during hour h as the mean power consumption during hour h for the entire cluster .an effective consumer segmentation algorithm should yield a large increase in availability the completeness score where n c t c d is the number of consumers assigned to cluster c t in the training set and cluster c d in the validation set .a availability	the increase in availability from consumer segmentation is the maximum availability over all clusters divided by the availability of all of the appliances in the entire datasetwhere is the set of all time indices associated with hour h and p j t is the power consumption of consumer j at time t in the test set 
1	108292	8292	an effective consumer segmentation algorithm should yield a large increase in availability the completeness score where n c t c d is the number of consumers assigned to cluster c t in the training set and cluster c d in the validation set .the increase in availability from consumer segmentation is the maximum availability over all clusters divided by the availability of all of the appliances in the entire datasetwhere is the set of all time indices associated with hour h and p j t is the power consumption of consumer j at time t in the test set .this analysis assumes that consumer power consumption patterns remain similar between the training validation and test sets such that a perfect clustering algorithm would recover identical clusters finally we used the intra cluster variation of the samples as a measure of cluster quality .a availability	an effective consumer segmentation algorithm should yield a large increase in availability the completeness score where n c t c d is the number of consumers assigned to cluster c t in the training set and cluster c d in the validation set 
1	108293	8293	this analysis assumes that consumer power consumption patterns remain similar between the training validation and test sets such that a perfect clustering algorithm would recover identical clusters finally we used the intra cluster variation of the samples as a measure of cluster quality .an effective consumer segmentation algorithm should yield a large increase in availability the completeness score where n c t c d is the number of consumers assigned to cluster c t in the training set and cluster c d in the validation set .the kl divergence as defined in equation 1 was used a distance measure between samples .a availability	this analysis assumes that consumer power consumption patterns remain similar between the training validation and test sets such that a perfect clustering algorithm would recover identical clusters finally we used the intra cluster variation of the samples as a measure of cluster quality 
0	108294	8294	the kl divergence as defined in equation 1 was used a distance measure between samples .this analysis assumes that consumer power consumption patterns remain similar between the training validation and test sets such that a perfect clustering algorithm would recover identical clusters finally we used the intra cluster variation of the samples as a measure of cluster quality .the elbow method.a availability	the kl divergence as defined in equation 1 was used a distance measure between samples 
0	108295	8295	the elbow method.the kl divergence as defined in equation 1 was used a distance measure between samples .last sentence.a availability	the elbow method
0	108296	8296	the variability of consumer power consumption patterns was also analyzed using unsupervised learning .first sentence.first k means clustering was used to cluster the 24 hour power consumption profiles of all homes for each specific appliance into k load shape types .b variability	the variability of consumer power consumption patterns was also analyzed using unsupervised learning 
1	108297	8297	first k means clustering was used to cluster the 24 hour power consumption profiles of all homes for each specific appliance into k load shape types .the variability of consumer power consumption patterns was also analyzed using unsupervised learning .this resulted in 365 load shape cluster assignments for each home and appliance .b variability	first k means clustering was used to cluster the 24 hour power consumption profiles of all homes for each specific appliance into k load shape types 
0	108298	8298	this resulted in 365 load shape cluster assignments for each home and appliance .first k means clustering was used to cluster the 24 hour power consumption profiles of all homes for each specific appliance into k load shape types .the distribution over these load shape types q j r k for each home and appliance over the entire training set was calculated using laplace smoothing .b variability	this resulted in 365 load shape cluster assignments for each home and appliance 
0	108299	8299	the distribution over these load shape types q j r k for each home and appliance over the entire training set was calculated using laplace smoothing .this resulted in 365 load shape cluster assignments for each home and appliance .the entropy of q j gives a measure of the variability of consumer use patterns .b variability	the distribution over these load shape types q j r k for each home and appliance over the entire training set was calculated using laplace smoothing 
0	108300	8300	the entropy of q j gives a measure of the variability of consumer use patterns .the distribution over these load shape types q j r k for each home and appliance over the entire training set was calculated using laplace smoothing .last sentence.b variability	the entropy of q j gives a measure of the variability of consumer use patterns 
1	108301	8301	three methods were compared for predicting the responsiveness of the power consumption of each consumer to changes in electricity price linear regression with recursive feature selection k nearest neighbors knn regression and random forests with recursive feature selection .first sentence.the tuning parameters for each algorithm were selected to minimize the mean squared error mse in the validation set .c flexibility	three methods were compared for predicting the responsiveness of the power consumption of each consumer to changes in electricity price linear regression with recursive feature selection k nearest neighbors knn regression and random forests with recursive feature selection 
0	108302	8302	the tuning parameters for each algorithm were selected to minimize the mean squared error mse in the validation set .three methods were compared for predicting the responsiveness of the power consumption of each consumer to changes in electricity price linear regression with recursive feature selection k nearest neighbors knn regression and random forests with recursive feature selection .tuning parameters included the number of features for linear regression and random forests the number of neighbors for knn and the number of estimators and the maximum tree depth for random forests .c flexibility	the tuning parameters for each algorithm were selected to minimize the mean squared error mse in the validation set 
0	108303	8303	tuning parameters included the number of features for linear regression and random forests the number of neighbors for knn and the number of estimators and the maximum tree depth for random forests .the tuning parameters for each algorithm were selected to minimize the mean squared error mse in the validation set .last sentence.c flexibility	tuning parameters included the number of features for linear regression and random forests the number of neighbors for knn and the number of estimators and the maximum tree depth for random forests 
1	108304	8304	in this project we developed methods for segmenting residential consumers based on their appliance level power consumption with respect to their availability variability and flexibility .first sentence.results indicated that segmentation using unsupervised learning methods can increase load availability for dr programs by up to a factor of two .vii conclusions and future work	in this project we developed methods for segmenting residential consumers based on their appliance level power consumption with respect to their availability variability and flexibility 
0	108305	8305	results indicated that segmentation using unsupervised learning methods can increase load availability for dr programs by up to a factor of two .in this project we developed methods for segmenting residential consumers based on their appliance level power consumption with respect to their availability variability and flexibility .hierarchical clustering applied to the start time probability distributions of deferrable appliances produced the most consistent performance .vii conclusions and future work	results indicated that segmentation using unsupervised learning methods can increase load availability for dr programs by up to a factor of two 
0	108306	8306	hierarchical clustering applied to the start time probability distributions of deferrable appliances produced the most consistent performance .results indicated that segmentation using unsupervised learning methods can increase load availability for dr programs by up to a factor of two .results indicated that cluster assignments can vary significantly from one appliance to another and can differ from the cluster assignments obtained by only analyzing the total power consumption of each home .vii conclusions and future work	hierarchical clustering applied to the start time probability distributions of deferrable appliances produced the most consistent performance 
1	108307	8307	results indicated that cluster assignments can vary significantly from one appliance to another and can differ from the cluster assignments obtained by only analyzing the total power consumption of each home .hierarchical clustering applied to the start time probability distributions of deferrable appliances produced the most consistent performance .this highlights the importance of performing consumer segmentation based on appliance level power consumption data .vii conclusions and future work	results indicated that cluster assignments can vary significantly from one appliance to another and can differ from the cluster assignments obtained by only analyzing the total power consumption of each home 
0	108308	8308	this highlights the importance of performing consumer segmentation based on appliance level power consumption data .results indicated that cluster assignments can vary significantly from one appliance to another and can differ from the cluster assignments obtained by only analyzing the total power consumption of each home .power consumption variability was assessed by calculating the entropy of the distribution of load profile types for individual consumers identified using kmeans clustering .vii conclusions and future work	this highlights the importance of performing consumer segmentation based on appliance level power consumption data 
0	108309	8309	power consumption variability was assessed by calculating the entropy of the distribution of load profile types for individual consumers identified using kmeans clustering .this highlights the importance of performing consumer segmentation based on appliance level power consumption data .results indicated notable differences in the variability of power consumption of different appliance types and segments of the population which could be exploited by a dr program provider .vii conclusions and future work	power consumption variability was assessed by calculating the entropy of the distribution of load profile types for individual consumers identified using kmeans clustering 
0	108310	8310	results indicated notable differences in the variability of power consumption of different appliance types and segments of the population which could be exploited by a dr program provider .power consumption variability was assessed by calculating the entropy of the distribution of load profile types for individual consumers identified using kmeans clustering .we tested three different supervised learning approaches for predicting consumer responsiveness to electricity prices and found that low variance models such as linear regression paired with recursive feature selection resulted in the lowest test error future work may investigate incorporating additional variables such as day of the week and season into the availability analysis .vii conclusions and future work	results indicated notable differences in the variability of power consumption of different appliance types and segments of the population which could be exploited by a dr program provider 
1	108311	8311	we tested three different supervised learning approaches for predicting consumer responsiveness to electricity prices and found that low variance models such as linear regression paired with recursive feature selection resulted in the lowest test error future work may investigate incorporating additional variables such as day of the week and season into the availability analysis .results indicated notable differences in the variability of power consumption of different appliance types and segments of the population which could be exploited by a dr program provider .expanding the analysis to a larger dataset may provide more insight into the generalizability of the results code for this project can be found at https github com ebuech cs229 .vii conclusions and future work	we tested three different supervised learning approaches for predicting consumer responsiveness to electricity prices and found that low variance models such as linear regression paired with recursive feature selection resulted in the lowest test error future work may investigate incorporating additional variables such as day of the week and season into the availability analysis 
0	108312	8312	expanding the analysis to a larger dataset may provide more insight into the generalizability of the results code for this project can be found at https github com ebuech cs229 .we tested three different supervised learning approaches for predicting consumer responsiveness to electricity prices and found that low variance models such as linear regression paired with recursive feature selection resulted in the lowest test error future work may investigate incorporating additional variables such as day of the week and season into the availability analysis .last sentence.vii conclusions and future work	expanding the analysis to a larger dataset may provide more insight into the generalizability of the results code for this project can be found at https github com ebuech cs229 
1	108313	8313	lily buechler performed feature extraction on the raw data implemented k means clustering and hierarchical clustering for the availability and variability analysis implemented the three supervised learning methods for the flexibility analysis and contributed to the poster and report zonghe chua implemented the gmm and lda models and performed the cluster number selection analysis using the elbow method and silhouette scores on all the unsupervised algorithms for the availability segmentation .first sentence.he also contributed to the poster and report .viii contributions	lily buechler performed feature extraction on the raw data implemented k means clustering and hierarchical clustering for the availability and variability analysis implemented the three supervised learning methods for the flexibility analysis and contributed to the poster and report zonghe chua implemented the gmm and lda models and performed the cluster number selection analysis using the elbow method and silhouette scores on all the unsupervised algorithms for the availability segmentation 
0	108314	8314	he also contributed to the poster and report .lily buechler performed feature extraction on the raw data implemented k means clustering and hierarchical clustering for the availability and variability analysis implemented the three supervised learning methods for the flexibility analysis and contributed to the poster and report zonghe chua implemented the gmm and lda models and performed the cluster number selection analysis using the elbow method and silhouette scores on all the unsupervised algorithms for the availability segmentation .last sentence.viii contributions	he also contributed to the poster and report 
1	108315	8315	in this project we seek to take a time series of current amplitudes collected by by phononsensitive detectors called qets quasiparticle trapping assisted electrothermal feedback transitionedge sensors and identify the start time of the pulse .first sentence.currently our techniques for registering a pulse are not accurate and often classify detector noise as a pulse .abstract	in this project we seek to take a time series of current amplitudes collected by by phononsensitive detectors called qets quasiparticle trapping assisted electrothermal feedback transitionedge sensors and identify the start time of the pulse 
0	108316	8316	currently our techniques for registering a pulse are not accurate and often classify detector noise as a pulse .in this project we seek to take a time series of current amplitudes collected by by phononsensitive detectors called qets quasiparticle trapping assisted electrothermal feedback transitionedge sensors and identify the start time of the pulse .this issue is significant because the separation of signal and noise is critical to the success of the experiment and is easily generalized to any other detector of this type .abstract	currently our techniques for registering a pulse are not accurate and often classify detector noise as a pulse 
1	108317	8317	this issue is significant because the separation of signal and noise is critical to the success of the experiment and is easily generalized to any other detector of this type .currently our techniques for registering a pulse are not accurate and often classify detector noise as a pulse .furthermore the problem of processing data to find pulses and characterizing them occur in many other fields were signals need to processed therefore it has the potential for a wider range of uses .abstract	this issue is significant because the separation of signal and noise is critical to the success of the experiment and is easily generalized to any other detector of this type 
1	108318	8318	furthermore the problem of processing data to find pulses and characterizing them occur in many other fields were signals need to processed therefore it has the potential for a wider range of uses .this issue is significant because the separation of signal and noise is critical to the success of the experiment and is easily generalized to any other detector of this type .we implement various machine learning models and found the most success with pca fcnn .abstract	furthermore the problem of processing data to find pulses and characterizing them occur in many other fields were signals need to processed therefore it has the potential for a wider range of uses 
1	108319	8319	we implement various machine learning models and found the most success with pca fcnn .furthermore the problem of processing data to find pulses and characterizing them occur in many other fields were signals need to processed therefore it has the potential for a wider range of uses .last sentence.abstract	we implement various machine learning models and found the most success with pca fcnn 
0	108320	8320	the cryogenic dark matter search cdms research group seeks to directly detect the most frequent form of matter in the universe dark matter .first sentence.to do so we study the behaviour inside semiconducting crystals at cryogenic temperatures .introduction	the cryogenic dark matter search cdms research group seeks to directly detect the most frequent form of matter in the universe dark matter 
0	108321	8321	to do so we study the behaviour inside semiconducting crystals at cryogenic temperatures .the cryogenic dark matter search cdms research group seeks to directly detect the most frequent form of matter in the universe dark matter .when a dark matter particle or another form of radiation interacts with the crystal a cloud of electrons and holes is produced at the interaction site .introduction	to do so we study the behaviour inside semiconducting crystals at cryogenic temperatures 
0	108322	8322	when a dark matter particle or another form of radiation interacts with the crystal a cloud of electrons and holes is produced at the interaction site .to do so we study the behaviour inside semiconducting crystals at cryogenic temperatures .these charges are then drifted through the crystal by an applied electric field and produce phonons that are collected by phonon sensitive detectors called qets quasiparticle trappingassisted electrothermal feedback transition edge sensors .introduction	when a dark matter particle or another form of radiation interacts with the crystal a cloud of electrons and holes is produced at the interaction site 
0	108323	8323	these charges are then drifted through the crystal by an applied electric field and produce phonons that are collected by phonon sensitive detectors called qets quasiparticle trappingassisted electrothermal feedback transition edge sensors .when a dark matter particle or another form of radiation interacts with the crystal a cloud of electrons and holes is produced at the interaction site .once the a signal pulse is received we seek to determine the start time of the interaction from the raw data .introduction	these charges are then drifted through the crystal by an applied electric field and produce phonons that are collected by phonon sensitive detectors called qets quasiparticle trappingassisted electrothermal feedback transition edge sensors 
1	108324	8324	once the a signal pulse is received we seek to determine the start time of the interaction from the raw data .these charges are then drifted through the crystal by an applied electric field and produce phonons that are collected by phonon sensitive detectors called qets quasiparticle trappingassisted electrothermal feedback transition edge sensors .we use logistic regression a shallow fully connected neural networks fcnn linear and kernelized principle component analysis pca with fcnn and convolutional recurrent neural networks cnn rnn .introduction	once the a signal pulse is received we seek to determine the start time of the interaction from the raw data 
1	108325	8325	we use logistic regression a shallow fully connected neural networks fcnn linear and kernelized principle component analysis pca with fcnn and convolutional recurrent neural networks cnn rnn .once the a signal pulse is received we seek to determine the start time of the interaction from the raw data .last sentence.introduction	we use logistic regression a shallow fully connected neural networks fcnn linear and kernelized principle component analysis pca with fcnn and convolutional recurrent neural networks cnn rnn 
0	108326	8326	dr andrew watson s dissertation.first sentence.last sentence.related work	dr andrew watson s dissertation
1	108327	8327	while we dont have enough real data 1 to train on we have do have a monte carlo simulation of our experiment built with g4cmp 2 .first sentence.combining the results from simulating with real noise we created our dataset as represented by.dataset creation	while we dont have enough real data 1 to train on we have do have a monte carlo simulation of our experiment built with g4cmp 2 
1	108328	8328	combining the results from simulating with real noise we created our dataset as represented by.while we dont have enough real data 1 to train on we have do have a monte carlo simulation of our experiment built with g4cmp 2 .last sentence.dataset creation	combining the results from simulating with real noise we created our dataset as represented by
1	108329	8329	in input features were traces with two channels a i r 2048 2 .first sentence.for the logistic regression and fcnn we flattened the trace to a 4096 dimensional vector .features	in input features were traces with two channels a i r 2048 2 
1	108330	8330	for the logistic regression and fcnn we flattened the trace to a 4096 dimensional vector .in input features were traces with two channels a i r 2048 2 .for the cnn rnn we kept the shape of the trace .features	for the logistic regression and fcnn we flattened the trace to a 4096 dimensional vector 
1	108331	8331	for the cnn rnn we kept the shape of the trace .for the logistic regression and fcnn we flattened the trace to a 4096 dimensional vector .for pca fcnn model we flattened the traces and used pca to find the first 1024 principle of the components pcs using 20 of the training set which explain 89 83 of the variance .features	for the cnn rnn we kept the shape of the trace 
1	108332	8332	for pca fcnn model we flattened the traces and used pca to find the first 1024 principle of the components pcs using 20 of the training set which explain 89 83 of the variance .for the cnn rnn we kept the shape of the trace .we decided to try pca because we plotted the correlation matrix of the traces 1 by real data we mean data that is produced by the physical detector instead of by the monter carlo simulation 2 https github com kelseymh g4cmp 3 each simulated event has an associated trace for each channel .features	for pca fcnn model we flattened the traces and used pca to find the first 1024 principle of the components pcs using 20 of the training set which explain 89 83 of the variance 
1	108333	8333	we decided to try pca because we plotted the correlation matrix of the traces 1 by real data we mean data that is produced by the physical detector instead of by the monter carlo simulation 2 https github com kelseymh g4cmp 3 each simulated event has an associated trace for each channel .for pca fcnn model we flattened the traces and used pca to find the first 1024 principle of the components pcs using 20 of the training set which explain 89 83 of the variance .a trace is a time series defined by an array of 2048 values where each value represents a current measured by the detector .features	we decided to try pca because we plotted the correlation matrix of the traces 1 by real data we mean data that is produced by the physical detector instead of by the monter carlo simulation 2 https github com kelseymh g4cmp 3 each simulated event has an associated trace for each channel 
0	108334	8334	a trace is a time series defined by an array of 2048 values where each value represents a current measured by the detector .we decided to try pca because we plotted the correlation matrix of the traces 1 by real data we mean data that is produced by the physical detector instead of by the monter carlo simulation 2 https github com kelseymh g4cmp 3 each simulated event has an associated trace for each channel .radial basis kernel and 1024 pcs .features	a trace is a time series defined by an array of 2048 values where each value represents a current measured by the detector 
1	108335	8335	radial basis kernel and 1024 pcs .a trace is a time series defined by an array of 2048 values where each value represents a current measured by the detector .we tried this too because we thought the relationship between the projected features could be non linear .features	radial basis kernel and 1024 pcs 
1	108336	8336	we tried this too because we thought the relationship between the projected features could be non linear .radial basis kernel and 1024 pcs .last sentence.features	we tried this too because we thought the relationship between the projected features could be non linear 
1	108337	8337	we represent a trance example input as a i r 2048 2 the flattened version as a i flatten a i r 4096 the true value of the time as t i and the prediction ast although for applied purposes we are more interested in reporting the mean absolute error mae .first sentence.last sentence.methods	we represent a trance example input as a i r 2048 2 the flattened version as a i flatten a i r 4096 the true value of the time as t i and the prediction ast although for applied purposes we are more interested in reporting the mean absolute error mae 
1	108338	8338	we begin the training modelling phase by fitting a logistic regression as our most basic baseline .first sentence.we use a i s as inputs and t i discretized into one hot encoded vector with 1048 classes as outputs .baselines logistic regression and shallow fcnn	we begin the training modelling phase by fitting a logistic regression as our most basic baseline 
0	108339	8339	we use a i s as inputs and t i discretized into one hot encoded vector with 1048 classes as outputs .we begin the training modelling phase by fitting a logistic regression as our most basic baseline .as a secondary baseline and to get a sense of the power of neural networks on this task we make a fcnn with one layer with 512 nodes .baselines logistic regression and shallow fcnn	we use a i s as inputs and t i discretized into one hot encoded vector with 1048 classes as outputs 
1	108340	8340	as a secondary baseline and to get a sense of the power of neural networks on this task we make a fcnn with one layer with 512 nodes .we use a i s as inputs and t i discretized into one hot encoded vector with 1048 classes as outputs .last sentence.baselines logistic regression and shallow fcnn	as a secondary baseline and to get a sense of the power of neural networks on this task we make a fcnn with one layer with 512 nodes 
1	108341	8341	we perform pca by finding the eigen basis of the correlation matrix c and then finding the projections by where v k t is the k th principle .first sentence.we then feed these projections into a fcnn explained in the next subsection .pca fcnn	we perform pca by finding the eigen basis of the correlation matrix c and then finding the projections by where v k t is the k th principle 
0	108342	8342	we then feed these projections into a fcnn explained in the next subsection .we perform pca by finding the eigen basis of the correlation matrix c and then finding the projections by where v k t is the k th principle .last sentence.pca fcnn	we then feed these projections into a fcnn explained in the next subsection 
1	108343	8343	we perform the kernel trick on linear pca as follows this was the structure used.first sentence.last sentence.kernelized pca fcnn	we perform the kernel trick on linear pca as follows this was the structure used
1	108344	8344	base on the suggestion of ta ashwini ramamoorthy we implemented a long short term memory lstm neural network as this type of nn is especially suited to dealing with time series data .first sentence.we built the model with two layers of max pooling with a stride of 4 followed by two dense hidden layers of 256 and 16 nodes respectively .lstm	base on the suggestion of ta ashwini ramamoorthy we implemented a long short term memory lstm neural network as this type of nn is especially suited to dealing with time series data 
1	108345	8345	we built the model with two layers of max pooling with a stride of 4 followed by two dense hidden layers of 256 and 16 nodes respectively .base on the suggestion of ta ashwini ramamoorthy we implemented a long short term memory lstm neural network as this type of nn is especially suited to dealing with time series data .training the lstm proved to be much slower than previous methods .lstm	we built the model with two layers of max pooling with a stride of 4 followed by two dense hidden layers of 256 and 16 nodes respectively 
0	108346	8346	training the lstm proved to be much slower than previous methods .we built the model with two layers of max pooling with a stride of 4 followed by two dense hidden layers of 256 and 16 nodes respectively .on the other hand the lstm predictions would converge withing a relatively smaller number of epochs less than 20 so little training was required .lstm	training the lstm proved to be much slower than previous methods 
1	108347	8347	on the other hand the lstm predictions would converge withing a relatively smaller number of epochs less than 20 so little training was required .training the lstm proved to be much slower than previous methods .unfortunately despite trying different structures of the model we were unable to attain an mean average error with this method that came close to that of pca fcnn .lstm	on the other hand the lstm predictions would converge withing a relatively smaller number of epochs less than 20 so little training was required 
1	108348	8348	unfortunately despite trying different structures of the model we were unable to attain an mean average error with this method that came close to that of pca fcnn .on the other hand the lstm predictions would converge withing a relatively smaller number of epochs less than 20 so little training was required .last sentence.lstm	unfortunately despite trying different structures of the model we were unable to attain an mean average error with this method that came close to that of pca fcnn 
0	108349	8349	the results table represents the average mae scaled to 2048 the total number of bins .first sentence.from this project were able to develop a methodology to construct an effective tool that we might use as part of physics experiment to determine the start time of pulses measured by our detector .results and discussion	the results table represents the average mae scaled to 2048 the total number of bins 
1	108350	8350	from this project were able to develop a methodology to construct an effective tool that we might use as part of physics experiment to determine the start time of pulses measured by our detector .the results table represents the average mae scaled to 2048 the total number of bins .after constructing our dataset from our monte carlo simulations we trained logistic regression fcnn cnn lstm a standard pca fed into fcnn and kpca with a radial basis kernel fed into a fcnn to predict t i and had the best test set mae with the standard pca fcnn method .results and discussion	from this project were able to develop a methodology to construct an effective tool that we might use as part of physics experiment to determine the start time of pulses measured by our detector 
1	108351	8351	after constructing our dataset from our monte carlo simulations we trained logistic regression fcnn cnn lstm a standard pca fed into fcnn and kpca with a radial basis kernel fed into a fcnn to predict t i and had the best test set mae with the standard pca fcnn method .from this project were able to develop a methodology to construct an effective tool that we might use as part of physics experiment to determine the start time of pulses measured by our detector .our goal was to get to a mae around 1 or 2 however the lowest we ever got on training was 4 .results and discussion	after constructing our dataset from our monte carlo simulations we trained logistic regression fcnn cnn lstm a standard pca fed into fcnn and kpca with a radial basis kernel fed into a fcnn to predict t i and had the best test set mae with the standard pca fcnn method 
1	108352	8352	our goal was to get to a mae around 1 or 2 however the lowest we ever got on training was 4 .after constructing our dataset from our monte carlo simulations we trained logistic regression fcnn cnn lstm a standard pca fed into fcnn and kpca with a radial basis kernel fed into a fcnn to predict t i and had the best test set mae with the standard pca fcnn method .this is likely due to the fact that the pulses are so noisy which is why we chose this challenging problem in the first place .results and discussion	our goal was to get to a mae around 1 or 2 however the lowest we ever got on training was 4 
1	108353	8353	this is likely due to the fact that the pulses are so noisy which is why we chose this challenging problem in the first place .our goal was to get to a mae around 1 or 2 however the lowest we ever got on training was 4 .an important insight from this project was that more complex models dont always produce better results as can be seen comparing the lstm cnn and kpca fcnn with the pca fcnn .results and discussion	this is likely due to the fact that the pulses are so noisy which is why we chose this challenging problem in the first place 
1	108354	8354	an important insight from this project was that more complex models dont always produce better results as can be seen comparing the lstm cnn and kpca fcnn with the pca fcnn .this is likely due to the fact that the pulses are so noisy which is why we chose this challenging problem in the first place .another lesson we learned was that producing the dataset and preparing it for training can be the most time intensive step .results and discussion	an important insight from this project was that more complex models dont always produce better results as can be seen comparing the lstm cnn and kpca fcnn with the pca fcnn 
1	108355	8355	another lesson we learned was that producing the dataset and preparing it for training can be the most time intensive step .an important insight from this project was that more complex models dont always produce better results as can be seen comparing the lstm cnn and kpca fcnn with the pca fcnn .finally while we didnt accomplish exactly what we set out to do we are content with out results and will continue improving on them .results and discussion	another lesson we learned was that producing the dataset and preparing it for training can be the most time intensive step 
1	108356	8356	finally while we didnt accomplish exactly what we set out to do we are content with out results and will continue improving on them .another lesson we learned was that producing the dataset and preparing it for training can be the most time intensive step .if we had more time we would try other models tune hyper parameters more methodically keeping track of all results and use a larger dataset with more examples per energy and also a wider range of energies .results and discussion	finally while we didnt accomplish exactly what we set out to do we are content with out results and will continue improving on them 
1	108357	8357	if we had more time we would try other models tune hyper parameters more methodically keeping track of all results and use a larger dataset with more examples per energy and also a wider range of energies .finally while we didnt accomplish exactly what we set out to do we are content with out results and will continue improving on them .last sentence.results and discussion	if we had more time we would try other models tune hyper parameters more methodically keeping track of all results and use a larger dataset with more examples per energy and also a wider range of energies 
1	108358	8358	we would like to thank to chin yu for suggesting this project and providing key insights about machine learning the northwestern supercdms team for providing the mc simulation and the sample noise traces from which we generated our dataset and finally the cs299 tas and professors for giving us the opportunity to do this project and teaching us all the material .first sentence.last sentence.acknowledgements	we would like to thank to chin yu for suggesting this project and providing key insights about machine learning the northwestern supercdms team for providing the mc simulation and the sample noise traces from which we generated our dataset and finally the cs299 tas and professors for giving us the opportunity to do this project and teaching us all the material 
0	108359	8359	we present an unsupervised machine learning model for computing approximate electromagnetic fields in a cavity containing an arbitrary spatial dielectric permittivity distribution .first sentence.our model achieves good predictive performance and is over 10 faster than identically sized finite difference frequency domain simulations suggesting possible applications for accelerating optical inverse design algorithms .abstract	we present an unsupervised machine learning model for computing approximate electromagnetic fields in a cavity containing an arbitrary spatial dielectric permittivity distribution 
0	108360	8360	our model achieves good predictive performance and is over 10 faster than identically sized finite difference frequency domain simulations suggesting possible applications for accelerating optical inverse design algorithms .we present an unsupervised machine learning model for computing approximate electromagnetic fields in a cavity containing an arbitrary spatial dielectric permittivity distribution .last sentence.abstract	our model achieves good predictive performance and is over 10 faster than identically sized finite difference frequency domain simulations suggesting possible applications for accelerating optical inverse design algorithms 
1	108361	8361	inverse design problems computational design of structures by specifying an objective functionare pervasive throughout physics especially in photonics where inverse design methods have been used to design many highly compact optical components the iterative fdfd simulations although exact can be computationally expensive and scale poorly with the design dimensions .first sentence.for many applications an approximate field solution is sufficient .introduction	inverse design problems computational design of structures by specifying an objective functionare pervasive throughout physics especially in photonics where inverse design methods have been used to design many highly compact optical components the iterative fdfd simulations although exact can be computationally expensive and scale poorly with the design dimensions 
0	108362	8362	for many applications an approximate field solution is sufficient .inverse design problems computational design of structures by specifying an objective functionare pervasive throughout physics especially in photonics where inverse design methods have been used to design many highly compact optical components the iterative fdfd simulations although exact can be computationally expensive and scale poorly with the design dimensions .a machine learning model which could quickly compute approximate electromagnetic fields for a dielectric structure could reduce this computational bottleneck allowing for much faster inverse design processes .introduction	for many applications an approximate field solution is sufficient 
0	108363	8363	a machine learning model which could quickly compute approximate electromagnetic fields for a dielectric structure could reduce this computational bottleneck allowing for much faster inverse design processes .for many applications an approximate field solution is sufficient .last sentence.introduction	a machine learning model which could quickly compute approximate electromagnetic fields for a dielectric structure could reduce this computational bottleneck allowing for much faster inverse design processes 
0	108364	8364	we were able to find a small body of existing work related to this problem .first sentence.shan et al .related work	we were able to find a small body of existing work related to this problem 
0	108365	8365	shan et al .we were able to find a small body of existing work related to this problem .lagaris et al .related work	shan et al 
0	108366	8366	lagaris et al .shan et al .last sentence.related work	lagaris et al 
0	108367	8367	our model computes approximate electromagnetic field solutions for a specific type of scenario formalized here .first sentence.suppose we have a perfectly reflective d dimensional 2 cavity of length l with an electromagnetic source at the center .problem summary and approach	our model computes approximate electromagnetic field solutions for a specific type of scenario formalized here 
0	108368	8368	suppose we have a perfectly reflective d dimensional 2 cavity of length l with an electromagnetic source at the center .our model computes approximate electromagnetic field solutions for a specific type of scenario formalized here .the cavity contains material forming an arbitrary spatial distribution of dielectric permittivity x .problem summary and approach	suppose we have a perfectly reflective d dimensional 2 cavity of length l with an electromagnetic source at the center 
0	108369	8369	the cavity contains material forming an arbitrary spatial distribution of dielectric permittivity x .suppose we have a perfectly reflective d dimensional 2 cavity of length l with an electromagnetic source at the center .discretizing the cavity into pixels of size l the permittivity at each point in space can be expressed as a vector of size n l l d .problem summary and approach	the cavity contains material forming an arbitrary spatial distribution of dielectric permittivity x 
0	108370	8370	discretizing the cavity into pixels of size l the permittivity at each point in space can be expressed as a vector of size n l l d .the cavity contains material forming an arbitrary spatial distribution of dielectric permittivity x .given an input permittivity vector and knowledge of the source location the model outputs an identicallysized vector e pred representing the electric field amplitude at each point in space .problem summary and approach	discretizing the cavity into pixels of size l the permittivity at each point in space can be expressed as a vector of size n l l d 
0	108371	8371	given an input permittivity vector and knowledge of the source location the model outputs an identicallysized vector e pred representing the electric field amplitude at each point in space .discretizing the cavity into pixels of size l the permittivity at each point in space can be expressed as a vector of size n l l d .the cavity scenario was chosen to impose dirichlet boundary conditions of e 0 at the cavity edges ensuring the electric fields are standing waves and thus real up to a global phase .problem summary and approach	given an input permittivity vector and knowledge of the source location the model outputs an identicallysized vector e pred representing the electric field amplitude at each point in space 
0	108372	8372	the cavity scenario was chosen to impose dirichlet boundary conditions of e 0 at the cavity edges ensuring the electric fields are standing waves and thus real up to a global phase .given an input permittivity vector and knowledge of the source location the model outputs an identicallysized vector e pred representing the electric field amplitude at each point in space .the structure of the model is loosely analogous to a generative adversarial network pred .problem summary and approach	the cavity scenario was chosen to impose dirichlet boundary conditions of e 0 at the cavity edges ensuring the electric fields are standing waves and thus real up to a global phase 
0	108373	8373	the structure of the model is loosely analogous to a generative adversarial network pred .the cavity scenario was chosen to impose dirichlet boundary conditions of e 0 at the cavity edges ensuring the electric fields are standing waves and thus real up to a global phase .the second part is a discriminator 5 which computes the maxwell residual of the predicted field as d providing a measure of how physically realistic the generator s outputs are .problem summary and approach	the structure of the model is loosely analogous to a generative adversarial network pred 
0	108374	8374	the second part is a discriminator 5 which computes the maxwell residual of the predicted field as d providing a measure of how physically realistic the generator s outputs are .the structure of the model is loosely analogous to a generative adversarial network pred .in both cases the loss of the total model is.problem summary and approach	the second part is a discriminator 5 which computes the maxwell residual of the predicted field as d providing a measure of how physically realistic the generator s outputs are 
0	108375	8375	in both cases the loss of the total model is.the second part is a discriminator 5 which computes the maxwell residual of the predicted field as d providing a measure of how physically realistic the generator s outputs are .last sentence.problem summary and approach	in both cases the loss of the total model is
0	108376	8376	maxwell s equations govern the dynamics and propagation of electromagnetic fields in materials and form the foundation for classical electromagnetism .first sentence.where e b are electric and magnetic fields at a given point in space and time are the permittivity and permeability of the material t is time is charge density and j is current density .unsupervised training with maxwell residuals	maxwell s equations govern the dynamics and propagation of electromagnetic fields in materials and form the foundation for classical electromagnetism 
0	108377	8377	where e b are electric and magnetic fields at a given point in space and time are the permittivity and permeability of the material t is time is charge density and j is current density .maxwell s equations govern the dynamics and propagation of electromagnetic fields in materials and form the foundation for classical electromagnetism .in a nonmagnetic electrically neutral linear material such as many cases of interest 0 0 and these equations can be simplified to where h b 0 is the magnetizing field .unsupervised training with maxwell residuals	where e b are electric and magnetic fields at a given point in space and time are the permittivity and permeability of the material t is time is charge density and j is current density 
0	108378	8378	in a nonmagnetic electrically neutral linear material such as many cases of interest 0 0 and these equations can be simplified to where h b 0 is the magnetizing field .where e b are electric and magnetic fields at a given point in space and time are the permittivity and permeability of the material t is time is charge density and j is current density .in a steady state frequency domain solution such as the ones found with fdfd methods e t ee i t so t where is frequency .unsupervised training with maxwell residuals	in a nonmagnetic electrically neutral linear material such as many cases of interest 0 0 and these equations can be simplified to where h b 0 is the magnetizing field 
0	108379	8379	in a steady state frequency domain solution such as the ones found with fdfd methods e t ee i t so t where is frequency .in a nonmagnetic electrically neutral linear material such as many cases of interest 0 0 and these equations can be simplified to where h b 0 is the magnetizing field .we can combine if the electromagnetic field is polarized say with e z polarization then e e and j j at each point in space .unsupervised training with maxwell residuals	in a steady state frequency domain solution such as the ones found with fdfd methods e t ee i t so t where is frequency 
0	108380	8380	we can combine if the electromagnetic field is polarized say with e z polarization then e e and j j at each point in space .in a steady state frequency domain solution such as the ones found with fdfd methods e t ee i t so t where is frequency .we can then vectorize this such that e and j are the electric field and free current amplitudes in the direction and is the dielectric permittivity at each point in space .unsupervised training with maxwell residuals	we can combine if the electromagnetic field is polarized say with e z polarization then e e and j j at each point in space 
0	108381	8381	we can then vectorize this such that e and j are the electric field and free current amplitudes in the direction and is the dielectric permittivity at each point in space .we can combine if the electromagnetic field is polarized say with e z polarization then e e and j j at each point in space .if we have a model which takes in a permittivity distribution and a source term j and returns a predicted field e pred then we use eq .unsupervised training with maxwell residuals	we can then vectorize this such that e and j are the electric field and free current amplitudes in the direction and is the dielectric permittivity at each point in space 
0	108382	8382	if we have a model which takes in a permittivity distribution and a source term j and returns a predicted field e pred then we use eq .we can then vectorize this such that e and j are the electric field and free current amplitudes in the direction and is the dielectric permittivity at each point in space .4 to define the maxwell residual l m as the maxwell residual provides an element wise measure of the physical realism of the predicted field e pred a measure of how far the predicted solution is from satisfying maxwell s equations at each point .unsupervised training with maxwell residuals	if we have a model which takes in a permittivity distribution and a source term j and returns a predicted field e pred then we use eq 
0	108383	8383	4 to define the maxwell residual l m as the maxwell residual provides an element wise measure of the physical realism of the predicted field e pred a measure of how far the predicted solution is from satisfying maxwell s equations at each point .if we have a model which takes in a permittivity distribution and a source term j and returns a predicted field e pred then we use eq .if the model can sufficiently minimize l m then it can produce solutions which approximately satisfy maxwell s equations at each point and thus are approximate global electromagnetic field solutions for the system described by and j .unsupervised training with maxwell residuals	4 to define the maxwell residual l m as the maxwell residual provides an element wise measure of the physical realism of the predicted field e pred a measure of how far the predicted solution is from satisfying maxwell s equations at each point 
0	108384	8384	if the model can sufficiently minimize l m then it can produce solutions which approximately satisfy maxwell s equations at each point and thus are approximate global electromagnetic field solutions for the system described by and j .4 to define the maxwell residual l m as the maxwell residual provides an element wise measure of the physical realism of the predicted field e pred a measure of how far the predicted solution is from satisfying maxwell s equations at each point .this training does not require the model to ever see the exact fdfd field solution the outputs it attempts to replicate and is thus unsupervised .unsupervised training with maxwell residuals	if the model can sufficiently minimize l m then it can produce solutions which approximately satisfy maxwell s equations at each point and thus are approximate global electromagnetic field solutions for the system described by and j 
0	108385	8385	this training does not require the model to ever see the exact fdfd field solution the outputs it attempts to replicate and is thus unsupervised .if the model can sufficiently minimize l m then it can produce solutions which approximately satisfy maxwell s equations at each point and thus are approximate global electromagnetic field solutions for the system described by and j .last sentence.unsupervised training with maxwell residuals	this training does not require the model to ever see the exact fdfd field solution the outputs it attempts to replicate and is thus unsupervised 
1	108386	8386	we found that when trained or more precisely overfit to predict the field of a single permittivity distribution virtually any network architecture would allow the predicted field to converge to the true field given enough training time .first sentence.for more on this see section 4 1 .model architecture and implementation	we found that when trained or more precisely overfit to predict the field of a single permittivity distribution virtually any network architecture would allow the predicted field to converge to the true field given enough training time 
0	108387	8387	for more on this see section 4 1 .we found that when trained or more precisely overfit to predict the field of a single permittivity distribution virtually any network architecture would allow the predicted field to converge to the true field given enough training time .the challenge was finding a network architecture with the correct structure to capture the generalized transformation e when trained on a large number of possible permittivities we tested many different network architectures for this project .model architecture and implementation	for more on this see section 4 1 
0	108388	8388	the challenge was finding a network architecture with the correct structure to capture the generalized transformation e when trained on a large number of possible permittivities we tested many different network architectures for this project .for more on this see section 4 1 .purely convolutional architectures like the ones used by ref .model architecture and implementation	the challenge was finding a network architecture with the correct structure to capture the generalized transformation e when trained on a large number of possible permittivities we tested many different network architectures for this project 
0	108389	8389	purely convolutional architectures like the ones used by ref .the challenge was finding a network architecture with the correct structure to capture the generalized transformation e when trained on a large number of possible permittivities we tested many different network architectures for this project .our final network architecture employed a hybrid convolutional dense deconvolutional approach and is shown in during training the network outputs the maxwell residual l m e .model architecture and implementation	purely convolutional architectures like the ones used by ref 
0	108390	8390	our final network architecture employed a hybrid convolutional dense deconvolutional approach and is shown in during training the network outputs the maxwell residual l m e .purely convolutional architectures like the ones used by ref .dropout layers with p 0 1 and relu activations are present after every layer except the last one .model architecture and implementation	our final network architecture employed a hybrid convolutional dense deconvolutional approach and is shown in during training the network outputs the maxwell residual l m e 
0	108391	8391	dropout layers with p 0 1 and relu activations are present after every layer except the last one .our final network architecture employed a hybrid convolutional dense deconvolutional approach and is shown in during training the network outputs the maxwell residual l m e .our model was implemented using pytorch.model architecture and implementation	dropout layers with p 0 1 and relu activations are present after every layer except the last one 
0	108392	8392	our model was implemented using pytorch.dropout layers with p 0 1 and relu activations are present after every layer except the last one .last sentence.model architecture and implementation	our model was implemented using pytorch
0	108393	8393	as an initial experiment we trained the model to predict the field of only a single input using the maxwell residual method described in section 3 2 .first sentence.the evolution of the predicted field as the network is trained on a sample permittivity is shown in.experiments 4 1 fitting to single	as an initial experiment we trained the model to predict the field of only a single input using the maxwell residual method described in section 3 2 
0	108394	8394	the evolution of the predicted field as the network is trained on a sample permittivity is shown in.as an initial experiment we trained the model to predict the field of only a single input using the maxwell residual method described in section 3 2 .last sentence.experiments 4 1 fitting to single	the evolution of the predicted field as the network is trained on a sample permittivity is shown in
0	108395	8395	for the main experiment in this paper we trained a model with the architecture described in to evaluate the results of the trained model a test set of 10 4 new permittivity samples was generated using the same generation procedure .first sentence.the model was run on each of these inputs the loss for each sample was calculated average loss of 8 8 10 4 and the results were sorted from best to worst .training on permittivity datasets	for the main experiment in this paper we trained a model with the architecture described in to evaluate the results of the trained model a test set of 10 4 new permittivity samples was generated using the same generation procedure 
0	108396	8396	the model was run on each of these inputs the loss for each sample was calculated average loss of 8 8 10 4 and the results were sorted from best to worst .for the main experiment in this paper we trained a model with the architecture described in to evaluate the results of the trained model a test set of 10 4 new permittivity samples was generated using the same generation procedure .example good best 10 10000 typical middle 10 10000 and bad worst 10 10000 field predictions from the test set are shown in the first three panels of finally we tested the model s capability to generalize to inputs outside of the training distributionthat is permittivities representing a different set of structures than the ones generated for the training and test sets .training on permittivity datasets	the model was run on each of these inputs the loss for each sample was calculated average loss of 8 8 10 4 and the results were sorted from best to worst 
1	108397	8397	example good best 10 10000 typical middle 10 10000 and bad worst 10 10000 field predictions from the test set are shown in the first three panels of finally we tested the model s capability to generalize to inputs outside of the training distributionthat is permittivities representing a different set of structures than the ones generated for the training and test sets .the model was run on each of these inputs the loss for each sample was calculated average loss of 8 8 10 4 and the results were sorted from best to worst .as an example the predicted field amplitudes for a sample where each point in space has a permittivity value randomly chosen between vacuum and silicon is shown in the last panel of.training on permittivity datasets	example good best 10 10000 typical middle 10 10000 and bad worst 10 10000 field predictions from the test set are shown in the first three panels of finally we tested the model s capability to generalize to inputs outside of the training distributionthat is permittivities representing a different set of structures than the ones generated for the training and test sets 
0	108398	8398	as an example the predicted field amplitudes for a sample where each point in space has a permittivity value randomly chosen between vacuum and silicon is shown in the last panel of.example good best 10 10000 typical middle 10 10000 and bad worst 10 10000 field predictions from the test set are shown in the first three panels of finally we tested the model s capability to generalize to inputs outside of the training distributionthat is permittivities representing a different set of structures than the ones generated for the training and test sets .last sentence.training on permittivity datasets	as an example the predicted field amplitudes for a sample where each point in space has a permittivity value randomly chosen between vacuum and silicon is shown in the last panel of
0	108399	8399	in this paper we presented a machine learning model capable of computing approximate solutions to maxwell s equations over an arbitrary dielectric permittivity in a cavity .first sentence.the model was trained using an unsupervised approach where it learned to minimize the maxwell residual of its predicted fields thereby maximizing the physical realism of the solutions .discussion	in this paper we presented a machine learning model capable of computing approximate solutions to maxwell s equations over an arbitrary dielectric permittivity in a cavity 
0	108400	8400	the model was trained using an unsupervised approach where it learned to minimize the maxwell residual of its predicted fields thereby maximizing the physical realism of the solutions .in this paper we presented a machine learning model capable of computing approximate solutions to maxwell s equations over an arbitrary dielectric permittivity in a cavity .our model demonstrates good predictive performance and is over 10 faster than comparable fdfd simulations suggesting applications for accelerating optical inverse design algorithms for future work we would like to implement a complex valued model to solve the more general problem of predicting fields outside of a cavity environment .discussion	the model was trained using an unsupervised approach where it learned to minimize the maxwell residual of its predicted fields thereby maximizing the physical realism of the solutions 
1	108401	8401	our model demonstrates good predictive performance and is over 10 faster than comparable fdfd simulations suggesting applications for accelerating optical inverse design algorithms for future work we would like to implement a complex valued model to solve the more general problem of predicting fields outside of a cavity environment .the model was trained using an unsupervised approach where it learned to minimize the maxwell residual of its predicted fields thereby maximizing the physical realism of the solutions .our choice of the cavity problem was driven primarily by pytorch s lack of support for complex tensors .discussion	our model demonstrates good predictive performance and is over 10 faster than comparable fdfd simulations suggesting applications for accelerating optical inverse design algorithms for future work we would like to implement a complex valued model to solve the more general problem of predicting fields outside of a cavity environment 
0	108402	8402	our choice of the cavity problem was driven primarily by pytorch s lack of support for complex tensors .our model demonstrates good predictive performance and is over 10 faster than comparable fdfd simulations suggesting applications for accelerating optical inverse design algorithms for future work we would like to implement a complex valued model to solve the more general problem of predicting fields outside of a cavity environment .in the project repository we have an initial implementation of this which explicitly parameterizes e and e although this approach was only mildly successful .discussion	our choice of the cavity problem was driven primarily by pytorch s lack of support for complex tensors 
0	108403	8403	in the project repository we have an initial implementation of this which explicitly parameterizes e and e although this approach was only mildly successful .our choice of the cavity problem was driven primarily by pytorch s lack of support for complex tensors .we would also like to explore using our model for dimensionality reduction especially for 2d and 3d problems .discussion	in the project repository we have an initial implementation of this which explicitly parameterizes e and e although this approach was only mildly successful 
0	108404	8404	we would also like to explore using our model for dimensionality reduction especially for 2d and 3d problems .in the project repository we have an initial implementation of this which explicitly parameterizes e and e although this approach was only mildly successful .we were able to achieve a 1 16 dimensionality reduction with our model applied to a 32 32 2d input of permittivities by adjusting the network parameters to force a 64 value chokepoint in the middle dense layers of the network .discussion	we would also like to explore using our model for dimensionality reduction especially for 2d and 3d problems 
1	108405	8405	we were able to achieve a 1 16 dimensionality reduction with our model applied to a 32 32 2d input of permittivities by adjusting the network parameters to force a 64 value chokepoint in the middle dense layers of the network .we would also like to explore using our model for dimensionality reduction especially for 2d and 3d problems .this figure is present in the poster but omitted here due to length constraints .discussion	we were able to achieve a 1 16 dimensionality reduction with our model applied to a 32 32 2d input of permittivities by adjusting the network parameters to force a 64 value chokepoint in the middle dense layers of the network 
0	108406	8406	this figure is present in the poster but omitted here due to length constraints .we were able to achieve a 1 16 dimensionality reduction with our model applied to a 32 32 2d input of permittivities by adjusting the network parameters to force a 64 value chokepoint in the middle dense layers of the network .this could force the model to learn more efficient representations of the relationships between permittivities and fields .discussion	this figure is present in the poster but omitted here due to length constraints 
0	108407	8407	this could force the model to learn more efficient representations of the relationships between permittivities and fields .this figure is present in the poster but omitted here due to length constraints .last sentence.discussion	this could force the model to learn more efficient representations of the relationships between permittivities and fields 
0	108408	8408	we would like to thank shanhui fan sunil pai and tyler hughes for several illuminating discussions relating to this work all source code used for this paper is available at https github com bencbartlett neural maxwell .first sentence.trained model parameters were too large to include in the repository but are are available upon request .source code	we would like to thank shanhui fan sunil pai and tyler hughes for several illuminating discussions relating to this work all source code used for this paper is available at https github com bencbartlett neural maxwell 
0	108409	8409	trained model parameters were too large to include in the repository but are are available upon request .we would like to thank shanhui fan sunil pai and tyler hughes for several illuminating discussions relating to this work all source code used for this paper is available at https github com bencbartlett neural maxwell .last sentence.source code	trained model parameters were too large to include in the repository but are are available upon request 
1	108410	8410	since the 1970s the seismic bridge design process has gone through a great change from capacity based design to performance based design and the performance of bridges has become a great concern to engineers .first sentence.the safety margins of the contributive factors vary from case to case and the trends are still unclear partially if not all because of the change in the design logics in the past decades .introduction	since the 1970s the seismic bridge design process has gone through a great change from capacity based design to performance based design and the performance of bridges has become a great concern to engineers 
1	108411	8411	the safety margins of the contributive factors vary from case to case and the trends are still unclear partially if not all because of the change in the design logics in the past decades .since the 1970s the seismic bridge design process has gone through a great change from capacity based design to performance based design and the performance of bridges has become a great concern to engineers .therefore having an on hand trained model on bridge performance prediction would be to some extent helpful for knowing how well badly an existing bridge would perform in a future earthquake as well as guiding the design of a new bridge to survive a future earthquake in this project we are trying to train a prediction model for bridge performance under earthquakes with supervised learning .introduction	the safety margins of the contributive factors vary from case to case and the trends are still unclear partially if not all because of the change in the design logics in the past decades 
1	108412	8412	therefore having an on hand trained model on bridge performance prediction would be to some extent helpful for knowing how well badly an existing bridge would perform in a future earthquake as well as guiding the design of a new bridge to survive a future earthquake in this project we are trying to train a prediction model for bridge performance under earthquakes with supervised learning .the safety margins of the contributive factors vary from case to case and the trends are still unclear partially if not all because of the change in the design logics in the past decades .the inputs to our algorithm are the age of a bridge the magnitude of the earthquake and the distance between that bridge and the epicenter .introduction	therefore having an on hand trained model on bridge performance prediction would be to some extent helpful for knowing how well badly an existing bridge would perform in a future earthquake as well as guiding the design of a new bridge to survive a future earthquake in this project we are trying to train a prediction model for bridge performance under earthquakes with supervised learning 
1	108413	8413	the inputs to our algorithm are the age of a bridge the magnitude of the earthquake and the distance between that bridge and the epicenter .therefore having an on hand trained model on bridge performance prediction would be to some extent helpful for knowing how well badly an existing bridge would perform in a future earthquake as well as guiding the design of a new bridge to survive a future earthquake in this project we are trying to train a prediction model for bridge performance under earthquakes with supervised learning .we then use logistic regression quadratic discriminative analysis and k nearest neighbor classifier to output a predicted performance categorized to be positive damaged or negative undamaged of the bridge under the given earthquake .introduction	the inputs to our algorithm are the age of a bridge the magnitude of the earthquake and the distance between that bridge and the epicenter 
1	108414	8414	we then use logistic regression quadratic discriminative analysis and k nearest neighbor classifier to output a predicted performance categorized to be positive damaged or negative undamaged of the bridge under the given earthquake .the inputs to our algorithm are the age of a bridge the magnitude of the earthquake and the distance between that bridge and the epicenter .last sentence.introduction	we then use logistic regression quadratic discriminative analysis and k nearest neighbor classifier to output a predicted performance categorized to be positive damaged or negative undamaged of the bridge under the given earthquake 
0	108415	8415	prof kiremidjian s paper greatly inspires our interest on data driven earthquake engineering .first sentence.when trying to select the related features we refer to a comprehensive study of u s bridge failures from mceer technical report .related work	prof kiremidjian s paper greatly inspires our interest on data driven earthquake engineering 
0	108416	8416	when trying to select the related features we refer to a comprehensive study of u s bridge failures from mceer technical report .prof kiremidjian s paper greatly inspires our interest on data driven earthquake engineering .we also learn the advantages of bootstrap resampling in managing unbalanced data from dupret and koda s paper the problem we encountered is actually a sub problem of other projects on application of machine learning in earthquake engineering in the previous cs 229 projects .related work	when trying to select the related features we refer to a comprehensive study of u s bridge failures from mceer technical report 
1	108417	8417	we also learn the advantages of bootstrap resampling in managing unbalanced data from dupret and koda s paper the problem we encountered is actually a sub problem of other projects on application of machine learning in earthquake engineering in the previous cs 229 projects .when trying to select the related features we refer to a comprehensive study of u s bridge failures from mceer technical report .in this project we implement some similar methodologies such as knn but with a different approach to select the optimal k value .related work	we also learn the advantages of bootstrap resampling in managing unbalanced data from dupret and koda s paper the problem we encountered is actually a sub problem of other projects on application of machine learning in earthquake engineering in the previous cs 229 projects 
1	108418	8418	in this project we implement some similar methodologies such as knn but with a different approach to select the optimal k value .we also learn the advantages of bootstrap resampling in managing unbalanced data from dupret and koda s paper the problem we encountered is actually a sub problem of other projects on application of machine learning in earthquake engineering in the previous cs 229 projects .last sentence.related work	in this project we implement some similar methodologies such as knn but with a different approach to select the optimal k value 
0	108419	8419	the data collection is a big challenge for us because we have limited records of bridge failure due to earthquakes .first sentence.therefore we make an assumption in advance if an earthquake occurs near the site of a bridge but there is no record showing that the bridge is damaged then this should be identified as an example of no damage negative examples we first search for bridge failure records in previous famous earthquakes in the reports published by the u s government find the site location of the bridge and mark it as a positive example .dataset and features	the data collection is a big challenge for us because we have limited records of bridge failure due to earthquakes 
1	108420	8420	therefore we make an assumption in advance if an earthquake occurs near the site of a bridge but there is no record showing that the bridge is damaged then this should be identified as an example of no damage negative examples we first search for bridge failure records in previous famous earthquakes in the reports published by the u s government find the site location of the bridge and mark it as a positive example .the data collection is a big challenge for us because we have limited records of bridge failure due to earthquakes .next we search for other earthquake records happened in bridge s history near the site location on usgs earthquake search catalog and mark them as negative examples .dataset and features	therefore we make an assumption in advance if an earthquake occurs near the site of a bridge but there is no record showing that the bridge is damaged then this should be identified as an example of no damage negative examples we first search for bridge failure records in previous famous earthquakes in the reports published by the u s government find the site location of the bridge and mark it as a positive example 
1	108421	8421	next we search for other earthquake records happened in bridge s history near the site location on usgs earthquake search catalog and mark them as negative examples .therefore we make an assumption in advance if an earthquake occurs near the site of a bridge but there is no record showing that the bridge is damaged then this should be identified as an example of no damage negative examples we first search for bridge failure records in previous famous earthquakes in the reports published by the u s government find the site location of the bridge and mark it as a positive example .finally we split the dataset into training 70 and test 30 sets because of the intrinsic scarcity of positive examples our dataset is unbalanced .dataset and features	next we search for other earthquake records happened in bridge s history near the site location on usgs earthquake search catalog and mark them as negative examples 
1	108422	8422	finally we split the dataset into training 70 and test 30 sets because of the intrinsic scarcity of positive examples our dataset is unbalanced .next we search for other earthquake records happened in bridge s history near the site location on usgs earthquake search catalog and mark them as negative examples .the positive tonegative ratio is about 1 10 .dataset and features	finally we split the dataset into training 70 and test 30 sets because of the intrinsic scarcity of positive examples our dataset is unbalanced 
0	108423	8423	the positive tonegative ratio is about 1 10 .finally we split the dataset into training 70 and test 30 sets because of the intrinsic scarcity of positive examples our dataset is unbalanced .to mitigate this problem we use bootstrap resampling with replacement to up sample the positive class and generate training sets with more reasonable positive to negative ratio .dataset and features	the positive tonegative ratio is about 1 10 
1	108424	8424	to mitigate this problem we use bootstrap resampling with replacement to up sample the positive class and generate training sets with more reasonable positive to negative ratio .the positive tonegative ratio is about 1 10 .in our project this ratio varies from 0 2 to 1 0 the raw features selected for the model are the age of the bridge the magnitude of the earthquake and the distance between the bridge and the epicenter all of which are continuous variables .dataset and features	to mitigate this problem we use bootstrap resampling with replacement to up sample the positive class and generate training sets with more reasonable positive to negative ratio 
1	108425	8425	in our project this ratio varies from 0 2 to 1 0 the raw features selected for the model are the age of the bridge the magnitude of the earthquake and the distance between the bridge and the epicenter all of which are continuous variables .to mitigate this problem we use bootstrap resampling with replacement to up sample the positive class and generate training sets with more reasonable positive to negative ratio .for future work we plan to explore more features continuous and discrete such as the material type the structure type the annual average daily traffic etc .dataset and features	in our project this ratio varies from 0 2 to 1 0 the raw features selected for the model are the age of the bridge the magnitude of the earthquake and the distance between the bridge and the epicenter all of which are continuous variables 
0	108426	8426	for future work we plan to explore more features continuous and discrete such as the material type the structure type the annual average daily traffic etc .in our project this ratio varies from 0 2 to 1 0 the raw features selected for the model are the age of the bridge the magnitude of the earthquake and the distance between the bridge and the epicenter all of which are continuous variables .last sentence.dataset and features	for future work we plan to explore more features continuous and discrete such as the material type the structure type the annual average daily traffic etc 
1	108427	8427	the goal of the algorithm for this binary classification problem is to predict the correct or more likely bridge performance given data on the earthquake and the bridge itself we use logistic regression quadratic discriminative analysis qda and k nearest neighbor classifier knn to train the model respectively and independently .first sentence.last sentence.methods	the goal of the algorithm for this binary classification problem is to predict the correct or more likely bridge performance given data on the earthquake and the bridge itself we use logistic regression quadratic discriminative analysis qda and k nearest neighbor classifier knn to train the model respectively and independently 
0	108428	8428	the logistic regression updates a set of parameters that maximize the log likelihood 2 .first sentence.quadratic discriminative analysis qda qda is one of the most commonly used generative models where we assume that the results from each class are normally distributed .logistic regression	the logistic regression updates a set of parameters that maximize the log likelihood 2 
0	108429	8429	quadratic discriminative analysis qda qda is one of the most commonly used generative models where we assume that the results from each class are normally distributed .the logistic regression updates a set of parameters that maximize the log likelihood 2 .in binary case with means 0 1 and 0 1 the log likelihood can be computed as .logistic regression	quadratic discriminative analysis qda qda is one of the most commonly used generative models where we assume that the results from each class are normally distributed 
0	108430	8430	in binary case with means 0 1 and 0 1 the log likelihood can be computed as .quadratic discriminative analysis qda qda is one of the most commonly used generative models where we assume that the results from each class are normally distributed .last sentence.logistic regression	in binary case with means 0 1 and 0 1 the log likelihood can be computed as 
0	108431	8431	the k nearest neighbor classifier considers k neighbors of the current data point compares the number of positives and negatives within the k neighbors and outputs the label i e .first sentence.positive or negative of the one that has a greater number than the other .k nearest neighbor classifier knn 	the k nearest neighbor classifier considers k neighbors of the current data point compares the number of positives and negatives within the k neighbors and outputs the label i e 
0	108432	8432	positive or negative of the one that has a greater number than the other .the k nearest neighbor classifier considers k neighbors of the current data point compares the number of positives and negatives within the k neighbors and outputs the label i e .to determine the optimal value of k we use a cross validation set and plot its accuracy with respect to the value of k from 1 to 10 as shown below based on the figure shown above the optimal k value in this case is 2 .k nearest neighbor classifier knn 	positive or negative of the one that has a greater number than the other 
1	108433	8433	to determine the optimal value of k we use a cross validation set and plot its accuracy with respect to the value of k from 1 to 10 as shown below based on the figure shown above the optimal k value in this case is 2 .positive or negative of the one that has a greater number than the other .last sentence.k nearest neighbor classifier knn 	to determine the optimal value of k we use a cross validation set and plot its accuracy with respect to the value of k from 1 to 10 as shown below based on the figure shown above the optimal k value in this case is 2 
1	108434	8434	the following training and testing accuracy are the average results from 5 cases with different positive to negative ratio after resampling since bootstrap resampling is a randomized sampling algorithm we run 100 iterations for each model and plot both training and testing accuracy vs numbers of iterations with different positive to negative ratio as tabulated below .first sentence.last sentence.results and discussion	the following training and testing accuracy are the average results from 5 cases with different positive to negative ratio after resampling since bootstrap resampling is a randomized sampling algorithm we run 100 iterations for each model and plot both training and testing accuracy vs numbers of iterations with different positive to negative ratio as tabulated below 
0	108435	8435	algorithm pos neg 0 2 pos neg 1 0.first sentence.last sentence.model	algorithm pos neg 0 2 pos neg 1 0
0	108436	8436	there are a few observations from these plots 1 .first sentence.the training accuracy is relatively stable in terms of number of iterations and only fluctuates about 2 5 for each algorithm .knn	there are a few observations from these plots 1 
1	108437	8437	the training accuracy is relatively stable in terms of number of iterations and only fluctuates about 2 5 for each algorithm .there are a few observations from these plots 1 .2 .knn	the training accuracy is relatively stable in terms of number of iterations and only fluctuates about 2 5 for each algorithm 
1	108438	8438	the accuracy decreases as the size of resampling increases .2 .however since the test set is also unbalanced a very high testing accuracy may not be meaningful as it always tends to predict the result to be negative .knn	the accuracy decreases as the size of resampling increases 
1	108439	8439	however since the test set is also unbalanced a very high testing accuracy may not be meaningful as it always tends to predict the result to be negative .the accuracy decreases as the size of resampling increases .additionally simply resampling the size does not truly add any valuable data point into the data set so the scarcity of positive examples is not improved .knn	however since the test set is also unbalanced a very high testing accuracy may not be meaningful as it always tends to predict the result to be negative 
1	108440	8440	additionally simply resampling the size does not truly add any valuable data point into the data set so the scarcity of positive examples is not improved .however since the test set is also unbalanced a very high testing accuracy may not be meaningful as it always tends to predict the result to be negative .3 .knn	additionally simply resampling the size does not truly add any valuable data point into the data set so the scarcity of positive examples is not improved 
1	108441	8441	the testing accuracies of knn for a positive to negative ratio of 1 0 appear to be a straight line or closely which indicates that knn algorithm tends to make the same predictions for random input of positive examples .3 .this also makes sense because in case of 2 a larger positive example size will make the output of knn more stable as it only uses the nearest 2 neighbors for prediction from all the plots and tables shown above qda seems to be the most accurate one among the 3 models we choose for this project .knn	the testing accuracies of knn for a positive to negative ratio of 1 0 appear to be a straight line or closely which indicates that knn algorithm tends to make the same predictions for random input of positive examples 
1	108442	8442	this also makes sense because in case of 2 a larger positive example size will make the output of knn more stable as it only uses the nearest 2 neighbors for prediction from all the plots and tables shown above qda seems to be the most accurate one among the 3 models we choose for this project .the testing accuracies of knn for a positive to negative ratio of 1 0 appear to be a straight line or closely which indicates that knn algorithm tends to make the same predictions for random input of positive examples .also its accuracy does not decrease too much as we expand the size of our positive class .knn	this also makes sense because in case of 2 a larger positive example size will make the output of knn more stable as it only uses the nearest 2 neighbors for prediction from all the plots and tables shown above qda seems to be the most accurate one among the 3 models we choose for this project 
1	108443	8443	also its accuracy does not decrease too much as we expand the size of our positive class .this also makes sense because in case of 2 a larger positive example size will make the output of knn more stable as it only uses the nearest 2 neighbors for prediction from all the plots and tables shown above qda seems to be the most accurate one among the 3 models we choose for this project .the testing accuracy is even more stable than that of qda always around 80 but it behaves poorly when making predictions on the positive class .knn	also its accuracy does not decrease too much as we expand the size of our positive class 
1	108444	8444	the testing accuracy is even more stable than that of qda always around 80 but it behaves poorly when making predictions on the positive class .also its accuracy does not decrease too much as we expand the size of our positive class .last sentence.knn	the testing accuracy is even more stable than that of qda always around 80 but it behaves poorly when making predictions on the positive class 
1	108445	8445	in this project we pre process the raw data set with bootstrap resampling and implement 3 supervised learning models on the training set .first sentence.during training process we observe that the accuracy decreases as we increase the size of resampling .conclusion and future work	in this project we pre process the raw data set with bootstrap resampling and implement 3 supervised learning models on the training set 
1	108446	8446	during training process we observe that the accuracy decreases as we increase the size of resampling .in this project we pre process the raw data set with bootstrap resampling and implement 3 supervised learning models on the training set .however since the test set is also unbalanced a very high testing accuracy may not be meaningful as it always tends to predict the result to be negative for future work we expect to expand the size of dataset more explicitly to increase the number of positive examples to run tests on other generative models and to implement multi class classification so we may obtain a more meaningful and practical model as we desired thinking about the nature of the problem helps us understand the observations above .conclusion and future work	during training process we observe that the accuracy decreases as we increase the size of resampling 
1	108447	8447	however since the test set is also unbalanced a very high testing accuracy may not be meaningful as it always tends to predict the result to be negative for future work we expect to expand the size of dataset more explicitly to increase the number of positive examples to run tests on other generative models and to implement multi class classification so we may obtain a more meaningful and practical model as we desired thinking about the nature of the problem helps us understand the observations above .during training process we observe that the accuracy decreases as we increase the size of resampling .in a civil engineering perspective seismic performance of structures is highly uncertain and most of them are hard to predict .conclusion and future work	however since the test set is also unbalanced a very high testing accuracy may not be meaningful as it always tends to predict the result to be negative for future work we expect to expand the size of dataset more explicitly to increase the number of positive examples to run tests on other generative models and to implement multi class classification so we may obtain a more meaningful and practical model as we desired thinking about the nature of the problem helps us understand the observations above 
1	108448	8448	in a civil engineering perspective seismic performance of structures is highly uncertain and most of them are hard to predict .however since the test set is also unbalanced a very high testing accuracy may not be meaningful as it always tends to predict the result to be negative for future work we expect to expand the size of dataset more explicitly to increase the number of positive examples to run tests on other generative models and to implement multi class classification so we may obtain a more meaningful and practical model as we desired thinking about the nature of the problem helps us understand the observations above .to view this study in a broader scope it may be observed that there are usually lots of different constraints on features data size physical meanings of results etc .conclusion and future work	in a civil engineering perspective seismic performance of structures is highly uncertain and most of them are hard to predict 
1	108449	8449	to view this study in a broader scope it may be observed that there are usually lots of different constraints on features data size physical meanings of results etc .in a civil engineering perspective seismic performance of structures is highly uncertain and most of them are hard to predict .in civil engineering scenarios which may impact the practicality of machine learning in such kind of studies .conclusion and future work	to view this study in a broader scope it may be observed that there are usually lots of different constraints on features data size physical meanings of results etc 
1	108450	8450	in civil engineering scenarios which may impact the practicality of machine learning in such kind of studies .to view this study in a broader scope it may be observed that there are usually lots of different constraints on features data size physical meanings of results etc .last sentence.conclusion and future work	in civil engineering scenarios which may impact the practicality of machine learning in such kind of studies 
0	108451	8451	xiao and ziyang came up with the topic and scope of the project together .first sentence.we both engaged in data collection and feature selection .contributions	xiao and ziyang came up with the topic and scope of the project together 
0	108452	8452	we both engaged in data collection and feature selection .xiao and ziyang came up with the topic and scope of the project together .ziyang mainly ran the algorithms to train the model and test the model .contributions	we both engaged in data collection and feature selection 
0	108453	8453	ziyang mainly ran the algorithms to train the model and test the model .we both engaged in data collection and feature selection .xiao and ziyang interpreted the results and made observations together .contributions	ziyang mainly ran the algorithms to train the model and test the model 
0	108454	8454	xiao and ziyang interpreted the results and made observations together .ziyang mainly ran the algorithms to train the model and test the model .ziyang prepared the proposal and the milestone whereas xiao made much of the poster and the final report .contributions	xiao and ziyang interpreted the results and made observations together 
0	108455	8455	ziyang prepared the proposal and the milestone whereas xiao made much of the poster and the final report .xiao and ziyang interpreted the results and made observations together .last sentence.contributions	ziyang prepared the proposal and the milestone whereas xiao made much of the poster and the final report 
0	108456	8456	the link to the github repository containing all the datasets codes and output are given below https github com jzy95310 cs229 fall 2018 final report.first sentence.last sentence.github repository	the link to the github repository containing all the datasets codes and output are given below https github com jzy95310 cs229 fall 2018 final report
1	108457	8457	we are motivated by the opportunity of using machine learning on novel vr based mental healthcare datasets to improve understanding and diagnosis of mood and emotional disorders .first sentence.recently researchers have established a relationship between degree frequency and type of head movement and the degree of clinical depression experienced by an individual we are given the time series data gathered from oculus sensors for 148 engage study participants .introduction	we are motivated by the opportunity of using machine learning on novel vr based mental healthcare datasets to improve understanding and diagnosis of mood and emotional disorders 
1	108458	8458	recently researchers have established a relationship between degree frequency and type of head movement and the degree of clinical depression experienced by an individual we are given the time series data gathered from oculus sensors for 148 engage study participants .we are motivated by the opportunity of using machine learning on novel vr based mental healthcare datasets to improve understanding and diagnosis of mood and emotional disorders .the time series represent two channels of yaw pitch and roll values of the participant s head location .introduction	recently researchers have established a relationship between degree frequency and type of head movement and the degree of clinical depression experienced by an individual we are given the time series data gathered from oculus sensors for 148 engage study participants 
0	108459	8459	the time series represent two channels of yaw pitch and roll values of the participant s head location .recently researchers have established a relationship between degree frequency and type of head movement and the degree of clinical depression experienced by an individual we are given the time series data gathered from oculus sensors for 148 engage study participants .this is our input data and our output labels are binary true if the participant s gad7 a score for level of anxiety value is above 10 false otherwise .introduction	the time series represent two channels of yaw pitch and roll values of the participant s head location 
0	108460	8460	this is our input data and our output labels are binary true if the participant s gad7 a score for level of anxiety value is above 10 false otherwise .the time series represent two channels of yaw pitch and roll values of the participant s head location .we featurize this data in two ways 1 summary statistics across time and 2 a 30 point discrete fourier transform .introduction	this is our input data and our output labels are binary true if the participant s gad7 a score for level of anxiety value is above 10 false otherwise 
1	108461	8461	we featurize this data in two ways 1 summary statistics across time and 2 a 30 point discrete fourier transform .this is our input data and our output labels are binary true if the participant s gad7 a score for level of anxiety value is above 10 false otherwise .we feed both of these input featurizations to three different classifiers logistic regression naive bayes and decision tree .introduction	we featurize this data in two ways 1 summary statistics across time and 2 a 30 point discrete fourier transform 
1	108462	8462	we feed both of these input featurizations to three different classifiers logistic regression naive bayes and decision tree .we featurize this data in two ways 1 summary statistics across time and 2 a 30 point discrete fourier transform .so in total there are six classification nodes .introduction	we feed both of these input featurizations to three different classifiers logistic regression naive bayes and decision tree 
0	108463	8463	so in total there are six classification nodes .we feed both of these input featurizations to three different classifiers logistic regression naive bayes and decision tree .the outputs of these 6 classifiers are fed into an ensembled learned weights voting node and the output of that is our final prediction .introduction	so in total there are six classification nodes 
1	108464	8464	the outputs of these 6 classifiers are fed into an ensembled learned weights voting node and the output of that is our final prediction .so in total there are six classification nodes .we compare the efficacy of this approach for several weighting threshold schemes against a convolutional neural network using the unfeaturized time series as input and also against predict 1 and predict random baselines finding our best model to improve upon baselines .introduction	the outputs of these 6 classifiers are fed into an ensembled learned weights voting node and the output of that is our final prediction 
1	108465	8465	we compare the efficacy of this approach for several weighting threshold schemes against a convolutional neural network using the unfeaturized time series as input and also against predict 1 and predict random baselines finding our best model to improve upon baselines .the outputs of these 6 classifiers are fed into an ensembled learned weights voting node and the output of that is our final prediction .last sentence.introduction	we compare the efficacy of this approach for several weighting threshold schemes against a convolutional neural network using the unfeaturized time series as input and also against predict 1 and predict random baselines finding our best model to improve upon baselines 
1	108466	8466	to our knowledge this is the first time the task of psychological disorder prediction using machine learning has been explored for time series head movement datasets gathered from virtual reality experiences .first sentence.that said we drew inspiration from discussion with members of the panlab and chose machine learning methods based on their success with similar tasks .related work	to our knowledge this is the first time the task of psychological disorder prediction using machine learning has been explored for time series head movement datasets gathered from virtual reality experiences 
1	108467	8467	that said we drew inspiration from discussion with members of the panlab and chose machine learning methods based on their success with similar tasks .to our knowledge this is the first time the task of psychological disorder prediction using machine learning has been explored for time series head movement datasets gathered from virtual reality experiences .we chose to use a convolutional neural network in hope that it captures patterns that hand crafted features do not .related work	that said we drew inspiration from discussion with members of the panlab and chose machine learning methods based on their success with similar tasks 
1	108468	8468	we chose to use a convolutional neural network in hope that it captures patterns that hand crafted features do not .that said we drew inspiration from discussion with members of the panlab and chose machine learning methods based on their success with similar tasks .a study done by hoppe and bulling proposes a convolutional neural network for learning featurizations for classification tasks on eye movement data.related work	we chose to use a convolutional neural network in hope that it captures patterns that hand crafted features do not 
1	108469	8469	a study done by hoppe and bulling proposes a convolutional neural network for learning featurizations for classification tasks on eye movement data.we chose to use a convolutional neural network in hope that it captures patterns that hand crafted features do not .last sentence.related work	a study done by hoppe and bulling proposes a convolutional neural network for learning featurizations for classification tasks on eye movement data
1	108470	8470	the data we have available to us is head angle data recorded over time by two sensors of an oculus vr .first sentence.the data is recorded at 0 2 6 and 12 month sessions and there are five types of virtual reality experiences during each session .dataset and featurization	the data we have available to us is head angle data recorded over time by two sensors of an oculus vr 
0	108471	8471	the data is recorded at 0 2 6 and 12 month sessions and there are five types of virtual reality experiences during each session .the data we have available to us is head angle data recorded over time by two sensors of an oculus vr .an example of the time series yaw pitch roll data for a single vr experience can be seen in in our experiments we attempt to predict the gad7 score an indicator of anxiety based on the feature vector discussed above .dataset and featurization	the data is recorded at 0 2 6 and 12 month sessions and there are five types of virtual reality experiences during each session 
1	108472	8472	an example of the time series yaw pitch roll data for a single vr experience can be seen in in our experiments we attempt to predict the gad7 score an indicator of anxiety based on the feature vector discussed above .the data is recorded at 0 2 6 and 12 month sessions and there are five types of virtual reality experiences during each session .computing these features requires that we have the associated score labels and the tracking data for each experiment type for a given participant and month v gad7 148 is the number of pairs of participant month for which we have the required data .dataset and featurization	an example of the time series yaw pitch roll data for a single vr experience can be seen in in our experiments we attempt to predict the gad7 score an indicator of anxiety based on the feature vector discussed above 
1	108473	8473	computing these features requires that we have the associated score labels and the tracking data for each experiment type for a given participant and month v gad7 148 is the number of pairs of participant month for which we have the required data .an example of the time series yaw pitch roll data for a single vr experience can be seen in in our experiments we attempt to predict the gad7 score an indicator of anxiety based on the feature vector discussed above .because of the small number of examples we use a hold one out cross validation scheme and for testing we set aside 30 test examples .dataset and featurization	computing these features requires that we have the associated score labels and the tracking data for each experiment type for a given participant and month v gad7 148 is the number of pairs of participant month for which we have the required data 
1	108474	8474	because of the small number of examples we use a hold one out cross validation scheme and for testing we set aside 30 test examples .computing these features requires that we have the associated score labels and the tracking data for each experiment type for a given participant and month v gad7 148 is the number of pairs of participant month for which we have the required data .this leaves us with a training matrix x gad7 and a label matrix y gad7 whose rows correspond respectively to the feature and label for all p m leftover for training .dataset and featurization	because of the small number of examples we use a hold one out cross validation scheme and for testing we set aside 30 test examples 
1	108475	8475	this leaves us with a training matrix x gad7 and a label matrix y gad7 whose rows correspond respectively to the feature and label for all p m leftover for training .because of the small number of examples we use a hold one out cross validation scheme and for testing we set aside 30 test examples .in our experiments x gad7 has 118 observations and 120 summary statistics features or 360 frequency domain features depending on the featurization scheme used .dataset and featurization	this leaves us with a training matrix x gad7 and a label matrix y gad7 whose rows correspond respectively to the feature and label for all p m leftover for training 
1	108476	8476	in our experiments x gad7 has 118 observations and 120 summary statistics features or 360 frequency domain features depending on the featurization scheme used .this leaves us with a training matrix x gad7 and a label matrix y gad7 whose rows correspond respectively to the feature and label for all p m leftover for training .last sentence.dataset and featurization	in our experiments x gad7 has 118 observations and 120 summary statistics features or 360 frequency domain features depending on the featurization scheme used 
1	108477	8477	we compute summary statistics on both the time series and the differences between subsequent elements of the time series as our features each piece of head tracking data is a matrix t whose columns are the roll pitch yaw gathered by sensor 1 concatenated with that of sensor 2 .first sentence.now we compute a difference matrix then for every column of t we compute the mean and variance and every column of d we compute the sum and variance .summary statistics	we compute summary statistics on both the time series and the differences between subsequent elements of the time series as our features each piece of head tracking data is a matrix t whose columns are the roll pitch yaw gathered by sensor 1 concatenated with that of sensor 2 
1	108478	8478	now we compute a difference matrix then for every column of t we compute the mean and variance and every column of d we compute the sum and variance .we compute summary statistics on both the time series and the differences between subsequent elements of the time series as our features each piece of head tracking data is a matrix t whose columns are the roll pitch yaw gathered by sensor 1 concatenated with that of sensor 2 .we concatenate these statistics across all experience types to form a feature vector for each participant month pair .summary statistics	now we compute a difference matrix then for every column of t we compute the mean and variance and every column of d we compute the sum and variance 
1	108479	8479	we concatenate these statistics across all experience types to form a feature vector for each participant month pair .now we compute a difference matrix then for every column of t we compute the mean and variance and every column of d we compute the sum and variance .last sentence.summary statistics	we concatenate these statistics across all experience types to form a feature vector for each participant month pair 
0	108480	8480	we use a 30 pt discrete fourier transform dft computed on each time axis for our second featurization .first sentence.the n point dft is defined as follows where x k is called the discrete fourier transform of the sequence x n .discrete fourier transform	we use a 30 pt discrete fourier transform dft computed on each time axis for our second featurization 
0	108481	8481	the n point dft is defined as follows where x k is called the discrete fourier transform of the sequence x n .we use a 30 pt discrete fourier transform dft computed on each time axis for our second featurization .x k can be thought of as periodic with period n or as of length n hence it is called the n point dft and for our purposes is used to compute n features there exists an aptly named algorithm called the fast fourier transform fft which is a computationally efficient implementation of an n point dft .discrete fourier transform	the n point dft is defined as follows where x k is called the discrete fourier transform of the sequence x n 
1	108482	8482	x k can be thought of as periodic with period n or as of length n hence it is called the n point dft and for our purposes is used to compute n features there exists an aptly named algorithm called the fast fourier transform fft which is a computationally efficient implementation of an n point dft .the n point dft is defined as follows where x k is called the discrete fourier transform of the sequence x n .we use numpy s fft implementation to featurize our time series head movement data across each each channel s yaw pitch and roll time axis .discrete fourier transform	x k can be thought of as periodic with period n or as of length n hence it is called the n point dft and for our purposes is used to compute n features there exists an aptly named algorithm called the fast fourier transform fft which is a computationally efficient implementation of an n point dft 
0	108483	8483	we use numpy s fft implementation to featurize our time series head movement data across each each channel s yaw pitch and roll time axis .x k can be thought of as periodic with period n or as of length n hence it is called the n point dft and for our purposes is used to compute n features there exists an aptly named algorithm called the fast fourier transform fft which is a computationally efficient implementation of an n point dft .last sentence.discrete fourier transform	we use numpy s fft implementation to featurize our time series head movement data across each each channel s yaw pitch and roll time axis 
1	108484	8484	the primary challenge of our task beyond uncertainty in featurization due to the novelty of the task is data scarcity .first sentence.we have only 118 participants on which to train our systems and a complex phenomenon to model .methods	the primary challenge of our task beyond uncertainty in featurization due to the novelty of the task is data scarcity 
0	108485	8485	we have only 118 participants on which to train our systems and a complex phenomenon to model .the primary challenge of our task beyond uncertainty in featurization due to the novelty of the task is data scarcity .we pursue three broad avenues to tackle these challenges 1 simple classifiers with different inductive biases and trained off of each of our two featurizations 2 ensembles of simple classifiers with vote weighting determined through random search and 3 a small convolutional neural network with pooling across time simple models we train multinomial naive bayes classifiers decision trees and logistic regression classifiers on each of our 2 featurization types for a total of 6 simple classifiers .methods	we have only 118 participants on which to train our systems and a complex phenomenon to model 
1	108486	8486	we pursue three broad avenues to tackle these challenges 1 simple classifiers with different inductive biases and trained off of each of our two featurizations 2 ensembles of simple classifiers with vote weighting determined through random search and 3 a small convolutional neural network with pooling across time simple models we train multinomial naive bayes classifiers decision trees and logistic regression classifiers on each of our 2 featurization types for a total of 6 simple classifiers .we have only 118 participants on which to train our systems and a complex phenomenon to model .in short the naive bayes models each observation as having been generated by sampling the class and then sampling all features independently given the class the decision tree iteratively splits the data to minimize the gini loss based on individual parameter differences and the logisitic regression learns a linear combination of the features to minimize the difference between the true and predicted probabilities of anxiety disorder for each patient .methods	we pursue three broad avenues to tackle these challenges 1 simple classifiers with different inductive biases and trained off of each of our two featurizations 2 ensembles of simple classifiers with vote weighting determined through random search and 3 a small convolutional neural network with pooling across time simple models we train multinomial naive bayes classifiers decision trees and logistic regression classifiers on each of our 2 featurization types for a total of 6 simple classifiers 
1	108487	8487	in short the naive bayes models each observation as having been generated by sampling the class and then sampling all features independently given the class the decision tree iteratively splits the data to minimize the gini loss based on individual parameter differences and the logisitic regression learns a linear combination of the features to minimize the difference between the true and predicted probabilities of anxiety disorder for each patient .we pursue three broad avenues to tackle these challenges 1 simple classifiers with different inductive biases and trained off of each of our two featurizations 2 ensembles of simple classifiers with vote weighting determined through random search and 3 a small convolutional neural network with pooling across time simple models we train multinomial naive bayes classifiers decision trees and logistic regression classifiers on each of our 2 featurization types for a total of 6 simple classifiers .we ran manual search in development deciding to use a max tree depth of 5 and a regularization precision of 1 for our logistic regression ensemble weighting through random search because of the high variance nature of the data and the different inductive biases in each of our featurizations and simple model architectures we hypothesized that an ensemble of models may improve over the performance of any individual model .methods	in short the naive bayes models each observation as having been generated by sampling the class and then sampling all features independently given the class the decision tree iteratively splits the data to minimize the gini loss based on individual parameter differences and the logisitic regression learns a linear combination of the features to minimize the difference between the true and predicted probabilities of anxiety disorder for each patient 
1	108488	8488	we ran manual search in development deciding to use a max tree depth of 5 and a regularization precision of 1 for our logistic regression ensemble weighting through random search because of the high variance nature of the data and the different inductive biases in each of our featurizations and simple model architectures we hypothesized that an ensemble of models may improve over the performance of any individual model .in short the naive bayes models each observation as having been generated by sampling the class and then sampling all features independently given the class the decision tree iteratively splits the data to minimize the gini loss based on individual parameter differences and the logisitic regression learns a linear combination of the features to minimize the difference between the true and predicted probabilities of anxiety disorder for each patient .we start with a simple majority vote ensembling baseline .methods	we ran manual search in development deciding to use a max tree depth of 5 and a regularization precision of 1 for our logistic regression ensemble weighting through random search because of the high variance nature of the data and the different inductive biases in each of our featurizations and simple model architectures we hypothesized that an ensemble of models may improve over the performance of any individual model 
0	108489	8489	we start with a simple majority vote ensembling baseline .we ran manual search in development deciding to use a max tree depth of 5 and a regularization precision of 1 for our logistic regression ensemble weighting through random search because of the high variance nature of the data and the different inductive biases in each of our featurizations and simple model architectures we hypothesized that an ensemble of models may improve over the performance of any individual model .to leverage the intuition that 1 the individual simple models are not of the same quality and 2 the tradeoff between recall and precision may be controlled through the voting threshold for predicting anxiety we run a random search on our development set to find high quality model weights and decision thresholds for precision recall and f1 specifically for each of the 6 simple models we draw a value from a gamma distribution with parameters shape 2 scale 1 and then normalize the weights by the sum of all .methods	we start with a simple majority vote ensembling baseline 
1	108490	8490	to leverage the intuition that 1 the individual simple models are not of the same quality and 2 the tradeoff between recall and precision may be controlled through the voting threshold for predicting anxiety we run a random search on our development set to find high quality model weights and decision thresholds for precision recall and f1 specifically for each of the 6 simple models we draw a value from a gamma distribution with parameters shape 2 scale 1 and then normalize the weights by the sum of all .we start with a simple majority vote ensembling baseline .we chose this distribution because it should give some variation between model weights without deviation in extremes .methods	to leverage the intuition that 1 the individual simple models are not of the same quality and 2 the tradeoff between recall and precision may be controlled through the voting threshold for predicting anxiety we run a random search on our development set to find high quality model weights and decision thresholds for precision recall and f1 specifically for each of the 6 simple models we draw a value from a gamma distribution with parameters shape 2 scale 1 and then normalize the weights by the sum of all 
1	108491	8491	we chose this distribution because it should give some variation between model weights without deviation in extremes .to leverage the intuition that 1 the individual simple models are not of the same quality and 2 the tradeoff between recall and precision may be controlled through the voting threshold for predicting anxiety we run a random search on our development set to find high quality model weights and decision thresholds for precision recall and f1 specifically for each of the 6 simple models we draw a value from a gamma distribution with parameters shape 2 scale 1 and then normalize the weights by the sum of all .for the threshold we sampled from a uniform between 4 and 6 as we found that sampling from a greater range led to degenerate results like extremely high recall by predicting all participants to have high anxiety .methods	we chose this distribution because it should give some variation between model weights without deviation in extremes 
1	108492	8492	for the threshold we sampled from a uniform between 4 and 6 as we found that sampling from a greater range led to degenerate results like extremely high recall by predicting all participants to have high anxiety .we chose this distribution because it should give some variation between model weights without deviation in extremes .for each sampled set of hyperparameters we ran all models 5 times using hold one out evaluation and averaged the scores .methods	for the threshold we sampled from a uniform between 4 and 6 as we found that sampling from a greater range led to degenerate results like extremely high recall by predicting all participants to have high anxiety 
1	108493	8493	for each sampled set of hyperparameters we ran all models 5 times using hold one out evaluation and averaged the scores .for the threshold we sampled from a uniform between 4 and 6 as we found that sampling from a greater range led to degenerate results like extremely high recall by predicting all participants to have high anxiety .we then picked the set of hyperparameters for each of f1 precision and recall that led to the best development result to run on the test set .methods	for each sampled set of hyperparameters we ran all models 5 times using hold one out evaluation and averaged the scores 
1	108494	8494	we then picked the set of hyperparameters for each of f1 precision and recall that led to the best development result to run on the test set .for each sampled set of hyperparameters we ran all models 5 times using hold one out evaluation and averaged the scores .this model is visualized in.methods	we then picked the set of hyperparameters for each of f1 precision and recall that led to the best development result to run on the test set 
0	108495	8495	this model is visualized in.we then picked the set of hyperparameters for each of f1 precision and recall that led to the best development result to run on the test set .last sentence.methods	this model is visualized in
1	108496	8496	for our second model we consider a small 1 dimensional convolutional neural network that uses the unfeaturized raw head movement data across the 6 channels of roll pitch yaw for both the right and left sensors .first sentence.we felt that a 1d cnn model was well suited for our data given that we were working with time series data where the exact time of head movement may not be as important as the amount or speed of movement in short intervals for predicting anxiety levels .1 dimensional convolutional neural network	for our second model we consider a small 1 dimensional convolutional neural network that uses the unfeaturized raw head movement data across the 6 channels of roll pitch yaw for both the right and left sensors 
1	108497	8497	we felt that a 1d cnn model was well suited for our data given that we were working with time series data where the exact time of head movement may not be as important as the amount or speed of movement in short intervals for predicting anxiety levels .for our second model we consider a small 1 dimensional convolutional neural network that uses the unfeaturized raw head movement data across the 6 channels of roll pitch yaw for both the right and left sensors .our cnn architecture consists of five layers .1 dimensional convolutional neural network	we felt that a 1d cnn model was well suited for our data given that we were working with time series data where the exact time of head movement may not be as important as the amount or speed of movement in short intervals for predicting anxiety levels 
0	108498	8498	our cnn architecture consists of five layers .we felt that a 1d cnn model was well suited for our data given that we were working with time series data where the exact time of head movement may not be as important as the amount or speed of movement in short intervals for predicting anxiety levels .the first layer is a 1 dimensional convolutional layer followed by an average pooling layer a relu activation layer and two dense layers that also have relu activations .1 dimensional convolutional neural network	our cnn architecture consists of five layers 
1	108499	8499	the first layer is a 1 dimensional convolutional layer followed by an average pooling layer a relu activation layer and two dense layers that also have relu activations .our cnn architecture consists of five layers .we included dropout as a form of regularization as well and chose the dropout rate during our hyperparameter search on the development set .1 dimensional convolutional neural network	the first layer is a 1 dimensional convolutional layer followed by an average pooling layer a relu activation layer and two dense layers that also have relu activations 
1	108500	8500	we included dropout as a form of regularization as well and chose the dropout rate during our hyperparameter search on the development set .the first layer is a 1 dimensional convolutional layer followed by an average pooling layer a relu activation layer and two dense layers that also have relu activations .this model s layout is visualized in.1 dimensional convolutional neural network	we included dropout as a form of regularization as well and chose the dropout rate during our hyperparameter search on the development set 
0	108501	8501	this model s layout is visualized in.we included dropout as a form of regularization as well and chose the dropout rate during our hyperparameter search on the development set .last sentence.1 dimensional convolutional neural network	this model s layout is visualized in
1	108502	8502	because the split of high anxiety to low anxiety participants was roughly 20 to 80 we report precision recall and f1 score across all experiments .first sentence.last sentence.experiments results	because the split of high anxiety to low anxiety participants was roughly 20 to 80 we report precision recall and f1 score across all experiments 
1	108503	8503	in our random search we sampled 50 ensemble configurations .first sentence.a few interesting patterns emerged .ensemble weighting through random search	in our random search we sampled 50 ensemble configurations 
0	108504	8504	a few interesting patterns emerged .in our random search we sampled 50 ensemble configurations .the top recall ensemble simply chose to ignore all models but the naive bayes on summary statistic features .ensemble weighting through random search	a few interesting patterns emerged 
1	108505	8505	the top recall ensemble simply chose to ignore all models but the naive bayes on summary statistic features .a few interesting patterns emerged .the top precision ensemble as might be expected used the highest threshold of the three at 60 of voting weight required to predict high anxiety .ensemble weighting through random search	the top recall ensemble simply chose to ignore all models but the naive bayes on summary statistic features 
1	108506	8506	the top precision ensemble as might be expected used the highest threshold of the three at 60 of voting weight required to predict high anxiety .the top recall ensemble simply chose to ignore all models but the naive bayes on summary statistic features .the top f1 ensemble assigned almost all its weight equally across the summary statistic models .ensemble weighting through random search	the top precision ensemble as might be expected used the highest threshold of the three at 60 of voting weight required to predict high anxiety 
1	108507	8507	the top f1 ensemble assigned almost all its weight equally across the summary statistic models .the top precision ensemble as might be expected used the highest threshold of the three at 60 of voting weight required to predict high anxiety .last sentence.ensemble weighting through random search	the top f1 ensemble assigned almost all its weight equally across the summary statistic models 
1	108508	8508	a convolutional neural network layer passes the same feature detector across all spatial steps of the data and in our case uses a pooling function to aggregate features across all timesteps .first sentence.for our cnn we found that the set of hyperparameters that resulted in the highest f1 score on the development set was a filter count of 16 a kernel size of 10 and a dropout rate of 0 5 and ultimately used this choice of values for our final model .1d convolutional neural network hyperparameter tuning	a convolutional neural network layer passes the same feature detector across all spatial steps of the data and in our case uses a pooling function to aggregate features across all timesteps 
1	108509	8509	for our cnn we found that the set of hyperparameters that resulted in the highest f1 score on the development set was a filter count of 16 a kernel size of 10 and a dropout rate of 0 5 and ultimately used this choice of values for our final model .a convolutional neural network layer passes the same feature detector across all spatial steps of the data and in our case uses a pooling function to aggregate features across all timesteps .we report numbers on cnns trained only the first 400 timesteps of the data as the results were robust to the number of timesteps used .1d convolutional neural network hyperparameter tuning	for our cnn we found that the set of hyperparameters that resulted in the highest f1 score on the development set was a filter count of 16 a kernel size of 10 and a dropout rate of 0 5 and ultimately used this choice of values for our final model 
1	108510	8510	we report numbers on cnns trained only the first 400 timesteps of the data as the results were robust to the number of timesteps used .for our cnn we found that the set of hyperparameters that resulted in the highest f1 score on the development set was a filter count of 16 a kernel size of 10 and a dropout rate of 0 5 and ultimately used this choice of values for our final model .one interesting takeaway from our hyperparameter search was that smaller models tended to do best reflecting small data problem but our smallest models started to degrade as well perhaps signaling limitations of raw head movement feature format with so little data .1d convolutional neural network hyperparameter tuning	we report numbers on cnns trained only the first 400 timesteps of the data as the results were robust to the number of timesteps used 
1	108511	8511	one interesting takeaway from our hyperparameter search was that smaller models tended to do best reflecting small data problem but our smallest models started to degrade as well perhaps signaling limitations of raw head movement feature format with so little data .we report numbers on cnns trained only the first 400 timesteps of the data as the results were robust to the number of timesteps used .last sentence.1d convolutional neural network hyperparameter tuning	one interesting takeaway from our hyperparameter search was that smaller models tended to do best reflecting small data problem but our smallest models started to degrade as well perhaps signaling limitations of raw head movement feature format with so little data 
1	108512	8512	our results are summarized in our ensembles underperform our expectations perhaps due to the high variation in model quality and the bias of our random search to being close to uniform .first sentence.however while they do not in general outperform the individual models we do see the desired behavior of each weighted ensemble f1 precision recall tending to bias towards that metric while not sacrificing too much on the other metrics .test set results	our results are summarized in our ensembles underperform our expectations perhaps due to the high variation in model quality and the bias of our random search to being close to uniform 
1	108513	8513	however while they do not in general outperform the individual models we do see the desired behavior of each weighted ensemble f1 precision recall tending to bias towards that metric while not sacrificing too much on the other metrics .our results are summarized in our ensembles underperform our expectations perhaps due to the high variation in model quality and the bias of our random search to being close to uniform .finally our cnn model underperforms all baselines despite hyperparameter tuning to attempt to avoid overfitting .test set results	however while they do not in general outperform the individual models we do see the desired behavior of each weighted ensemble f1 precision recall tending to bias towards that metric while not sacrificing too much on the other metrics 
1	108514	8514	finally our cnn model underperforms all baselines despite hyperparameter tuning to attempt to avoid overfitting .however while they do not in general outperform the individual models we do see the desired behavior of each weighted ensemble f1 precision recall tending to bias towards that metric while not sacrificing too much on the other metrics .we expect that this is because the small amount of data and somewhat abstract problem make joint feature detection and generalization infeasible finally we conducted qualitative error exploration on our development set .test set results	finally our cnn model underperforms all baselines despite hyperparameter tuning to attempt to avoid overfitting 
1	108515	8515	we expect that this is because the small amount of data and somewhat abstract problem make joint feature detection and generalization infeasible finally we conducted qualitative error exploration on our development set .finally our cnn model underperforms all baselines despite hyperparameter tuning to attempt to avoid overfitting .we sampled two highanxiety participants one of which our naive bayes classified correctly and the other incorrectly .test set results	we expect that this is because the small amount of data and somewhat abstract problem make joint feature detection and generalization infeasible finally we conducted qualitative error exploration on our development set 
0	108516	8516	we sampled two highanxiety participants one of which our naive bayes classified correctly and the other incorrectly .we expect that this is because the small amount of data and somewhat abstract problem make joint feature detection and generalization infeasible finally we conducted qualitative error exploration on our development set .qualitatively the examples seem quite similar in the amount of movement as seen in.test set results	we sampled two highanxiety participants one of which our naive bayes classified correctly and the other incorrectly 
0	108517	8517	qualitatively the examples seem quite similar in the amount of movement as seen in.we sampled two highanxiety participants one of which our naive bayes classified correctly and the other incorrectly .last sentence.test set results	qualitatively the examples seem quite similar in the amount of movement as seen in
1	108518	8518	in this work we explored head movements a noisy signal in the medical domain which we confirm to be useful for predicting patient anxiety disorder .first sentence.we faced an inherently small data problem since controlled participation in a vr experience is costly to collect .discussion conclusion	in this work we explored head movements a noisy signal in the medical domain which we confirm to be useful for predicting patient anxiety disorder 
1	108519	8519	we faced an inherently small data problem since controlled participation in a vr experience is costly to collect .in this work we explored head movements a noisy signal in the medical domain which we confirm to be useful for predicting patient anxiety disorder .as such we focused on featurization and model comparison to determine what features and methods are promising for evaluating anxiety through head movement .discussion conclusion	we faced an inherently small data problem since controlled participation in a vr experience is costly to collect 
1	108520	8520	as such we focused on featurization and model comparison to determine what features and methods are promising for evaluating anxiety through head movement .we faced an inherently small data problem since controlled participation in a vr experience is costly to collect .our best model improved on an informationless baseline by 10 2 points f1 a modest but potentially useful result when combined with other predictors of anxiety in a hypothetical future system .discussion conclusion	as such we focused on featurization and model comparison to determine what features and methods are promising for evaluating anxiety through head movement 
1	108521	8521	our best model improved on an informationless baseline by 10 2 points f1 a modest but potentially useful result when combined with other predictors of anxiety in a hypothetical future system .as such we focused on featurization and model comparison to determine what features and methods are promising for evaluating anxiety through head movement .by ensembling models and running a random search on the ensemble voting weights and decision threshold we were able to control the tradeoff between precision and recall but not improve upon the f1 score of individual models a mixed result .discussion conclusion	our best model improved on an informationless baseline by 10 2 points f1 a modest but potentially useful result when combined with other predictors of anxiety in a hypothetical future system 
1	108522	8522	by ensembling models and running a random search on the ensemble voting weights and decision threshold we were able to control the tradeoff between precision and recall but not improve upon the f1 score of individual models a mixed result .our best model improved on an informationless baseline by 10 2 points f1 a modest but potentially useful result when combined with other predictors of anxiety in a hypothetical future system .for thoroughness we compared our featurizations and simple models to a low parameter cnn finding as we expected that the cnn underperformed models with hand crafted features .discussion conclusion	by ensembling models and running a random search on the ensemble voting weights and decision threshold we were able to control the tradeoff between precision and recall but not improve upon the f1 score of individual models a mixed result 
1	108523	8523	for thoroughness we compared our featurizations and simple models to a low parameter cnn finding as we expected that the cnn underperformed models with hand crafted features .by ensembling models and running a random search on the ensemble voting weights and decision threshold we were able to control the tradeoff between precision and recall but not improve upon the f1 score of individual models a mixed result .we hypothesize this was due to the rather small data setting .discussion conclusion	for thoroughness we compared our featurizations and simple models to a low parameter cnn finding as we expected that the cnn underperformed models with hand crafted features 
0	108524	8524	we hypothesize this was due to the rather small data setting .for thoroughness we compared our featurizations and simple models to a low parameter cnn finding as we expected that the cnn underperformed models with hand crafted features .our findings suggest that head movement data has signal for predicting anxiety disorder and suggest that future work may leverage richer representations of each patient in combination with head tracking to improve predictiveness and eventually improve professionals ability to care for patients .discussion conclusion	we hypothesize this was due to the rather small data setting 
1	108525	8525	our findings suggest that head movement data has signal for predicting anxiety disorder and suggest that future work may leverage richer representations of each patient in combination with head tracking to improve predictiveness and eventually improve professionals ability to care for patients .we hypothesize this was due to the rather small data setting .last sentence.discussion conclusion	our findings suggest that head movement data has signal for predicting anxiety disorder and suggest that future work may leverage richer representations of each patient in combination with head tracking to improve predictiveness and eventually improve professionals ability to care for patients 
1	108526	8526	in this paper we explore the applications of machine learning to sports betting by focusing on predicting the number of total points scored by both teams in an nba game .first sentence.we use neural networks as well as recurrent models for this task and manage to achieve results that are similar to those of the sports books .abstract	in this paper we explore the applications of machine learning to sports betting by focusing on predicting the number of total points scored by both teams in an nba game 
1	108527	8527	we use neural networks as well as recurrent models for this task and manage to achieve results that are similar to those of the sports books .in this paper we explore the applications of machine learning to sports betting by focusing on predicting the number of total points scored by both teams in an nba game .on average our best models can beat the house 51 5 of the time .abstract	we use neural networks as well as recurrent models for this task and manage to achieve results that are similar to those of the sports books 
0	108528	8528	on average our best models can beat the house 51 5 of the time .we use neural networks as well as recurrent models for this task and manage to achieve results that are similar to those of the sports books .last sentence.abstract	on average our best models can beat the house 51 5 of the time 
0	108529	8529	last may the united states supreme court legalized sports betting .first sentence.while it is still up to every state to pass legislation on the issues many have already been working on bills .introduction	last may the united states supreme court legalized sports betting 
0	108530	8530	while it is still up to every state to pass legislation on the issues many have already been working on bills .last may the united states supreme court legalized sports betting .as tracked by espn eight states have already legalized sports gambling with two more states having signed bills that will legalize gambling in the near future .introduction	while it is still up to every state to pass legislation on the issues many have already been working on bills 
0	108531	8531	as tracked by espn eight states have already legalized sports gambling with two more states having signed bills that will legalize gambling in the near future .while it is still up to every state to pass legislation on the issues many have already been working on bills .with its ruling the supreme court paved way to the opening of whole new legal and thus taxable market of size 150 billion according to the american gaming association in this project we will explore the applications of machine learning in the field of sports betting using a case study of the national basketball association .introduction	as tracked by espn eight states have already legalized sports gambling with two more states having signed bills that will legalize gambling in the near future 
1	108532	8532	with its ruling the supreme court paved way to the opening of whole new legal and thus taxable market of size 150 billion according to the american gaming association in this project we will explore the applications of machine learning in the field of sports betting using a case study of the national basketball association .as tracked by espn eight states have already legalized sports gambling with two more states having signed bills that will legalize gambling in the near future .more specifically we wish to design sets of models that predict betting indicators for nba matches .introduction	with its ruling the supreme court paved way to the opening of whole new legal and thus taxable market of size 150 billion according to the american gaming association in this project we will explore the applications of machine learning in the field of sports betting using a case study of the national basketball association 
0	108533	8533	more specifically we wish to design sets of models that predict betting indicators for nba matches .with its ruling the supreme court paved way to the opening of whole new legal and thus taxable market of size 150 billion according to the american gaming association in this project we will explore the applications of machine learning in the field of sports betting using a case study of the national basketball association .out of the three main indicators overunder money line and point spread we focus on the over under .introduction	more specifically we wish to design sets of models that predict betting indicators for nba matches 
0	108534	8534	out of the three main indicators overunder money line and point spread we focus on the over under .more specifically we wish to design sets of models that predict betting indicators for nba matches .in other words we predict for every nba game how many combined points the two teams will score .introduction	out of the three main indicators overunder money line and point spread we focus on the over under 
0	108535	8535	in other words we predict for every nba game how many combined points the two teams will score .out of the three main indicators overunder money line and point spread we focus on the over under .ideally we would be able to come up with an estimate for this indicator that is more accurate than that of some betting platforms and use this to our advantage to place bets .introduction	in other words we predict for every nba game how many combined points the two teams will score 
1	108536	8536	ideally we would be able to come up with an estimate for this indicator that is more accurate than that of some betting platforms and use this to our advantage to place bets .in other words we predict for every nba game how many combined points the two teams will score .last sentence.introduction	ideally we would be able to come up with an estimate for this indicator that is more accurate than that of some betting platforms and use this to our advantage to place bets 
0	108537	8537	the paper predicting margin of victory in nfl games the paper football match prediction using deep learning .first sentence.last sentence.related work	the paper predicting margin of victory in nfl games the paper football match prediction using deep learning 
1	108538	8538	the data collected so far can be classified into two groups betting odds data and game data which consists of both data describing team performance and player performance we found odds data on sports book review online a website that compiles betting data for every nba games since the 2007 2008 season .first sentence.the website offers a downloadable excel file for each season .data collection	the data collected so far can be classified into two groups betting odds data and game data which consists of both data describing team performance and player performance we found odds data on sports book review online a website that compiles betting data for every nba games since the 2007 2008 season 
0	108539	8539	the website offers a downloadable excel file for each season .the data collected so far can be classified into two groups betting odds data and game data which consists of both data describing team performance and player performance we found odds data on sports book review online a website that compiles betting data for every nba games since the 2007 2008 season .using a short script we were able to retrieve all the necessary target variables and the corresponding betting odds offered .data collection	the website offers a downloadable excel file for each season 
0	108540	8540	using a short script we were able to retrieve all the necessary target variables and the corresponding betting odds offered .the website offers a downloadable excel file for each season .the betting indicators scraped were the following i over under the total number of points scored in a game ii spread the number of points by which the home team wins or loses iii money line a number encoding the amount of money won from placing a bet on the winning team of a game in the rest of this project we focus on the total number of points scored and we use the over under data collected to solely evaluate our models and do not include the above data in our features for game data we retrieved data from basketball reference using fran goitia s nba crawler.data collection	using a short script we were able to retrieve all the necessary target variables and the corresponding betting odds offered 
1	108541	8541	the betting indicators scraped were the following i over under the total number of points scored in a game ii spread the number of points by which the home team wins or loses iii money line a number encoding the amount of money won from placing a bet on the winning team of a game in the rest of this project we focus on the total number of points scored and we use the over under data collected to solely evaluate our models and do not include the above data in our features for game data we retrieved data from basketball reference using fran goitia s nba crawler.using a short script we were able to retrieve all the necessary target variables and the corresponding betting odds offered .last sentence.data collection	the betting indicators scraped were the following i over under the total number of points scored in a game ii spread the number of points by which the home team wins or loses iii money line a number encoding the amount of money won from placing a bet on the winning team of a game in the rest of this project we focus on the total number of points scored and we use the over under data collected to solely evaluate our models and do not include the above data in our features for game data we retrieved data from basketball reference using fran goitia s nba crawler
0	108542	8542	using the retrieved data from basketball reference we build a featurized dataset .first sentence.for every matchup between two teams we decide to look at both team s past three games .feature building	using the retrieved data from basketball reference we build a featurized dataset 
0	108543	8543	for every matchup between two teams we decide to look at both team s past three games .using the retrieved data from basketball reference we build a featurized dataset .we included simple features such as points scored points scored against or total rebounds and also more complicated metrics such as offensive rating and plus minus .feature building	for every matchup between two teams we decide to look at both team s past three games 
1	108544	8544	we included simple features such as points scored points scored against or total rebounds and also more complicated metrics such as offensive rating and plus minus .for every matchup between two teams we decide to look at both team s past three games .in order to account for opponent strength we also added each opponent s season averages in all above metrics .feature building	we included simple features such as points scored points scored against or total rebounds and also more complicated metrics such as offensive rating and plus minus 
1	108545	8545	in order to account for opponent strength we also added each opponent s season averages in all above metrics .we included simple features such as points scored points scored against or total rebounds and also more complicated metrics such as offensive rating and plus minus .finally to account for player fatigue we added the number of days since the last game as well as the distance traveled see we trained a random forest as a baseline before moving on to more complex models .feature building	in order to account for opponent strength we also added each opponent s season averages in all above metrics 
1	108546	8546	finally to account for player fatigue we added the number of days since the last game as well as the distance traveled see we trained a random forest as a baseline before moving on to more complex models .in order to account for opponent strength we also added each opponent s season averages in all above metrics .last sentence.feature building	finally to account for player fatigue we added the number of days since the last game as well as the distance traveled see we trained a random forest as a baseline before moving on to more complex models 
1	108547	8547	this problem is similar to the user item rating prediction information about the outcome of previous games between other teams can inform our predicted output for the current matchup .first sentence.to leverage this we used singular value decomposition we hope that the rows of matrices u and v capture information about teams at home and away respectively and we then predict u i v t j total points scored in a game between teams i home and j away where b k is the k th row of matrix b .collaborative filtering	this problem is similar to the user item rating prediction information about the outcome of previous games between other teams can inform our predicted output for the current matchup 
0	108548	8548	to leverage this we used singular value decomposition we hope that the rows of matrices u and v capture information about teams at home and away respectively and we then predict u i v t j total points scored in a game between teams i home and j away where b k is the k th row of matrix b .this problem is similar to the user item rating prediction information about the outcome of previous games between other teams can inform our predicted output for the current matchup .last sentence.collaborative filtering	to leverage this we used singular value decomposition we hope that the rows of matrices u and v capture information about teams at home and away respectively and we then predict u i v t j total points scored in a game between teams i home and j away where b k is the k th row of matrix b 
1	108549	8549	like the collaborative filtering model the neural network captures information from the outcomes of previous games between other teams as during training the network is provided the results of previous games as input along with the identity of the two teams involved in the game .first sentence.however the neural network has an advantage over collaborative filtering in that it is also able to take features of both teams involved as inputs .neural network	like the collaborative filtering model the neural network captures information from the outcomes of previous games between other teams as during training the network is provided the results of previous games as input along with the identity of the two teams involved in the game 
0	108550	8550	however the neural network has an advantage over collaborative filtering in that it is also able to take features of both teams involved as inputs .like the collaborative filtering model the neural network captures information from the outcomes of previous games between other teams as during training the network is provided the results of previous games as input along with the identity of the two teams involved in the game .therefore it can draw on not only the outcomes of previous training examples but also the offensive rating of each of the teams involved over their past three games etc passing in the feature set created in 3 2 we train a neural network with fully connected layers and relu activations .neural network	however the neural network has an advantage over collaborative filtering in that it is also able to take features of both teams involved as inputs 
1	108551	8551	therefore it can draw on not only the outcomes of previous training examples but also the offensive rating of each of the teams involved over their past three games etc passing in the feature set created in 3 2 we train a neural network with fully connected layers and relu activations .however the neural network has an advantage over collaborative filtering in that it is also able to take features of both teams involved as inputs .the input data is flattened into a size of 1524 features .neural network	therefore it can draw on not only the outcomes of previous training examples but also the offensive rating of each of the teams involved over their past three games etc passing in the feature set created in 3 2 we train a neural network with fully connected layers and relu activations 
0	108552	8552	the input data is flattened into a size of 1524 features .therefore it can draw on not only the outcomes of previous training examples but also the offensive rating of each of the teams involved over their past three games etc passing in the feature set created in 3 2 we train a neural network with fully connected layers and relu activations .the relu activations are used for ease of training and to reduce the likelihood of gradient vanishing see.neural network	the input data is flattened into a size of 1524 features 
0	108553	8553	the relu activations are used for ease of training and to reduce the likelihood of gradient vanishing see.the input data is flattened into a size of 1524 features .last sentence.neural network	the relu activations are used for ease of training and to reduce the likelihood of gradient vanishing see
1	108554	8554	given that games are played sequentially we decided to use an lstm to process the past three games one by one .first sentence.as described in long short term memory for our task we implemented an lstm model with a fully connected layer at the end to output the number of points scored by the two teams for the desired game see.long short term memory network lstm 	given that games are played sequentially we decided to use an lstm to process the past three games one by one 
1	108555	8555	as described in long short term memory for our task we implemented an lstm model with a fully connected layer at the end to output the number of points scored by the two teams for the desired game see.given that games are played sequentially we decided to use an lstm to process the past three games one by one .last sentence.long short term memory network lstm 	as described in long short term memory for our task we implemented an lstm model with a fully connected layer at the end to output the number of points scored by the two teams for the desired game see
1	108556	8556	after tuning our models on the validation set to minimize validation mse we found the following architectures to perform best ii neural network we use four fully connected layers reducing the input of dimension 1524 to size 500 then 100 then 20 then finally a 1 dimensional output that predicts the overunder of the desired game see iii lstm we use one layer with hidden dimension 20 and a fully connected layer at the end see we evaluate the performance of our models primarily through the mean squared error mse between the over under value predicted by the model and the true point total observed in the game .first sentence.the over under values predicted by the sports books can serve as a benchmark as calculating the mse between the sports books predictions and the observed point totals gives us a sense of how well our models are performing relative to the books .results	after tuning our models on the validation set to minimize validation mse we found the following architectures to perform best ii neural network we use four fully connected layers reducing the input of dimension 1524 to size 500 then 100 then 20 then finally a 1 dimensional output that predicts the overunder of the desired game see iii lstm we use one layer with hidden dimension 20 and a fully connected layer at the end see we evaluate the performance of our models primarily through the mean squared error mse between the over under value predicted by the model and the true point total observed in the game 
1	108557	8557	the over under values predicted by the sports books can serve as a benchmark as calculating the mse between the sports books predictions and the observed point totals gives us a sense of how well our models are performing relative to the books .after tuning our models on the validation set to minimize validation mse we found the following architectures to perform best ii neural network we use four fully connected layers reducing the input of dimension 1524 to size 500 then 100 then 20 then finally a 1 dimensional output that predicts the overunder of the desired game see iii lstm we use one layer with hidden dimension 20 and a fully connected layer at the end see we evaluate the performance of our models primarily through the mean squared error mse between the over under value predicted by the model and the true point total observed in the game .in addition we can also calculate the percentage of the time that our model would have correctly predicted that the true point total was either over or under the over under number provided by the sports books we found that the neural network correctly chooses over or under around 51 5 of the time while the collaborative filtering beats the line 51 of the time on average .results	the over under values predicted by the sports books can serve as a benchmark as calculating the mse between the sports books predictions and the observed point totals gives us a sense of how well our models are performing relative to the books 
1	108558	8558	in addition we can also calculate the percentage of the time that our model would have correctly predicted that the true point total was either over or under the over under number provided by the sports books we found that the neural network correctly chooses over or under around 51 5 of the time while the collaborative filtering beats the line 51 of the time on average .the over under values predicted by the sports books can serve as a benchmark as calculating the mse between the sports books predictions and the observed point totals gives us a sense of how well our models are performing relative to the books .last sentence.results	in addition we can also calculate the percentage of the time that our model would have correctly predicted that the true point total was either over or under the over under number provided by the sports books we found that the neural network correctly chooses over or under around 51 5 of the time while the collaborative filtering beats the line 51 of the time on average 
1	108559	8559	due to the rapidly changing nature of the nba it is difficult to acquire sufficient training data that reflects the way the game is currently played .first sentence.this can be seen most prominently in the increasing frequency with which nba teams are attempting and making three point shots .discussion	due to the rapidly changing nature of the nba it is difficult to acquire sufficient training data that reflects the way the game is currently played 
0	108560	8560	this can be seen most prominently in the increasing frequency with which nba teams are attempting and making three point shots .due to the rapidly changing nature of the nba it is difficult to acquire sufficient training data that reflects the way the game is currently played .according to the nba teams around the league combined for around 15 000 three pointers made in the 2009 2010 season and broke the 25 000 mark last year 2017 2018 when we were training the models we found while while we had data available starting from it is worth noting that the collaborative filtering model which doesn t use any team features significantly outperformed the random forest .discussion	this can be seen most prominently in the increasing frequency with which nba teams are attempting and making three point shots 
1	108561	8561	according to the nba teams around the league combined for around 15 000 three pointers made in the 2009 2010 season and broke the 25 000 mark last year 2017 2018 when we were training the models we found while while we had data available starting from it is worth noting that the collaborative filtering model which doesn t use any team features significantly outperformed the random forest .this can be seen most prominently in the increasing frequency with which nba teams are attempting and making three point shots .this shows the high variance in our data as well as the strong seasonal trends that a model needs to encompass in order to be accurate on this task on we achieved a test mse of 369 84 for our best model the neural network .discussion	according to the nba teams around the league combined for around 15 000 three pointers made in the 2009 2010 season and broke the 25 000 mark last year 2017 2018 when we were training the models we found while while we had data available starting from it is worth noting that the collaborative filtering model which doesn t use any team features significantly outperformed the random forest 
1	108562	8562	this shows the high variance in our data as well as the strong seasonal trends that a model needs to encompass in order to be accurate on this task on we achieved a test mse of 369 84 for our best model the neural network .according to the nba teams around the league combined for around 15 000 three pointers made in the 2009 2010 season and broke the 25 000 mark last year 2017 2018 when we were training the models we found while while we had data available starting from it is worth noting that the collaborative filtering model which doesn t use any team features significantly outperformed the random forest .this value is higher than the mse of the sports books predictions which is 320 70 but is relatively close to the level of accuracy of the books .discussion	this shows the high variance in our data as well as the strong seasonal trends that a model needs to encompass in order to be accurate on this task on we achieved a test mse of 369 84 for our best model the neural network 
1	108563	8563	this value is higher than the mse of the sports books predictions which is 320 70 but is relatively close to the level of accuracy of the books .this shows the high variance in our data as well as the strong seasonal trends that a model needs to encompass in order to be accurate on this task on we achieved a test mse of 369 84 for our best model the neural network .when considering that nba teams are scoring roughly a combined 200 points per game mse values of over 300 from even sports books may seem high .discussion	this value is higher than the mse of the sports books predictions which is 320 70 but is relatively close to the level of accuracy of the books 
1	108564	8564	when considering that nba teams are scoring roughly a combined 200 points per game mse values of over 300 from even sports books may seem high .this value is higher than the mse of the sports books predictions which is 320 70 but is relatively close to the level of accuracy of the books .however this is simply a reflection of the high variance nature of nba games where anything from injuries to players to a strong shooting night or the decision of a coach to rest his star players after building a lead can all lead to huge swings in the overall point totals of a game .discussion	when considering that nba teams are scoring roughly a combined 200 points per game mse values of over 300 from even sports books may seem high 
1	108565	8565	however this is simply a reflection of the high variance nature of nba games where anything from injuries to players to a strong shooting night or the decision of a coach to rest his star players after building a lead can all lead to huge swings in the overall point totals of a game .when considering that nba teams are scoring roughly a combined 200 points per game mse values of over 300 from even sports books may seem high .the variance of the data was also reflected in the need for a relatively large weight decay parameter while training the neural network to prevent overfitting encouragingly when placing bets against the sports books on games in the test dataset using the neural network the predictions made were correct with respect to the actual outcomes 51 5 of the time as mentioned earlier .discussion	however this is simply a reflection of the high variance nature of nba games where anything from injuries to players to a strong shooting night or the decision of a coach to rest his star players after building a lead can all lead to huge swings in the overall point totals of a game 
1	108566	8566	the variance of the data was also reflected in the need for a relatively large weight decay parameter while training the neural network to prevent overfitting encouragingly when placing bets against the sports books on games in the test dataset using the neural network the predictions made were correct with respect to the actual outcomes 51 5 of the time as mentioned earlier .however this is simply a reflection of the high variance nature of nba games where anything from injuries to players to a strong shooting night or the decision of a coach to rest his star players after building a lead can all lead to huge swings in the overall point totals of a game .however note that in the real world the fees associated with betting mean that successful long term betting patterns need to be correct at least 52 53 of the time .discussion	the variance of the data was also reflected in the need for a relatively large weight decay parameter while training the neural network to prevent overfitting encouragingly when placing bets against the sports books on games in the test dataset using the neural network the predictions made were correct with respect to the actual outcomes 51 5 of the time as mentioned earlier 
1	108567	8567	however note that in the real world the fees associated with betting mean that successful long term betting patterns need to be correct at least 52 53 of the time .the variance of the data was also reflected in the need for a relatively large weight decay parameter while training the neural network to prevent overfitting encouragingly when placing bets against the sports books on games in the test dataset using the neural network the predictions made were correct with respect to the actual outcomes 51 5 of the time as mentioned earlier .in the future it would be interesting to explore if directly predicting whether the true result of the game is over of under a sports book s over under prediction would lead to higher levels of accuracy in this secondary metric .discussion	however note that in the real world the fees associated with betting mean that successful long term betting patterns need to be correct at least 52 53 of the time 
1	108568	8568	in the future it would be interesting to explore if directly predicting whether the true result of the game is over of under a sports book s over under prediction would lead to higher levels of accuracy in this secondary metric .however note that in the real world the fees associated with betting mean that successful long term betting patterns need to be correct at least 52 53 of the time .last sentence.discussion	in the future it would be interesting to explore if directly predicting whether the true result of the game is over of under a sports book s over under prediction would lead to higher levels of accuracy in this secondary metric 
1	108569	8569	overall we were able to achieve encouraging results using our models as our best predictions rivaled the accuracy of sports books .first sentence.indeed our neural network had a mse of 369 84 compared to the 320 696 mse of sports book .conclusion and future work	overall we were able to achieve encouraging results using our models as our best predictions rivaled the accuracy of sports books 
1	108570	8570	indeed our neural network had a mse of 369 84 compared to the 320 696 mse of sports book .overall we were able to achieve encouraging results using our models as our best predictions rivaled the accuracy of sports books .on average our model was able to pick the correct side of the over under 51 5 of the time in the future there is definitely potential to improve the model through augmenting our data set .conclusion and future work	indeed our neural network had a mse of 369 84 compared to the 320 696 mse of sports book 
1	108571	8571	on average our model was able to pick the correct side of the over under 51 5 of the time in the future there is definitely potential to improve the model through augmenting our data set .indeed our neural network had a mse of 369 84 compared to the 320 696 mse of sports book .first of all the odds lines offered by sports books themselves offer a large amount of information on the potential outcomes of games including data that indicates the direction the odds lines are moving in the hours before the game .conclusion and future work	on average our model was able to pick the correct side of the over under 51 5 of the time in the future there is definitely potential to improve the model through augmenting our data set 
1	108572	8572	first of all the odds lines offered by sports books themselves offer a large amount of information on the potential outcomes of games including data that indicates the direction the odds lines are moving in the hours before the game .on average our model was able to pick the correct side of the over under 51 5 of the time in the future there is definitely potential to improve the model through augmenting our data set .it is entirely possible that the odds lines would indicate that the sports books are very good at predicting the outcomes of games involving certain teams but tend to skew in some direction when trying to predict the outcomes of games involving other teams .conclusion and future work	first of all the odds lines offered by sports books themselves offer a large amount of information on the potential outcomes of games including data that indicates the direction the odds lines are moving in the hours before the game 
1	108573	8573	it is entirely possible that the odds lines would indicate that the sports books are very good at predicting the outcomes of games involving certain teams but tend to skew in some direction when trying to predict the outcomes of games involving other teams .first of all the odds lines offered by sports books themselves offer a large amount of information on the potential outcomes of games including data that indicates the direction the odds lines are moving in the hours before the game .in addition while our current models only use team level data incorporating player level data could add additional layers of nuance while accounting for the impact of injuries or player fatigue further exploration of model architectures could potentially improve our results as well .conclusion and future work	it is entirely possible that the odds lines would indicate that the sports books are very good at predicting the outcomes of games involving certain teams but tend to skew in some direction when trying to predict the outcomes of games involving other teams 
1	108574	8574	in addition while our current models only use team level data incorporating player level data could add additional layers of nuance while accounting for the impact of injuries or player fatigue further exploration of model architectures could potentially improve our results as well .it is entirely possible that the odds lines would indicate that the sports books are very good at predicting the outcomes of games involving certain teams but tend to skew in some direction when trying to predict the outcomes of games involving other teams .due to the similarity of our current problem predicting betting indicators from the home team and away team to classical recommendation systems predicting ratings for a given user and item we could definitely explore adapting algorithms used by netflix amazon and others for recommendation .conclusion and future work	in addition while our current models only use team level data incorporating player level data could add additional layers of nuance while accounting for the impact of injuries or player fatigue further exploration of model architectures could potentially improve our results as well 
1	108575	8575	due to the similarity of our current problem predicting betting indicators from the home team and away team to classical recommendation systems predicting ratings for a given user and item we could definitely explore adapting algorithms used by netflix amazon and others for recommendation .in addition while our current models only use team level data incorporating player level data could add additional layers of nuance while accounting for the impact of injuries or player fatigue further exploration of model architectures could potentially improve our results as well .last sentence.conclusion and future work	due to the similarity of our current problem predicting betting indicators from the home team and away team to classical recommendation systems predicting ratings for a given user and item we could definitely explore adapting algorithms used by netflix amazon and others for recommendation 
0	108576	8576	in this project both vishnu and alexandre contributed equally to the writing of this report .first sentence.alexandre completed around two thirds of the data retrieval cleaning process built the collaborative filtering model and the lstm network .contributions	in this project both vishnu and alexandre contributed equally to the writing of this report 
0	108577	8577	alexandre completed around two thirds of the data retrieval cleaning process built the collaborative filtering model and the lstm network .in this project both vishnu and alexandre contributed equally to the writing of this report .vishnu completed the remaining third of the data retrieval cleaning process built the baseline models as well as the neural network .contributions	alexandre completed around two thirds of the data retrieval cleaning process built the collaborative filtering model and the lstm network 
0	108578	8578	vishnu completed the remaining third of the data retrieval cleaning process built the baseline models as well as the neural network .alexandre completed around two thirds of the data retrieval cleaning process built the collaborative filtering model and the lstm network .the rest was achieved jointly by both members .contributions	vishnu completed the remaining third of the data retrieval cleaning process built the baseline models as well as the neural network 
0	108579	8579	the rest was achieved jointly by both members .vishnu completed the remaining third of the data retrieval cleaning process built the baseline models as well as the neural network .last sentence.contributions	the rest was achieved jointly by both members 
1	108580	8580	several prominent pathologies such as cerebral palsy parkinson s disease and alzheimer s disease can manifest themselves in an abnormal walking gait .first sentence.abnormal gait is characterized by irregular patterns in step length step cadence joint angles and poor balance .introduction and background	several prominent pathologies such as cerebral palsy parkinson s disease and alzheimer s disease can manifest themselves in an abnormal walking gait 
0	108581	8581	abnormal gait is characterized by irregular patterns in step length step cadence joint angles and poor balance .several prominent pathologies such as cerebral palsy parkinson s disease and alzheimer s disease can manifest themselves in an abnormal walking gait .early detection of these abnormalities can help in diagnosis treatment and effective post treatment monitoring of patients a comprehensive metric used to assess the extent of gait pathology is the gait deviation index score or gdi the current gold standard method of measuring gdi marker based motion capture places a set of reflective markers on body segments and tracks their trajectories over time .introduction and background	abnormal gait is characterized by irregular patterns in step length step cadence joint angles and poor balance 
1	108582	8582	early detection of these abnormalities can help in diagnosis treatment and effective post treatment monitoring of patients a comprehensive metric used to assess the extent of gait pathology is the gait deviation index score or gdi the current gold standard method of measuring gdi marker based motion capture places a set of reflective markers on body segments and tracks their trajectories over time .abnormal gait is characterized by irregular patterns in step length step cadence joint angles and poor balance .sessions with patients can last multiple hours and cost hundreds or thousands of dollars .introduction and background	early detection of these abnormalities can help in diagnosis treatment and effective post treatment monitoring of patients a comprehensive metric used to assess the extent of gait pathology is the gait deviation index score or gdi the current gold standard method of measuring gdi marker based motion capture places a set of reflective markers on body segments and tracks their trajectories over time 
0	108583	8583	sessions with patients can last multiple hours and cost hundreds or thousands of dollars .early detection of these abnormalities can help in diagnosis treatment and effective post treatment monitoring of patients a comprehensive metric used to assess the extent of gait pathology is the gait deviation index score or gdi the current gold standard method of measuring gdi marker based motion capture places a set of reflective markers on body segments and tracks their trajectories over time .a potential less expensive and less time consuming alternative is to analyze video captured by commodity devices i e .introduction and background	sessions with patients can last multiple hours and cost hundreds or thousands of dollars 
0	108584	8584	a potential less expensive and less time consuming alternative is to analyze video captured by commodity devices i e .sessions with patients can last multiple hours and cost hundreds or thousands of dollars .mobile camera phone using machine learning algorithms to predict gdi previous attempts have been made to predict gdi from monocular video footage using a projection of joint centers onto the two dimensional plane of the camera .introduction and background	a potential less expensive and less time consuming alternative is to analyze video captured by commodity devices i e 
1	108585	8585	mobile camera phone using machine learning algorithms to predict gdi previous attempts have been made to predict gdi from monocular video footage using a projection of joint centers onto the two dimensional plane of the camera .a potential less expensive and less time consuming alternative is to analyze video captured by commodity devices i e .in this project we leverage cutting edge computer vision tactics to extract three dimensional features from each frame of video .introduction and background	mobile camera phone using machine learning algorithms to predict gdi previous attempts have been made to predict gdi from monocular video footage using a projection of joint centers onto the two dimensional plane of the camera 
0	108586	8586	in this project we leverage cutting edge computer vision tactics to extract three dimensional features from each frame of video .mobile camera phone using machine learning algorithms to predict gdi previous attempts have been made to predict gdi from monocular video footage using a projection of joint centers onto the two dimensional plane of the camera .by stacking processed frames into a video sequence relevant spatiotemporal features can be modeled for gait characterization .introduction and background	in this project we leverage cutting edge computer vision tactics to extract three dimensional features from each frame of video 
1	108587	8587	by stacking processed frames into a video sequence relevant spatiotemporal features can be modeled for gait characterization .in this project we leverage cutting edge computer vision tactics to extract three dimensional features from each frame of video .thus the project goal is to use monocular video footage to predict gdi score with lower root mean squared error rmse than existing methods .introduction and background	by stacking processed frames into a video sequence relevant spatiotemporal features can be modeled for gait characterization 
1	108588	8588	thus the project goal is to use monocular video footage to predict gdi score with lower root mean squared error rmse than existing methods .by stacking processed frames into a video sequence relevant spatiotemporal features can be modeled for gait characterization .last sentence.introduction and background	thus the project goal is to use monocular video footage to predict gdi score with lower root mean squared error rmse than existing methods 
1	108589	8589	our work builds on the efforts of many machine learning scientists who developed models to extract spatiotemporal features from video as well as biomechanists who have analyzed human motion to help physicians assess neuromuscular pathology .first sentence.a leader in this field is lukasz kidzinski a member of stanford s mobilize center who was our advisor for this project .related work	our work builds on the efforts of many machine learning scientists who developed models to extract spatiotemporal features from video as well as biomechanists who have analyzed human motion to help physicians assess neuromuscular pathology 
0	108590	8590	a leader in this field is lukasz kidzinski a member of stanford s mobilize center who was our advisor for this project .our work builds on the efforts of many machine learning scientists who developed models to extract spatiotemporal features from video as well as biomechanists who have analyzed human motion to help physicians assess neuromuscular pathology .we met with lukasz throughout the project for guidance on data processing and model generation .related work	a leader in this field is lukasz kidzinski a member of stanford s mobilize center who was our advisor for this project 
0	108591	8591	we met with lukasz throughout the project for guidance on data processing and model generation .a leader in this field is lukasz kidzinski a member of stanford s mobilize center who was our advisor for this project .in 2017 lukasz and his team used a temporal convolutional network built on videos processed through openpose to predict gdi a critical component to our analysis is the refeaturization of images into a spatial representation of human pose .related work	we met with lukasz throughout the project for guidance on data processing and model generation 
1	108592	8592	in 2017 lukasz and his team used a temporal convolutional network built on videos processed through openpose to predict gdi a critical component to our analysis is the refeaturization of images into a spatial representation of human pose .we met with lukasz throughout the project for guidance on data processing and model generation .specifically we leveraged the densepose algorithm which converts red green blue images toeach pixel in an image to one of thousands of surface locations on a modeled human mesh .related work	in 2017 lukasz and his team used a temporal convolutional network built on videos processed through openpose to predict gdi a critical component to our analysis is the refeaturization of images into a spatial representation of human pose 
1	108593	8593	specifically we leveraged the densepose algorithm which converts red green blue images toeach pixel in an image to one of thousands of surface locations on a modeled human mesh .in 2017 lukasz and his team used a temporal convolutional network built on videos processed through openpose to predict gdi a critical component to our analysis is the refeaturization of images into a spatial representation of human pose .densepose builds on prior work in human pose estimation most notably the skinned multi person linear model our models and experiments were motivated by researchers who have used machine learning to extract spatial and spatiotemporal features from video .related work	specifically we leveraged the densepose algorithm which converts red green blue images toeach pixel in an image to one of thousands of surface locations on a modeled human mesh 
1	108594	8594	densepose builds on prior work in human pose estimation most notably the skinned multi person linear model our models and experiments were motivated by researchers who have used machine learning to extract spatial and spatiotemporal features from video .specifically we leveraged the densepose algorithm which converts red green blue images toeach pixel in an image to one of thousands of surface locations on a modeled human mesh .ibm used a cnn with a multi layer perceptron to classify images into one of many types a guiding work for extracting spatiotemporal features from images was harvey s blog post.related work	densepose builds on prior work in human pose estimation most notably the skinned multi person linear model our models and experiments were motivated by researchers who have used machine learning to extract spatial and spatiotemporal features from video 
1	108595	8595	ibm used a cnn with a multi layer perceptron to classify images into one of many types a guiding work for extracting spatiotemporal features from images was harvey s blog post.densepose builds on prior work in human pose estimation most notably the skinned multi person linear model our models and experiments were motivated by researchers who have used machine learning to extract spatial and spatiotemporal features from video .last sentence.related work	ibm used a cnn with a multi layer perceptron to classify images into one of many types a guiding work for extracting spatiotemporal features from images was harvey s blog post
1	108596	8596	our dataset comprises of 3 000 videos of patients walking in a room at gillette children s specialty healthcare center for gait and motion analysis the videos have a resolution of 640x480 and are 25 frames per second .first sentence.each frame is processed using densepose which maps all pixels of an rgb image to the surface of a modeled human mesh.dataset	our dataset comprises of 3 000 videos of patients walking in a room at gillette children s specialty healthcare center for gait and motion analysis the videos have a resolution of 640x480 and are 25 frames per second 
1	108597	8597	each frame is processed using densepose which maps all pixels of an rgb image to the surface of a modeled human mesh.our dataset comprises of 3 000 videos of patients walking in a room at gillette children s specialty healthcare center for gait and motion analysis the videos have a resolution of 640x480 and are 25 frames per second .last sentence.dataset	each frame is processed using densepose which maps all pixels of an rgb image to the surface of a modeled human mesh
0	108598	8598	densepose right .first sentence.each pixel is assigned to a corresponding point on a human skin mesh the processed frames consist of three channels i part index u and v coordinates and are passed as inputs to the different learning models .figure 1 a sample rgb image left processed by	densepose right 
1	108599	8599	each pixel is assigned to a corresponding point on a human skin mesh the processed frames consist of three channels i part index u and v coordinates and are passed as inputs to the different learning models .densepose right .in the case of a model with a temporal component 10 outputs i e .figure 1 a sample rgb image left processed by	each pixel is assigned to a corresponding point on a human skin mesh the processed frames consist of three channels i part index u and v coordinates and are passed as inputs to the different learning models 
0	108600	8600	in the case of a model with a temporal component 10 outputs i e .each pixel is assigned to a corresponding point on a human skin mesh the processed frames consist of three channels i part index u and v coordinates and are passed as inputs to the different learning models .processed frames are concatenated in sequence per training example before running any initial experiments substantial work was performed to process data .figure 1 a sample rgb image left processed by	in the case of a model with a temporal component 10 outputs i e 
1	108601	8601	processed frames are concatenated in sequence per training example before running any initial experiments substantial work was performed to process data .in the case of a model with a temporal component 10 outputs i e .this included running densepose algorithm on top of thousands of videos and organizing them into folders .figure 1 a sample rgb image left processed by	processed frames are concatenated in sequence per training example before running any initial experiments substantial work was performed to process data 
0	108602	8602	this included running densepose algorithm on top of thousands of videos and organizing them into folders .processed frames are concatenated in sequence per training example before running any initial experiments substantial work was performed to process data .subsequently the folders were assigned a gdi score based on the corresponding examid from a joined file of physician assessments .figure 1 a sample rgb image left processed by	this included running densepose algorithm on top of thousands of videos and organizing them into folders 
0	108603	8603	subsequently the folders were assigned a gdi score based on the corresponding examid from a joined file of physician assessments .this included running densepose algorithm on top of thousands of videos and organizing them into folders .due to the massive data volume and limits of memory we did not leverage the entire dataset in our experiments .figure 1 a sample rgb image left processed by	subsequently the folders were assigned a gdi score based on the corresponding examid from a joined file of physician assessments 
0	108604	8604	due to the massive data volume and limits of memory we did not leverage the entire dataset in our experiments .subsequently the folders were assigned a gdi score based on the corresponding examid from a joined file of physician assessments .we typically accessed 500 1 500 videos depending on the model s computational demands .figure 1 a sample rgb image left processed by	due to the massive data volume and limits of memory we did not leverage the entire dataset in our experiments 
0	108605	8605	we typically accessed 500 1 500 videos depending on the model s computational demands .due to the massive data volume and limits of memory we did not leverage the entire dataset in our experiments .as such though we considered using video slicing image mirroring or other data augmentation techniques we decided not to implement these as generating additional augmented data was not necessary .figure 1 a sample rgb image left processed by	we typically accessed 500 1 500 videos depending on the model s computational demands 
0	108606	8606	as such though we considered using video slicing image mirroring or other data augmentation techniques we decided not to implement these as generating additional augmented data was not necessary .we typically accessed 500 1 500 videos depending on the model s computational demands .last sentence.figure 1 a sample rgb image left processed by	as such though we considered using video slicing image mirroring or other data augmentation techniques we decided not to implement these as generating additional augmented data was not necessary 
1	108607	8607	patient videos are transformed into i u v channels by densepose and are then passed as inputs to our model which we call gdi net .first sentence.gdi net is a machine learning algorithm with spatial and temporal components to predict a patient s gdi score .methods and experiments	patient videos are transformed into i u v channels by densepose and are then passed as inputs to our model which we call gdi net 
1	108608	8608	gdi net is a machine learning algorithm with spatial and temporal components to predict a patient s gdi score .patient videos are transformed into i u v channels by densepose and are then passed as inputs to our model which we call gdi net .gdi net was implemented in keras an open source neural network library which runs on top of tensorflow environment our initial implementations were quick and simple .methods and experiments	gdi net is a machine learning algorithm with spatial and temporal components to predict a patient s gdi score 
0	108609	8609	gdi net was implemented in keras an open source neural network library which runs on top of tensorflow environment our initial implementations were quick and simple .gdi net is a machine learning algorithm with spatial and temporal components to predict a patient s gdi score .we built a linear regression model implemented in scikit learn package using 5 frames of 480x640x3 resolution for each training example as the model complexity was gradually increased a sole spatial component was added to gdi net architecture .methods and experiments	gdi net was implemented in keras an open source neural network library which runs on top of tensorflow environment our initial implementations were quick and simple 
1	108610	8610	we built a linear regression model implemented in scikit learn package using 5 frames of 480x640x3 resolution for each training example as the model complexity was gradually increased a sole spatial component was added to gdi net architecture .gdi net was implemented in keras an open source neural network library which runs on top of tensorflow environment our initial implementations were quick and simple .we trained the spatial component which consisted of vgg16 an off the shelf cnn architecture or a custom 2d cnn with 3 834 training frames of 480x640x3 dimension .methods and experiments	we built a linear regression model implemented in scikit learn package using 5 frames of 480x640x3 resolution for each training example as the model complexity was gradually increased a sole spatial component was added to gdi net architecture 
0	108611	8611	we trained the spatial component which consisted of vgg16 an off the shelf cnn architecture or a custom 2d cnn with 3 834 training frames of 480x640x3 dimension .we built a linear regression model implemented in scikit learn package using 5 frames of 480x640x3 resolution for each training example as the model complexity was gradually increased a sole spatial component was added to gdi net architecture .the model was validated against 951 examples to quantify performance on unseen examples the vgg16 model is pre trained and only the last layer of the model was replaced by a linear function and trained to perform our regression task .methods and experiments	we trained the spatial component which consisted of vgg16 an off the shelf cnn architecture or a custom 2d cnn with 3 834 training frames of 480x640x3 dimension 
1	108612	8612	the model was validated against 951 examples to quantify performance on unseen examples the vgg16 model is pre trained and only the last layer of the model was replaced by a linear function and trained to perform our regression task .we trained the spatial component which consisted of vgg16 an off the shelf cnn architecture or a custom 2d cnn with 3 834 training frames of 480x640x3 dimension .the motivation behind using the pre trained weights was to investigate the possibility of transfer learning .methods and experiments	the model was validated against 951 examples to quantify performance on unseen examples the vgg16 model is pre trained and only the last layer of the model was replaced by a linear function and trained to perform our regression task 
0	108613	8613	the motivation behind using the pre trained weights was to investigate the possibility of transfer learning .the model was validated against 951 examples to quantify performance on unseen examples the vgg16 model is pre trained and only the last layer of the model was replaced by a linear function and trained to perform our regression task .since collecting patient data and building custom models is expensive and cumbersome transfer learning is desirable and could reduce the lead time of any application development .methods and experiments	the motivation behind using the pre trained weights was to investigate the possibility of transfer learning 
0	108614	8614	since collecting patient data and building custom models is expensive and cumbersome transfer learning is desirable and could reduce the lead time of any application development .the motivation behind using the pre trained weights was to investigate the possibility of transfer learning .after receiving advice from cs229 course assistants at the course poster session we also ran a vgg16 model in which all weights were re trained a challenge in executing vgg16 was that it requires an image with a standard size of 224x224x3 as an input .methods and experiments	since collecting patient data and building custom models is expensive and cumbersome transfer learning is desirable and could reduce the lead time of any application development 
1	108615	8615	after receiving advice from cs229 course assistants at the course poster session we also ran a vgg16 model in which all weights were re trained a challenge in executing vgg16 was that it requires an image with a standard size of 224x224x3 as an input .since collecting patient data and building custom models is expensive and cumbersome transfer learning is desirable and could reduce the lead time of any application development .since the densepose output has a resolution of 480x640x3 the outputs had to be cropped before being passed to vgg16 .methods and experiments	after receiving advice from cs229 course assistants at the course poster session we also ran a vgg16 model in which all weights were re trained a challenge in executing vgg16 was that it requires an image with a standard size of 224x224x3 as an input 
1	108616	8616	since the densepose output has a resolution of 480x640x3 the outputs had to be cropped before being passed to vgg16 .after receiving advice from cs229 course assistants at the course poster session we also ran a vgg16 model in which all weights were re trained a challenge in executing vgg16 was that it requires an image with a standard size of 224x224x3 as an input .the crop was made in a manner to preserve as much information as possible i e .methods and experiments	since the densepose output has a resolution of 480x640x3 the outputs had to be cropped before being passed to vgg16 
0	108617	8617	the crop was made in a manner to preserve as much information as possible i e .since the densepose output has a resolution of 480x640x3 the outputs had to be cropped before being passed to vgg16 .by selecting pixel values where patients are most likely to appear in the frame however some loss is inevitable densepose outputs were passed whole to the custom cnn .methods and experiments	the crop was made in a manner to preserve as much information as possible i e 
1	108618	8618	by selecting pixel values where patients are most likely to appear in the frame however some loss is inevitable densepose outputs were passed whole to the custom cnn .the crop was made in a manner to preserve as much information as possible i e .the inspiration behind the cnn s architecture was mahendran et al .methods and experiments	by selecting pixel values where patients are most likely to appear in the frame however some loss is inevitable densepose outputs were passed whole to the custom cnn 
0	108619	8619	the inspiration behind the cnn s architecture was mahendran et al .by selecting pixel values where patients are most likely to appear in the frame however some loss is inevitable densepose outputs were passed whole to the custom cnn .s model that leveraged a cnn to predict the pose of a vehicle within a continuous regression framework .methods and experiments	the inspiration behind the cnn s architecture was mahendran et al 
1	108620	8620	s model that leveraged a cnn to predict the pose of a vehicle within a continuous regression framework .the inspiration behind the cnn s architecture was mahendran et al .in addition lukasz kidzinski provided instrumental guidance for making important architecture choices in the cnn although the spatial component identifies human poses in a frame it cannot track the pose trajectories over time .methods and experiments	s model that leveraged a cnn to predict the pose of a vehicle within a continuous regression framework 
1	108621	8621	in addition lukasz kidzinski provided instrumental guidance for making important architecture choices in the cnn although the spatial component identifies human poses in a frame it cannot track the pose trajectories over time .s model that leveraged a cnn to predict the pose of a vehicle within a continuous regression framework .to detect temporal characteristics a temporal model either an lstm or 1d cnn was added to gdi net .methods and experiments	in addition lukasz kidzinski provided instrumental guidance for making important architecture choices in the cnn although the spatial component identifies human poses in a frame it cannot track the pose trajectories over time 
1	108622	8622	to detect temporal characteristics a temporal model either an lstm or 1d cnn was added to gdi net .in addition lukasz kidzinski provided instrumental guidance for making important architecture choices in the cnn although the spatial component identifies human poses in a frame it cannot track the pose trajectories over time .the input to a temporal model had 2 920 training examples of 10 frames of 480x640x3 resolution concatenated into a single array .methods and experiments	to detect temporal characteristics a temporal model either an lstm or 1d cnn was added to gdi net 
1	108623	8623	the input to a temporal model had 2 920 training examples of 10 frames of 480x640x3 resolution concatenated into a single array .to detect temporal characteristics a temporal model either an lstm or 1d cnn was added to gdi net .hyperparameter tuning was performed to further optimize models mainly tuning learning rate batch size and dropout .methods and experiments	the input to a temporal model had 2 920 training examples of 10 frames of 480x640x3 resolution concatenated into a single array 
1	108624	8624	hyperparameter tuning was performed to further optimize models mainly tuning learning rate batch size and dropout .the input to a temporal model had 2 920 training examples of 10 frames of 480x640x3 resolution concatenated into a single array .the majority of this effort focused on tuning hyperparameters for our most promising model the cnn with lstm .methods and experiments	hyperparameter tuning was performed to further optimize models mainly tuning learning rate batch size and dropout 
0	108625	8625	the majority of this effort focused on tuning hyperparameters for our most promising model the cnn with lstm .hyperparameter tuning was performed to further optimize models mainly tuning learning rate batch size and dropout .the complete architecture of the highest performing model is outlined in the results and discussion section of this paper the immense size of the dataset and the desire to concatenate frames sometimes led to issues with memory overload .methods and experiments	the majority of this effort focused on tuning hyperparameters for our most promising model the cnn with lstm 
1	108626	8626	the complete architecture of the highest performing model is outlined in the results and discussion section of this paper the immense size of the dataset and the desire to concatenate frames sometimes led to issues with memory overload .the majority of this effort focused on tuning hyperparameters for our most promising model the cnn with lstm .in those scenarios hyperparameters such as batch size and kernel size were adjusted to avoid memory limits .methods and experiments	the complete architecture of the highest performing model is outlined in the results and discussion section of this paper the immense size of the dataset and the desire to concatenate frames sometimes led to issues with memory overload 
0	108627	8627	in those scenarios hyperparameters such as batch size and kernel size were adjusted to avoid memory limits .the complete architecture of the highest performing model is outlined in the results and discussion section of this paper the immense size of the dataset and the desire to concatenate frames sometimes led to issues with memory overload .further for computationally expensive models we leveraged sherlock a high performance computing cluster available to stanford university affiliates to reduce run time.methods and experiments	in those scenarios hyperparameters such as batch size and kernel size were adjusted to avoid memory limits 
0	108628	8628	further for computationally expensive models we leveraged sherlock a high performance computing cluster available to stanford university affiliates to reduce run time.in those scenarios hyperparameters such as batch size and kernel size were adjusted to avoid memory limits .last sentence.methods and experiments	further for computationally expensive models we leveraged sherlock a high performance computing cluster available to stanford university affiliates to reduce run time
0	108629	8629	one of the interesting findings from our experiments was the relatively poor performance from using an off the shelf model .first sentence.the initial goal in these experiments was to exploit transfer learning to build complex networks with millions of parameters that are pre trained .a summary of experiments and results is shown in	one of the interesting findings from our experiments was the relatively poor performance from using an off the shelf model 
0	108630	8630	the initial goal in these experiments was to exploit transfer learning to build complex networks with millions of parameters that are pre trained .one of the interesting findings from our experiments was the relatively poor performance from using an off the shelf model .we used vgg16 a model that is readily available within the keras api and has reported success with image classification tasks frame specific models that only captured spatial features of a given frame did not perform well .a summary of experiments and results is shown in	the initial goal in these experiments was to exploit transfer learning to build complex networks with millions of parameters that are pre trained 
1	108631	8631	we used vgg16 a model that is readily available within the keras api and has reported success with image classification tasks frame specific models that only captured spatial features of a given frame did not perform well .the initial goal in these experiments was to exploit transfer learning to build complex networks with millions of parameters that are pre trained .this is expected as gdi is largely determined by trajectories of body parts and individual frames do not hold temporal information describing how the patient moves over time .a summary of experiments and results is shown in	we used vgg16 a model that is readily available within the keras api and has reported success with image classification tasks frame specific models that only captured spatial features of a given frame did not perform well 
1	108632	8632	this is expected as gdi is largely determined by trajectories of body parts and individual frames do not hold temporal information describing how the patient moves over time .we used vgg16 a model that is readily available within the keras api and has reported success with image classification tasks frame specific models that only captured spatial features of a given frame did not perform well .as such we spent most of our time and effort experimenting with spatiotemporal models the best performing model combined a 2d cnn on each frame and an lstm to capture temporal patterns .a summary of experiments and results is shown in	this is expected as gdi is largely determined by trajectories of body parts and individual frames do not hold temporal information describing how the patient moves over time 
1	108633	8633	as such we spent most of our time and effort experimenting with spatiotemporal models the best performing model combined a 2d cnn on each frame and an lstm to capture temporal patterns .this is expected as gdi is largely determined by trajectories of body parts and individual frames do not hold temporal information describing how the patient moves over time .the learning curve regression plot and detailed architecture are outlined in.a summary of experiments and results is shown in	as such we spent most of our time and effort experimenting with spatiotemporal models the best performing model combined a 2d cnn on each frame and an lstm to capture temporal patterns 
0	108634	8634	the learning curve regression plot and detailed architecture are outlined in.as such we spent most of our time and effort experimenting with spatiotemporal models the best performing model combined a 2d cnn on each frame and an lstm to capture temporal patterns .last sentence.a summary of experiments and results is shown in	the learning curve regression plot and detailed architecture are outlined in
0	108635	8635	in this study we implemented a machine learning approach to develop a cheap and automated way to assess gait abnormalities .first sentence.the results especially that of a cnn with lstm model are very promising .conclusions and future work	in this study we implemented a machine learning approach to develop a cheap and automated way to assess gait abnormalities 
0	108636	8636	the results especially that of a cnn with lstm model are very promising .in this study we implemented a machine learning approach to develop a cheap and automated way to assess gait abnormalities .we are excited by our model s potential to help diagnose and track the progression of neuromuscular disease although the cnn with lstm model was able to achieve a low rmse error analysis should be performed to investigate the deviations between the model and ground truth .conclusions and future work	the results especially that of a cnn with lstm model are very promising 
1	108637	8637	we are excited by our model s potential to help diagnose and track the progression of neuromuscular disease although the cnn with lstm model was able to achieve a low rmse error analysis should be performed to investigate the deviations between the model and ground truth .the results especially that of a cnn with lstm model are very promising .once the nature of these errors is understood the model and the architecture can be fine tuned for better accuracy an option to enhance our architecture is to implement 3d convolution blocks instead of a separate spatial and temporal component .conclusions and future work	we are excited by our model s potential to help diagnose and track the progression of neuromuscular disease although the cnn with lstm model was able to achieve a low rmse error analysis should be performed to investigate the deviations between the model and ground truth 
1	108638	8638	once the nature of these errors is understood the model and the architecture can be fine tuned for better accuracy an option to enhance our architecture is to implement 3d convolution blocks instead of a separate spatial and temporal component .we are excited by our model s potential to help diagnose and track the progression of neuromuscular disease although the cnn with lstm model was able to achieve a low rmse error analysis should be performed to investigate the deviations between the model and ground truth .we hypothesize that it may perform better than the spatiotemporal model as it can capture lower level features in time and is not affected by the way data is passed from the spatial component to the temporal component in our experiments we observed a growing gap between training error and validation error as training progressed which suggests overfitting .conclusions and future work	once the nature of these errors is understood the model and the architecture can be fine tuned for better accuracy an option to enhance our architecture is to implement 3d convolution blocks instead of a separate spatial and temporal component 
1	108639	8639	we hypothesize that it may perform better than the spatiotemporal model as it can capture lower level features in time and is not affected by the way data is passed from the spatial component to the temporal component in our experiments we observed a growing gap between training error and validation error as training progressed which suggests overfitting .once the nature of these errors is understood the model and the architecture can be fine tuned for better accuracy an option to enhance our architecture is to implement 3d convolution blocks instead of a separate spatial and temporal component .although we attempted to mitigate this using dropout more could be done to generalize our model .conclusions and future work	we hypothesize that it may perform better than the spatiotemporal model as it can capture lower level features in time and is not affected by the way data is passed from the spatial component to the temporal component in our experiments we observed a growing gap between training error and validation error as training progressed which suggests overfitting 
0	108640	8640	although we attempted to mitigate this using dropout more could be done to generalize our model .we hypothesize that it may perform better than the spatiotemporal model as it can capture lower level features in time and is not affected by the way data is passed from the spatial component to the temporal component in our experiments we observed a growing gap between training error and validation error as training progressed which suggests overfitting .future experiments can focus on reducing model complexity or incorporating l2 regularization an interesting approach we would like to implement is building a classification network by bucketing gdi scores to the nearest integer .conclusions and future work	although we attempted to mitigate this using dropout more could be done to generalize our model 
1	108641	8641	future experiments can focus on reducing model complexity or incorporating l2 regularization an interesting approach we would like to implement is building a classification network by bucketing gdi scores to the nearest integer .although we attempted to mitigate this using dropout more could be done to generalize our model .using a softmax layer we can take the probability weighted sum of bucket values to determine a scalar gdi score .conclusions and future work	future experiments can focus on reducing model complexity or incorporating l2 regularization an interesting approach we would like to implement is building a classification network by bucketing gdi scores to the nearest integer 
0	108642	8642	using a softmax layer we can take the probability weighted sum of bucket values to determine a scalar gdi score .future experiments can focus on reducing model complexity or incorporating l2 regularization an interesting approach we would like to implement is building a classification network by bucketing gdi scores to the nearest integer .in other words we can train the network as a classification task but derive the scalar gdi score using the appropriate weighted sum of the softmax output .conclusions and future work	using a softmax layer we can take the probability weighted sum of bucket values to determine a scalar gdi score 
0	108643	8643	in other words we can train the network as a classification task but derive the scalar gdi score using the appropriate weighted sum of the softmax output .using a softmax layer we can take the probability weighted sum of bucket values to determine a scalar gdi score .this option also opens the opportunity to use off the shelf classification frameworks for our task our capacity to experiment was constrained by memory overload issues .conclusions and future work	in other words we can train the network as a classification task but derive the scalar gdi score using the appropriate weighted sum of the softmax output 
0	108644	8644	this option also opens the opportunity to use off the shelf classification frameworks for our task our capacity to experiment was constrained by memory overload issues .in other words we can train the network as a classification task but derive the scalar gdi score using the appropriate weighted sum of the softmax output .the efficient management of memory and resource utilization would allow for more rapid experimentation .conclusions and future work	this option also opens the opportunity to use off the shelf classification frameworks for our task our capacity to experiment was constrained by memory overload issues 
0	108645	8645	the efficient management of memory and resource utilization would allow for more rapid experimentation .this option also opens the opportunity to use off the shelf classification frameworks for our task our capacity to experiment was constrained by memory overload issues .chunking memory swapping or simply accessing machines with larger random access memory can be applied to address this issue future experiments should explore refeaturizing the processed densepose outputs to global x y z coordinates .conclusions and future work	the efficient management of memory and resource utilization would allow for more rapid experimentation 
1	108646	8646	chunking memory swapping or simply accessing machines with larger random access memory can be applied to address this issue future experiments should explore refeaturizing the processed densepose outputs to global x y z coordinates .the efficient management of memory and resource utilization would allow for more rapid experimentation .this would allow us to manually engineer additional relevant features such as knee flexion angle that are expected correlates of gdi score .conclusions and future work	chunking memory swapping or simply accessing machines with larger random access memory can be applied to address this issue future experiments should explore refeaturizing the processed densepose outputs to global x y z coordinates 
1	108647	8647	this would allow us to manually engineer additional relevant features such as knee flexion angle that are expected correlates of gdi score .chunking memory swapping or simply accessing machines with larger random access memory can be applied to address this issue future experiments should explore refeaturizing the processed densepose outputs to global x y z coordinates .we can further compress our data by considering only the x y z coordinates of the most relevant body landmarks as movements of the hip knee and ankle are particularly important for gait analysis .conclusions and future work	this would allow us to manually engineer additional relevant features such as knee flexion angle that are expected correlates of gdi score 
1	108648	8648	we can further compress our data by considering only the x y z coordinates of the most relevant body landmarks as movements of the hip knee and ankle are particularly important for gait analysis .this would allow us to manually engineer additional relevant features such as knee flexion angle that are expected correlates of gdi score .this process would require the manipulation of densepose outputs to a customized smpl human body model the potential for future work is enormous as we have just scratched the surface of densepose s capabilities .conclusions and future work	we can further compress our data by considering only the x y z coordinates of the most relevant body landmarks as movements of the hip knee and ankle are particularly important for gait analysis 
1	108649	8649	this process would require the manipulation of densepose outputs to a customized smpl human body model the potential for future work is enormous as we have just scratched the surface of densepose s capabilities .we can further compress our data by considering only the x y z coordinates of the most relevant body landmarks as movements of the hip knee and ankle are particularly important for gait analysis .a determined effort can lead to the development of a robust reliable and low cost alternative to analyzing human gait .conclusions and future work	this process would require the manipulation of densepose outputs to a customized smpl human body model the potential for future work is enormous as we have just scratched the surface of densepose s capabilities 
0	108650	8650	a determined effort can lead to the development of a robust reliable and low cost alternative to analyzing human gait .this process would require the manipulation of densepose outputs to a customized smpl human body model the potential for future work is enormous as we have just scratched the surface of densepose s capabilities .last sentence.conclusions and future work	a determined effort can lead to the development of a robust reliable and low cost alternative to analyzing human gait 
0	108651	8651	adam gotlin collaborated closely with the stanford mobilize center and other project partners to secure data and share knowledge .first sentence.he orchestrated meetings with advisors and experts and helped identify best practices for time series and movement data .contributions	adam gotlin collaborated closely with the stanford mobilize center and other project partners to secure data and share knowledge 
0	108652	8652	he orchestrated meetings with advisors and experts and helped identify best practices for time series and movement data .adam gotlin collaborated closely with the stanford mobilize center and other project partners to secure data and share knowledge .he led efforts on building temporal cnn models apurva pancholi spearheaded initial experiments and was involved in data preprocessing .contributions	he orchestrated meetings with advisors and experts and helped identify best practices for time series and movement data 
0	108653	8653	he led efforts on building temporal cnn models apurva pancholi spearheaded initial experiments and was involved in data preprocessing .he orchestrated meetings with advisors and experts and helped identify best practices for time series and movement data .he owned data transfer from the mobilize center to our project team .contributions	he led efforts on building temporal cnn models apurva pancholi spearheaded initial experiments and was involved in data preprocessing 
0	108654	8654	he owned data transfer from the mobilize center to our project team .he led efforts on building temporal cnn models apurva pancholi spearheaded initial experiments and was involved in data preprocessing .apurva developed a custom cnn that was the primary spatial component for our highest performing model .contributions	he owned data transfer from the mobilize center to our project team 
0	108655	8655	apurva developed a custom cnn that was the primary spatial component for our highest performing model .he owned data transfer from the mobilize center to our project team .umang agarwal led data processing and consolidation .contributions	apurva developed a custom cnn that was the primary spatial component for our highest performing model 
0	108656	8656	umang agarwal led data processing and consolidation .apurva developed a custom cnn that was the primary spatial component for our highest performing model .he owned efforts to read and interpret densepose source code in order to generate gdi net model inputs .contributions	umang agarwal led data processing and consolidation 
0	108657	8657	he owned efforts to read and interpret densepose source code in order to generate gdi net model inputs .umang agarwal led data processing and consolidation .he led efforts to exploit transfer learning and contributed to tuning neural network models to maximize performance .contributions	he owned efforts to read and interpret densepose source code in order to generate gdi net model inputs 
0	108658	8658	he led efforts to exploit transfer learning and contributed to tuning neural network models to maximize performance .he owned efforts to read and interpret densepose source code in order to generate gdi net model inputs .last sentence.contributions	he led efforts to exploit transfer learning and contributed to tuning neural network models to maximize performance 
0	108659	8659	code for this project can be found at https github com agotlin cs229dp.first sentence.last sentence.code	code for this project can be found at https github com agotlin cs229dp
0	108660	8660	using words can be limited when communicating across cultures and literacy levels .first sentence.drawing images is a shared communication method that can bridge those divides .motivation	using words can be limited when communicating across cultures and literacy levels 
0	108661	8661	drawing images is a shared communication method that can bridge those divides .using words can be limited when communicating across cultures and literacy levels .if successful this model can be applied for a variety of interesting tasks including a new search interface where someone can draw what they need and search for it or an app where a language learner can draw an image and get the translation immediately .motivation	drawing images is a shared communication method that can bridge those divides 
1	108662	8662	if successful this model can be applied for a variety of interesting tasks including a new search interface where someone can draw what they need and search for it or an app where a language learner can draw an image and get the translation immediately .drawing images is a shared communication method that can bridge those divides .these applications require computers to understand our quick line drawings or doodles .motivation	if successful this model can be applied for a variety of interesting tasks including a new search interface where someone can draw what they need and search for it or an app where a language learner can draw an image and get the translation immediately 
0	108663	8663	these applications require computers to understand our quick line drawings or doodles .if successful this model can be applied for a variety of interesting tasks including a new search interface where someone can draw what they need and search for it or an app where a language learner can draw an image and get the translation immediately .last sentence.motivation	these applications require computers to understand our quick line drawings or doodles 
0	108664	8664	our goal is to develop an efficient system to recognize labels of hand drawn images from google s quickdraw dataset .first sentence.the input to our algorithm is an image .goal	our goal is to develop an efficient system to recognize labels of hand drawn images from google s quickdraw dataset 
0	108665	8665	the input to our algorithm is an image .our goal is to develop an efficient system to recognize labels of hand drawn images from google s quickdraw dataset .we use logistic regression support vector machines svms convolutional neural networks cnns and transfer learning to output a predicted class .goal	the input to our algorithm is an image 
0	108666	8666	we use logistic regression support vector machines svms convolutional neural networks cnns and transfer learning to output a predicted class .the input to our algorithm is an image .last sentence.goal	we use logistic regression support vector machines svms convolutional neural networks cnns and transfer learning to output a predicted class 
0	108667	8667	deep learning has proven to be very successful in general image classification .first sentence.past imagenet 2 1 1 .image recognition	deep learning has proven to be very successful in general image classification 
0	108668	8668	past imagenet 2 1 1 .deep learning has proven to be very successful in general image classification .transfer learning .image recognition	past imagenet 2 1 1 
0	108669	8669	transfer learning .past imagenet 2 1 1 .the idea behind transfer learning is that we can apply knowledge from a generalized area to a novel task while pre trained imagenet models have been widely used in transfer learning for other natural image classification tasks they have not commonly been used for handdrawn images .image recognition	transfer learning 
1	108670	8670	the idea behind transfer learning is that we can apply knowledge from a generalized area to a novel task while pre trained imagenet models have been widely used in transfer learning for other natural image classification tasks they have not commonly been used for handdrawn images .transfer learning .however researchers lagunas and garces have successfully used transfer learning with vgg.image recognition	the idea behind transfer learning is that we can apply knowledge from a generalized area to a novel task while pre trained imagenet models have been widely used in transfer learning for other natural image classification tasks they have not commonly been used for handdrawn images 
0	108671	8671	however researchers lagunas and garces have successfully used transfer learning with vgg.the idea behind transfer learning is that we can apply knowledge from a generalized area to a novel task while pre trained imagenet models have been widely used in transfer learning for other natural image classification tasks they have not commonly been used for handdrawn images .last sentence.image recognition	however researchers lagunas and garces have successfully used transfer learning with vgg
0	108672	8672	we are working on a relatively new dataset released under two years ago with a focus on efficiency .first sentence.our project could be seen as within the domain of image recognition .our contributions	we are working on a relatively new dataset released under two years ago with a focus on efficiency 
0	108673	8673	our project could be seen as within the domain of image recognition .we are working on a relatively new dataset released under two years ago with a focus on efficiency .however doodles only consist of lines and have two colors black and white which could render complex features learned by image recognition models useless .our contributions	our project could be seen as within the domain of image recognition 
0	108674	8674	however doodles only consist of lines and have two colors black and white which could render complex features learned by image recognition models useless .our project could be seen as within the domain of image recognition .we want to focus on not just accuracy but also efficiency .our contributions	however doodles only consist of lines and have two colors black and white which could render complex features learned by image recognition models useless 
0	108675	8675	we want to focus on not just accuracy but also efficiency .however doodles only consist of lines and have two colors black and white which could render complex features learned by image recognition models useless .efficiency i e model size training time is critical to allow deployment of the system in real life applications yet it has not received sufficient attention in research .our contributions	we want to focus on not just accuracy but also efficiency 
1	108676	8676	efficiency i e model size training time is critical to allow deployment of the system in real life applications yet it has not received sufficient attention in research .we want to focus on not just accuracy but also efficiency .we used the bitmap version of the data .our contributions	efficiency i e model size training time is critical to allow deployment of the system in real life applications yet it has not received sufficient attention in research 
0	108677	8677	we used the bitmap version of the data .efficiency i e model size training time is critical to allow deployment of the system in real life applications yet it has not received sufficient attention in research .each drawing consists of 28 by 28 raw pixel inputs with values from 0 to 255 .our contributions	we used the bitmap version of the data 
0	108678	8678	each drawing consists of 28 by 28 raw pixel inputs with values from 0 to 255 .we used the bitmap version of the data .we took advantage of the fact that each image has only two colors black and white to binarize the pixels for a more compact representation .our contributions	each drawing consists of 28 by 28 raw pixel inputs with values from 0 to 255 
0	108679	8679	we took advantage of the fact that each image has only two colors black and white to binarize the pixels for a more compact representation .each drawing consists of 28 by 28 raw pixel inputs with values from 0 to 255 .last sentence.our contributions	we took advantage of the fact that each image has only two colors black and white to binarize the pixels for a more compact representation 
0	108680	8680	to make training more tractable on modest computing resources we elected to work with a subset of the data .first sentence.the classes selected were chosen randomly from the overall pool of classes and were fixed throughout our experimentation .dataset	to make training more tractable on modest computing resources we elected to work with a subset of the data 
0	108681	8681	the classes selected were chosen randomly from the overall pool of classes and were fixed throughout our experimentation .to make training more tractable on modest computing resources we elected to work with a subset of the data .the number of examples per class is 20 000 .dataset	the classes selected were chosen randomly from the overall pool of classes and were fixed throughout our experimentation 
0	108682	8682	the number of examples per class is 20 000 .the classes selected were chosen randomly from the overall pool of classes and were fixed throughout our experimentation .we picked 3 10 and 50 classes to train .dataset	the number of examples per class is 20 000 
0	108683	8683	we picked 3 10 and 50 classes to train .the number of examples per class is 20 000 .for each of the classes we split our data into training validation and test sets with the ratio of 80 10 10 .dataset	we picked 3 10 and 50 classes to train 
0	108684	8684	for each of the classes we split our data into training validation and test sets with the ratio of 80 10 10 .we picked 3 10 and 50 classes to train .last sentence.dataset	for each of the classes we split our data into training validation and test sets with the ratio of 80 10 10 
0	108685	8685	broadly speaking we looked at two classes of algorithms for our task traditional machine learning approaches and deep learning techniques .first sentence.the link to our github with our code is here https github com jervisfm cs229 project .methods	broadly speaking we looked at two classes of algorithms for our task traditional machine learning approaches and deep learning techniques 
0	108686	8686	the link to our github with our code is here https github com jervisfm cs229 project .broadly speaking we looked at two classes of algorithms for our task traditional machine learning approaches and deep learning techniques .last sentence.methods	the link to our github with our code is here https github com jervisfm cs229 project 
0	108687	8687	4 1 1 .first sentence.logistic regression .classical machine learning	4 1 1 
0	108688	8688	logistic regression .4 1 1 .for our baseline we used logistic regression a simple and fast model to train the log likelihood for our logistic model where h x 1 1 e t x 4 1 2 .classical machine learning	logistic regression 
1	108689	8689	for our baseline we used logistic regression a simple and fast model to train the log likelihood for our logistic model where h x 1 1 e t x 4 1 2 .logistic regression .support vector machine .classical machine learning	for our baseline we used logistic regression a simple and fast model to train the log likelihood for our logistic model where h x 1 1 e t x 4 1 2 
0	108690	8690	support vector machine .for our baseline we used logistic regression a simple and fast model to train the log likelihood for our logistic model where h x 1 1 e t x 4 1 2 .support vector machines are optimal margin classifiers and the optimization objective for these models we explored using support vector machines with various kernels to find empirically the kernel most suited for the task of doodle classification .classical machine learning	support vector machine 
1	108691	8691	support vector machines are optimal margin classifiers and the optimization objective for these models we explored using support vector machines with various kernels to find empirically the kernel most suited for the task of doodle classification .support vector machine .the types of kernels we experimented with are linear rbf radial basis function polynomial and sigmoid .classical machine learning	support vector machines are optimal margin classifiers and the optimization objective for these models we explored using support vector machines with various kernels to find empirically the kernel most suited for the task of doodle classification 
0	108692	8692	the types of kernels we experimented with are linear rbf radial basis function polynomial and sigmoid .support vector machines are optimal margin classifiers and the optimization objective for these models we explored using support vector machines with various kernels to find empirically the kernel most suited for the task of doodle classification .last sentence.classical machine learning	the types of kernels we experimented with are linear rbf radial basis function polynomial and sigmoid 
0	108693	8693	we started out with a cnn a natural candidate for image recognition given the convolutional layer s ability to capture spatial dependency .first sentence.a key insight is that since a doodle is a simple image some components of the cnn may be unnecessary .deep learning 4 2 1 convolutional neural network cnn 	we started out with a cnn a natural candidate for image recognition given the convolutional layer s ability to capture spatial dependency 
0	108694	8694	a key insight is that since a doodle is a simple image some components of the cnn may be unnecessary .we started out with a cnn a natural candidate for image recognition given the convolutional layer s ability to capture spatial dependency .by identifying and removing these layers we developed a compact model that is both fast to train and still accurate .deep learning 4 2 1 convolutional neural network cnn 	a key insight is that since a doodle is a simple image some components of the cnn may be unnecessary 
0	108695	8695	by identifying and removing these layers we developed a compact model that is both fast to train and still accurate .a key insight is that since a doodle is a simple image some components of the cnn may be unnecessary .the cnn architecture is given in.deep learning 4 2 1 convolutional neural network cnn 	by identifying and removing these layers we developed a compact model that is both fast to train and still accurate 
0	108696	8696	the cnn architecture is given in.by identifying and removing these layers we developed a compact model that is both fast to train and still accurate .last sentence.deep learning 4 2 1 convolutional neural network cnn 	the cnn architecture is given in
0	108697	8697	even with a simple cnn we noticed that training a deep learning model from the ground up can be time and resource intensive .first sentence.since a doodle is also an image we explored if it is feasible to transfer knowledge from winning imagenet architectures to our specific problem of doodle classification via transfer learning .transfer learning 	even with a simple cnn we noticed that training a deep learning model from the ground up can be time and resource intensive 
1	108698	8698	since a doodle is also an image we explored if it is feasible to transfer knowledge from winning imagenet architectures to our specific problem of doodle classification via transfer learning .even with a simple cnn we noticed that training a deep learning model from the ground up can be time and resource intensive .we used four different baseline models namely inception v3 vgg mobilenet and resnet50 from the imagenet competition and extended them for doodle classification figure 4 .transfer learning 	since a doodle is also an image we explored if it is feasible to transfer knowledge from winning imagenet architectures to our specific problem of doodle classification via transfer learning 
0	108699	8699	we used four different baseline models namely inception v3 vgg mobilenet and resnet50 from the imagenet competition and extended them for doodle classification figure 4 .since a doodle is also an image we explored if it is feasible to transfer knowledge from winning imagenet architectures to our specific problem of doodle classification via transfer learning .more specifically we added a global spatial average pooling layer after the original architecture followed by a dense layer of 128 units with relu activation and finally a softmax layer for classification .transfer learning 	we used four different baseline models namely inception v3 vgg mobilenet and resnet50 from the imagenet competition and extended them for doodle classification figure 4 
1	108700	8700	more specifically we added a global spatial average pooling layer after the original architecture followed by a dense layer of 128 units with relu activation and finally a softmax layer for classification .we used four different baseline models namely inception v3 vgg mobilenet and resnet50 from the imagenet competition and extended them for doodle classification figure 4 .we used stochastic gradient descent with an adam optimizer to fine tune the added layers .transfer learning 	more specifically we added a global spatial average pooling layer after the original architecture followed by a dense layer of 128 units with relu activation and finally a softmax layer for classification 
0	108701	8701	we used stochastic gradient descent with an adam optimizer to fine tune the added layers .more specifically we added a global spatial average pooling layer after the original architecture followed by a dense layer of 128 units with relu activation and finally a softmax layer for classification .due to constraints on computational resources we optionally finetuned the top two layers of the original network .transfer learning 	we used stochastic gradient descent with an adam optimizer to fine tune the added layers 
0	108702	8702	due to constraints on computational resources we optionally finetuned the top two layers of the original network .we used stochastic gradient descent with an adam optimizer to fine tune the added layers .last sentence.transfer learning 	due to constraints on computational resources we optionally finetuned the top two layers of the original network 
0	108703	8703	for logistic regression and svms we ran these models locally .first sentence.due to our local machines computational power constraints we ran the cnns and transfer learning models on google cloud on a virtual machine with 320gb of local disk 12 cores of cpu 64gb of ram and an nvidia p100 gpu with 16gb of memory .efficiency we measured training time in seconds 	for logistic regression and svms we ran these models locally 
0	108704	8704	due to our local machines computational power constraints we ran the cnns and transfer learning models on google cloud on a virtual machine with 320gb of local disk 12 cores of cpu 64gb of ram and an nvidia p100 gpu with 16gb of memory .for logistic regression and svms we ran these models locally .we used google cloud deep learning machine image.efficiency we measured training time in seconds 	due to our local machines computational power constraints we ran the cnns and transfer learning models on google cloud on a virtual machine with 320gb of local disk 12 cores of cpu 64gb of ram and an nvidia p100 gpu with 16gb of memory 
0	108705	8705	we used google cloud deep learning machine image.due to our local machines computational power constraints we ran the cnns and transfer learning models on google cloud on a virtual machine with 320gb of local disk 12 cores of cpu 64gb of ram and an nvidia p100 gpu with 16gb of memory .last sentence.efficiency we measured training time in seconds 	we used google cloud deep learning machine image
1	108706	8706	logistic regression was implemented using python s scikit learn framework putting these numbers in context for the 50 class dataset a classifier that randomly guesses the class would have expected accuracy of 2 .first sentence.we see that the logistic regression classifier did relatively well that said there was still room for improvement and we performed error analysis next to understand the types of errors that the classifier was making .logistic regression	logistic regression was implemented using python s scikit learn framework putting these numbers in context for the 50 class dataset a classifier that randomly guesses the class would have expected accuracy of 2 
1	108707	8707	we see that the logistic regression classifier did relatively well that said there was still room for improvement and we performed error analysis next to understand the types of errors that the classifier was making .logistic regression was implemented using python s scikit learn framework putting these numbers in context for the 50 class dataset a classifier that randomly guesses the class would have expected accuracy of 2 .looking at the confusion matrix we can see that logistic regression performs relatively well .logistic regression	we see that the logistic regression classifier did relatively well that said there was still room for improvement and we performed error analysis next to understand the types of errors that the classifier was making 
0	108708	8708	looking at the confusion matrix we can see that logistic regression performs relatively well .we see that the logistic regression classifier did relatively well that said there was still room for improvement and we performed error analysis next to understand the types of errors that the classifier was making .the diagonal of the confusion matrix carries the most weight indicating it often makes the correct prediction .logistic regression	looking at the confusion matrix we can see that logistic regression performs relatively well 
0	108709	8709	the diagonal of the confusion matrix carries the most weight indicating it often makes the correct prediction .looking at the confusion matrix we can see that logistic regression performs relatively well .we notice that in the wrongly classified regions the true label banana is highly misclassified with hockey stick .logistic regression	the diagonal of the confusion matrix carries the most weight indicating it often makes the correct prediction 
0	108710	8710	we notice that in the wrongly classified regions the true label banana is highly misclassified with hockey stick .the diagonal of the confusion matrix carries the most weight indicating it often makes the correct prediction .this is expected as a hand drawn banana is very similar to a hand drawn hockey stick as seen in figures 6 and 7 .logistic regression	we notice that in the wrongly classified regions the true label banana is highly misclassified with hockey stick 
0	108711	8711	this is expected as a hand drawn banana is very similar to a hand drawn hockey stick as seen in figures 6 and 7 .we notice that in the wrongly classified regions the true label banana is highly misclassified with hockey stick .thus this experiment suggests that in order to predict hand drawn doodles contrary to our initial belief we may need a more sophisticated model instead of a simpler model because the quality of the drawing may not be very good .logistic regression	this is expected as a hand drawn banana is very similar to a hand drawn hockey stick as seen in figures 6 and 7 
1	108712	8712	thus this experiment suggests that in order to predict hand drawn doodles contrary to our initial belief we may need a more sophisticated model instead of a simpler model because the quality of the drawing may not be very good .this is expected as a hand drawn banana is very similar to a hand drawn hockey stick as seen in figures 6 and 7 .to our surprise the svms performed worse than linear regression overall .logistic regression	thus this experiment suggests that in order to predict hand drawn doodles contrary to our initial belief we may need a more sophisticated model instead of a simpler model because the quality of the drawing may not be very good 
0	108713	8713	to our surprise the svms performed worse than linear regression overall .thus this experiment suggests that in order to predict hand drawn doodles contrary to our initial belief we may need a more sophisticated model instead of a simpler model because the quality of the drawing may not be very good .however this could be due to the fact that we have not done extensive hyperparameter tuning for svms among our different choices of kernels the rbf kernel performed the best followed by the polynomial kernel with degree 5 then the linear kernel .logistic regression	to our surprise the svms performed worse than linear regression overall 
1	108714	8714	however this could be due to the fact that we have not done extensive hyperparameter tuning for svms among our different choices of kernels the rbf kernel performed the best followed by the polynomial kernel with degree 5 then the linear kernel .to our surprise the svms performed worse than linear regression overall .the sigmoid kernel performed the worst with an accuracy equitable to assigning a category at random this result is consistent with what we expected .logistic regression	however this could be due to the fact that we have not done extensive hyperparameter tuning for svms among our different choices of kernels the rbf kernel performed the best followed by the polynomial kernel with degree 5 then the linear kernel 
1	108715	8715	the sigmoid kernel performed the worst with an accuracy equitable to assigning a category at random this result is consistent with what we expected .however this could be due to the fact that we have not done extensive hyperparameter tuning for svms among our different choices of kernels the rbf kernel performed the best followed by the polynomial kernel with degree 5 then the linear kernel .the accuracy corresponds to the complexity of the feature space with rbf corresponding to an infinite feature space and polynomial and linear having fewer features .logistic regression	the sigmoid kernel performed the worst with an accuracy equitable to assigning a category at random this result is consistent with what we expected 
0	108716	8716	the accuracy corresponds to the complexity of the feature space with rbf corresponding to an infinite feature space and polynomial and linear having fewer features .the sigmoid kernel performed the worst with an accuracy equitable to assigning a category at random this result is consistent with what we expected .although the sigmoid corresponds to a higher dimensional feature space its corresponding kernel matrix is not guaranteed to be positive semi definite .logistic regression	the accuracy corresponds to the complexity of the feature space with rbf corresponding to an infinite feature space and polynomial and linear having fewer features 
0	108717	8717	although the sigmoid corresponds to a higher dimensional feature space its corresponding kernel matrix is not guaranteed to be positive semi definite .the accuracy corresponds to the complexity of the feature space with rbf corresponding to an infinite feature space and polynomial and linear having fewer features .therefore if the chosen parameters are not well tuned the algorithm can perform worse than random.logistic regression	although the sigmoid corresponds to a higher dimensional feature space its corresponding kernel matrix is not guaranteed to be positive semi definite 
0	108718	8718	therefore if the chosen parameters are not well tuned the algorithm can perform worse than random.although the sigmoid corresponds to a higher dimensional feature space its corresponding kernel matrix is not guaranteed to be positive semi definite .last sentence.logistic regression	therefore if the chosen parameters are not well tuned the algorithm can perform worse than random
0	108719	8719	we tested four different variations of a basic cnn using binarized and non binarized data .first sentence.a base v1 model included two convolutions and two max pool layers from which we then iteratively simplified as shown in we find that the best performing cnn on the larger classification task of 50 classes was v2 which attained a validation set accuracy of 81 83 with the confusion matrix below .convolutional neural network	we tested four different variations of a basic cnn using binarized and non binarized data 
1	108720	8720	a base v1 model included two convolutions and two max pool layers from which we then iteratively simplified as shown in we find that the best performing cnn on the larger classification task of 50 classes was v2 which attained a validation set accuracy of 81 83 with the confusion matrix below .we tested four different variations of a basic cnn using binarized and non binarized data .there is a close correlation between the training accuracy and validation accuracy .convolutional neural network	a base v1 model included two convolutions and two max pool layers from which we then iteratively simplified as shown in we find that the best performing cnn on the larger classification task of 50 classes was v2 which attained a validation set accuracy of 81 83 with the confusion matrix below 
0	108721	8721	there is a close correlation between the training accuracy and validation accuracy .a base v1 model included two convolutions and two max pool layers from which we then iteratively simplified as shown in we find that the best performing cnn on the larger classification task of 50 classes was v2 which attained a validation set accuracy of 81 83 with the confusion matrix below .testing this v2 model on the test set we obtained a final accuracy score of 81 87 .convolutional neural network	there is a close correlation between the training accuracy and validation accuracy 
0	108722	8722	testing this v2 model on the test set we obtained a final accuracy score of 81 87 .there is a close correlation between the training accuracy and validation accuracy .this shows that our trained model is able to generalize well to unseen data in general simple cnns performed well for our task .convolutional neural network	testing this v2 model on the test set we obtained a final accuracy score of 81 87 
1	108723	8723	this shows that our trained model is able to generalize well to unseen data in general simple cnns performed well for our task .testing this v2 model on the test set we obtained a final accuracy score of 81 87 .with fewer classes the accuracy is roughly the same when taking away layers but the training time decreases .convolutional neural network	this shows that our trained model is able to generalize well to unseen data in general simple cnns performed well for our task 
0	108724	8724	with fewer classes the accuracy is roughly the same when taking away layers but the training time decreases .this shows that our trained model is able to generalize well to unseen data in general simple cnns performed well for our task .with more classes accuracy decreases with fewer layers as expected but the lowest accuracy is still significantly greater than the accuracy found using logistic regression .convolutional neural network	with fewer classes the accuracy is roughly the same when taking away layers but the training time decreases 
1	108725	8725	with more classes accuracy decreases with fewer layers as expected but the lowest accuracy is still significantly greater than the accuracy found using logistic regression .with fewer classes the accuracy is roughly the same when taking away layers but the training time decreases .last sentence.convolutional neural network	with more classes accuracy decreases with fewer layers as expected but the lowest accuracy is still significantly greater than the accuracy found using logistic regression 
1	108726	8726	we ran transfer learning with weights pre trained on the imagenet dataset with four different architectures vgg a model proposed in the imagenet 2013 challenge which was widely used due to its simple architecture consisting of only repeated units of convolutional layers followed by max pooling the results of transfer learning experiments are discussed below we noticed however that the best performing model on the quickdraw dataset vgg is also the one with the simplest architecture .first sentence.this suggests that more complex architectures such as the inception module used in inception v3 or the residual block used in resnet50 may not be beneficial for the problem of doodle classification .transfer learning	we ran transfer learning with weights pre trained on the imagenet dataset with four different architectures vgg a model proposed in the imagenet 2013 challenge which was widely used due to its simple architecture consisting of only repeated units of convolutional layers followed by max pooling the results of transfer learning experiments are discussed below we noticed however that the best performing model on the quickdraw dataset vgg is also the one with the simplest architecture 
1	108727	8727	this suggests that more complex architectures such as the inception module used in inception v3 or the residual block used in resnet50 may not be beneficial for the problem of doodle classification .we ran transfer learning with weights pre trained on the imagenet dataset with four different architectures vgg a model proposed in the imagenet 2013 challenge which was widely used due to its simple architecture consisting of only repeated units of convolutional layers followed by max pooling the results of transfer learning experiments are discussed below we noticed however that the best performing model on the quickdraw dataset vgg is also the one with the simplest architecture .since doodles are simple drawings only using the classic convolutional and max pool layers may be the best .transfer learning	this suggests that more complex architectures such as the inception module used in inception v3 or the residual block used in resnet50 may not be beneficial for the problem of doodle classification 
0	108728	8728	since doodles are simple drawings only using the classic convolutional and max pool layers may be the best .this suggests that more complex architectures such as the inception module used in inception v3 or the residual block used in resnet50 may not be beneficial for the problem of doodle classification .this also corroborates our earlier finding with cnn where our simple cnn built from the ground up did well on the quickdraw dataset .transfer learning	since doodles are simple drawings only using the classic convolutional and max pool layers may be the best 
0	108729	8729	this also corroborates our earlier finding with cnn where our simple cnn built from the ground up did well on the quickdraw dataset .since doodles are simple drawings only using the classic convolutional and max pool layers may be the best .in terms of training time mobilenet performed the best .transfer learning	this also corroborates our earlier finding with cnn where our simple cnn built from the ground up did well on the quickdraw dataset 
0	108730	8730	in terms of training time mobilenet performed the best .this also corroborates our earlier finding with cnn where our simple cnn built from the ground up did well on the quickdraw dataset .this is expected since the model is optimized for efficiency and has the smallest number of parameters .transfer learning	in terms of training time mobilenet performed the best 
0	108731	8731	this is expected since the model is optimized for efficiency and has the smallest number of parameters .in terms of training time mobilenet performed the best .overall the trend in training time follows the number of parameters in the base model which is expected .transfer learning	this is expected since the model is optimized for efficiency and has the smallest number of parameters 
0	108732	8732	overall the trend in training time follows the number of parameters in the base model which is expected .this is expected since the model is optimized for efficiency and has the smallest number of parameters .the exception of inception v3 whose training time is the largest despite it having the second largest number of parameters of all the four models .transfer learning	overall the trend in training time follows the number of parameters in the base model which is expected 
0	108733	8733	the exception of inception v3 whose training time is the largest despite it having the second largest number of parameters of all the four models .overall the trend in training time follows the number of parameters in the base model which is expected .this was due to us additionally fine tuning the parameters of the two top most layers of inception v3 .transfer learning	the exception of inception v3 whose training time is the largest despite it having the second largest number of parameters of all the four models 
0	108734	8734	this was due to us additionally fine tuning the parameters of the two top most layers of inception v3 .the exception of inception v3 whose training time is the largest despite it having the second largest number of parameters of all the four models .this was because transfer learning with inception v3 was performing poorly in terms of accuracy and we wanted to see if further tuning hyper parameters would help .transfer learning	this was due to us additionally fine tuning the parameters of the two top most layers of inception v3 
1	108735	8735	this was because transfer learning with inception v3 was performing poorly in terms of accuracy and we wanted to see if further tuning hyper parameters would help .this was due to us additionally fine tuning the parameters of the two top most layers of inception v3 .overall we find that optimizing the model by reducing the number of parameters will help with reducing training time which further supports our initial push for simplifying the models to achieve higher efficiency .transfer learning	this was because transfer learning with inception v3 was performing poorly in terms of accuracy and we wanted to see if further tuning hyper parameters would help 
1	108736	8736	overall we find that optimizing the model by reducing the number of parameters will help with reducing training time which further supports our initial push for simplifying the models to achieve higher efficiency .this was because transfer learning with inception v3 was performing poorly in terms of accuracy and we wanted to see if further tuning hyper parameters would help .last sentence.transfer learning	overall we find that optimizing the model by reducing the number of parameters will help with reducing training time which further supports our initial push for simplifying the models to achieve higher efficiency 
0	108737	8737	our project aimed to recognize the meaning of doodles a critical first task in order to build any system that uses hand drawn images for communication .first sentence.we focused on doodle recognition with an emphasis on efficiency in conjunction with accuracy .conclusion	our project aimed to recognize the meaning of doodles a critical first task in order to build any system that uses hand drawn images for communication 
0	108738	8738	we focused on doodle recognition with an emphasis on efficiency in conjunction with accuracy .our project aimed to recognize the meaning of doodles a critical first task in order to build any system that uses hand drawn images for communication .after implementing logistic regression svms cnns and transfer learning and analyzing our results we found that a simplified cnn was best for the task balancing both accuracy and training time .conclusion	we focused on doodle recognition with an emphasis on efficiency in conjunction with accuracy 
1	108739	8739	after implementing logistic regression svms cnns and transfer learning and analyzing our results we found that a simplified cnn was best for the task balancing both accuracy and training time .we focused on doodle recognition with an emphasis on efficiency in conjunction with accuracy .we also found that for simpler images such as doodles using classic architectures such as a combination of convolutional and max pool layers can outperform complex architectures for future work we would further develop our most promising approach by performing more extensive experiments to determine the effect of each layer in the cnn .conclusion	after implementing logistic regression svms cnns and transfer learning and analyzing our results we found that a simplified cnn was best for the task balancing both accuracy and training time 
1	108740	8740	we also found that for simpler images such as doodles using classic architectures such as a combination of convolutional and max pool layers can outperform complex architectures for future work we would further develop our most promising approach by performing more extensive experiments to determine the effect of each layer in the cnn .after implementing logistic regression svms cnns and transfer learning and analyzing our results we found that a simplified cnn was best for the task balancing both accuracy and training time .last sentence.conclusion	we also found that for simpler images such as doodles using classic architectures such as a combination of convolutional and max pool layers can outperform complex architectures for future work we would further develop our most promising approach by performing more extensive experiments to determine the effect of each layer in the cnn 
1	108741	8741	we would also explore using transfer learning as a fixed feature extractor for logistic regression our fastest model .first sentence.given more time we would also love to explore working on efficiency in conjunction with smaller datasets each team member contributed equally to this project .contributions	we would also explore using transfer learning as a fixed feature extractor for logistic regression our fastest model 
0	108742	8742	given more time we would also love to explore working on efficiency in conjunction with smaller datasets each team member contributed equally to this project .we would also explore using transfer learning as a fixed feature extractor for logistic regression our fastest model .last sentence.contributions	given more time we would also love to explore working on efficiency in conjunction with smaller datasets each team member contributed equally to this project 
0	108743	8743	in this paper we are exploring the generation of depthmaps from a sequence of images .first sentence.compared to similar projects in the field we have decided to incorporate both spatial cnn and temporal lstm aspects in our model by creating convlstm cells .abstract	in this paper we are exploring the generation of depthmaps from a sequence of images 
0	108744	8744	compared to similar projects in the field we have decided to incorporate both spatial cnn and temporal lstm aspects in our model by creating convlstm cells .in this paper we are exploring the generation of depthmaps from a sequence of images .these are used in a u net encoder decoder architecture .abstract	compared to similar projects in the field we have decided to incorporate both spatial cnn and temporal lstm aspects in our model by creating convlstm cells 
0	108745	8745	these are used in a u net encoder decoder architecture .compared to similar projects in the field we have decided to incorporate both spatial cnn and temporal lstm aspects in our model by creating convlstm cells .the results indicate some potential in such an approach .abstract	these are used in a u net encoder decoder architecture 
0	108746	8746	the results indicate some potential in such an approach .these are used in a u net encoder decoder architecture .last sentence.abstract	the results indicate some potential in such an approach 
0	108747	8747	hardware progress has enabled solutions which were historically computationally intractable .first sentence.this is particularly true in video analysis .introduction	hardware progress has enabled solutions which were historically computationally intractable 
0	108748	8748	this is particularly true in video analysis .hardware progress has enabled solutions which were historically computationally intractable .this technological advance has opened a new frontier of problems .introduction	this is particularly true in video analysis 
0	108749	8749	this technological advance has opened a new frontier of problems .this is particularly true in video analysis .within this expanse we have chosen the classic problem of depth inference from images .introduction	this technological advance has opened a new frontier of problems 
0	108750	8750	within this expanse we have chosen the classic problem of depth inference from images .this technological advance has opened a new frontier of problems .specifically given a sequence of images captured over time we output depth maps corresponding one to one with the input sequence .introduction	within this expanse we have chosen the classic problem of depth inference from images 
1	108751	8751	specifically given a sequence of images captured over time we output depth maps corresponding one to one with the input sequence .within this expanse we have chosen the classic problem of depth inference from images .as a spatiotemporal problem we were motivated to model it with convolutions spatial and lstms temporal the input to our algorithm is a sequence of images .introduction	specifically given a sequence of images captured over time we output depth maps corresponding one to one with the input sequence 
1	108752	8752	as a spatiotemporal problem we were motivated to model it with convolutions spatial and lstms temporal the input to our algorithm is a sequence of images .specifically given a sequence of images captured over time we output depth maps corresponding one to one with the input sequence .we then use a neural network u net encoder decoder architecture with bi convlstm cells for encoding and convolutions and transconvolutions to decode to output a predicted depth map sequence .introduction	as a spatiotemporal problem we were motivated to model it with convolutions spatial and lstms temporal the input to our algorithm is a sequence of images 
1	108753	8753	we then use a neural network u net encoder decoder architecture with bi convlstm cells for encoding and convolutions and transconvolutions to decode to output a predicted depth map sequence .as a spatiotemporal problem we were motivated to model it with convolutions spatial and lstms temporal the input to our algorithm is a sequence of images .as we deal with sequences of images this process is many to many where for each input image we output one depth map .introduction	we then use a neural network u net encoder decoder architecture with bi convlstm cells for encoding and convolutions and transconvolutions to decode to output a predicted depth map sequence 
1	108754	8754	as we deal with sequences of images this process is many to many where for each input image we output one depth map .we then use a neural network u net encoder decoder architecture with bi convlstm cells for encoding and convolutions and transconvolutions to decode to output a predicted depth map sequence .solutions to the above problem would enable 3d world generation from simple video input with applications from vr to robotics .introduction	as we deal with sequences of images this process is many to many where for each input image we output one depth map 
0	108755	8755	solutions to the above problem would enable 3d world generation from simple video input with applications from vr to robotics .as we deal with sequences of images this process is many to many where for each input image we output one depth map .while there are hardware approaches to depth determination problems such as lidar or multiple lenses software solutions provide flexibility in their application .introduction	solutions to the above problem would enable 3d world generation from simple video input with applications from vr to robotics 
0	108756	8756	while there are hardware approaches to depth determination problems such as lidar or multiple lenses software solutions provide flexibility in their application .solutions to the above problem would enable 3d world generation from simple video input with applications from vr to robotics .last sentence.introduction	while there are hardware approaches to depth determination problems such as lidar or multiple lenses software solutions provide flexibility in their application 
0	108757	8757	after researching this initial problem in depth we became familiar with literature on depth maps their algorithms and datasets .first sentence.this presented itself as a sensible path forward as it seemed simpler and better scoped .in depth	after researching this initial problem in depth we became familiar with literature on depth maps their algorithms and datasets 
0	108758	8758	this presented itself as a sensible path forward as it seemed simpler and better scoped .after researching this initial problem in depth we became familiar with literature on depth maps their algorithms and datasets .this area is a classic one with not only history but ongoing and recent progress .in depth	this presented itself as a sensible path forward as it seemed simpler and better scoped 
0	108759	8759	this area is a classic one with not only history but ongoing and recent progress .this presented itself as a sensible path forward as it seemed simpler and better scoped .concerning depth maps there are various families of problems single image to depth map depth map alignments from sparse to dense but given the background research we d done on the image depth map sequence we were naturally drawn to the most similar problem from a sequence of images generate a sequence of depth maps there are many reasons to be excited about such a problem especially as the interest for spatiotemporal models is booming .in depth	this area is a classic one with not only history but ongoing and recent progress 
1	108760	8760	concerning depth maps there are various families of problems single image to depth map depth map alignments from sparse to dense but given the background research we d done on the image depth map sequence we were naturally drawn to the most similar problem from a sequence of images generate a sequence of depth maps there are many reasons to be excited about such a problem especially as the interest for spatiotemporal models is booming .this area is a classic one with not only history but ongoing and recent progress .for us however we wanted to learn about rnns and cnns and as space time lends itself to natural conceptions of convolutions and recurrent networks we proceeded down that path quite excited to apply modern rnn and cnn techniques we were both disappointed and relieved to find extremely relevant literature depthnet while there some people praise cnn to the detriment of rnn we wanted to explore this avenue further .in depth	concerning depth maps there are various families of problems single image to depth map depth map alignments from sparse to dense but given the background research we d done on the image depth map sequence we were naturally drawn to the most similar problem from a sequence of images generate a sequence of depth maps there are many reasons to be excited about such a problem especially as the interest for spatiotemporal models is booming 
1	108761	8761	for us however we wanted to learn about rnns and cnns and as space time lends itself to natural conceptions of convolutions and recurrent networks we proceeded down that path quite excited to apply modern rnn and cnn techniques we were both disappointed and relieved to find extremely relevant literature depthnet while there some people praise cnn to the detriment of rnn we wanted to explore this avenue further .concerning depth maps there are various families of problems single image to depth map depth map alignments from sparse to dense but given the background research we d done on the image depth map sequence we were naturally drawn to the most similar problem from a sequence of images generate a sequence of depth maps there are many reasons to be excited about such a problem especially as the interest for spatiotemporal models is booming .in pursuit of this approach we have our own opinion as will be discussed at the end .in depth	for us however we wanted to learn about rnns and cnns and as space time lends itself to natural conceptions of convolutions and recurrent networks we proceeded down that path quite excited to apply modern rnn and cnn techniques we were both disappointed and relieved to find extremely relevant literature depthnet while there some people praise cnn to the detriment of rnn we wanted to explore this avenue further 
0	108762	8762	in pursuit of this approach we have our own opinion as will be discussed at the end .for us however we wanted to learn about rnns and cnns and as space time lends itself to natural conceptions of convolutions and recurrent networks we proceeded down that path quite excited to apply modern rnn and cnn techniques we were both disappointed and relieved to find extremely relevant literature depthnet while there some people praise cnn to the detriment of rnn we wanted to explore this avenue further .last sentence.in depth	in pursuit of this approach we have our own opinion as will be discussed at the end 
1	108763	8763	it is fitting to begin with paper that introduced the core unit of our model convolutional lstm a machine learning approach for precipitation nowcasting we chose depthnet there are many great people and great ideas .first sentence.last sentence.related work	it is fitting to begin with paper that introduced the core unit of our model convolutional lstm a machine learning approach for precipitation nowcasting we chose depthnet there are many great people and great ideas 
1	108764	8764	in the search for a dataset with both picture and depth map we have decided to use the kitti dataset.first sentence.last sentence.descriptive overview	in the search for a dataset with both picture and depth map we have decided to use the kitti dataset
0	108765	8765	first we organized the data and store image sequences in subfolders as it seems to simplify and speed up the training.first sentence.last sentence.preprocessing	first we organized the data and store image sequences in subfolders as it seems to simplify and speed up the training
1	108766	8766	convlstms are more than a convolutional layer into an lstm layer they convolve on the hidden state and the input together .first sentence.this functional difference has led to some speculation as to the merits of one over the other where convlstms sometimes prove more effective as in very deep convolutional networks for end to end speech recognition the specific math for a convlstm is where refers to a convolution operation and to the hadamard product .convlstm bi convlstm	convlstms are more than a convolutional layer into an lstm layer they convolve on the hidden state and the input together 
1	108767	8767	this functional difference has led to some speculation as to the merits of one over the other where convlstms sometimes prove more effective as in very deep convolutional networks for end to end speech recognition the specific math for a convlstm is where refers to a convolution operation and to the hadamard product .convlstms are more than a convolutional layer into an lstm layer they convolve on the hidden state and the input together .last sentence.convlstm bi convlstm	this functional difference has led to some speculation as to the merits of one over the other where convlstms sometimes prove more effective as in very deep convolutional networks for end to end speech recognition the specific math for a convlstm is where refers to a convolution operation and to the hadamard product 
0	108768	8768	we picked this model to optimize for simplicity of approach while maintain sophistication of the model s capacity .first sentence.the u net structure is symmetrical and conceptually simple and the main complexity is within its subcomponent bi convlstm .architecture u net encoder decoder	we picked this model to optimize for simplicity of approach while maintain sophistication of the model s capacity 
0	108769	8769	the u net structure is symmetrical and conceptually simple and the main complexity is within its subcomponent bi convlstm .we picked this model to optimize for simplicity of approach while maintain sophistication of the model s capacity .this subcomponent could be tinkered without alteration to the whole .architecture u net encoder decoder	the u net structure is symmetrical and conceptually simple and the main complexity is within its subcomponent bi convlstm 
0	108770	8770	this subcomponent could be tinkered without alteration to the whole .the u net structure is symmetrical and conceptually simple and the main complexity is within its subcomponent bi convlstm .the inputs to our network are 5d tensors b n c h w where b refers to batch size n to sequence length c to channels h to height and w to width .architecture u net encoder decoder	this subcomponent could be tinkered without alteration to the whole 
0	108771	8771	the inputs to our network are 5d tensors b n c h w where b refers to batch size n to sequence length c to channels h to height and w to width .this subcomponent could be tinkered without alteration to the whole .per layer the number of filters and therefore output channels of that layer increase during the encoding phase starting from 3 rgb and decrease during the decoding phase finishing at 1 depth .architecture u net encoder decoder	the inputs to our network are 5d tensors b n c h w where b refers to batch size n to sequence length c to channels h to height and w to width 
0	108772	8772	per layer the number of filters and therefore output channels of that layer increase during the encoding phase starting from 3 rgb and decrease during the decoding phase finishing at 1 depth .the inputs to our network are 5d tensors b n c h w where b refers to batch size n to sequence length c to channels h to height and w to width .we use relu activation functions for each encoding layer and at the last step of the decoding phase .architecture u net encoder decoder	per layer the number of filters and therefore output channels of that layer increase during the encoding phase starting from 3 rgb and decrease during the decoding phase finishing at 1 depth 
0	108773	8773	we use relu activation functions for each encoding layer and at the last step of the decoding phase .per layer the number of filters and therefore output channels of that layer increase during the encoding phase starting from 3 rgb and decrease during the decoding phase finishing at 1 depth .skip connections in the u net structure pass forward outputs to later layers concatenating with the output of the directly previous layer .architecture u net encoder decoder	we use relu activation functions for each encoding layer and at the last step of the decoding phase 
0	108774	8774	skip connections in the u net structure pass forward outputs to later layers concatenating with the output of the directly previous layer .we use relu activation functions for each encoding layer and at the last step of the decoding phase .see figures 2 3 for greater details .architecture u net encoder decoder	skip connections in the u net structure pass forward outputs to later layers concatenating with the output of the directly previous layer 
0	108775	8775	see figures 2 3 for greater details .skip connections in the u net structure pass forward outputs to later layers concatenating with the output of the directly previous layer .last sentence.architecture u net encoder decoder	see figures 2 3 for greater details 
0	108776	8776	for this project we are using two separate machines with both a recent nvidia gpu 1080ti and p100 .first sentence.our current implementation of the model uses pytorch 0 4 1.experiments	for this project we are using two separate machines with both a recent nvidia gpu 1080ti and p100 
0	108777	8777	our current implementation of the model uses pytorch 0 4 1.for this project we are using two separate machines with both a recent nvidia gpu 1080ti and p100 .last sentence.experiments	our current implementation of the model uses pytorch 0 4 1
0	108778	8778	we used multiple metrics for training and evaluation purposes based on properties distinct to each function .first sentence.specifically rmse irmse mae imae and a custom scale invariant loss .metrics	we used multiple metrics for training and evaluation purposes based on properties distinct to each function 
0	108779	8779	specifically rmse irmse mae imae and a custom scale invariant loss .we used multiple metrics for training and evaluation purposes based on properties distinct to each function .rmsewhere d i log y i log y i for the i th pixel n the number of pixels and 0 5 succinctly rmse is standard and easy to implement while providing comparison against other models .metrics	specifically rmse irmse mae imae and a custom scale invariant loss 
1	108780	8780	rmsewhere d i log y i log y i for the i th pixel n the number of pixels and 0 5 succinctly rmse is standard and easy to implement while providing comparison against other models .specifically rmse irmse mae imae and a custom scale invariant loss .it is distinct from mae in that rmse significantly larger than mae indicates large variance of error distribution frequency finally a1 a2 and a3 metrics are accuracies that represent the percent of pixels that fall within a threshold ratio of inferred depth value and ground truth depth value .metrics	rmsewhere d i log y i log y i for the i th pixel n the number of pixels and 0 5 succinctly rmse is standard and easy to implement while providing comparison against other models 
1	108781	8781	it is distinct from mae in that rmse significantly larger than mae indicates large variance of error distribution frequency finally a1 a2 and a3 metrics are accuracies that represent the percent of pixels that fall within a threshold ratio of inferred depth value and ground truth depth value .rmsewhere d i log y i log y i for the i th pixel n the number of pixels and 0 5 succinctly rmse is standard and easy to implement while providing comparison against other models .a refers to a base and 1 2 3 refer to powers of that base ie a3 is the most lenient and a1 the strictest .metrics	it is distinct from mae in that rmse significantly larger than mae indicates large variance of error distribution frequency finally a1 a2 and a3 metrics are accuracies that represent the percent of pixels that fall within a threshold ratio of inferred depth value and ground truth depth value 
0	108782	8782	a refers to a base and 1 2 3 refer to powers of that base ie a3 is the most lenient and a1 the strictest .it is distinct from mae in that rmse significantly larger than mae indicates large variance of error distribution frequency finally a1 a2 and a3 metrics are accuracies that represent the percent of pixels that fall within a threshold ratio of inferred depth value and ground truth depth value .these accuracies are independent of image size and therefore ideal for baseline comparison .metrics	a refers to a base and 1 2 3 refer to powers of that base ie a3 is the most lenient and a1 the strictest 
0	108783	8783	these accuracies are independent of image size and therefore ideal for baseline comparison .a refers to a base and 1 2 3 refer to powers of that base ie a3 is the most lenient and a1 the strictest .also whereas losses provide an unintuitive metric of goodness and progress accuracy is more comprehensible .metrics	these accuracies are independent of image size and therefore ideal for baseline comparison 
0	108784	8784	also whereas losses provide an unintuitive metric of goodness and progress accuracy is more comprehensible .these accuracies are independent of image size and therefore ideal for baseline comparison .multiple a values indicate the distribution of inferences .metrics	also whereas losses provide an unintuitive metric of goodness and progress accuracy is more comprehensible 
0	108785	8785	multiple a values indicate the distribution of inferences .also whereas losses provide an unintuitive metric of goodness and progress accuracy is more comprehensible .last sentence.metrics	multiple a values indicate the distribution of inferences 
0	108786	8786	we are comparing ourselves most directly to depthnet and other kitti competitors with the corresponding loss measures .first sentence.the current two leaders are dl 61 dorn and dl sord sq .baseline measures	we are comparing ourselves most directly to depthnet and other kitti competitors with the corresponding loss measures 
0	108787	8787	the current two leaders are dl 61 dorn and dl sord sq .we are comparing ourselves most directly to depthnet and other kitti competitors with the corresponding loss measures .last sentence.baseline measures	the current two leaders are dl 61 dorn and dl sord sq 
0	108788	8788	comparing sequences of length 1 to 6 we see that the 6 outperformed the 1 on every metric .first sentence.this implies that the lstm does provide utility to analysis and that over time information holds clues to depth .discussion	comparing sequences of length 1 to 6 we see that the 6 outperformed the 1 on every metric 
0	108789	8789	this implies that the lstm does provide utility to analysis and that over time information holds clues to depth .comparing sequences of length 1 to 6 we see that the 6 outperformed the 1 on every metric .this somewhat justifies the theory behind the model and is consistent with the results of previous teams such as depthnet .discussion	this implies that the lstm does provide utility to analysis and that over time information holds clues to depth 
0	108790	8790	this somewhat justifies the theory behind the model and is consistent with the results of previous teams such as depthnet .this implies that the lstm does provide utility to analysis and that over time information holds clues to depth .last sentence.discussion	this somewhat justifies the theory behind the model and is consistent with the results of previous teams such as depthnet 
0	108791	8791	we presented a different approach to image to depthmap implementation .first sentence.using jointly a u net architecture and convlstm cells we tried to incorporate both spacial and temporal elements to insure consistency when generating depth maps for sequences of images i e .conclusion and future work	we presented a different approach to image to depthmap implementation 
1	108792	8792	using jointly a u net architecture and convlstm cells we tried to incorporate both spacial and temporal elements to insure consistency when generating depth maps for sequences of images i e .we presented a different approach to image to depthmap implementation .video there are several areas to continue our work here .conclusion and future work	using jointly a u net architecture and convlstm cells we tried to incorporate both spacial and temporal elements to insure consistency when generating depth maps for sequences of images i e 
0	108793	8793	video there are several areas to continue our work here .using jointly a u net architecture and convlstm cells we tried to incorporate both spacial and temporal elements to insure consistency when generating depth maps for sequences of images i e .we d first like to train for an extra week and see if we continue to progress towards convergence .conclusion and future work	video there are several areas to continue our work here 
0	108794	8794	we d first like to train for an extra week and see if we continue to progress towards convergence .video there are several areas to continue our work here .first increasing sequence length .conclusion and future work	we d first like to train for an extra week and see if we continue to progress towards convergence 
0	108795	8795	first increasing sequence length .we d first like to train for an extra week and see if we continue to progress towards convergence .while we limited ourselves to a length of 6 we are curious as to the impact a sequence of 600 would compare .conclusion and future work	first increasing sequence length 
0	108796	8796	while we limited ourselves to a length of 6 we are curious as to the impact a sequence of 600 would compare .first increasing sequence length .second data processing .conclusion and future work	while we limited ourselves to a length of 6 we are curious as to the impact a sequence of 600 would compare 
0	108797	8797	second data processing .while we limited ourselves to a length of 6 we are curious as to the impact a sequence of 600 would compare .there are many transformation alternatives to be played with .conclusion and future work	second data processing 
0	108798	8798	there are many transformation alternatives to be played with .second data processing .we especially would like to train on bigger image sizes if we had more time and compute .conclusion and future work	there are many transformation alternatives to be played with 
0	108799	8799	we especially would like to train on bigger image sizes if we had more time and compute .there are many transformation alternatives to be played with .there are also multiple ways to iterate over the data as for loss functions we used many of them for evaluation and it d be interesting to explore if any of those is better than our custom loss for guiding the gradient and training .conclusion and future work	we especially would like to train on bigger image sizes if we had more time and compute 
1	108800	8800	there are also multiple ways to iterate over the data as for loss functions we used many of them for evaluation and it d be interesting to explore if any of those is better than our custom loss for guiding the gradient and training .we especially would like to train on bigger image sizes if we had more time and compute .beyond that we would like to play with kernel size and the number of filters per layer as there are interesting questions in the optimal number per layer .conclusion and future work	there are also multiple ways to iterate over the data as for loss functions we used many of them for evaluation and it d be interesting to explore if any of those is better than our custom loss for guiding the gradient and training 
1	108801	8801	beyond that we would like to play with kernel size and the number of filters per layer as there are interesting questions in the optimal number per layer .there are also multiple ways to iterate over the data as for loss functions we used many of them for evaluation and it d be interesting to explore if any of those is better than our custom loss for guiding the gradient and training .expanding in creativity how would increasing the number of encodingdecoding layers affect performance .conclusion and future work	beyond that we would like to play with kernel size and the number of filters per layer as there are interesting questions in the optimal number per layer 
0	108802	8802	expanding in creativity how would increasing the number of encodingdecoding layers affect performance .beyond that we would like to play with kernel size and the number of filters per layer as there are interesting questions in the optimal number per layer .we did not nearly approach overfitting problems .conclusion and future work	expanding in creativity how would increasing the number of encodingdecoding layers affect performance 
0	108803	8803	we did not nearly approach overfitting problems .expanding in creativity how would increasing the number of encodingdecoding layers affect performance .more dramatically what are the effects of u net .conclusion and future work	we did not nearly approach overfitting problems 
0	108804	8804	more dramatically what are the effects of u net .we did not nearly approach overfitting problems .if we were to remove the skip connections how would we perform .conclusion and future work	more dramatically what are the effects of u net 
0	108805	8805	if we were to remove the skip connections how would we perform .more dramatically what are the effects of u net .other advanced techniques invite exploration .conclusion and future work	if we were to remove the skip connections how would we perform 
0	108806	8806	other advanced techniques invite exploration .if we were to remove the skip connections how would we perform .pyramids for convolutions attention for lstm drop the rnn and just use a 3d convolution perhaps use a 3d convolution inside a convlstm multiple timesteps to each lstm timestep .conclusion and future work	other advanced techniques invite exploration 
1	108807	8807	pyramids for convolutions attention for lstm drop the rnn and just use a 3d convolution perhaps use a 3d convolution inside a convlstm multiple timesteps to each lstm timestep .other advanced techniques invite exploration .there are many possibilities .conclusion and future work	pyramids for convolutions attention for lstm drop the rnn and just use a 3d convolution perhaps use a 3d convolution inside a convlstm multiple timesteps to each lstm timestep 
0	108808	8808	there are many possibilities .pyramids for convolutions attention for lstm drop the rnn and just use a 3d convolution perhaps use a 3d convolution inside a convlstm multiple timesteps to each lstm timestep .last sentence.conclusion and future work	there are many possibilities 
0	108809	8809	while everyone has done some of everything the largest contributions from each to any particulars may be described as follows .first sentence.john has researched the literature designed the models and assisted manuel with several of the models and metrics .appendices contributions	while everyone has done some of everything the largest contributions from each to any particulars may be described as follows 
0	108810	8810	john has researched the literature designed the models and assisted manuel with several of the models and metrics .while everyone has done some of everything the largest contributions from each to any particulars may be described as follows .manuel implemented the final model and metrics and ran multiple training runs .appendices contributions	john has researched the literature designed the models and assisted manuel with several of the models and metrics 
0	108811	8811	manuel implemented the final model and metrics and ran multiple training runs .john has researched the literature designed the models and assisted manuel with several of the models and metrics .geoffrey has set up the infrastructure for preprocessing data with various transforms saving and loading models and worked on metrics .appendices contributions	manuel implemented the final model and metrics and ran multiple training runs 
0	108812	8812	geoffrey has set up the infrastructure for preprocessing data with various transforms saving and loading models and worked on metrics .manuel implemented the final model and metrics and ran multiple training runs .last sentence.appendices contributions	geoffrey has set up the infrastructure for preprocessing data with various transforms saving and loading models and worked on metrics 
1	108813	8813	existing studies have shown that excessive alcohol drinking can impact the normal structural development of brain anatomy during adolescence in this project our goal is to design a classification model to predict if a subject is a heavy drinker based on their resting state fmri data stored as blood oxygen level dependent bold signals .first sentence.after pre processing the inputs to our models are parcellated fmri data as bold signals as well as patient demographic information age sex scanner type .introduction	existing studies have shown that excessive alcohol drinking can impact the normal structural development of brain anatomy during adolescence in this project our goal is to design a classification model to predict if a subject is a heavy drinker based on their resting state fmri data stored as blood oxygen level dependent bold signals 
1	108814	8814	after pre processing the inputs to our models are parcellated fmri data as bold signals as well as patient demographic information age sex scanner type .existing studies have shown that excessive alcohol drinking can impact the normal structural development of brain anatomy during adolescence in this project our goal is to design a classification model to predict if a subject is a heavy drinker based on their resting state fmri data stored as blood oxygen level dependent bold signals .we then used each model logistic regression svm deep learning to output a predicted classification of the patient as a heavy drinker versus non heavy drinker .introduction	after pre processing the inputs to our models are parcellated fmri data as bold signals as well as patient demographic information age sex scanner type 
1	108815	8815	we then used each model logistic regression svm deep learning to output a predicted classification of the patient as a heavy drinker versus non heavy drinker .after pre processing the inputs to our models are parcellated fmri data as bold signals as well as patient demographic information age sex scanner type .last sentence.introduction	we then used each model logistic regression svm deep learning to output a predicted classification of the patient as a heavy drinker versus non heavy drinker 
0	108816	8816	as more and more neuroimaging databases become publicly available machine learning models are becoming increasingly useful in functional neuroimaging classification .first sentence.over the past decade there have been several attempts to leverage machine learning on fmri data to classify neurodegenerative diseases or different tasks .related work	as more and more neuroimaging databases become publicly available machine learning models are becoming increasingly useful in functional neuroimaging classification 
0	108817	8817	over the past decade there have been several attempts to leverage machine learning on fmri data to classify neurodegenerative diseases or different tasks .as more and more neuroimaging databases become publicly available machine learning models are becoming increasingly useful in functional neuroimaging classification .these fmri classifications are often compared to traditional manual classification methods using clinical behavioral data such as the dsm iv criteria for psychological disorders .related work	over the past decade there have been several attempts to leverage machine learning on fmri data to classify neurodegenerative diseases or different tasks 
1	108818	8818	these fmri classifications are often compared to traditional manual classification methods using clinical behavioral data such as the dsm iv criteria for psychological disorders .over the past decade there have been several attempts to leverage machine learning on fmri data to classify neurodegenerative diseases or different tasks .last sentence.related work	these fmri classifications are often compared to traditional manual classification methods using clinical behavioral data such as the dsm iv criteria for psychological disorders 
1	108819	8819	the earliest experiments we found mostly relied on support vector machines svms or linear classifiers which achieved accuracies between 69 92 .first sentence.chanel et al .svm and linear classifiers	the earliest experiments we found mostly relied on support vector machines svms or linear classifiers which achieved accuracies between 69 92 
0	108820	8820	chanel et al .the earliest experiments we found mostly relied on support vector machines svms or linear classifiers which achieved accuracies between 69 92 .used svms for classifying austistic spectrum disorder asd from both task based and resting based fmri.svm and linear classifiers	chanel et al 
1	108821	8821	used svms for classifying austistic spectrum disorder asd from both task based and resting based fmri.chanel et al .last sentence.svm and linear classifiers	used svms for classifying austistic spectrum disorder asd from both task based and resting based fmri
1	108822	8822	recent and current techniques for classifying on fmri data seem to take more advantage of recurrent neural networks rnns which naturally lends itself to time series data .first sentence.chen and hu developed a rnn based model that was able to identify individuals by their fmri functional brain fingerprints with up to 94 accuracy.recurrent neural networks	recent and current techniques for classifying on fmri data seem to take more advantage of recurrent neural networks rnns which naturally lends itself to time series data 
1	108823	8823	chen and hu developed a rnn based model that was able to identify individuals by their fmri functional brain fingerprints with up to 94 accuracy.recent and current techniques for classifying on fmri data seem to take more advantage of recurrent neural networks rnns which naturally lends itself to time series data .last sentence.recurrent neural networks	chen and hu developed a rnn based model that was able to identify individuals by their fmri functional brain fingerprints with up to 94 accuracy
1	108824	8824	for alcohol abuse classification little work has been done with machine learning models on fmri data with most analyses being done statistically or using basic regression models these papers vary on their feature selection and validation methods .first sentence.based on the results presented by these works a mixed cnn rnn based approach seemed promising due to the nature of our data which consists of resting state fmri only .alcohol abuse and our problem	for alcohol abuse classification little work has been done with machine learning models on fmri data with most analyses being done statistically or using basic regression models these papers vary on their feature selection and validation methods 
1	108825	8825	based on the results presented by these works a mixed cnn rnn based approach seemed promising due to the nature of our data which consists of resting state fmri only .for alcohol abuse classification little work has been done with machine learning models on fmri data with most analyses being done statistically or using basic regression models these papers vary on their feature selection and validation methods .as a result we chose to build a cnn rnn based model to classify heavy drinkers as well as compare the performance against other neural networkbased models as well as regression and svm based models .alcohol abuse and our problem	based on the results presented by these works a mixed cnn rnn based approach seemed promising due to the nature of our data which consists of resting state fmri only 
1	108826	8826	as a result we chose to build a cnn rnn based model to classify heavy drinkers as well as compare the performance against other neural networkbased models as well as regression and svm based models .based on the results presented by these works a mixed cnn rnn based approach seemed promising due to the nature of our data which consists of resting state fmri only .last sentence.alcohol abuse and our problem	as a result we chose to build a cnn rnn based model to classify heavy drinkers as well as compare the performance against other neural networkbased models as well as regression and svm based models 
0	108827	8827	our original dataset consists of fmri scans from 715 adolescents young adults from the ncanda database .first sentence.the scans measure the bold signal from each brain region per second over t 269 timesteps 2 2 seconds frame .data	our original dataset consists of fmri scans from 715 adolescents young adults from the ncanda database 
0	108828	8828	the scans measure the bold signal from each brain region per second over t 269 timesteps 2 2 seconds frame .our original dataset consists of fmri scans from 715 adolescents young adults from the ncanda database .preliminary visualization of the data such as time series plots of individual patients.data	the scans measure the bold signal from each brain region per second over t 269 timesteps 2 2 seconds frame 
0	108829	8829	preliminary visualization of the data such as time series plots of individual patients.the scans measure the bold signal from each brain region per second over t 269 timesteps 2 2 seconds frame .last sentence.data	preliminary visualization of the data such as time series plots of individual patients
0	108830	8830	fundamentally bold signals were normalized by z score to reduce variability between patients .first sentence.moreover there was significant class imbalance within the dataset 122 17 heavy drinkers out of 715 .pre processing and derived features	fundamentally bold signals were normalized by z score to reduce variability between patients 
0	108831	8831	moreover there was significant class imbalance within the dataset 122 17 heavy drinkers out of 715 .fundamentally bold signals were normalized by z score to reduce variability between patients .after setting class balance 50 50 our dataset size was reduced to 244 patients because we were limited by the size of our dataset it was imperative for us to make smart use of our data .pre processing and derived features	moreover there was significant class imbalance within the dataset 122 17 heavy drinkers out of 715 
0	108832	8832	after setting class balance 50 50 our dataset size was reduced to 244 patients because we were limited by the size of our dataset it was imperative for us to make smart use of our data .moreover there was significant class imbalance within the dataset 122 17 heavy drinkers out of 715 .because neural networks have a high tendency to overfit we needed a proper dev set in addition to train test .pre processing and derived features	after setting class balance 50 50 our dataset size was reduced to 244 patients because we were limited by the size of our dataset it was imperative for us to make smart use of our data 
1	108833	8833	because neural networks have a high tendency to overfit we needed a proper dev set in addition to train test .after setting class balance 50 50 our dataset size was reduced to 244 patients because we were limited by the size of our dataset it was imperative for us to make smart use of our data .we took a random sample of 5 of our dataset 13 taken 231 leftover and set it aside as our dev set which will not be used during kfold cross validation of the test training data .pre processing and derived features	because neural networks have a high tendency to overfit we needed a proper dev set in addition to train test 
0	108834	8834	we took a random sample of 5 of our dataset 13 taken 231 leftover and set it aside as our dev set which will not be used during kfold cross validation of the test training data .because neural networks have a high tendency to overfit we needed a proper dev set in addition to train test .we decided on 5 of the dataset because we wanted to keep this set size small but still enough to not be overly skewed towards one class based on the cumulative probability of the binomial distribution likelihood of getting 3 or less of a single class is 17 .pre processing and derived features	we took a random sample of 5 of our dataset 13 taken 231 leftover and set it aside as our dev set which will not be used during kfold cross validation of the test training data 
1	108835	8835	we decided on 5 of the dataset because we wanted to keep this set size small but still enough to not be overly skewed towards one class based on the cumulative probability of the binomial distribution likelihood of getting 3 or less of a single class is 17 .we took a random sample of 5 of our dataset 13 taken 231 leftover and set it aside as our dev set which will not be used during kfold cross validation of the test training data .for 10 fold validation the remaining 231 samples were split 90 10 resulting in 208 training and 23 test samples .pre processing and derived features	we decided on 5 of the dataset because we wanted to keep this set size small but still enough to not be overly skewed towards one class based on the cumulative probability of the binomial distribution likelihood of getting 3 or less of a single class is 17 
0	108836	8836	for 10 fold validation the remaining 231 samples were split 90 10 resulting in 208 training and 23 test samples .we decided on 5 of the dataset because we wanted to keep this set size small but still enough to not be overly skewed towards one class based on the cumulative probability of the binomial distribution likelihood of getting 3 or less of a single class is 17 .last sentence.pre processing and derived features	for 10 fold validation the remaining 231 samples were split 90 10 resulting in 208 training and 23 test samples 
0	108837	8837	other features included demographic information sex age and scanner type .first sentence.last sentence.train test dev total	other features included demographic information sex age and scanner type 
1	108838	8838	to classify heavy drinkers versus non heavy drinkers we implemented a series of models based on related experiments found in the literature .first sentence.for our baseline we utilized logistic regression .methods	to classify heavy drinkers versus non heavy drinkers we implemented a series of models based on related experiments found in the literature 
1	108839	8839	for our baseline we utilized logistic regression .to classify heavy drinkers versus non heavy drinkers we implemented a series of models based on related experiments found in the literature .for our main exploration of the project we experimented with deep learning models .methods	for our baseline we utilized logistic regression 
0	108840	8840	for our main exploration of the project we experimented with deep learning models .for our baseline we utilized logistic regression .we briefly attempted support vector machines as a foil for deep learning .methods	for our main exploration of the project we experimented with deep learning models 
0	108841	8841	we briefly attempted support vector machines as a foil for deep learning .for our main exploration of the project we experimented with deep learning models .last sentence.methods	we briefly attempted support vector machines as a foil for deep learning 
1	108842	8842	logistic regression a common binary classification algorithm utilizes the sigmoid function also known as the logistic function .first sentence.incorporated with linear prediction parameters and the features of x the classification prediction is given by the following probability distribution whose log likelihood is given by the following due to our low number of features 25 when using derived features from ica parcellated data we used newton s method for convergence .logistic regression	logistic regression a common binary classification algorithm utilizes the sigmoid function also known as the logistic function 
1	108843	8843	incorporated with linear prediction parameters and the features of x the classification prediction is given by the following probability distribution whose log likelihood is given by the following due to our low number of features 25 when using derived features from ica parcellated data we used newton s method for convergence .logistic regression a common binary classification algorithm utilizes the sigmoid function also known as the logistic function .newton s method requires the hessian of the loss with respect to the features to be calculated which is impractical for high dimensional data .logistic regression	incorporated with linear prediction parameters and the features of x the classification prediction is given by the following probability distribution whose log likelihood is given by the following due to our low number of features 25 when using derived features from ica parcellated data we used newton s method for convergence 
0	108844	8844	newton s method requires the hessian of the loss with respect to the features to be calculated which is impractical for high dimensional data .incorporated with linear prediction parameters and the features of x the classification prediction is given by the following probability distribution whose log likelihood is given by the following due to our low number of features 25 when using derived features from ica parcellated data we used newton s method for convergence .for fewer features newton s method has the benefit of converging quickly which also allows us to use batch gradient ascent .logistic regression	newton s method requires the hessian of the loss with respect to the features to be calculated which is impractical for high dimensional data 
0	108845	8845	for fewer features newton s method has the benefit of converging quickly which also allows us to use batch gradient ascent .newton s method requires the hessian of the loss with respect to the features to be calculated which is impractical for high dimensional data .newton s method update rule is given by the following .logistic regression	for fewer features newton s method has the benefit of converging quickly which also allows us to use batch gradient ascent 
0	108846	8846	newton s method update rule is given by the following .for fewer features newton s method has the benefit of converging quickly which also allows us to use batch gradient ascent .last sentence.logistic regression	newton s method update rule is given by the following 
0	108847	8847	support vector machines svms map a given set of features to a higher dimensional space so that nonlinear classifications can be made .first sentence.as opposed to logistic regression which minimizes functional margin defined by the following equation support vector machines seek to minimize the geometric margin which is defined by the following equation in doing so the convergence of the algorithm takes the norm of the parameters into account and essentially the parameters become invariant to random meaningless scaling .support vector machines	support vector machines svms map a given set of features to a higher dimensional space so that nonlinear classifications can be made 
1	108848	8848	as opposed to logistic regression which minimizes functional margin defined by the following equation support vector machines seek to minimize the geometric margin which is defined by the following equation in doing so the convergence of the algorithm takes the norm of the parameters into account and essentially the parameters become invariant to random meaningless scaling .support vector machines svms map a given set of features to a higher dimensional space so that nonlinear classifications can be made .this is important in allowing the parameter change to be small enough for the algorithm to converge appropriately .support vector machines	as opposed to logistic regression which minimizes functional margin defined by the following equation support vector machines seek to minimize the geometric margin which is defined by the following equation in doing so the convergence of the algorithm takes the norm of the parameters into account and essentially the parameters become invariant to random meaningless scaling 
0	108849	8849	this is important in allowing the parameter change to be small enough for the algorithm to converge appropriately .as opposed to logistic regression which minimizes functional margin defined by the following equation support vector machines seek to minimize the geometric margin which is defined by the following equation in doing so the convergence of the algorithm takes the norm of the parameters into account and essentially the parameters become invariant to random meaningless scaling .as various svms have been used on fmri classification models in the literature we ran 4 svms with different kernels 1 linear 2 polynomial degree 2 3 sigmoid and 4 radial basis function rbf .support vector machines	this is important in allowing the parameter change to be small enough for the algorithm to converge appropriately 
1	108850	8850	as various svms have been used on fmri classification models in the literature we ran 4 svms with different kernels 1 linear 2 polynomial degree 2 3 sigmoid and 4 radial basis function rbf .this is important in allowing the parameter change to be small enough for the algorithm to converge appropriately .last sentence.support vector machines	as various svms have been used on fmri classification models in the literature we ran 4 svms with different kernels 1 linear 2 polynomial degree 2 3 sigmoid and 4 radial basis function rbf 
1	108851	8851	finally we chose to primarily use deep learning in our most promising model as we saw an analogy between our data image processing and natural language processing .first sentence.last sentence.deep learning	finally we chose to primarily use deep learning in our most promising model as we saw an analogy between our data image processing and natural language processing 
1	108852	8852	convolutional neural networks cnns are used frequently in image processing to recognize patterns that can be anywhere throughout the picture in our case the fmri data is a time series data for different brain regions .first sentence.we are trying to recognize patterns of brain activity at any time point throughout the time series along multiple channels brain regions we tried using 1 d convolution each region as an input channel time series for convolution .convolutional neural networks	convolutional neural networks cnns are used frequently in image processing to recognize patterns that can be anywhere throughout the picture in our case the fmri data is a time series data for different brain regions 
1	108853	8853	we are trying to recognize patterns of brain activity at any time point throughout the time series along multiple channels brain regions we tried using 1 d convolution each region as an input channel time series for convolution .convolutional neural networks cnns are used frequently in image processing to recognize patterns that can be anywhere throughout the picture in our case the fmri data is a time series data for different brain regions .formally the equation for computing 1 d convolution looks like the following where o represents the output of the convolution m represents the m th filter output in the convolution i represents the axis for time series b represents the bias term in the convolution w represents the weight matrix associated with a given output filter x represents the input data and n represents the n th input channel brain region in this case .convolutional neural networks	we are trying to recognize patterns of brain activity at any time point throughout the time series along multiple channels brain regions we tried using 1 d convolution each region as an input channel time series for convolution 
0	108854	8854	formally the equation for computing 1 d convolution looks like the following where o represents the output of the convolution m represents the m th filter output in the convolution i represents the axis for time series b represents the bias term in the convolution w represents the weight matrix associated with a given output filter x represents the input data and n represents the n th input channel brain region in this case .we are trying to recognize patterns of brain activity at any time point throughout the time series along multiple channels brain regions we tried using 1 d convolution each region as an input channel time series for convolution .note convolution above assumes stride 1 .convolutional neural networks	formally the equation for computing 1 d convolution looks like the following where o represents the output of the convolution m represents the m th filter output in the convolution i represents the axis for time series b represents the bias term in the convolution w represents the weight matrix associated with a given output filter x represents the input data and n represents the n th input channel brain region in this case 
0	108855	8855	note convolution above assumes stride 1 .formally the equation for computing 1 d convolution looks like the following where o represents the output of the convolution m represents the m th filter output in the convolution i represents the axis for time series b represents the bias term in the convolution w represents the weight matrix associated with a given output filter x represents the input data and n represents the n th input channel brain region in this case .last sentence.convolutional neural networks	note convolution above assumes stride 1 
0	108856	8856	our most promising model.first sentence.last sentence.our deep learning models	our most promising model
1	108857	8857	in general for all models because we used 10 fold cross validation as described in the data section to get an accurate measurement of the measurement of our model small dataset means cross validation is imperative we had to make sure to set aside a dev set that will not be seen during cross validation but will be used as a means for adjust for the hyperparameters .first sentence.this was the best way to properly adjust hyperparameters without being biased by the very data that we eventually train the model on .experiments	in general for all models because we used 10 fold cross validation as described in the data section to get an accurate measurement of the measurement of our model small dataset means cross validation is imperative we had to make sure to set aside a dev set that will not be seen during cross validation but will be used as a means for adjust for the hyperparameters 
1	108858	8858	this was the best way to properly adjust hyperparameters without being biased by the very data that we eventually train the model on .in general for all models because we used 10 fold cross validation as described in the data section to get an accurate measurement of the measurement of our model small dataset means cross validation is imperative we had to make sure to set aside a dev set that will not be seen during cross validation but will be used as a means for adjust for the hyperparameters .last sentence.experiments	this was the best way to properly adjust hyperparameters without being biased by the very data that we eventually train the model on 
1	108859	8859	for logistic regression we either used fmri data in addition to all demographic information or with all demographic information excluding age .first sentence.as expected of newton s method the algorithm converged very quickly 1 6 iterations depending on the threshold for convergence batch gradient descent .logistic regression hyperparameters	for logistic regression we either used fmri data in addition to all demographic information or with all demographic information excluding age 
1	108860	8860	as expected of newton s method the algorithm converged very quickly 1 6 iterations depending on the threshold for convergence batch gradient descent .for logistic regression we either used fmri data in addition to all demographic information or with all demographic information excluding age .the criteria for convergence c was measured by the magnitude of change in the parameter c d 2 2 where the threshold for convergence was determined manually using the dev set .logistic regression hyperparameters	as expected of newton s method the algorithm converged very quickly 1 6 iterations depending on the threshold for convergence batch gradient descent 
0	108861	8861	the criteria for convergence c was measured by the magnitude of change in the parameter c d 2 2 where the threshold for convergence was determined manually using the dev set .as expected of newton s method the algorithm converged very quickly 1 6 iterations depending on the threshold for convergence batch gradient descent .we started with a value of 100 for and decreased by a factor of 10 and observed the effect on the average difference in accuracy between the train set and dev set over the k fold validation the dev set never enters the train or test set .logistic regression hyperparameters	the criteria for convergence c was measured by the magnitude of change in the parameter c d 2 2 where the threshold for convergence was determined manually using the dev set 
1	108862	8862	we started with a value of 100 for and decreased by a factor of 10 and observed the effect on the average difference in accuracy between the train set and dev set over the k fold validation the dev set never enters the train or test set .the criteria for convergence c was measured by the magnitude of change in the parameter c d 2 2 where the threshold for convergence was determined manually using the dev set .the average difference in accuracy was invariant to the changes in see chart below .logistic regression hyperparameters	we started with a value of 100 for and decreased by a factor of 10 and observed the effect on the average difference in accuracy between the train set and dev set over the k fold validation the dev set never enters the train or test set 
0	108863	8863	the average difference in accuracy was invariant to the changes in see chart below .we started with a value of 100 for and decreased by a factor of 10 and observed the effect on the average difference in accuracy between the train set and dev set over the k fold validation the dev set never enters the train or test set .in other words 1 iteration was enough for meaningful convergence of the algorithm and that any additional iterations will not cause significant overfitting .logistic regression hyperparameters	the average difference in accuracy was invariant to the changes in see chart below 
0	108864	8864	in other words 1 iteration was enough for meaningful convergence of the algorithm and that any additional iterations will not cause significant overfitting .the average difference in accuracy was invariant to the changes in see chart below .therefore we left the value of at 1e 5 a value used in class .logistic regression hyperparameters	in other words 1 iteration was enough for meaningful convergence of the algorithm and that any additional iterations will not cause significant overfitting 
0	108865	8865	therefore we left the value of at 1e 5 a value used in class .in other words 1 iteration was enough for meaningful convergence of the algorithm and that any additional iterations will not cause significant overfitting .last sentence.logistic regression hyperparameters	therefore we left the value of at 1e 5 a value used in class 
0	108866	8866	1 1e 1 1e 2 avg train dev accuracy 16 16 14 17.first sentence.last sentence.10	1 1e 1 1e 2 avg train dev accuracy 16 16 14 17
0	108867	8867	using sklearn s package for svms.first sentence.last sentence.support vector machine hyperparameters	using sklearn s package for svms
1	108868	8868	for the deep learning models which was implemented through keras with theano backend to that end we established three conditions for exiting iterations .first sentence.as the model was training the model was to exit iterating if 1 t rainacc devacc t rainacc devacc 20 t rainacc devacc t hreshold the difference in accuracy between train and dev sets is fairly small and they both have reached a threshold value of our interest 0 6 better than random guessing 2 t rainacc devacc t rainacc devacc 20 t rainacc m ax t rainacc devacc the difference in accuracy between train and dev sets is getting too large the train accuracy is too high 0 65 and it is greater than the dev accuracy .deep learning hyperparameters	for the deep learning models which was implemented through keras with theano backend to that end we established three conditions for exiting iterations 
1	108869	8869	as the model was training the model was to exit iterating if 1 t rainacc devacc t rainacc devacc 20 t rainacc devacc t hreshold the difference in accuracy between train and dev sets is fairly small and they both have reached a threshold value of our interest 0 6 better than random guessing 2 t rainacc devacc t rainacc devacc 20 t rainacc m ax t rainacc devacc the difference in accuracy between train and dev sets is getting too large the train accuracy is too high 0 65 and it is greater than the dev accuracy .for the deep learning models which was implemented through keras with theano backend to that end we established three conditions for exiting iterations .suggests that the model is beginning to overfit .deep learning hyperparameters	as the model was training the model was to exit iterating if 1 t rainacc devacc t rainacc devacc 20 t rainacc devacc t hreshold the difference in accuracy between train and dev sets is fairly small and they both have reached a threshold value of our interest 0 6 better than random guessing 2 t rainacc devacc t rainacc devacc 20 t rainacc m ax t rainacc devacc the difference in accuracy between train and dev sets is getting too large the train accuracy is too high 0 65 and it is greater than the dev accuracy 
0	108870	8870	suggests that the model is beginning to overfit .as the model was training the model was to exit iterating if 1 t rainacc devacc t rainacc devacc 20 t rainacc devacc t hreshold the difference in accuracy between train and dev sets is fairly small and they both have reached a threshold value of our interest 0 6 better than random guessing 2 t rainacc devacc t rainacc devacc 20 t rainacc m ax t rainacc devacc the difference in accuracy between train and dev sets is getting too large the train accuracy is too high 0 65 and it is greater than the dev accuracy .last sentence.deep learning hyperparameters	suggests that the model is beginning to overfit 
0	108871	8871	the train accuracy is high 0 65 while the dev accuracy is low 0 55 .first sentence.suggests that the model is beginning to overfit as for window size stride and output filter dimensions for cnn and output dimensions for rnn we manually judged based on the rate of training and changes in accuracy differences between the train and dev set to have window size 5 stride 1 output filter dimensions 5 and output dimensions for rnn 5 except for when rnn was the only layer present .3 t rainacc m ax devacc m in	the train accuracy is high 0 65 while the dev accuracy is low 0 55 
1	108872	8872	suggests that the model is beginning to overfit as for window size stride and output filter dimensions for cnn and output dimensions for rnn we manually judged based on the rate of training and changes in accuracy differences between the train and dev set to have window size 5 stride 1 output filter dimensions 5 and output dimensions for rnn 5 except for when rnn was the only layer present .the train accuracy is high 0 65 while the dev accuracy is low 0 55 .last sentence.3 t rainacc m ax devacc m in	suggests that the model is beginning to overfit as for window size stride and output filter dimensions for cnn and output dimensions for rnn we manually judged based on the rate of training and changes in accuracy differences between the train and dev set to have window size 5 stride 1 output filter dimensions 5 and output dimensions for rnn 5 except for when rnn was the only layer present 
1	108873	8873	to measure the final performance of all our models we utilized accuracy and f1 scoresince we balanced our classes accuracy gave us a good indications of how the models performed .first sentence.however f1 score was a way to formally take both recall what proportion of correct predictions was for heavy drinkers over non heavy drinkers .performance evaluation metrics	to measure the final performance of all our models we utilized accuracy and f1 scoresince we balanced our classes accuracy gave us a good indications of how the models performed 
0	108874	8874	however f1 score was a way to formally take both recall what proportion of correct predictions was for heavy drinkers over non heavy drinkers .to measure the final performance of all our models we utilized accuracy and f1 scoresince we balanced our classes accuracy gave us a good indications of how the models performed .and precision what proportion of heavy drinker predictions was correct .performance evaluation metrics	however f1 score was a way to formally take both recall what proportion of correct predictions was for heavy drinkers over non heavy drinkers 
0	108875	8875	and precision what proportion of heavy drinker predictions was correct .however f1 score was a way to formally take both recall what proportion of correct predictions was for heavy drinkers over non heavy drinkers .into account .performance evaluation metrics	and precision what proportion of heavy drinker predictions was correct 
0	108876	8876	into account .and precision what proportion of heavy drinker predictions was correct .logistic regression with our derived features and demographics performed the best.performance evaluation metrics	into account 
1	108877	8877	logistic regression with our derived features and demographics performed the best.into account .last sentence.performance evaluation metrics	logistic regression with our derived features and demographics performed the best
0	108878	8878	generally svms were not our major focus .first sentence.purely based on data we obtained while adjusting for the threshold hyperparameter we saw that all svms either underfit or overfit .results discussion	generally svms were not our major focus 
1	108879	8879	purely based on data we obtained while adjusting for the threshold hyperparameter we saw that all svms either underfit or overfit .generally svms were not our major focus .to prevent overfitting we were able to adjust our threshold but we may need to explore other hyperparameters kernels relevant to svms the bulk of our work went into attempting deeplearning models .results discussion	purely based on data we obtained while adjusting for the threshold hyperparameter we saw that all svms either underfit or overfit 
1	108880	8880	to prevent overfitting we were able to adjust our threshold but we may need to explore other hyperparameters kernels relevant to svms the bulk of our work went into attempting deeplearning models .purely based on data we obtained while adjusting for the threshold hyperparameter we saw that all svms either underfit or overfit .as we initially experimented with different hyperparameters one of the first things we noticed was the tendency to underfit or overfit .results discussion	to prevent overfitting we were able to adjust our threshold but we may need to explore other hyperparameters kernels relevant to svms the bulk of our work went into attempting deeplearning models 
0	108881	8881	as we initially experimented with different hyperparameters one of the first things we noticed was the tendency to underfit or overfit .to prevent overfitting we were able to adjust our threshold but we may need to explore other hyperparameters kernels relevant to svms the bulk of our work went into attempting deeplearning models .we had to make sure to have a dev set we could test on such that we could determine window size output filter dimension and output dimension for rnn .results discussion	as we initially experimented with different hyperparameters one of the first things we noticed was the tendency to underfit or overfit 
0	108882	8882	we had to make sure to have a dev set we could test on such that we could determine window size output filter dimension and output dimension for rnn .as we initially experimented with different hyperparameters one of the first things we noticed was the tendency to underfit or overfit .we also attempted using regularization to no avail as well as drop out .results discussion	we had to make sure to have a dev set we could test on such that we could determine window size output filter dimension and output dimension for rnn 
0	108883	8883	we also attempted using regularization to no avail as well as drop out .we had to make sure to have a dev set we could test on such that we could determine window size output filter dimension and output dimension for rnn .our most important use of the dev set was to know when to drop after an epoch .results discussion	we also attempted using regularization to no avail as well as drop out 
0	108884	8884	our most important use of the dev set was to know when to drop after an epoch .we also attempted using regularization to no avail as well as drop out .to ensure the model trained enough but not too much we reasoned that the best place to stop was when both the train and dev set accuracies were fairly close and above a certain threshold or when the two accuracies were getting too far apart .results discussion	our most important use of the dev set was to know when to drop after an epoch 
0	108885	8885	to ensure the model trained enough but not too much we reasoned that the best place to stop was when both the train and dev set accuracies were fairly close and above a certain threshold or when the two accuracies were getting too far apart .our most important use of the dev set was to know when to drop after an epoch .in doing so we were able to avoid overfitting .results discussion	to ensure the model trained enough but not too much we reasoned that the best place to stop was when both the train and dev set accuracies were fairly close and above a certain threshold or when the two accuracies were getting too far apart 
0	108886	8886	in doing so we were able to avoid overfitting .to ensure the model trained enough but not too much we reasoned that the best place to stop was when both the train and dev set accuracies were fairly close and above a certain threshold or when the two accuracies were getting too far apart .however all in all we saw that we could not yield any significant improvements in accuracy f1 score .results discussion	in doing so we were able to avoid overfitting 
0	108887	8887	however all in all we saw that we could not yield any significant improvements in accuracy f1 score .in doing so we were able to avoid overfitting .last sentence.results discussion	however all in all we saw that we could not yield any significant improvements in accuracy f1 score 
1	108888	8888	overall although we were able to control many of the hyperparameters to prevent under overfitting and adjust the number of layers we found that deep learning models did not perform very well .first sentence.this is either because our overall architecture was fundamentally flawed inclusion of demographics is crucial or we were simply lacking in data to be able to make meaningful distinctions as indicated by aggressive guessing of a single class .conclusion future work	overall although we were able to control many of the hyperparameters to prevent under overfitting and adjust the number of layers we found that deep learning models did not perform very well 
1	108889	8889	this is either because our overall architecture was fundamentally flawed inclusion of demographics is crucial or we were simply lacking in data to be able to make meaningful distinctions as indicated by aggressive guessing of a single class .overall although we were able to control many of the hyperparameters to prevent under overfitting and adjust the number of layers we found that deep learning models did not perform very well .as for svms they were not our primary focus for developing our models so we weren t able to see their full potential on the data .conclusion future work	this is either because our overall architecture was fundamentally flawed inclusion of demographics is crucial or we were simply lacking in data to be able to make meaningful distinctions as indicated by aggressive guessing of a single class 
0	108890	8890	as for svms they were not our primary focus for developing our models so we weren t able to see their full potential on the data .this is either because our overall architecture was fundamentally flawed inclusion of demographics is crucial or we were simply lacking in data to be able to make meaningful distinctions as indicated by aggressive guessing of a single class .nonetheless our most preliminary work suggests they will have similar issues as our deep learning models .conclusion future work	as for svms they were not our primary focus for developing our models so we weren t able to see their full potential on the data 
0	108891	8891	nonetheless our most preliminary work suggests they will have similar issues as our deep learning models .as for svms they were not our primary focus for developing our models so we weren t able to see their full potential on the data .although we do have our most successful model baseline logistic regression using derived features we also found that removing age as a factor reduces its efficacy suggesting that even logistic regression with our derived features was not any more successful than our other models moving forward there are multiple things we would like to try for our future directions .conclusion future work	nonetheless our most preliminary work suggests they will have similar issues as our deep learning models 
1	108892	8892	although we do have our most successful model baseline logistic regression using derived features we also found that removing age as a factor reduces its efficacy suggesting that even logistic regression with our derived features was not any more successful than our other models moving forward there are multiple things we would like to try for our future directions .nonetheless our most preliminary work suggests they will have similar issues as our deep learning models .as mentioned above we simply may not have had enough data to be able to pick up sensitive features .conclusion future work	although we do have our most successful model baseline logistic regression using derived features we also found that removing age as a factor reduces its efficacy suggesting that even logistic regression with our derived features was not any more successful than our other models moving forward there are multiple things we would like to try for our future directions 
0	108893	8893	as mentioned above we simply may not have had enough data to be able to pick up sensitive features .although we do have our most successful model baseline logistic regression using derived features we also found that removing age as a factor reduces its efficacy suggesting that even logistic regression with our derived features was not any more successful than our other models moving forward there are multiple things we would like to try for our future directions .to circumvent this issue we can use transfer learning which is commonly used for model development on medical images due to relatively modest sample sizes .conclusion future work	as mentioned above we simply may not have had enough data to be able to pick up sensitive features 
1	108894	8894	to circumvent this issue we can use transfer learning which is commonly used for model development on medical images due to relatively modest sample sizes .as mentioned above we simply may not have had enough data to be able to pick up sensitive features .this involves applying learned parameters from other large datasets ideally those that combine images and sequence data as our fmri dataset does and training our own final few layers to use features detected from larger data but predicting for or own another important direction is to incorporate demographics as features into our deep learning models .conclusion future work	to circumvent this issue we can use transfer learning which is commonly used for model development on medical images due to relatively modest sample sizes 
1	108895	8895	this involves applying learned parameters from other large datasets ideally those that combine images and sequence data as our fmri dataset does and training our own final few layers to use features detected from larger data but predicting for or own another important direction is to incorporate demographics as features into our deep learning models .to circumvent this issue we can use transfer learning which is commonly used for model development on medical images due to relatively modest sample sizes .although we were limited by the fact that all of us had just learned to use keras and didn t have the time to incorporate multiple data inputs we saw how important demographics can be .conclusion future work	this involves applying learned parameters from other large datasets ideally those that combine images and sequence data as our fmri dataset does and training our own final few layers to use features detected from larger data but predicting for or own another important direction is to incorporate demographics as features into our deep learning models 
0	108896	8896	although we were limited by the fact that all of us had just learned to use keras and didn t have the time to incorporate multiple data inputs we saw how important demographics can be .this involves applying learned parameters from other large datasets ideally those that combine images and sequence data as our fmri dataset does and training our own final few layers to use features detected from larger data but predicting for or own another important direction is to incorporate demographics as features into our deep learning models .moreover we have not normalized for an natural biological changes in brain function as an individual ages .conclusion future work	although we were limited by the fact that all of us had just learned to use keras and didn t have the time to incorporate multiple data inputs we saw how important demographics can be 
0	108897	8897	moreover we have not normalized for an natural biological changes in brain function as an individual ages .although we were limited by the fact that all of us had just learned to use keras and didn t have the time to incorporate multiple data inputs we saw how important demographics can be .including demographics may potentially drastically change our models performances another idea would be to look into different parcellation methods for pre processing the data craddock with more regions etc .conclusion future work	moreover we have not normalized for an natural biological changes in brain function as an individual ages 
1	108898	8898	including demographics may potentially drastically change our models performances another idea would be to look into different parcellation methods for pre processing the data craddock with more regions etc .moreover we have not normalized for an natural biological changes in brain function as an individual ages .as currently there is no consensus on the best parcellation method of fmri data lastly we can look into making our svm models more robust for our classification problem .conclusion future work	including demographics may potentially drastically change our models performances another idea would be to look into different parcellation methods for pre processing the data craddock with more regions etc 
1	108899	8899	as currently there is no consensus on the best parcellation method of fmri data lastly we can look into making our svm models more robust for our classification problem .including demographics may potentially drastically change our models performances another idea would be to look into different parcellation methods for pre processing the data craddock with more regions etc .for this project we kept most of the default scikit learn parameters except for tolerance as exploring svms was not the main focus of our project .conclusion future work	as currently there is no consensus on the best parcellation method of fmri data lastly we can look into making our svm models more robust for our classification problem 
0	108900	8900	for this project we kept most of the default scikit learn parameters except for tolerance as exploring svms was not the main focus of our project .as currently there is no consensus on the best parcellation method of fmri data lastly we can look into making our svm models more robust for our classification problem .however fine tuning additional parameters as in poly sigmoid rbf kernels or considering additional custom kernels may help with the issue we had of either extremely underfitting or extremely overfitting .conclusion future work	for this project we kept most of the default scikit learn parameters except for tolerance as exploring svms was not the main focus of our project 
0	108901	8901	however fine tuning additional parameters as in poly sigmoid rbf kernels or considering additional custom kernels may help with the issue we had of either extremely underfitting or extremely overfitting .for this project we kept most of the default scikit learn parameters except for tolerance as exploring svms was not the main focus of our project .last sentence.conclusion future work	however fine tuning additional parameters as in poly sigmoid rbf kernels or considering additional custom kernels may help with the issue we had of either extremely underfitting or extremely overfitting 
0	108902	8902	all team members contributed equally to writing this project report .first sentence.joseph noh researched models and built the frame work to use through keras theanos .contributions	all team members contributed equally to writing this project report 
0	108903	8903	joseph noh researched models and built the frame work to use through keras theanos .all team members contributed equally to writing this project report .he was primarily responsible for pioneering different methods models .contributions	joseph noh researched models and built the frame work to use through keras theanos 
0	108904	8904	he was primarily responsible for pioneering different methods models .joseph noh researched models and built the frame work to use through keras theanos .yong hun kim tested code for different model combinations and hyperparameters and recorded the resulting data .contributions	he was primarily responsible for pioneering different methods models 
0	108905	8905	yong hun kim tested code for different model combinations and hyperparameters and recorded the resulting data .he was primarily responsible for pioneering different methods models .cindy liu generated a large number of images tables for data and result visualizations implemented the svm models and created the model diagram we would like to acknowledge qingyu zhao for providing the pre processed data and serving as a mentor for the project giving us guidance on our set up and answering our questions about the data .contributions	yong hun kim tested code for different model combinations and hyperparameters and recorded the resulting data 
0	108906	8906	cindy liu generated a large number of images tables for data and result visualizations implemented the svm models and created the model diagram we would like to acknowledge qingyu zhao for providing the pre processed data and serving as a mentor for the project giving us guidance on our set up and answering our questions about the data .yong hun kim tested code for different model combinations and hyperparameters and recorded the resulting data .we would also like to thank tas atharva parulekar and raphael townshend for providing guidance and advice during project office hours as well as professors andrew ng and ron dror for teaching cs229 this quarter .contributions	cindy liu generated a large number of images tables for data and result visualizations implemented the svm models and created the model diagram we would like to acknowledge qingyu zhao for providing the pre processed data and serving as a mentor for the project giving us guidance on our set up and answering our questions about the data 
0	108907	8907	we would also like to thank tas atharva parulekar and raphael townshend for providing guidance and advice during project office hours as well as professors andrew ng and ron dror for teaching cs229 this quarter .cindy liu generated a large number of images tables for data and result visualizations implemented the svm models and created the model diagram we would like to acknowledge qingyu zhao for providing the pre processed data and serving as a mentor for the project giving us guidance on our set up and answering our questions about the data .last sentence.contributions	we would also like to thank tas atharva parulekar and raphael townshend for providing guidance and advice during project office hours as well as professors andrew ng and ron dror for teaching cs229 this quarter 
1	108908	8908	using a training set provided by the pacific earthquake engineering research peer center we build a classifier to label images of structures as damaged or undamaged using a variety of machine learning techniques k nearest neighbors logistic regression svm and convolutional neural networks cnn .first sentence.we find that when compared to classical machine learning techniques the performance of a cnn is best on our data set .abstract	using a training set provided by the pacific earthquake engineering research peer center we build a classifier to label images of structures as damaged or undamaged using a variety of machine learning techniques k nearest neighbors logistic regression svm and convolutional neural networks cnn 
1	108909	8909	we find that when compared to classical machine learning techniques the performance of a cnn is best on our data set .using a training set provided by the pacific earthquake engineering research peer center we build a classifier to label images of structures as damaged or undamaged using a variety of machine learning techniques k nearest neighbors logistic regression svm and convolutional neural networks cnn .we evaluate the mistakes made by our classifiers and we tune our models using information gleaned from learning curves .abstract	we find that when compared to classical machine learning techniques the performance of a cnn is best on our data set 
1	108910	8910	we evaluate the mistakes made by our classifiers and we tune our models using information gleaned from learning curves .we find that when compared to classical machine learning techniques the performance of a cnn is best on our data set .we find that our best performing model which uses transfer learning using inceptionv3 trained on imagenet with an added fully connected layer and softmax has a test accuracy of 83 .abstract	we evaluate the mistakes made by our classifiers and we tune our models using information gleaned from learning curves 
1	108911	8911	we find that our best performing model which uses transfer learning using inceptionv3 trained on imagenet with an added fully connected layer and softmax has a test accuracy of 83 .we evaluate the mistakes made by our classifiers and we tune our models using information gleaned from learning curves .last sentence.abstract	we find that our best performing model which uses transfer learning using inceptionv3 trained on imagenet with an added fully connected layer and softmax has a test accuracy of 83 
0	108912	8912	the pacific earthquake engineering research peer center has provided image datasets that can be used to classify structures in terms of damage the input to this project consisted of 5913 images .first sentence.of this set of images 3186 images were labeled as undamaged or 0 54 and 2727 images were labeled as damaged or 1 46 .introduction	the pacific earthquake engineering research peer center has provided image datasets that can be used to classify structures in terms of damage the input to this project consisted of 5913 images 
0	108913	8913	of this set of images 3186 images were labeled as undamaged or 0 54 and 2727 images were labeled as damaged or 1 46 .the pacific earthquake engineering research peer center has provided image datasets that can be used to classify structures in terms of damage the input to this project consisted of 5913 images .each image includes 224 by 224 eight bit rgb pixels .introduction	of this set of images 3186 images were labeled as undamaged or 0 54 and 2727 images were labeled as damaged or 1 46 
0	108914	8914	each image includes 224 by 224 eight bit rgb pixels .of this set of images 3186 images were labeled as undamaged or 0 54 and 2727 images were labeled as damaged or 1 46 .we split the images into the following sets 90 for training 2870 undamaged and 2451 damaged .introduction	each image includes 224 by 224 eight bit rgb pixels 
0	108915	8915	we split the images into the following sets 90 for training 2870 undamaged and 2451 damaged .each image includes 224 by 224 eight bit rgb pixels .46 are damaged 10 for validation 316 undamaged and 276 damaged .introduction	we split the images into the following sets 90 for training 2870 undamaged and 2451 damaged 
0	108916	8916	46 are damaged 10 for validation 316 undamaged and 276 damaged .we split the images into the following sets 90 for training 2870 undamaged and 2451 damaged .47 are damaged .introduction	46 are damaged 10 for validation 316 undamaged and 276 damaged 
0	108917	8917	47 are damaged .46 are damaged 10 for validation 316 undamaged and 276 damaged .we decided not to set aside images for testing because of the limited number of samples although if we were to eventually submit to an academic journal we would need to be more rigorous in this regard we used several models three classical machine learning models several variations of two deep learning models mobilenet and inceptionv3 convolutional neural networks and one model which combined classical and deep learning techniques the primary output of our classifier models is the accuracy as determined by the number of correctly predicted images over the total number of predicted images .introduction	47 are damaged 
1	108918	8918	we decided not to set aside images for testing because of the limited number of samples although if we were to eventually submit to an academic journal we would need to be more rigorous in this regard we used several models three classical machine learning models several variations of two deep learning models mobilenet and inceptionv3 convolutional neural networks and one model which combined classical and deep learning techniques the primary output of our classifier models is the accuracy as determined by the number of correctly predicted images over the total number of predicted images .47 are damaged .last sentence.introduction	we decided not to set aside images for testing because of the limited number of samples although if we were to eventually submit to an academic journal we would need to be more rigorous in this regard we used several models three classical machine learning models several variations of two deep learning models mobilenet and inceptionv3 convolutional neural networks and one model which combined classical and deep learning techniques the primary output of our classifier models is the accuracy as determined by the number of correctly predicted images over the total number of predicted images 
0	108919	8919	there are few references on image classification of damaged buildings .first sentence.one good survey paper on structural image classification is.related work	there are few references on image classification of damaged buildings 
0	108920	8920	one good survey paper on structural image classification is.there are few references on image classification of damaged buildings .last sentence.related work	one good survey paper on structural image classification is
0	108921	8921	minnie ho intel corporation minnie ho intel com.first sentence.last sentence.dataset and features	minnie ho intel corporation minnie ho intel com
0	108922	8922	google llc jatron google com a few examples from our dataset are shown in.first sentence.last sentence.jorge troncoso	google llc jatron google com a few examples from our dataset are shown in
0	108923	8923	we normalized the images so each of the 224x224 8 bit rgb pixels x was in the range 1 1 .first sentence.this was done by setting each pixel x to 128 1 .figure 1 example structural images	we normalized the images so each of the 224x224 8 bit rgb pixels x was in the range 1 1 
0	108924	8924	this was done by setting each pixel x to 128 1 .we normalized the images so each of the 224x224 8 bit rgb pixels x was in the range 1 1 .for k nearest neighbors logistic regression and support vector machine we also scaled and flattened the pictures before feeding them into the models .figure 1 example structural images	this was done by setting each pixel x to 128 1 
1	108925	8925	for k nearest neighbors logistic regression and support vector machine we also scaled and flattened the pictures before feeding them into the models .this was done by setting each pixel x to 128 1 .last sentence.figure 1 example structural images	for k nearest neighbors logistic regression and support vector machine we also scaled and flattened the pictures before feeding them into the models 
1	108926	8926	we built six models three classical machine learning models k nearest neighbors with k 5 logistic regression support vector machine with rbf kernel two deep learning models mobilenetv1 0 and inceptionv3 convolutional neural networks and one model which combined classical and deep learning techniques support vector machine based on activations earlier in the inceptionv3 network .first sentence.the performance of each of these models is summarized in the results section .methods	we built six models three classical machine learning models k nearest neighbors with k 5 logistic regression support vector machine with rbf kernel two deep learning models mobilenetv1 0 and inceptionv3 convolutional neural networks and one model which combined classical and deep learning techniques support vector machine based on activations earlier in the inceptionv3 network 
0	108927	8927	the performance of each of these models is summarized in the results section .we built six models three classical machine learning models k nearest neighbors with k 5 logistic regression support vector machine with rbf kernel two deep learning models mobilenetv1 0 and inceptionv3 convolutional neural networks and one model which combined classical and deep learning techniques support vector machine based on activations earlier in the inceptionv3 network .last sentence.methods	the performance of each of these models is summarized in the results section 
0	108928	8928	in k nearest neighbors an unlabeled vector is classified by assigning the label which is most frequent among the k training samples nearest to that query point .first sentence.last sentence.k nearest neighbors	in k nearest neighbors an unlabeled vector is classified by assigning the label which is most frequent among the k training samples nearest to that query point 
0	108929	8929	in logistic regression we use the sigmoid function to estimate the probability that an image belongs to a certain class .first sentence.this sigmoid function is parametrized by a vector which is obtained by maximizing the log likelihood .logistic regression	in logistic regression we use the sigmoid function to estimate the probability that an image belongs to a certain class 
0	108930	8930	this sigmoid function is parametrized by a vector which is obtained by maximizing the log likelihood .in logistic regression we use the sigmoid function to estimate the probability that an image belongs to a certain class .our logistic regression model included l2 regularization with 1 0 .logistic regression	this sigmoid function is parametrized by a vector which is obtained by maximizing the log likelihood 
1	108931	8931	our logistic regression model included l2 regularization with 1 0 .this sigmoid function is parametrized by a vector which is obtained by maximizing the log likelihood .due to the suboptimal results achieved by this model we did not spend additional time tuning the regularization parameters .logistic regression	our logistic regression model included l2 regularization with 1 0 
1	108932	8932	due to the suboptimal results achieved by this model we did not spend additional time tuning the regularization parameters .our logistic regression model included l2 regularization with 1 0 .last sentence.logistic regression	due to the suboptimal results achieved by this model we did not spend additional time tuning the regularization parameters 
1	108933	8933	during training support vector machines try to find the maximum margin hyperplane that divides data points with different labels .first sentence.supports vector machines can also efficiently perform non linear classification using what is called the kernel trick implicitly mapping the inputs into high dimensional feature spaces our support vector machine model performed non linear classification using the radial basis function kernel which is defined by the formula below .support vector machine	during training support vector machines try to find the maximum margin hyperplane that divides data points with different labels 
0	108934	8934	supports vector machines can also efficiently perform non linear classification using what is called the kernel trick implicitly mapping the inputs into high dimensional feature spaces our support vector machine model performed non linear classification using the radial basis function kernel which is defined by the formula below .during training support vector machines try to find the maximum margin hyperplane that divides data points with different labels .2 we set the penalty parameter c of the error term to 1 0 and the kernel coefficient for the rbf kernel to 0 001 .support vector machine	supports vector machines can also efficiently perform non linear classification using what is called the kernel trick implicitly mapping the inputs into high dimensional feature spaces our support vector machine model performed non linear classification using the radial basis function kernel which is defined by the formula below 
0	108935	8935	2 we set the penalty parameter c of the error term to 1 0 and the kernel coefficient for the rbf kernel to 0 001 .supports vector machines can also efficiently perform non linear classification using what is called the kernel trick implicitly mapping the inputs into high dimensional feature spaces our support vector machine model performed non linear classification using the radial basis function kernel which is defined by the formula below .due to the suboptimal results achieved by this first model we did not do further tuning of these parameters .support vector machine	2 we set the penalty parameter c of the error term to 1 0 and the kernel coefficient for the rbf kernel to 0 001 
1	108936	8936	due to the suboptimal results achieved by this first model we did not do further tuning of these parameters .2 we set the penalty parameter c of the error term to 1 0 and the kernel coefficient for the rbf kernel to 0 001 .last sentence.support vector machine	due to the suboptimal results achieved by this first model we did not do further tuning of these parameters 
0	108937	8937	mobilenetv1 and inceptionv3 are two convolutional neural network cnn architectures designed for image recognition tasks .first sentence.mobilenetv1 is a lighter lower latency neural network designed for use on mobile devices while inceptionv3 is a heavier architecture which tends to achieve better performance .mobilenetv1 and inceptionv3	mobilenetv1 and inceptionv3 are two convolutional neural network cnn architectures designed for image recognition tasks 
0	108938	8938	mobilenetv1 is a lighter lower latency neural network designed for use on mobile devices while inceptionv3 is a heavier architecture which tends to achieve better performance .mobilenetv1 and inceptionv3 are two convolutional neural network cnn architectures designed for image recognition tasks .since we only had a few thousand images training these networks from scratch would surely cause overfitting so instead we downloaded pre trained versions of these models using tensorflow with weights optimized to classify images in the imagenet dataset froze the weights of most of the layers of the pre trained networks and trained a new fully connected layer with a sigmoid or softmax activation placed on top of each of the pre trained networks .mobilenetv1 and inceptionv3	mobilenetv1 is a lighter lower latency neural network designed for use on mobile devices while inceptionv3 is a heavier architecture which tends to achieve better performance 
1	108939	8939	since we only had a few thousand images training these networks from scratch would surely cause overfitting so instead we downloaded pre trained versions of these models using tensorflow with weights optimized to classify images in the imagenet dataset froze the weights of most of the layers of the pre trained networks and trained a new fully connected layer with a sigmoid or softmax activation placed on top of each of the pre trained networks .mobilenetv1 is a lighter lower latency neural network designed for use on mobile devices while inceptionv3 is a heavier architecture which tends to achieve better performance .this is a common technique used in machine learning known as transfer learning.mobilenetv1 and inceptionv3	since we only had a few thousand images training these networks from scratch would surely cause overfitting so instead we downloaded pre trained versions of these models using tensorflow with weights optimized to classify images in the imagenet dataset froze the weights of most of the layers of the pre trained networks and trained a new fully connected layer with a sigmoid or softmax activation placed on top of each of the pre trained networks 
0	108940	8940	this is a common technique used in machine learning known as transfer learning.since we only had a few thousand images training these networks from scratch would surely cause overfitting so instead we downloaded pre trained versions of these models using tensorflow with weights optimized to classify images in the imagenet dataset froze the weights of most of the layers of the pre trained networks and trained a new fully connected layer with a sigmoid or softmax activation placed on top of each of the pre trained networks .last sentence.mobilenetv1 and inceptionv3	this is a common technique used in machine learning known as transfer learning
1	108941	8941	since our dataset was quite different from the imagenet dataset the features extracted at the top of the inceptionv3 network were probably not optimized for our application so we thought we might be able to achieve better performance by building an svm classifier based on activations earlier in the inceptionv3 network which contains more general features this was achieved by feeding the pretrained inceptionv3 network all of our images computing the output of the 288th later for reference the inceptionv3 network has 311 layers and using these outputs as features for an svm classifier .first sentence.here we also implemented model selection to find the optimal kernel coefficient gamma of the rbf kernel as shown in we used a google cloud deep learning vm instance for many of our simulation runs with tensorflow optimized for an nvdia p100 gpu and intel skylake 8 core cpu using intel mkl and nvidia cuda .support vector machine based on activations earlier in the inceptionv3 network	since our dataset was quite different from the imagenet dataset the features extracted at the top of the inceptionv3 network were probably not optimized for our application so we thought we might be able to achieve better performance by building an svm classifier based on activations earlier in the inceptionv3 network which contains more general features this was achieved by feeding the pretrained inceptionv3 network all of our images computing the output of the 288th later for reference the inceptionv3 network has 311 layers and using these outputs as features for an svm classifier 
1	108942	8942	here we also implemented model selection to find the optimal kernel coefficient gamma of the rbf kernel as shown in we used a google cloud deep learning vm instance for many of our simulation runs with tensorflow optimized for an nvdia p100 gpu and intel skylake 8 core cpu using intel mkl and nvidia cuda .since our dataset was quite different from the imagenet dataset the features extracted at the top of the inceptionv3 network were probably not optimized for our application so we thought we might be able to achieve better performance by building an svm classifier based on activations earlier in the inceptionv3 network which contains more general features this was achieved by feeding the pretrained inceptionv3 network all of our images computing the output of the 288th later for reference the inceptionv3 network has 311 layers and using these outputs as features for an svm classifier .we discovered an instance optimized for nvdia was faster on cnns but an instance optimized for intel was faster for sci py all of the code used in this project including many experiments whose results we did not include in this report due to lack of space is available in our github repository https github com jatron structural damage recognition .support vector machine based on activations earlier in the inceptionv3 network	here we also implemented model selection to find the optimal kernel coefficient gamma of the rbf kernel as shown in we used a google cloud deep learning vm instance for many of our simulation runs with tensorflow optimized for an nvdia p100 gpu and intel skylake 8 core cpu using intel mkl and nvidia cuda 
1	108943	8943	we discovered an instance optimized for nvdia was faster on cnns but an instance optimized for intel was faster for sci py all of the code used in this project including many experiments whose results we did not include in this report due to lack of space is available in our github repository https github com jatron structural damage recognition .here we also implemented model selection to find the optimal kernel coefficient gamma of the rbf kernel as shown in we used a google cloud deep learning vm instance for many of our simulation runs with tensorflow optimized for an nvdia p100 gpu and intel skylake 8 core cpu using intel mkl and nvidia cuda .last sentence.support vector machine based on activations earlier in the inceptionv3 network	we discovered an instance optimized for nvdia was faster on cnns but an instance optimized for intel was faster for sci py all of the code used in this project including many experiments whose results we did not include in this report due to lack of space is available in our github repository https github com jatron structural damage recognition 
1	108944	8944	the performance achieved by each of our models is summarized in the it is not surprising that the models based on cnns performed the best since the parameters could best take advantage of the spatial information in the images .first sentence.we note however that the mixed network svm plus inceptionv3 also did well after tuning the kernel coefficient gamma of the rbf kernel we were able to achieve 75 validation accuracy and 95 training accuracy with this model .experimental results	the performance achieved by each of our models is summarized in the it is not surprising that the models based on cnns performed the best since the parameters could best take advantage of the spatial information in the images 
1	108945	8945	we note however that the mixed network svm plus inceptionv3 also did well after tuning the kernel coefficient gamma of the rbf kernel we were able to achieve 75 validation accuracy and 95 training accuracy with this model .the performance achieved by each of our models is summarized in the it is not surprising that the models based on cnns performed the best since the parameters could best take advantage of the spatial information in the images .last sentence.experimental results	we note however that the mixed network svm plus inceptionv3 also did well after tuning the kernel coefficient gamma of the rbf kernel we were able to achieve 75 validation accuracy and 95 training accuracy with this model 
1	108946	8946	as mentioned earlier we had applied transfer learning in tensorflow to baseline inceptionv3 model originally trained using imagenet adding a fully connected and softmax layer similar to.first sentence.last sentence.bias versus variance	as mentioned earlier we had applied transfer learning in tensorflow to baseline inceptionv3 model originally trained using imagenet adding a fully connected and softmax layer similar to
1	108947	8947	we find that by using 4000 images for training 1000 images for testing on the retrained inceptionv3 model discussed in 5 2 we obtain the following test confusion matrix 448 86 117 349 .first sentence.after performing the prediction we checked manually through several hundred images to determine patterns in correctly predicted images false negatives and false positives .misclassified images and data augmentation	we find that by using 4000 images for training 1000 images for testing on the retrained inceptionv3 model discussed in 5 2 we obtain the following test confusion matrix 448 86 117 349 
1	108948	8948	after performing the prediction we checked manually through several hundred images to determine patterns in correctly predicted images false negatives and false positives .we find that by using 4000 images for training 1000 images for testing on the retrained inceptionv3 model discussed in 5 2 we obtain the following test confusion matrix 448 86 117 349 .examples of misclassified images are depicted in.misclassified images and data augmentation	after performing the prediction we checked manually through several hundred images to determine patterns in correctly predicted images false negatives and false positives 
0	108949	8949	examples of misclassified images are depicted in.after performing the prediction we checked manually through several hundred images to determine patterns in correctly predicted images false negatives and false positives .last sentence.misclassified images and data augmentation	examples of misclassified images are depicted in
1	108950	8950	we retrain the model from 5 2 using keras but this time we remove the top layer of the inceptionv3 network flatten the output of the penultimate layer add a fully connected layer and softmax activation .first sentence.we find that we are now overfitting.experiments with inceptionv3	we retrain the model from 5 2 using keras but this time we remove the top layer of the inceptionv3 network flatten the output of the penultimate layer add a fully connected layer and softmax activation 
0	108951	8951	we find that we are now overfitting.we retrain the model from 5 2 using keras but this time we remove the top layer of the inceptionv3 network flatten the output of the penultimate layer add a fully connected layer and softmax activation .last sentence.experiments with inceptionv3	we find that we are now overfitting
0	108952	8952	we conclude that a variation of a convolutional neural network performs best on our dataset .first sentence.furthermore while bias can be managed by training more parameters layers of the cnn we must be careful not to add so many parameters that we overfit .conclusions and next steps	we conclude that a variation of a convolutional neural network performs best on our dataset 
1	108953	8953	furthermore while bias can be managed by training more parameters layers of the cnn we must be careful not to add so many parameters that we overfit .we conclude that a variation of a convolutional neural network performs best on our dataset .however overfitting can be also managed by adding random images to data in terms of future work and next steps more controlled experimentation can be done to manage bias and variance .conclusions and next steps	furthermore while bias can be managed by training more parameters layers of the cnn we must be careful not to add so many parameters that we overfit 
1	108954	8954	however overfitting can be also managed by adding random images to data in terms of future work and next steps more controlled experimentation can be done to manage bias and variance .furthermore while bias can be managed by training more parameters layers of the cnn we must be careful not to add so many parameters that we overfit .we could improve validation accuracy by better managing the data correct mislabeled images add images similar to the false positives or negatives cropping irrelevant features understanding differences in texture or pattern vs damage and accommodating wide angle versus close up images .conclusions and next steps	however overfitting can be also managed by adding random images to data in terms of future work and next steps more controlled experimentation can be done to manage bias and variance 
1	108955	8955	we could improve validation accuracy by better managing the data correct mislabeled images add images similar to the false positives or negatives cropping irrelevant features understanding differences in texture or pattern vs damage and accommodating wide angle versus close up images .however overfitting can be also managed by adding random images to data in terms of future work and next steps more controlled experimentation can be done to manage bias and variance .furthermore other techniques such as ensemble averaging could perhaps lead to better performance .conclusions and next steps	we could improve validation accuracy by better managing the data correct mislabeled images add images similar to the false positives or negatives cropping irrelevant features understanding differences in texture or pattern vs damage and accommodating wide angle versus close up images 
0	108956	8956	furthermore other techniques such as ensemble averaging could perhaps lead to better performance .we could improve validation accuracy by better managing the data correct mislabeled images add images similar to the false positives or negatives cropping irrelevant features understanding differences in texture or pattern vs damage and accommodating wide angle versus close up images .last sentence.conclusions and next steps	furthermore other techniques such as ensemble averaging could perhaps lead to better performance 
0	108957	8957	we acknowledge sanjay govindjee who alerted us to this problem .first sentence.the guidance of fantine huot and mark daoust are also gratefully acknowledged .acknowledgments	we acknowledge sanjay govindjee who alerted us to this problem 
0	108958	8958	the guidance of fantine huot and mark daoust are also gratefully acknowledged .we acknowledge sanjay govindjee who alerted us to this problem .last sentence.acknowledgments	the guidance of fantine huot and mark daoust are also gratefully acknowledged 
1	108959	8959	autonomous fly by feel vehicles motivated by the supreme flight skills of birds a new concept called fly by feel fbf has been proposed to develop the next generation of intelligent aircrafts .first sentence.to achieve this goal stanford structures and composites lab sacl has developed a smart wing which embeds a multifunctional sensor network on the surface layup of the wing collected from a series of wind tunnel tests with different flight states the dataset explored in this study includes conditions of angle of attack from 0 to 15 degrees incremental step of 5 degrees and conditions of airflow velocity from 0 to 25 m s minimum incremental step of 0 5 m s .discussion and future work features references	autonomous fly by feel vehicles motivated by the supreme flight skills of birds a new concept called fly by feel fbf has been proposed to develop the next generation of intelligent aircrafts 
1	108960	8960	to achieve this goal stanford structures and composites lab sacl has developed a smart wing which embeds a multifunctional sensor network on the surface layup of the wing collected from a series of wind tunnel tests with different flight states the dataset explored in this study includes conditions of angle of attack from 0 to 15 degrees incremental step of 5 degrees and conditions of airflow velocity from 0 to 25 m s minimum incremental step of 0 5 m s .autonomous fly by feel vehicles motivated by the supreme flight skills of birds a new concept called fly by feel fbf has been proposed to develop the next generation of intelligent aircrafts .60 000 data points are collected from every piezoelectric sensor for each flight state .discussion and future work features references	to achieve this goal stanford structures and composites lab sacl has developed a smart wing which embeds a multifunctional sensor network on the surface layup of the wing collected from a series of wind tunnel tests with different flight states the dataset explored in this study includes conditions of angle of attack from 0 to 15 degrees incremental step of 5 degrees and conditions of airflow velocity from 0 to 25 m s minimum incremental step of 0 5 m s 
1	108961	8961	60 000 data points are collected from every piezoelectric sensor for each flight state .to achieve this goal stanford structures and composites lab sacl has developed a smart wing which embeds a multifunctional sensor network on the surface layup of the wing collected from a series of wind tunnel tests with different flight states the dataset explored in this study includes conditions of angle of attack from 0 to 15 degrees incremental step of 5 degrees and conditions of airflow velocity from 0 to 25 m s minimum incremental step of 0 5 m s .we perform data augmentation in the time domain by splitting 60 000 data points into numerous segments as samples 80 samples are used as training data 10 are used as validation data and the 10 else are used as testing data with uniform distribution among each flight state goal minimize misclassification rate 1 1 .discussion and future work features references	60 000 data points are collected from every piezoelectric sensor for each flight state 
1	108962	8962	we perform data augmentation in the time domain by splitting 60 000 data points into numerous segments as samples 80 samples are used as training data 10 are used as validation data and the 10 else are used as testing data with uniform distribution among each flight state goal minimize misclassification rate 1 1 .60 000 data points are collected from every piezoelectric sensor for each flight state .last sentence.discussion and future work features references	we perform data augmentation in the time domain by splitting 60 000 data points into numerous segments as samples 80 samples are used as training data 10 are used as validation data and the 10 else are used as testing data with uniform distribution among each flight state goal minimize misclassification rate 1 1 
0	108963	8963	the gini index categorical cross entropy convolutional neural network architecture 1 .first sentence.results of the decision tree algorithm indicate that mean and standard deviation of signal magnitudes and power spectrum are key features .decision tree	the gini index categorical cross entropy convolutional neural network architecture 1 
1	108964	8964	results of the decision tree algorithm indicate that mean and standard deviation of signal magnitudes and power spectrum are key features .the gini index categorical cross entropy convolutional neural network architecture 1 .when velocity interval becomes smaller features from different sensors are required to guarantee higher classification performance .decision tree	results of the decision tree algorithm indicate that mean and standard deviation of signal magnitudes and power spectrum are key features 
1	108965	8965	when velocity interval becomes smaller features from different sensors are required to guarantee higher classification performance .results of the decision tree algorithm indicate that mean and standard deviation of signal magnitudes and power spectrum are key features .2 .decision tree	when velocity interval becomes smaller features from different sensors are required to guarantee higher classification performance 
1	108966	8966	linear models work well with manually designed features .2 .feature selection improves linear separability of the data .decision tree	linear models work well with manually designed features 
1	108967	8967	feature selection improves linear separability of the data .linear models work well with manually designed features .3 .decision tree	feature selection improves linear separability of the data 
1	108968	8968	the convolutional neural network shows comparable performance by feeding in only standardized signal segments .3 .it is demonstrated that the convolutional neural network can be trained to capture important features from the original signal directly .decision tree	the convolutional neural network shows comparable performance by feeding in only standardized signal segments 
1	108969	8969	it is demonstrated that the convolutional neural network can be trained to capture important features from the original signal directly .the convolutional neural network shows comparable performance by feeding in only standardized signal segments .last sentence.decision tree	it is demonstrated that the convolutional neural network can be trained to capture important features from the original signal directly 
1	108970	8970	we are going to develop a regression model in the following 6 months .first sentence.discretized flight state has constrained application if the resolution is not sufficient and high resolution requirement with limited data also poses difficulties for classification .future work	we are going to develop a regression model in the following 6 months 
1	108971	8971	discretized flight state has constrained application if the resolution is not sufficient and high resolution requirement with limited data also poses difficulties for classification .we are going to develop a regression model in the following 6 months .we hope to train a regression model to provide an accurate estimate of the flight state for example aoa 9 8 airflow velocity 24 3 m s which would be more of practical use than specifying an approximate range of aoa and velocity .future work	discretized flight state has constrained application if the resolution is not sufficient and high resolution requirement with limited data also poses difficulties for classification 
1	108972	8972	we hope to train a regression model to provide an accurate estimate of the flight state for example aoa 9 8 airflow velocity 24 3 m s which would be more of practical use than specifying an approximate range of aoa and velocity .discretized flight state has constrained application if the resolution is not sufficient and high resolution requirement with limited data also poses difficulties for classification .1 f p .future work	we hope to train a regression model to provide an accurate estimate of the flight state for example aoa 9 8 airflow velocity 24 3 m s which would be more of practical use than specifying an approximate range of aoa and velocity 
1	108973	8973	1 f p .we hope to train a regression model to provide an accurate estimate of the flight state for example aoa 9 8 airflow velocity 24 3 m s which would be more of practical use than specifying an approximate range of aoa and velocity .kopsaftopoulos r nardari y h li p wang b ye f k chang experimental identification of structural dynamics and aeroelastic properties of a self sensing smart composite wing in proceedings of the 10th international workshop on structural health monitoring stanford ca usa 1 3 september 2015 .future work	1 f p 
1	108974	8974	kopsaftopoulos r nardari y h li p wang b ye f k chang experimental identification of structural dynamics and aeroelastic properties of a self sensing smart composite wing in proceedings of the 10th international workshop on structural health monitoring stanford ca usa 1 3 september 2015 .1 f p .2 x chen f p .future work	kopsaftopoulos r nardari y h li p wang b ye f k chang experimental identification of structural dynamics and aeroelastic properties of a self sensing smart composite wing in proceedings of the 10th international workshop on structural health monitoring stanford ca usa 1 3 september 2015 
1	108975	8975	2 x chen f p .kopsaftopoulos r nardari y h li p wang b ye f k chang experimental identification of structural dynamics and aeroelastic properties of a self sensing smart composite wing in proceedings of the 10th international workshop on structural health monitoring stanford ca usa 1 3 september 2015 .kopsaftopoulos q wu h ren f in this problem a large feature pool from both the time and frequency domains is created to obtain enough useful information from the raw signal data .future work	2 x chen f p 
1	108976	8976	kopsaftopoulos q wu h ren f in this problem a large feature pool from both the time and frequency domains is created to obtain enough useful information from the raw signal data .2 x chen f p .we split total data into 80 10 and 10 for training validation and test dataset respectively .future work	kopsaftopoulos q wu h ren f in this problem a large feature pool from both the time and frequency domains is created to obtain enough useful information from the raw signal data 
1	108977	8977	we split total data into 80 10 and 10 for training validation and test dataset respectively .kopsaftopoulos q wu h ren f in this problem a large feature pool from both the time and frequency domains is created to obtain enough useful information from the raw signal data .there are 4 743 training samples 522 validation samples and 522 test samples .future work	we split total data into 80 10 and 10 for training validation and test dataset respectively 
1	108978	8978	there are 4 743 training samples 522 validation samples and 522 test samples .we split total data into 80 10 and 10 for training validation and test dataset respectively .last sentence.future work	there are 4 743 training samples 522 validation samples and 522 test samples 
0	108979	8979	artist identification is the task of identifying the artist of a work given only the image with no other metadata .first sentence.many paintings have unknown or highly contested artists and experts in the field need a long time to learn the styles of various artists before being able to analyze paintings .introduction	artist identification is the task of identifying the artist of a work given only the image with no other metadata 
1	108980	8980	many paintings have unknown or highly contested artists and experts in the field need a long time to learn the styles of various artists before being able to analyze paintings .artist identification is the task of identifying the artist of a work given only the image with no other metadata .additionally even with significant expertise artist identification can be difficult because one artist s style can vary wildly from painting to painting .introduction	many paintings have unknown or highly contested artists and experts in the field need a long time to learn the styles of various artists before being able to analyze paintings 
1	108981	8981	additionally even with significant expertise artist identification can be difficult because one artist s style can vary wildly from painting to painting .many paintings have unknown or highly contested artists and experts in the field need a long time to learn the styles of various artists before being able to analyze paintings .with machine learning we can provide experts with baseline estimate to reduce necessary time and effort as well as making artist identification more accessible to those with less experience additionally this task was chosen because it is a fairly straightforward image classification task and we want to compare conventional machine learning techniques for image classification to more recent deep learning techniques .introduction	additionally even with significant expertise artist identification can be difficult because one artist s style can vary wildly from painting to painting 
1	108982	8982	with machine learning we can provide experts with baseline estimate to reduce necessary time and effort as well as making artist identification more accessible to those with less experience additionally this task was chosen because it is a fairly straightforward image classification task and we want to compare conventional machine learning techniques for image classification to more recent deep learning techniques .additionally even with significant expertise artist identification can be difficult because one artist s style can vary wildly from painting to painting .specifically in this report we will compare the use of feature extraction with an svm to the use of a cnn .introduction	with machine learning we can provide experts with baseline estimate to reduce necessary time and effort as well as making artist identification more accessible to those with less experience additionally this task was chosen because it is a fairly straightforward image classification task and we want to compare conventional machine learning techniques for image classification to more recent deep learning techniques 
1	108983	8983	specifically in this report we will compare the use of feature extraction with an svm to the use of a cnn .with machine learning we can provide experts with baseline estimate to reduce necessary time and effort as well as making artist identification more accessible to those with less experience additionally this task was chosen because it is a fairly straightforward image classification task and we want to compare conventional machine learning techniques for image classification to more recent deep learning techniques .our dataset contains 256 x 256 x 3 color images of paintings for the svm we extract features from these images as the input for the classifier while we feed in the raw image to the cnn .introduction	specifically in this report we will compare the use of feature extraction with an svm to the use of a cnn 
1	108984	8984	our dataset contains 256 x 256 x 3 color images of paintings for the svm we extract features from these images as the input for the classifier while we feed in the raw image to the cnn .specifically in this report we will compare the use of feature extraction with an svm to the use of a cnn .for both models the output is a predicted artist .introduction	our dataset contains 256 x 256 x 3 color images of paintings for the svm we extract features from these images as the input for the classifier while we feed in the raw image to the cnn 
0	108985	8985	for both models the output is a predicted artist .our dataset contains 256 x 256 x 3 color images of paintings for the svm we extract features from these images as the input for the classifier while we feed in the raw image to the cnn .the metrics we chose to compare the two methods are accuracy training and inference times and ease of implementation .introduction	for both models the output is a predicted artist 
0	108986	8986	the metrics we chose to compare the two methods are accuracy training and inference times and ease of implementation .for both models the output is a predicted artist .last sentence.introduction	the metrics we chose to compare the two methods are accuracy training and inference times and ease of implementation 
1	108987	8987	as previously stated artist identification is often a task done by human experts such as curators in museums art historians and other collectors .first sentence.however recent times has lead to a marked increase in computational methods for artist identification much of previous work on this task involves exploration into feature extraction and the subsequent application of a classifier like an svm .related work	as previously stated artist identification is often a task done by human experts such as curators in museums art historians and other collectors 
0	108988	8988	however recent times has lead to a marked increase in computational methods for artist identification much of previous work on this task involves exploration into feature extraction and the subsequent application of a classifier like an svm .as previously stated artist identification is often a task done by human experts such as curators in museums art historians and other collectors .blessing and wen uses features including dense sift hog2x2 ssim and texton histograms to classify works using an svm with 85 13 accuracy approaches using deep learning have also been very successful on this task .related work	however recent times has lead to a marked increase in computational methods for artist identification much of previous work on this task involves exploration into feature extraction and the subsequent application of a classifier like an svm 
1	108989	8989	blessing and wen uses features including dense sift hog2x2 ssim and texton histograms to classify works using an svm with 85 13 accuracy approaches using deep learning have also been very successful on this task .however recent times has lead to a marked increase in computational methods for artist identification much of previous work on this task involves exploration into feature extraction and the subsequent application of a classifier like an svm .viswanathan explores the use of three different cnn models demonstrating that features from imagenet are generally applicable to artist identification and thus showing that transfer learning can be very useful for this task most prior work has worked with datasets involving relatively few classes blessing and wen only used work from 7 artists.related work	blessing and wen uses features including dense sift hog2x2 ssim and texton histograms to classify works using an svm with 85 13 accuracy approaches using deep learning have also been very successful on this task 
1	108990	8990	viswanathan explores the use of three different cnn models demonstrating that features from imagenet are generally applicable to artist identification and thus showing that transfer learning can be very useful for this task most prior work has worked with datasets involving relatively few classes blessing and wen only used work from 7 artists.blessing and wen uses features including dense sift hog2x2 ssim and texton histograms to classify works using an svm with 85 13 accuracy approaches using deep learning have also been very successful on this task .last sentence.related work	viswanathan explores the use of three different cnn models demonstrating that features from imagenet are generally applicable to artist identification and thus showing that transfer learning can be very useful for this task most prior work has worked with datasets involving relatively few classes blessing and wen only used work from 7 artists
0	108991	8991	our data is obtained from the dataset for the kaggle competition painters by numbers .first sentence.last sentence.obtaining the data	our data is obtained from the dataset for the kaggle competition painters by numbers 
1	108992	8992	a set of 6 image descriptors were explored as features for the svm gist descriptors hu moments color histograms sift keypoints histogram of oriented gradients and haralick textures .first sentence.each of the 64 possible feature combinations was explored by concatenating feature vectors obtained from each method horizontally .feature extraction	a set of 6 image descriptors were explored as features for the svm gist descriptors hu moments color histograms sift keypoints histogram of oriented gradients and haralick textures 
0	108993	8993	each of the 64 possible feature combinations was explored by concatenating feature vectors obtained from each method horizontally .a set of 6 image descriptors were explored as features for the svm gist descriptors hu moments color histograms sift keypoints histogram of oriented gradients and haralick textures .after exploring the combinations thoroughly the 3 most useful features were found to be the gist descriptors hu moments and color histograms .feature extraction	each of the 64 possible feature combinations was explored by concatenating feature vectors obtained from each method horizontally 
0	108994	8994	after exploring the combinations thoroughly the 3 most useful features were found to be the gist descriptors hu moments and color histograms .each of the 64 possible feature combinations was explored by concatenating feature vectors obtained from each method horizontally .gist descriptors are a set of 5 attributes that represent intuitive properties of a scene naturalness openness roughness expansion and ruggedness .feature extraction	after exploring the combinations thoroughly the 3 most useful features were found to be the gist descriptors hu moments and color histograms 
0	108995	8995	gist descriptors are a set of 5 attributes that represent intuitive properties of a scene naturalness openness roughness expansion and ruggedness .after exploring the combinations thoroughly the 3 most useful features were found to be the gist descriptors hu moments and color histograms .these attributes were determined by aude oliva and antonio torralba hu moments are a set of 7 polynomial combinations of image moments that are defined as to be scale shift and rotation invariant .feature extraction	gist descriptors are a set of 5 attributes that represent intuitive properties of a scene naturalness openness roughness expansion and ruggedness 
0	108996	8996	these attributes were determined by aude oliva and antonio torralba hu moments are a set of 7 polynomial combinations of image moments that are defined as to be scale shift and rotation invariant .gist descriptors are a set of 5 attributes that represent intuitive properties of a scene naturalness openness roughness expansion and ruggedness .scale and shift invariance are ensured by using the central moments to account for shift and scaling by the 0th moment to account for scale .feature extraction	these attributes were determined by aude oliva and antonio torralba hu moments are a set of 7 polynomial combinations of image moments that are defined as to be scale shift and rotation invariant 
0	108997	8997	scale and shift invariance are ensured by using the central moments to account for shift and scaling by the 0th moment to account for scale .these attributes were determined by aude oliva and antonio torralba hu moments are a set of 7 polynomial combinations of image moments that are defined as to be scale shift and rotation invariant .rotation invariance is obtained by the specific polynomial combinations of moments from ming huei hu the color histogram is a simple set of 3 histograms representing the distribution of color appearances in the image .feature extraction	scale and shift invariance are ensured by using the central moments to account for shift and scaling by the 0th moment to account for scale 
0	108998	8998	rotation invariance is obtained by the specific polynomial combinations of moments from ming huei hu the color histogram is a simple set of 3 histograms representing the distribution of color appearances in the image .scale and shift invariance are ensured by using the central moments to account for shift and scaling by the 0th moment to account for scale .a histogram for each color was computed by quantizing the color values for each channel into 8 bins and counting appearances of each quantized color .feature extraction	rotation invariance is obtained by the specific polynomial combinations of moments from ming huei hu the color histogram is a simple set of 3 histograms representing the distribution of color appearances in the image 
0	108999	8999	a histogram for each color was computed by quantizing the color values for each channel into 8 bins and counting appearances of each quantized color .rotation invariance is obtained by the specific polynomial combinations of moments from ming huei hu the color histogram is a simple set of 3 histograms representing the distribution of color appearances in the image .the histogram has no representation of the spatial distribution of the colors only their appearance each feature combination was scaled using min max scaling so that each element was in the rage 0 1 .feature extraction	a histogram for each color was computed by quantizing the color values for each channel into 8 bins and counting appearances of each quantized color 
1	109000	9000	the histogram has no representation of the spatial distribution of the colors only their appearance each feature combination was scaled using min max scaling so that each element was in the rage 0 1 .a histogram for each color was computed by quantizing the color values for each channel into 8 bins and counting appearances of each quantized color .for input to the svm pca was applied with 100 components on the training features to determine the top 100 principal components .feature extraction	the histogram has no representation of the spatial distribution of the colors only their appearance each feature combination was scaled using min max scaling so that each element was in the rage 0 1 
1	109001	9001	for input to the svm pca was applied with 100 components on the training features to determine the top 100 principal components .the histogram has no representation of the spatial distribution of the colors only their appearance each feature combination was scaled using min max scaling so that each element was in the rage 0 1 .test data was projected onto the same principal components prior to prediction .feature extraction	for input to the svm pca was applied with 100 components on the training features to determine the top 100 principal components 
0	109002	9002	test data was projected onto the same principal components prior to prediction .for input to the svm pca was applied with 100 components on the training features to determine the top 100 principal components .examples of feature extraction can be seen in for each class where a one vs rest classification scheme was used so a classifier for each class was trained to identify that particular class .feature extraction	test data was projected onto the same principal components prior to prediction 
1	109003	9003	examples of feature extraction can be seen in for each class where a one vs rest classification scheme was used so a classifier for each class was trained to identify that particular class .test data was projected onto the same principal components prior to prediction .for each class a distinct svm finds the margin that best separates the data in one class from the data in the rest after the rbf kernel is applied while minimizing the allowable misclassification terms i .feature extraction	examples of feature extraction can be seen in for each class where a one vs rest classification scheme was used so a classifier for each class was trained to identify that particular class 
0	109004	9004	for each class a distinct svm finds the margin that best separates the data in one class from the data in the rest after the rbf kernel is applied while minimizing the allowable misclassification terms i .examples of feature extraction can be seen in for each class where a one vs rest classification scheme was used so a classifier for each class was trained to identify that particular class .last sentence.feature extraction	for each class a distinct svm finds the margin that best separates the data in one class from the data in the rest after the rbf kernel is applied while minimizing the allowable misclassification terms i 
0	109005	9005	as a deep learning approach to image classification the cnn is a neural network that consists of several layers of different types including convolutional max pooling relu activation dropout dense fully connected and softmax .first sentence.each convolutional layer is a set of learnable 2d filters which are applied to input data by the 2d cross correlation operation .cnn	as a deep learning approach to image classification the cnn is a neural network that consists of several layers of different types including convolutional max pooling relu activation dropout dense fully connected and softmax 
0	109006	9006	each convolutional layer is a set of learnable 2d filters which are applied to input data by the 2d cross correlation operation .as a deep learning approach to image classification the cnn is a neural network that consists of several layers of different types including convolutional max pooling relu activation dropout dense fully connected and softmax .in max pooling layers data is downsampled by a set factor n by choosing only the max value in each n n square to propagate forward .cnn	each convolutional layer is a set of learnable 2d filters which are applied to input data by the 2d cross correlation operation 
0	109007	9007	in max pooling layers data is downsampled by a set factor n by choosing only the max value in each n n square to propagate forward .each convolutional layer is a set of learnable 2d filters which are applied to input data by the 2d cross correlation operation .the relu activation function is a unit ramp function f x max x 0 that allows for nonlinearlity in the network .cnn	in max pooling layers data is downsampled by a set factor n by choosing only the max value in each n n square to propagate forward 
0	109008	9008	the relu activation function is a unit ramp function f x max x 0 that allows for nonlinearlity in the network .in max pooling layers data is downsampled by a set factor n by choosing only the max value in each n n square to propagate forward .in dense layers input data is flattened into 1d vectors multiplied by a matrix of learnable weights and added with a learnable bias .cnn	the relu activation function is a unit ramp function f x max x 0 that allows for nonlinearlity in the network 
0	109009	9009	in dense layers input data is flattened into 1d vectors multiplied by a matrix of learnable weights and added with a learnable bias .the relu activation function is a unit ramp function f x max x 0 that allows for nonlinearlity in the network .dropout removes a percentage of activations to help prevent overfitting .cnn	in dense layers input data is flattened into 1d vectors multiplied by a matrix of learnable weights and added with a learnable bias 
0	109010	9010	dropout removes a percentage of activations to help prevent overfitting .in dense layers input data is flattened into 1d vectors multiplied by a matrix of learnable weights and added with a learnable bias .finally the softmax layer computes the class probabilities for the data for our cnn we base our architecture on the winning submission to the kaggle painters by numbers competition j e fj where l i indicates the loss for example i y i is the correct class for example i j is one of the potential classes and f k indicates the score for class k according to the cnn .cnn	dropout removes a percentage of activations to help prevent overfitting 
1	109011	9011	finally the softmax layer computes the class probabilities for the data for our cnn we base our architecture on the winning submission to the kaggle painters by numbers competition j e fj where l i indicates the loss for example i y i is the correct class for example i j is one of the potential classes and f k indicates the score for class k according to the cnn .dropout removes a percentage of activations to help prevent overfitting .the architecture is shown in.cnn	finally the softmax layer computes the class probabilities for the data for our cnn we base our architecture on the winning submission to the kaggle painters by numbers competition j e fj where l i indicates the loss for example i y i is the correct class for example i j is one of the potential classes and f k indicates the score for class k according to the cnn 
0	109012	9012	the architecture is shown in.finally the softmax layer computes the class probabilities for the data for our cnn we base our architecture on the winning submission to the kaggle painters by numbers competition j e fj where l i indicates the loss for example i y i is the correct class for example i j is one of the potential classes and f k indicates the score for class k according to the cnn .last sentence.cnn	the architecture is shown in
1	109013	9013	our svm model is implemented with the scikit learn package to choose the best hyperparameters features and c for our svm we ran a grid search and 3 fold cross validation across 10 5 10 5 and c 10 5 10 5 using the training set for each feature combination .first sentence.the model for each feature combination with best and c was then tested on our validation data and the best was chosen as our best model .experiments	our svm model is implemented with the scikit learn package to choose the best hyperparameters features and c for our svm we ran a grid search and 3 fold cross validation across 10 5 10 5 and c 10 5 10 5 using the training set for each feature combination 
0	109014	9014	the model for each feature combination with best and c was then tested on our validation data and the best was chosen as our best model .our svm model is implemented with the scikit learn package to choose the best hyperparameters features and c for our svm we ran a grid search and 3 fold cross validation across 10 5 10 5 and c 10 5 10 5 using the training set for each feature combination .the cnn was also trained multiple times using different architectures adding and removing convolutional and max pooling layers and the best model was chosen using the final validation accuracy after training .experiments	the model for each feature combination with best and c was then tested on our validation data and the best was chosen as our best model 
1	109015	9015	the cnn was also trained multiple times using different architectures adding and removing convolutional and max pooling layers and the best model was chosen using the final validation accuracy after training .the model for each feature combination with best and c was then tested on our validation data and the best was chosen as our best model .last sentence.experiments	the cnn was also trained multiple times using different architectures adding and removing convolutional and max pooling layers and the best model was chosen using the final validation accuracy after training 
0	109016	9016	the metric we used for judging our models was accuracy on the test set .first sentence.our dataset was overall reasonably balanced so this should be a good measure for how our models perform on these classes .performance	the metric we used for judging our models was accuracy on the test set 
0	109017	9017	our dataset was overall reasonably balanced so this should be a good measure for how our models perform on these classes .the metric we used for judging our models was accuracy on the test set .we found that the 3 most useful features in classifying artists were the gist descriptors hu moments and color histograms .performance	our dataset was overall reasonably balanced so this should be a good measure for how our models perform on these classes 
0	109018	9018	we found that the 3 most useful features in classifying artists were the gist descriptors hu moments and color histograms .our dataset was overall reasonably balanced so this should be a good measure for how our models perform on these classes .from the results tabulated in we can see that compared to even the best svm results the cnn is superior .performance	we found that the 3 most useful features in classifying artists were the gist descriptors hu moments and color histograms 
0	109019	9019	from the results tabulated in we can see that compared to even the best svm results the cnn is superior .we found that the 3 most useful features in classifying artists were the gist descriptors hu moments and color histograms .it achieves higher test accuracy with less noticeable overfitting .performance	from the results tabulated in we can see that compared to even the best svm results the cnn is superior 
0	109020	9020	it achieves higher test accuracy with less noticeable overfitting .from the results tabulated in we can see that compared to even the best svm results the cnn is superior .looking at.performance	it achieves higher test accuracy with less noticeable overfitting 
0	109021	9021	looking at.it achieves higher test accuracy with less noticeable overfitting .last sentence.performance	looking at
1	109022	9022	to compare inference time fairly inference for both models were performed on the ec2 instance .first sentence.the results show that cnn took an order of magnitude less time to run inference while achieving higher accuracy .training and inference time	to compare inference time fairly inference for both models were performed on the ec2 instance 
0	109023	9023	the results show that cnn took an order of magnitude less time to run inference while achieving higher accuracy .to compare inference time fairly inference for both models were performed on the ec2 instance .the bulk of the inference time for the svm can also be accounted for by the feature extraction of the test data however so if feature extraction were accelerated the svm might achieve similar time results .training and inference time	the results show that cnn took an order of magnitude less time to run inference while achieving higher accuracy 
1	109024	9024	the bulk of the inference time for the svm can also be accounted for by the feature extraction of the test data however so if feature extraction were accelerated the svm might achieve similar time results .the results show that cnn took an order of magnitude less time to run inference while achieving higher accuracy .judging from our current results however the cnn is superior in inference each blue data point in.training and inference time	the bulk of the inference time for the svm can also be accounted for by the feature extraction of the test data however so if feature extraction were accelerated the svm might achieve similar time results 
0	109025	9025	judging from our current results however the cnn is superior in inference each blue data point in.the bulk of the inference time for the svm can also be accounted for by the feature extraction of the test data however so if feature extraction were accelerated the svm might achieve similar time results .last sentence.training and inference time	judging from our current results however the cnn is superior in inference each blue data point in
1	109026	9026	in terms of ease of implementation the cnn was superior all it required was setting up the initial architecture and feeding in the raw images .first sentence.in contrast implementing the svm required a lot of overhead in researching potential features correctly extracting these features from the images and then tuning different combinations to be used with the model .ease of implementation	in terms of ease of implementation the cnn was superior all it required was setting up the initial architecture and feeding in the raw images 
1	109027	9027	in contrast implementing the svm required a lot of overhead in researching potential features correctly extracting these features from the images and then tuning different combinations to be used with the model .in terms of ease of implementation the cnn was superior all it required was setting up the initial architecture and feeding in the raw images .last sentence.ease of implementation	in contrast implementing the svm required a lot of overhead in researching potential features correctly extracting these features from the images and then tuning different combinations to be used with the model 
1	109028	9028	we explored the task of artist identification using a dataset of 7462 paintings from 15 artists and compare the performance training and inference time and ease of implementation for both the classical method of feature extraction with an svm classifier and the deep learning method of a cnn .first sentence.our best result came from the cnn with an accuracy of 74 7 in comparison to our best svm using gist features and hu moments with an accuracy of 68 1 for future work we would like to address the overfitting in our svm models .conclusion and future work	we explored the task of artist identification using a dataset of 7462 paintings from 15 artists and compare the performance training and inference time and ease of implementation for both the classical method of feature extraction with an svm classifier and the deep learning method of a cnn 
1	109029	9029	our best result came from the cnn with an accuracy of 74 7 in comparison to our best svm using gist features and hu moments with an accuracy of 68 1 for future work we would like to address the overfitting in our svm models .we explored the task of artist identification using a dataset of 7462 paintings from 15 artists and compare the performance training and inference time and ease of implementation for both the classical method of feature extraction with an svm classifier and the deep learning method of a cnn .despite the tuning of the regularization parameter our svms are overfitting heavily to the training data this may be mitigated by other regularization techniques such as early stopping we would also like to investigate additional features such as classemes given our rather small dataset we believe that transfer learning would have a lot of success on this task .conclusion and future work	our best result came from the cnn with an accuracy of 74 7 in comparison to our best svm using gist features and hu moments with an accuracy of 68 1 for future work we would like to address the overfitting in our svm models 
1	109030	9030	despite the tuning of the regularization parameter our svms are overfitting heavily to the training data this may be mitigated by other regularization techniques such as early stopping we would also like to investigate additional features such as classemes given our rather small dataset we believe that transfer learning would have a lot of success on this task .our best result came from the cnn with an accuracy of 74 7 in comparison to our best svm using gist features and hu moments with an accuracy of 68 1 for future work we would like to address the overfitting in our svm models .training our cnn with a large dataset such as the imagenet dataset and then fine tuning for artist classification with our smaller dataset can help our model gain knowledge about object recognition which has already proven useful in features for our svm .conclusion and future work	despite the tuning of the regularization parameter our svms are overfitting heavily to the training data this may be mitigated by other regularization techniques such as early stopping we would also like to investigate additional features such as classemes given our rather small dataset we believe that transfer learning would have a lot of success on this task 
1	109031	9031	training our cnn with a large dataset such as the imagenet dataset and then fine tuning for artist classification with our smaller dataset can help our model gain knowledge about object recognition which has already proven useful in features for our svm .despite the tuning of the regularization parameter our svms are overfitting heavily to the training data this may be mitigated by other regularization techniques such as early stopping we would also like to investigate additional features such as classemes given our rather small dataset we believe that transfer learning would have a lot of success on this task .last sentence.conclusion and future work	training our cnn with a large dataset such as the imagenet dataset and then fine tuning for artist classification with our smaller dataset can help our model gain knowledge about object recognition which has already proven useful in features for our svm 
0	109032	9032	the initial dataset and implementations of the cnn.first sentence.last sentence.contributions	the initial dataset and implementations of the cnn
0	109033	9033	our code can be found at www github com jchen437 artist classification .first sentence.last sentence.code	our code can be found at www github com jchen437 artist classification 
1	109034	9034	many present day facial recognition systems focus on making verification or identification facial feature invariant .first sentence.while these systems are highly effective for fully automated tasks manual facial recognition is still required in numerous real life scenarios that involve comparing against id photos .category computer vision	many present day facial recognition systems focus on making verification or identification facial feature invariant 
1	109035	9035	while these systems are highly effective for fully automated tasks manual facial recognition is still required in numerous real life scenarios that involve comparing against id photos .many present day facial recognition systems focus on making verification or identification facial feature invariant .our goal is to build a system that is capable of transforming faces to include or exclude glasses and beard .category computer vision	while these systems are highly effective for fully automated tasks manual facial recognition is still required in numerous real life scenarios that involve comparing against id photos 
0	109036	9036	our goal is to build a system that is capable of transforming faces to include or exclude glasses and beard .while these systems are highly effective for fully automated tasks manual facial recognition is still required in numerous real life scenarios that involve comparing against id photos .such a system should be able to handle a wider range of facial features with small modifications .category computer vision	our goal is to build a system that is capable of transforming faces to include or exclude glasses and beard 
0	109037	9037	such a system should be able to handle a wider range of facial features with small modifications .our goal is to build a system that is capable of transforming faces to include or exclude glasses and beard .a few network structures have been tested for this purpose and we have found that cyclegan 1 is the most capable compared to other vanilla gan systems .category computer vision	such a system should be able to handle a wider range of facial features with small modifications 
1	109038	9038	a few network structures have been tested for this purpose and we have found that cyclegan 1 is the most capable compared to other vanilla gan systems .such a system should be able to handle a wider range of facial features with small modifications .generated images from test set are presented and their inception scores 2 are analyzed .category computer vision	a few network structures have been tested for this purpose and we have found that cyclegan 1 is the most capable compared to other vanilla gan systems 
1	109039	9039	generated images from test set are presented and their inception scores 2 are analyzed .a few network structures have been tested for this purpose and we have found that cyclegan 1 is the most capable compared to other vanilla gan systems .details regarding characteristics of these generated images are also included in our discussion .category computer vision	generated images from test set are presented and their inception scores 2 are analyzed 
0	109040	9040	details regarding characteristics of these generated images are also included in our discussion .generated images from test set are presented and their inception scores 2 are analyzed .potential future improvements could involve making our system more generic or introducing semi supervised learning to expand usable data sources .category computer vision	details regarding characteristics of these generated images are also included in our discussion 
1	109041	9041	potential future improvements could involve making our system more generic or introducing semi supervised learning to expand usable data sources .details regarding characteristics of these generated images are also included in our discussion .source code for this project is available on github .category computer vision	potential future improvements could involve making our system more generic or introducing semi supervised learning to expand usable data sources 
0	109042	9042	source code for this project is available on github .potential future improvements could involve making our system more generic or introducing semi supervised learning to expand usable data sources .last sentence.category computer vision	source code for this project is available on github 
0	109043	9043	there have been significant improvement in our capability to identify and verify human faces over the past few years .first sentence.device makers are already taking advantage of such development by equipping their latest phones and tablets with ai co processor and powerful image processing algorithms .i introduction	there have been significant improvement in our capability to identify and verify human faces over the past few years 
0	109044	9044	device makers are already taking advantage of such development by equipping their latest phones and tablets with ai co processor and powerful image processing algorithms .there have been significant improvement in our capability to identify and verify human faces over the past few years .however the recent trend has mostly focused on making facial identification and verification invariant to facial features .i introduction	device makers are already taking advantage of such development by equipping their latest phones and tablets with ai co processor and powerful image processing algorithms 
0	109045	9045	however the recent trend has mostly focused on making facial identification and verification invariant to facial features .device makers are already taking advantage of such development by equipping their latest phones and tablets with ai co processor and powerful image processing algorithms .these works certainly help machine recognize human faces however most humans are interested in seeing people in the natural state without any facial disguise a system that can recover undisguised faces could be helpful for criminal investigation .i introduction	however the recent trend has mostly focused on making facial identification and verification invariant to facial features 
1	109046	9046	these works certainly help machine recognize human faces however most humans are interested in seeing people in the natural state without any facial disguise a system that can recover undisguised faces could be helpful for criminal investigation .however the recent trend has mostly focused on making facial identification and verification invariant to facial features .in particular witnesses should be able to make use of these processed images to identify the criminal among a series of id photos which typically include no disguise or in person among a number of held suspects .i introduction	these works certainly help machine recognize human faces however most humans are interested in seeing people in the natural state without any facial disguise a system that can recover undisguised faces could be helpful for criminal investigation 
1	109047	9047	in particular witnesses should be able to make use of these processed images to identify the criminal among a series of id photos which typically include no disguise or in person among a number of held suspects .these works certainly help machine recognize human faces however most humans are interested in seeing people in the natural state without any facial disguise a system that can recover undisguised faces could be helpful for criminal investigation .people utilizing online dating apps could also utilize this system to reveal the real person behind facial disguise a feature that many find useful we build on current work related to gan based style transform methods that are commonly employed for applying facial disguise .i introduction	in particular witnesses should be able to make use of these processed images to identify the criminal among a series of id photos which typically include no disguise or in person among a number of held suspects 
1	109048	9048	people utilizing online dating apps could also utilize this system to reveal the real person behind facial disguise a feature that many find useful we build on current work related to gan based style transform methods that are commonly employed for applying facial disguise .in particular witnesses should be able to make use of these processed images to identify the criminal among a series of id photos which typically include no disguise or in person among a number of held suspects .recent works have demonstrated much success in related areas we train our generative neural network using a facial disguise database from hong kong polytechnic university.i introduction	people utilizing online dating apps could also utilize this system to reveal the real person behind facial disguise a feature that many find useful we build on current work related to gan based style transform methods that are commonly employed for applying facial disguise 
1	109049	9049	recent works have demonstrated much success in related areas we train our generative neural network using a facial disguise database from hong kong polytechnic university.people utilizing online dating apps could also utilize this system to reveal the real person behind facial disguise a feature that many find useful we build on current work related to gan based style transform methods that are commonly employed for applying facial disguise .last sentence.i introduction	recent works have demonstrated much success in related areas we train our generative neural network using a facial disguise database from hong kong polytechnic university
1	109050	9050	the seminal paper on gans was first published in 2014 where g tries to minimize this function where an adversary tries to maximize it creating the min max optimization problem min g max dy l gan another related work that builds on top of the traditional gan is called the cyclegan.first sentence.last sentence.ii related work	the seminal paper on gans was first published in 2014 where g tries to minimize this function where an adversary tries to maximize it creating the min max optimization problem min g max dy l gan another related work that builds on top of the traditional gan is called the cyclegan
0	109051	9051	finding an appropriate dataset is one of the most important task for this work .first sentence.unfortunately due to privacy concerns and inherent difficulties in obtaining ground truth associated with human faces.iii dataset a data sources	finding an appropriate dataset is one of the most important task for this work 
0	109052	9052	unfortunately due to privacy concerns and inherent difficulties in obtaining ground truth associated with human faces.finding an appropriate dataset is one of the most important task for this work .last sentence.iii dataset a data sources	unfortunately due to privacy concerns and inherent difficulties in obtaining ground truth associated with human faces
1	109053	9053	we expect neural networks to produce better results when faces are intelligently selected from the images .first sentence.cropping out faces help the neural network select area of interest and reduces input size .b pre processing	we expect neural networks to produce better results when faces are intelligently selected from the images 
0	109054	9054	cropping out faces help the neural network select area of interest and reduces input size .we expect neural networks to produce better results when faces are intelligently selected from the images .with a reduced input size the network can spend resources on applying feature transformations and on identifying features .b pre processing	cropping out faces help the neural network select area of interest and reduces input size 
1	109055	9055	with a reduced input size the network can spend resources on applying feature transformations and on identifying features .cropping out faces help the neural network select area of interest and reduces input size .for this project we used image sizes of 64 64 to decrease demand on gpu memory our dataset from hong kong polytechnic comes with cropped images .b pre processing	with a reduced input size the network can spend resources on applying feature transformations and on identifying features 
1	109056	9056	for this project we used image sizes of 64 64 to decrease demand on gpu memory our dataset from hong kong polytechnic comes with cropped images .with a reduced input size the network can spend resources on applying feature transformations and on identifying features .celeba in contrast contains too much background for the dataset to be generic enough for a variety of tasks .b pre processing	for this project we used image sizes of 64 64 to decrease demand on gpu memory our dataset from hong kong polytechnic comes with cropped images 
0	109057	9057	celeba in contrast contains too much background for the dataset to be generic enough for a variety of tasks .for this project we used image sizes of 64 64 to decrease demand on gpu memory our dataset from hong kong polytechnic comes with cropped images .we made use of opencv s haar cascades.b pre processing	celeba in contrast contains too much background for the dataset to be generic enough for a variety of tasks 
0	109058	9058	we made use of opencv s haar cascades.celeba in contrast contains too much background for the dataset to be generic enough for a variety of tasks .last sentence.b pre processing	we made use of opencv s haar cascades
1	109059	9059	neural network used for our purpose are much more sophisticated than typical generative adversarial networks that deals with mnist datasets .first sentence.multiple network structures have been attempted and their differences will be presented in the results section .a vanilla gan structure	neural network used for our purpose are much more sophisticated than typical generative adversarial networks that deals with mnist datasets 
0	109060	9060	multiple network structures have been attempted and their differences will be presented in the results section .neural network used for our purpose are much more sophisticated than typical generative adversarial networks that deals with mnist datasets .given the theoretical background of generative adversarial neural network as discussed in section ii a vanila gan can be roughly represented by the project milestone has demonstrated that vanila gans have limited capability in terms of both beard removal and facial feature reconstruction .a vanilla gan structure	multiple network structures have been attempted and their differences will be presented in the results section 
1	109061	9061	given the theoretical background of generative adversarial neural network as discussed in section ii a vanila gan can be roughly represented by the project milestone has demonstrated that vanila gans have limited capability in terms of both beard removal and facial feature reconstruction .multiple network structures have been attempted and their differences will be presented in the results section .this as described in section ii can be tackled by introducing coupling implemented as a cyclegan like structure shown in the forward generator g maps disguised faces to original faces whereas the backward generator f maps original faces back to disguised faces .a vanilla gan structure	given the theoretical background of generative adversarial neural network as discussed in section ii a vanila gan can be roughly represented by the project milestone has demonstrated that vanila gans have limited capability in terms of both beard removal and facial feature reconstruction 
1	109062	9062	this as described in section ii can be tackled by introducing coupling implemented as a cyclegan like structure shown in the forward generator g maps disguised faces to original faces whereas the backward generator f maps original faces back to disguised faces .given the theoretical background of generative adversarial neural network as discussed in section ii a vanila gan can be roughly represented by the project milestone has demonstrated that vanila gans have limited capability in terms of both beard removal and facial feature reconstruction .we apply adversarial loss functions to both gan s in addition to the adversarial loss functions we have an additional cycle consistency loss to preserve the individual identities through the generation process such that our full objective would be where is a hyperparameter that controls the relative importance of the two objectiv losses we tested cyclegan using relatively simple network structure .a vanilla gan structure	this as described in section ii can be tackled by introducing coupling implemented as a cyclegan like structure shown in the forward generator g maps disguised faces to original faces whereas the backward generator f maps original faces back to disguised faces 
1	109063	9063	we apply adversarial loss functions to both gan s in addition to the adversarial loss functions we have an additional cycle consistency loss to preserve the individual identities through the generation process such that our full objective would be where is a hyperparameter that controls the relative importance of the two objectiv losses we tested cyclegan using relatively simple network structure .this as described in section ii can be tackled by introducing coupling implemented as a cyclegan like structure shown in the forward generator g maps disguised faces to original faces whereas the backward generator f maps original faces back to disguised faces .however the cyclegan structure has two sets of generator discriminator pairing effectively doubling the size of the network .a vanilla gan structure	we apply adversarial loss functions to both gan s in addition to the adversarial loss functions we have an additional cycle consistency loss to preserve the individual identities through the generation process such that our full objective would be where is a hyperparameter that controls the relative importance of the two objectiv losses we tested cyclegan using relatively simple network structure 
1	109064	9064	however the cyclegan structure has two sets of generator discriminator pairing effectively doubling the size of the network .we apply adversarial loss functions to both gan s in addition to the adversarial loss functions we have an additional cycle consistency loss to preserve the individual identities through the generation process such that our full objective would be where is a hyperparameter that controls the relative importance of the two objectiv losses we tested cyclegan using relatively simple network structure .structure of both pairs are the same as presented in taking the exponential makes it easier for us to compare the values .a vanilla gan structure	however the cyclegan structure has two sets of generator discriminator pairing effectively doubling the size of the network 
1	109065	9065	structure of both pairs are the same as presented in taking the exponential makes it easier for us to compare the values .however the cyclegan structure has two sets of generator discriminator pairing effectively doubling the size of the network .last sentence.a vanilla gan structure	structure of both pairs are the same as presented in taking the exponential makes it easier for us to compare the values 
0	109066	9066	our experiments are conducted on google cloud vm instances with nvidia k80 gpus .first sentence.this setup significantly speeds up the training process compared to running on cpu only machines decreasing discriminator training time from over an hour to less than a minute and generator training from 10 to 15 minutes per batch on a simple cnn structure to a few seconds we built the training infrastructure using keras .v experiments a experiment environment	our experiments are conducted on google cloud vm instances with nvidia k80 gpus 
0	109067	9067	this setup significantly speeds up the training process compared to running on cpu only machines decreasing discriminator training time from over an hour to less than a minute and generator training from 10 to 15 minutes per batch on a simple cnn structure to a few seconds we built the training infrastructure using keras .our experiments are conducted on google cloud vm instances with nvidia k80 gpus .in addition we have developed a generic infrastructure that is capable of handling difference generators and discriminators in a plug and go fashion .v experiments a experiment environment	this setup significantly speeds up the training process compared to running on cpu only machines decreasing discriminator training time from over an hour to less than a minute and generator training from 10 to 15 minutes per batch on a simple cnn structure to a few seconds we built the training infrastructure using keras 
0	109068	9068	in addition we have developed a generic infrastructure that is capable of handling difference generators and discriminators in a plug and go fashion .this setup significantly speeds up the training process compared to running on cpu only machines decreasing discriminator training time from over an hour to less than a minute and generator training from 10 to 15 minutes per batch on a simple cnn structure to a few seconds we built the training infrastructure using keras .this modular infrastructure has significantly lowered overhead associated with experimenting with a wide range of network structures .v experiments a experiment environment	in addition we have developed a generic infrastructure that is capable of handling difference generators and discriminators in a plug and go fashion 
0	109069	9069	this modular infrastructure has significantly lowered overhead associated with experimenting with a wide range of network structures .in addition we have developed a generic infrastructure that is capable of handling difference generators and discriminators in a plug and go fashion .our custom code referenced vanilla gan implementation from.v experiments a experiment environment	this modular infrastructure has significantly lowered overhead associated with experimenting with a wide range of network structures 
0	109070	9070	our custom code referenced vanilla gan implementation from.this modular infrastructure has significantly lowered overhead associated with experimenting with a wide range of network structures .last sentence.v experiments a experiment environment	our custom code referenced vanilla gan implementation from
0	109071	9071	we present generated images from different networks that we have experimented with .first sentence.last sentence.b results and discussion	we present generated images from different networks that we have experimented with 
0	109072	9072	simple convolutional neural network as shown in fig 5 generated images are a lot smoother than that from multilayer perceptron .b .there is also traces of beard mustache region being modified by the generator network .fig 4 generated images from multilayer perceptron	simple convolutional neural network as shown in fig 5 generated images are a lot smoother than that from multilayer perceptron 
0	109073	9073	there is also traces of beard mustache region being modified by the generator network .simple convolutional neural network as shown in fig 5 generated images are a lot smoother than that from multilayer perceptron .also the generator seems to be brightening columns near nose where mustache typically appears .fig 4 generated images from multilayer perceptron	there is also traces of beard mustache region being modified by the generator network 
0	109074	9074	also the generator seems to be brightening columns near nose where mustache typically appears .there is also traces of beard mustache region being modified by the generator network .last sentence.fig 4 generated images from multilayer perceptron	also the generator seems to be brightening columns near nose where mustache typically appears 
1	109075	9075	c residual convolutional neural network residual networks are supposed to be better in retaining characteristics of the original image .first sentence.since this network also contains more convolutional layers the result shown in fig 6 has slightly higher quality than images generated using simple cnns .fig 5 generated images from simple cnn	c residual convolutional neural network residual networks are supposed to be better in retaining characteristics of the original image 
1	109076	9076	since this network also contains more convolutional layers the result shown in fig 6 has slightly higher quality than images generated using simple cnns .c residual convolutional neural network residual networks are supposed to be better in retaining characteristics of the original image .these images have far less bright dark bars .fig 5 generated images from simple cnn	since this network also contains more convolutional layers the result shown in fig 6 has slightly higher quality than images generated using simple cnns 
0	109077	9077	these images have far less bright dark bars .since this network also contains more convolutional layers the result shown in fig 6 has slightly higher quality than images generated using simple cnns .last sentence.fig 5 generated images from simple cnn	these images have far less bright dark bars 
0	109078	9078	d cyclegan a relatively simple cyclegan structure is implemented for this work .first sentence.this is because cycle gan consumes more than twice the memory compared to its vanilla counterparts .fig 6 generated images from residual cnn	d cyclegan a relatively simple cyclegan structure is implemented for this work 
0	109079	9079	this is because cycle gan consumes more than twice the memory compared to its vanilla counterparts .d cyclegan a relatively simple cyclegan structure is implemented for this work .expanding our network to support colored images also significantly limits complexity of the network .fig 6 generated images from residual cnn	this is because cycle gan consumes more than twice the memory compared to its vanilla counterparts 
0	109080	9080	expanding our network to support colored images also significantly limits complexity of the network .this is because cycle gan consumes more than twice the memory compared to its vanilla counterparts .nevertheless cyclegan produces high quality images as shown.fig 6 generated images from residual cnn	expanding our network to support colored images also significantly limits complexity of the network 
0	109081	9081	nevertheless cyclegan produces high quality images as shown.expanding our network to support colored images also significantly limits complexity of the network .last sentence.fig 6 generated images from residual cnn	nevertheless cyclegan produces high quality images as shown
1	109082	9082	clearly with the introduction of reconstruction and identity loss the generated images are of much higher quality .first sentence.not only that irrelevant features are modified our reconstructed images look almost identical to the original verifying that the reconstruction losses are highly effective plot of losses for cyclegan running the beard and glasses modification task is presented.fig 7 generated test images from cyclegan	clearly with the introduction of reconstruction and identity loss the generated images are of much higher quality 
1	109083	9083	not only that irrelevant features are modified our reconstructed images look almost identical to the original verifying that the reconstruction losses are highly effective plot of losses for cyclegan running the beard and glasses modification task is presented.clearly with the introduction of reconstruction and identity loss the generated images are of much higher quality .last sentence.fig 7 generated test images from cyclegan	not only that irrelevant features are modified our reconstructed images look almost identical to the original verifying that the reconstruction losses are highly effective plot of losses for cyclegan running the beard and glasses modification task is presented
0	109084	9084	all images presented here are faces of male .first sentence.this is because training the network with female faces introduces makeup to modified faces .fig 8 model losses of cyclegan	all images presented here are faces of male 
0	109085	9085	this is because training the network with female faces introduces makeup to modified faces .all images presented here are faces of male .for example removing beard adds lipstick regardless of gender .fig 8 model losses of cyclegan	this is because training the network with female faces introduces makeup to modified faces 
0	109086	9086	for example removing beard adds lipstick regardless of gender .this is because training the network with female faces introduces makeup to modified faces .similarly removing sunglasses frequently adds eyeshadow or eyeline .fig 8 model losses of cyclegan	for example removing beard adds lipstick regardless of gender 
0	109087	9087	similarly removing sunglasses frequently adds eyeshadow or eyeline .for example removing beard adds lipstick regardless of gender .another interesting phenomenon we notices is that old celebrities tend to get clear glasses whereas younger celebrities tend to get sunglasses .fig 8 model losses of cyclegan	similarly removing sunglasses frequently adds eyeshadow or eyeline 
0	109088	9088	another interesting phenomenon we notices is that old celebrities tend to get clear glasses whereas younger celebrities tend to get sunglasses .similarly removing sunglasses frequently adds eyeshadow or eyeline .though the network handles most images reasonably well we have noticed that it is still struggling with removing opaque sunglasses .fig 8 model losses of cyclegan	another interesting phenomenon we notices is that old celebrities tend to get clear glasses whereas younger celebrities tend to get sunglasses 
0	109089	9089	though the network handles most images reasonably well we have noticed that it is still struggling with removing opaque sunglasses .another interesting phenomenon we notices is that old celebrities tend to get clear glasses whereas younger celebrities tend to get sunglasses .this difficulty is expected because image with opaque sunglasses provides little information about wearers eyes .fig 8 model losses of cyclegan	though the network handles most images reasonably well we have noticed that it is still struggling with removing opaque sunglasses 
0	109090	9090	this difficulty is expected because image with opaque sunglasses provides little information about wearers eyes .though the network handles most images reasonably well we have noticed that it is still struggling with removing opaque sunglasses .the algorithm has nothing to construct the eyes from .fig 8 model losses of cyclegan	this difficulty is expected because image with opaque sunglasses provides little information about wearers eyes 
0	109091	9091	the algorithm has nothing to construct the eyes from .this difficulty is expected because image with opaque sunglasses provides little information about wearers eyes .instead it puts a generic eye in place of sunglasses which often look out of place .fig 8 model losses of cyclegan	the algorithm has nothing to construct the eyes from 
0	109092	9092	instead it puts a generic eye in place of sunglasses which often look out of place .the algorithm has nothing to construct the eyes from .this effect is observed among images in which glasses hide significant portion of eye brows .fig 8 model losses of cyclegan	instead it puts a generic eye in place of sunglasses which often look out of place 
0	109093	9093	this effect is observed among images in which glasses hide significant portion of eye brows .instead it puts a generic eye in place of sunglasses which often look out of place .reconstructed eye brows in those cases are of dubious quality since this project is generative in nature there is no accuracy to evaluate .fig 8 model losses of cyclegan	this effect is observed among images in which glasses hide significant portion of eye brows 
0	109094	9094	reconstructed eye brows in those cases are of dubious quality since this project is generative in nature there is no accuracy to evaluate .this effect is observed among images in which glasses hide significant portion of eye brows .inception score is perhaps the more appropriate numerical metric to include for the experiment .fig 8 model losses of cyclegan	reconstructed eye brows in those cases are of dubious quality since this project is generative in nature there is no accuracy to evaluate 
0	109095	9095	inception score is perhaps the more appropriate numerical metric to include for the experiment .reconstructed eye brows in those cases are of dubious quality since this project is generative in nature there is no accuracy to evaluate .inception score of all tested networks are presented in table vii .fig 8 model losses of cyclegan	inception score is perhaps the more appropriate numerical metric to include for the experiment 
0	109096	9096	inception score of all tested networks are presented in table vii .inception score is perhaps the more appropriate numerical metric to include for the experiment .since inception score for cifar 10 images.fig 8 model losses of cyclegan	inception score of all tested networks are presented in table vii 
0	109097	9097	since inception score for cifar 10 images.inception score of all tested networks are presented in table vii .last sentence.fig 8 model losses of cyclegan	since inception score for cifar 10 images
0	109098	9098	this project successfully identified a neural network structure to perform the task of modifying facial features .first sentence.although results of this work focuses exclusively on beard and glasses the same infrastructure can certainly be used for other features in the future we would like to build a generic infrastructure that is capable of handling any facial feature .vi conclusion and future works	this project successfully identified a neural network structure to perform the task of modifying facial features 
1	109099	9099	although results of this work focuses exclusively on beard and glasses the same infrastructure can certainly be used for other features in the future we would like to build a generic infrastructure that is capable of handling any facial feature .this project successfully identified a neural network structure to perform the task of modifying facial features .it would also be helpful to make the training process semisupervised .vi conclusion and future works	although results of this work focuses exclusively on beard and glasses the same infrastructure can certainly be used for other features in the future we would like to build a generic infrastructure that is capable of handling any facial feature 
0	109100	9100	it would also be helpful to make the training process semisupervised .although results of this work focuses exclusively on beard and glasses the same infrastructure can certainly be used for other features in the future we would like to build a generic infrastructure that is capable of handling any facial feature .this will allow us to include other datasets that do not have relevant tags .vi conclusion and future works	it would also be helpful to make the training process semisupervised 
0	109101	9101	this will allow us to include other datasets that do not have relevant tags .it would also be helpful to make the training process semisupervised .mirza s.vi conclusion and future works	this will allow us to include other datasets that do not have relevant tags 
0	109102	9102	mirza s.this will allow us to include other datasets that do not have relevant tags .last sentence.vi conclusion and future works	mirza s
0	109103	9103	our team has divided work evenly based on each team member s technical background and course load .first sentence.to be more specific jingbo worked on pre processing and testing neural network models boning worked on building various neural network models and meixian focused on plotting and writing reports poster .vii distribution of work	our team has divided work evenly based on each team member s technical background and course load 
0	109104	9104	to be more specific jingbo worked on pre processing and testing neural network models boning worked on building various neural network models and meixian focused on plotting and writing reports poster .our team has divided work evenly based on each team member s technical background and course load .last sentence.vii distribution of work	to be more specific jingbo worked on pre processing and testing neural network models boning worked on building various neural network models and meixian focused on plotting and writing reports poster 
1	109105	9105	traditional off the shelf lossy image compression techniques such as jpeg and webp are not designed specifically for the data being compressed and therefore do not achieve the best possible compression rates for images .first sentence.in this paper we construct a deep neural network based compression architecture using a generative model pretrained with the celeba faces dataset which consists of semantically related images .abstract	traditional off the shelf lossy image compression techniques such as jpeg and webp are not designed specifically for the data being compressed and therefore do not achieve the best possible compression rates for images 
1	109106	9106	in this paper we construct a deep neural network based compression architecture using a generative model pretrained with the celeba faces dataset which consists of semantically related images .traditional off the shelf lossy image compression techniques such as jpeg and webp are not designed specifically for the data being compressed and therefore do not achieve the best possible compression rates for images .our architecture compresses related images by reversing the generator of a gan and omits the encoder altogether .abstract	in this paper we construct a deep neural network based compression architecture using a generative model pretrained with the celeba faces dataset which consists of semantically related images 
1	109107	9107	our architecture compresses related images by reversing the generator of a gan and omits the encoder altogether .in this paper we construct a deep neural network based compression architecture using a generative model pretrained with the celeba faces dataset which consists of semantically related images .we report orders of magnitude improvements in the compression rate compared to standard methods such as the high quality jpeg and are able to achieve comparable compression magnitudes to the 1 quality jpeg while maintaining a much higher fidelity to the original image and being able to create much more perceptual reconstructions .abstract	our architecture compresses related images by reversing the generator of a gan and omits the encoder altogether 
1	109108	9108	we report orders of magnitude improvements in the compression rate compared to standard methods such as the high quality jpeg and are able to achieve comparable compression magnitudes to the 1 quality jpeg while maintaining a much higher fidelity to the original image and being able to create much more perceptual reconstructions .our architecture compresses related images by reversing the generator of a gan and omits the encoder altogether .finally we evaluate our reconstructions with mse psnr and ssim measures compare them with jpeg of different qualities and k means compression and report compression magnitudes in bits per pixel bbp and the compression ratio cr .abstract	we report orders of magnitude improvements in the compression rate compared to standard methods such as the high quality jpeg and are able to achieve comparable compression magnitudes to the 1 quality jpeg while maintaining a much higher fidelity to the original image and being able to create much more perceptual reconstructions 
0	109109	9109	finally we evaluate our reconstructions with mse psnr and ssim measures compare them with jpeg of different qualities and k means compression and report compression magnitudes in bits per pixel bbp and the compression ratio cr .we report orders of magnitude improvements in the compression rate compared to standard methods such as the high quality jpeg and are able to achieve comparable compression magnitudes to the 1 quality jpeg while maintaining a much higher fidelity to the original image and being able to create much more perceptual reconstructions .last sentence.abstract	finally we evaluate our reconstructions with mse psnr and ssim measures compare them with jpeg of different qualities and k means compression and report compression magnitudes in bits per pixel bbp and the compression ratio cr 
0	109110	9110	standard lossy image compression techniques such as jpeg and webp are not data specific i e .first sentence.they are not designed specifically to handle individual datasets in which the images are semantically related to each other .introduction and related work	standard lossy image compression techniques such as jpeg and webp are not data specific i e 
1	109111	9111	they are not designed specifically to handle individual datasets in which the images are semantically related to each other .standard lossy image compression techniques such as jpeg and webp are not data specific i e .hence these techniques do not make use of the semantic relations among the images in a specific dataset and do not achieve the best possible compression rates .introduction and related work	they are not designed specifically to handle individual datasets in which the images are semantically related to each other 
1	109112	9112	hence these techniques do not make use of the semantic relations among the images in a specific dataset and do not achieve the best possible compression rates .they are not designed specifically to handle individual datasets in which the images are semantically related to each other .this has led to a growth in the research towards deep neural network based compression architectures .introduction and related work	hence these techniques do not make use of the semantic relations among the images in a specific dataset and do not achieve the best possible compression rates 
0	109113	9113	this has led to a growth in the research towards deep neural network based compression architectures .hence these techniques do not make use of the semantic relations among the images in a specific dataset and do not achieve the best possible compression rates .these models tend to achieve orders of magnitude better compression rates while still maintaining higher accuracy and fidelity in their reconstructions .introduction and related work	this has led to a growth in the research towards deep neural network based compression architectures 
0	109114	9114	these models tend to achieve orders of magnitude better compression rates while still maintaining higher accuracy and fidelity in their reconstructions .this has led to a growth in the research towards deep neural network based compression architectures .another challenge in constructing a generative deep neural compressor rises from the fact that gans lack the encoder function .introduction and related work	these models tend to achieve orders of magnitude better compression rates while still maintaining higher accuracy and fidelity in their reconstructions 
0	109115	9115	another challenge in constructing a generative deep neural compressor rises from the fact that gans lack the encoder function .these models tend to achieve orders of magnitude better compression rates while still maintaining higher accuracy and fidelity in their reconstructions .the generator network of a gan can map from the smaller dimensional latent space to the larger dimensional image space but not the other way around .introduction and related work	another challenge in constructing a generative deep neural compressor rises from the fact that gans lack the encoder function 
1	109116	9116	the generator network of a gan can map from the smaller dimensional latent space to the larger dimensional image space but not the other way around .another challenge in constructing a generative deep neural compressor rises from the fact that gans lack the encoder function .in compression language this means that a gan can give us a decoder but not an encoder .introduction and related work	the generator network of a gan can map from the smaller dimensional latent space to the larger dimensional image space but not the other way around 
0	109117	9117	in compression language this means that a gan can give us a decoder but not an encoder .the generator network of a gan can map from the smaller dimensional latent space to the larger dimensional image space but not the other way around .addressing this issue is the work of.introduction and related work	in compression language this means that a gan can give us a decoder but not an encoder 
0	109118	9118	addressing this issue is the work of.in compression language this means that a gan can give us a decoder but not an encoder .last sentence.introduction and related work	addressing this issue is the work of
0	109119	9119	our main contribution in this paper is the introduction of this novel method of latent space vector recovery into the compression literature .first sentence.accordingly we construct a compressor using solely a pretrained gan generator omitting the encoder altogether .our contributions	our main contribution in this paper is the introduction of this novel method of latent space vector recovery into the compression literature 
0	109120	9120	accordingly we construct a compressor using solely a pretrained gan generator omitting the encoder altogether .our main contribution in this paper is the introduction of this novel method of latent space vector recovery into the compression literature .we refer to this method as gan reversal throughout the paper .our contributions	accordingly we construct a compressor using solely a pretrained gan generator omitting the encoder altogether 
0	109121	9121	we refer to this method as gan reversal throughout the paper .accordingly we construct a compressor using solely a pretrained gan generator omitting the encoder altogether .compression is done via training a vector in the latent space which is further compressed with bzip2 a standard lossless compression scheme .our contributions	we refer to this method as gan reversal throughout the paper 
1	109122	9122	compression is done via training a vector in the latent space which is further compressed with bzip2 a standard lossless compression scheme .we refer to this method as gan reversal throughout the paper .decompression of images is simply done with a forward propagation of the latent vector through the gan generator .our contributions	compression is done via training a vector in the latent space which is further compressed with bzip2 a standard lossless compression scheme 
1	109123	9123	decompression of images is simply done with a forward propagation of the latent vector through the gan generator .compression is done via training a vector in the latent space which is further compressed with bzip2 a standard lossless compression scheme .to the best of our knowledge we are not familiar with any other literature that uses this gan reversal scheme for image compression furthermore the mentioned paper of.our contributions	decompression of images is simply done with a forward propagation of the latent vector through the gan generator 
0	109124	9124	to the best of our knowledge we are not familiar with any other literature that uses this gan reversal scheme for image compression furthermore the mentioned paper of.decompression of images is simply done with a forward propagation of the latent vector through the gan generator .last sentence.our contributions	to the best of our knowledge we are not familiar with any other literature that uses this gan reversal scheme for image compression furthermore the mentioned paper of
0	109125	9125	a gan consists of two networks called the generator and the discriminator .first sentence.during training the network tries to minimize an adversarial loss function .models and methods	a gan consists of two networks called the generator and the discriminator 
0	109126	9126	during training the network tries to minimize an adversarial loss function .a gan consists of two networks called the generator and the discriminator .to achieve this the generator tries to create images that cannot be differentiated from true images while the discriminator tries to correctly classify images as real or generated .models and methods	during training the network tries to minimize an adversarial loss function 
1	109127	9127	to achieve this the generator tries to create images that cannot be differentiated from true images while the discriminator tries to correctly classify images as real or generated .during training the network tries to minimize an adversarial loss function .the discriminator is constructed just for the training purposes and is discarded after training .models and methods	to achieve this the generator tries to create images that cannot be differentiated from true images while the discriminator tries to correctly classify images as real or generated 
0	109128	9128	the discriminator is constructed just for the training purposes and is discarded after training .to achieve this the generator tries to create images that cannot be differentiated from true images while the discriminator tries to correctly classify images as real or generated .as stated in the introduction the remaining generator network maps from a low dimensional latent space to a higher dimensional image space .models and methods	the discriminator is constructed just for the training purposes and is discarded after training 
1	109129	9129	as stated in the introduction the remaining generator network maps from a low dimensional latent space to a higher dimensional image space .the discriminator is constructed just for the training purposes and is discarded after training .for the formulas below we will refer to the latent space as the z space and refer to the image space as the x space .models and methods	as stated in the introduction the remaining generator network maps from a low dimensional latent space to a higher dimensional image space 
0	109130	9130	for the formulas below we will refer to the latent space as the z space and refer to the image space as the x space .as stated in the introduction the remaining generator network maps from a low dimensional latent space to a higher dimensional image space .it is not an inherent feature of a gan network to perform a mapping from x z .models and methods	for the formulas below we will refer to the latent space as the z space and refer to the image space as the x space 
0	109131	9131	it is not an inherent feature of a gan network to perform a mapping from x z .for the formulas below we will refer to the latent space as the z space and refer to the image space as the x space .however this mapping is exactly the necessary encoding step of compression .models and methods	it is not an inherent feature of a gan network to perform a mapping from x z 
0	109132	9132	however this mapping is exactly the necessary encoding step of compression .it is not an inherent feature of a gan network to perform a mapping from x z .in this compression architecture we encode images to the latent space based on the work of the specific model construction is as follows first we either train a gan or acquire the pretrained generator of a gan that is capable of generating images of a specific domain such as human faces .models and methods	however this mapping is exactly the necessary encoding step of compression 
1	109133	9133	in this compression architecture we encode images to the latent space based on the work of the specific model construction is as follows first we either train a gan or acquire the pretrained generator of a gan that is capable of generating images of a specific domain such as human faces .however this mapping is exactly the necessary encoding step of compression .the weights of this generator network which exactly corresponds to the decoder of our compressor are kept frozen .models and methods	in this compression architecture we encode images to the latent space based on the work of the specific model construction is as follows first we either train a gan or acquire the pretrained generator of a gan that is capable of generating images of a specific domain such as human faces 
1	109134	9134	the weights of this generator network which exactly corresponds to the decoder of our compressor are kept frozen .in this compression architecture we encode images to the latent space based on the work of the specific model construction is as follows first we either train a gan or acquire the pretrained generator of a gan that is capable of generating images of a specific domain such as human faces .the gans that we use for image generation are from a specific category called dcgans introduced by because these mentioned loss functions are pixel wise distance metrics they have limitations in terms of outputting perceptual images and recovering the edges in the original images .models and methods	the weights of this generator network which exactly corresponds to the decoder of our compressor are kept frozen 
1	109135	9135	the gans that we use for image generation are from a specific category called dcgans introduced by because these mentioned loss functions are pixel wise distance metrics they have limitations in terms of outputting perceptual images and recovering the edges in the original images .the weights of this generator network which exactly corresponds to the decoder of our compressor are kept frozen .this motivates us to use perceptual similarity metrics for our training .models and methods	the gans that we use for image generation are from a specific category called dcgans introduced by because these mentioned loss functions are pixel wise distance metrics they have limitations in terms of outputting perceptual images and recovering the edges in the original images 
0	109136	9136	this motivates us to use perceptual similarity metrics for our training .the gans that we use for image generation are from a specific category called dcgans introduced by because these mentioned loss functions are pixel wise distance metrics they have limitations in terms of outputting perceptual images and recovering the edges in the original images .one of such metrics is the well known structural similarity index ssim .models and methods	this motivates us to use perceptual similarity metrics for our training 
0	109137	9137	one of such metrics is the well known structural similarity index ssim .this motivates us to use perceptual similarity metrics for our training .for two aligned windows x and y from different images this metric is defined as note that this function takes the neighboring pixels into account and it is a more perceptual metric than pixel wise metrics .models and methods	one of such metrics is the well known structural similarity index ssim 
1	109138	9138	for two aligned windows x and y from different images this metric is defined as note that this function takes the neighboring pixels into account and it is a more perceptual metric than pixel wise metrics .one of such metrics is the well known structural similarity index ssim .another remark is that ssim value increases up to 1 as the images become more and more similar so the corresponding loss function to be minimized is we minimize these loss functions with respect to the latent vector z via stochastic gradient descent .models and methods	for two aligned windows x and y from different images this metric is defined as note that this function takes the neighboring pixels into account and it is a more perceptual metric than pixel wise metrics 
0	109139	9139	another remark is that ssim value increases up to 1 as the images become more and more similar so the corresponding loss function to be minimized is we minimize these loss functions with respect to the latent vector z via stochastic gradient descent .for two aligned windows x and y from different images this metric is defined as note that this function takes the neighboring pixels into account and it is a more perceptual metric than pixel wise metrics .note that we actually know that the unknown latent vector was sampled from u 1 1 .models and methods	another remark is that ssim value increases up to 1 as the images become more and more similar so the corresponding loss function to be minimized is we minimize these loss functions with respect to the latent vector z via stochastic gradient descent 
1	109140	9140	note that we actually know that the unknown latent vector was sampled from u 1 1 .another remark is that ssim value increases up to 1 as the images become more and more similar so the corresponding loss function to be minimized is we minimize these loss functions with respect to the latent vector z via stochastic gradient descent .thus after each iteration we can clip the vector to stay in this range .models and methods	note that we actually know that the unknown latent vector was sampled from u 1 1 
0	109141	9141	thus after each iteration we can clip the vector to stay in this range .note that we actually know that the unknown latent vector was sampled from u 1 1 .another important remark is that this gan reversal training is non convex and we cannot guarantee to recover the same latent vector after each training .models and methods	thus after each iteration we can clip the vector to stay in this range 
1	109142	9142	another important remark is that this gan reversal training is non convex and we cannot guarantee to recover the same latent vector after each training .thus after each iteration we can clip the vector to stay in this range .multiple latent vectors can indeed map to the same image but for our compression purposes it does not matter which latent vector we recover as long as its corresponding image is close to the original .models and methods	another important remark is that this gan reversal training is non convex and we cannot guarantee to recover the same latent vector after each training 
1	109143	9143	multiple latent vectors can indeed map to the same image but for our compression purposes it does not matter which latent vector we recover as long as its corresponding image is close to the original .another important remark is that this gan reversal training is non convex and we cannot guarantee to recover the same latent vector after each training .last sentence.models and methods	multiple latent vectors can indeed map to the same image but for our compression purposes it does not matter which latent vector we recover as long as its corresponding image is close to the original 
0	109144	9144	in our initial experiments we use a gan architecture implemented according to the work of for the test set and for an unbiased evaluation of the models and baselines used we used a test set of 10 images from the celeba dataset face centered and resized to be of size 128 128 pixels .first sentence.the images in the test set are not outputs of the gan generator unlike in.experiments and dataset	in our initial experiments we use a gan architecture implemented according to the work of for the test set and for an unbiased evaluation of the models and baselines used we used a test set of 10 images from the celeba dataset face centered and resized to be of size 128 128 pixels 
1	109145	9145	the images in the test set are not outputs of the gan generator unlike in.in our initial experiments we use a gan architecture implemented according to the work of for the test set and for an unbiased evaluation of the models and baselines used we used a test set of 10 images from the celeba dataset face centered and resized to be of size 128 128 pixels .last sentence.experiments and dataset	the images in the test set are not outputs of the gan generator unlike in
1	109146	9146	in this paper we are aiming to design a better extreme compression scheme with two main objectives the scheme must achieve higher compression rates than other standard lossy image compression techniques and the reconstructed images must still be of high perceptual quality and true to their originals .first sentence.to assess how well our scheme is doing based on these objectives we have to use suitable metrics for image quality the metrics that are relevant to our work and that are most used in the literature include the mean squared error mse peak signal to noise ratio psnr and structural similarity index ssim .metrics used and baseline models	in this paper we are aiming to design a better extreme compression scheme with two main objectives the scheme must achieve higher compression rates than other standard lossy image compression techniques and the reconstructed images must still be of high perceptual quality and true to their originals 
0	109147	9147	to assess how well our scheme is doing based on these objectives we have to use suitable metrics for image quality the metrics that are relevant to our work and that are most used in the literature include the mean squared error mse peak signal to noise ratio psnr and structural similarity index ssim .in this paper we are aiming to design a better extreme compression scheme with two main objectives the scheme must achieve higher compression rates than other standard lossy image compression techniques and the reconstructed images must still be of high perceptual quality and true to their originals .while mse seems like the easiest metric to use it is actually not as indicative of perceptual or textural quality as the other metrics to measure how much compression our scheme is achieving we use the bits per pixel bpp metric which conveys the magnitude of the compression scheme by indicating the average number of bits needed to represent one pixel .metrics used and baseline models	to assess how well our scheme is doing based on these objectives we have to use suitable metrics for image quality the metrics that are relevant to our work and that are most used in the literature include the mean squared error mse peak signal to noise ratio psnr and structural similarity index ssim 
1	109148	9148	while mse seems like the easiest metric to use it is actually not as indicative of perceptual or textural quality as the other metrics to measure how much compression our scheme is achieving we use the bits per pixel bpp metric which conveys the magnitude of the compression scheme by indicating the average number of bits needed to represent one pixel .to assess how well our scheme is doing based on these objectives we have to use suitable metrics for image quality the metrics that are relevant to our work and that are most used in the literature include the mean squared error mse peak signal to noise ratio psnr and structural similarity index ssim .to put the numbers in context a non compressed image that represents each color channel of the pixel using one byte has a bpp of 24 .metrics used and baseline models	while mse seems like the easiest metric to use it is actually not as indicative of perceptual or textural quality as the other metrics to measure how much compression our scheme is achieving we use the bits per pixel bpp metric which conveys the magnitude of the compression scheme by indicating the average number of bits needed to represent one pixel 
1	109149	9149	to put the numbers in context a non compressed image that represents each color channel of the pixel using one byte has a bpp of 24 .while mse seems like the easiest metric to use it is actually not as indicative of perceptual or textural quality as the other metrics to measure how much compression our scheme is achieving we use the bits per pixel bpp metric which conveys the magnitude of the compression scheme by indicating the average number of bits needed to represent one pixel .however for fairness before reporting the bpp we losslessly compress the images from any scheme similar to what we do with the latent vectors in our gan reversal approach .metrics used and baseline models	to put the numbers in context a non compressed image that represents each color channel of the pixel using one byte has a bpp of 24 
1	109150	9150	however for fairness before reporting the bpp we losslessly compress the images from any scheme similar to what we do with the latent vectors in our gan reversal approach .to put the numbers in context a non compressed image that represents each color channel of the pixel using one byte has a bpp of 24 .therefore the bpp of the uncompressed png the other baseline we investigated was jpeg optimized which is a popular lossy image compression scheme .metrics used and baseline models	however for fairness before reporting the bpp we losslessly compress the images from any scheme similar to what we do with the latent vectors in our gan reversal approach 
1	109151	9151	therefore the bpp of the uncompressed png the other baseline we investigated was jpeg optimized which is a popular lossy image compression scheme .however for fairness before reporting the bpp we losslessly compress the images from any scheme similar to what we do with the latent vectors in our gan reversal approach .we used two values for the quality parameter of the jpeg schem 10 and 1 .metrics used and baseline models	therefore the bpp of the uncompressed png the other baseline we investigated was jpeg optimized which is a popular lossy image compression scheme 
0	109152	9152	we used two values for the quality parameter of the jpeg schem 10 and 1 .therefore the bpp of the uncompressed png the other baseline we investigated was jpeg optimized which is a popular lossy image compression scheme .note that for the purposes of this paper where we are aiming for extreme compression our main objective is beating the jpeg 1 baseline in the ssim metric which corresponds to better perceptual quality while still having a comparable compression ratio cr .metrics used and baseline models	we used two values for the quality parameter of the jpeg schem 10 and 1 
1	109153	9153	note that for the purposes of this paper where we are aiming for extreme compression our main objective is beating the jpeg 1 baseline in the ssim metric which corresponds to better perceptual quality while still having a comparable compression ratio cr .we used two values for the quality parameter of the jpeg schem 10 and 1 .last sentence.metrics used and baseline models	note that for the purposes of this paper where we are aiming for extreme compression our main objective is beating the jpeg 1 baseline in the ssim metric which corresponds to better perceptual quality while still having a comparable compression ratio cr 
0	109154	9154	the main future goal should be to improve the loss function used for latent vector training even further .first sentence.this will enable us to build compressors that achieve more perceptual reconstructions with higher fidelity .future work	the main future goal should be to improve the loss function used for latent vector training even further 
0	109155	9155	this will enable us to build compressors that achieve more perceptual reconstructions with higher fidelity .the main future goal should be to improve the loss function used for latent vector training even further .the additional challenge will be to build an automated process for the compressor .future work	this will enable us to build compressors that achieve more perceptual reconstructions with higher fidelity 
0	109156	9156	the additional challenge will be to build an automated process for the compressor .this will enable us to build compressors that achieve more perceptual reconstructions with higher fidelity .since the compression procedure utilizes a gradient descent based training scheme there are significant parts of compression that rely on human observation such as the hyperparameter tuning running other training sessions with differently initialized random vectors for improvement and picking the best perceptual output among all reconstructions .future work	the additional challenge will be to build an automated process for the compressor 
1	109157	9157	since the compression procedure utilizes a gradient descent based training scheme there are significant parts of compression that rely on human observation such as the hyperparameter tuning running other training sessions with differently initialized random vectors for improvement and picking the best perceptual output among all reconstructions .the additional challenge will be to build an automated process for the compressor .for a practical compressor all these processes must be automated .future work	since the compression procedure utilizes a gradient descent based training scheme there are significant parts of compression that rely on human observation such as the hyperparameter tuning running other training sessions with differently initialized random vectors for improvement and picking the best perceptual output among all reconstructions 
0	109158	9158	for a practical compressor all these processes must be automated .since the compression procedure utilizes a gradient descent based training scheme there are significant parts of compression that rely on human observation such as the hyperparameter tuning running other training sessions with differently initialized random vectors for improvement and picking the best perceptual output among all reconstructions .last sentence.future work	for a practical compressor all these processes must be automated 
0	109159	9159	we would like to thank kedar tatwawadi and shubham chandak for their very helpful discussions and comments .first sentence.last sentence.acknowledgments	we would like to thank kedar tatwawadi and shubham chandak for their very helpful discussions and comments 
0	109160	9160	behavioral cloning the model car is equipped with a mono frontal wide angle camera capturing 120x160 rgb images which are used for the training and testing inputs for the autopilot .first sentence.in addition the model car s steering angle and motor throttling values are used for the classification labels so the autopilot can estimate and output the best steering angle and throttling output given an input image in the testing phase .introduction	behavioral cloning the model car is equipped with a mono frontal wide angle camera capturing 120x160 rgb images which are used for the training and testing inputs for the autopilot 
1	109161	9161	in addition the model car s steering angle and motor throttling values are used for the classification labels so the autopilot can estimate and output the best steering angle and throttling output given an input image in the testing phase .behavioral cloning the model car is equipped with a mono frontal wide angle camera capturing 120x160 rgb images which are used for the training and testing inputs for the autopilot .last sentence.introduction	in addition the model car s steering angle and motor throttling values are used for the classification labels so the autopilot can estimate and output the best steering angle and throttling output given an input image in the testing phase 
0	109162	9162	bojarski et all .first sentence.1 have shown that it is possible to use a cnn based supervised model to drive a car .related works	bojarski et all 
0	109163	9163	1 have shown that it is possible to use a cnn based supervised model to drive a car .bojarski et all .the work has used three frontal cameras using the middle camera as the main source for the agent s inputs and using the side cameras to compensate the car s shift and rotation movements .related works	1 have shown that it is possible to use a cnn based supervised model to drive a car 
0	109164	9164	the work has used three frontal cameras using the middle camera as the main source for the agent s inputs and using the side cameras to compensate the car s shift and rotation movements .1 have shown that it is possible to use a cnn based supervised model to drive a car .it basically has relied only on the frontal captured images to classify the right steering angle to keep the car on the track .related works	the work has used three frontal cameras using the middle camera as the main source for the agent s inputs and using the side cameras to compensate the car s shift and rotation movements 
0	109165	9165	it basically has relied only on the frontal captured images to classify the right steering angle to keep the car on the track .the work has used three frontal cameras using the middle camera as the main source for the agent s inputs and using the side cameras to compensate the car s shift and rotation movements .this modeling is quite simple to come up with a decent performance if it is trained with sufficient amount of data .related works	it basically has relied only on the frontal captured images to classify the right steering angle to keep the car on the track 
1	109166	9166	this modeling is quite simple to come up with a decent performance if it is trained with sufficient amount of data .it basically has relied only on the frontal captured images to classify the right steering angle to keep the car on the track .however the biggest problem is that once it encounters a state which it has not seen before it is very easy for the agent to drift away significantly figure 1 behavioral cloning trajectory drifting image from where l s is the 0 1 loss of with respect to in state s as to optimize the bound of this approach the same work has suggested data aggregation which can achieve the cost j to be bounded linear to the trajectory t as shown from the following theorem from the same work letting n to designates the number of iteration to perform the data aggregation section 5 3 if n is o ut then there exists a policy 1 n such that.related works	this modeling is quite simple to come up with a decent performance if it is trained with sufficient amount of data 
1	109167	9167	however the biggest problem is that once it encounters a state which it has not seen before it is very easy for the agent to drift away significantly figure 1 behavioral cloning trajectory drifting image from where l s is the 0 1 loss of with respect to in state s as to optimize the bound of this approach the same work has suggested data aggregation which can achieve the cost j to be bounded linear to the trajectory t as shown from the following theorem from the same work letting n to designates the number of iteration to perform the data aggregation section 5 3 if n is o ut then there exists a policy 1 n such that.this modeling is quite simple to come up with a decent performance if it is trained with sufficient amount of data .last sentence.related works	however the biggest problem is that once it encounters a state which it has not seen before it is very easy for the agent to drift away significantly figure 1 behavioral cloning trajectory drifting image from where l s is the 0 1 loss of with respect to in state s as to optimize the bound of this approach the same work has suggested data aggregation which can achieve the cost j to be bounded linear to the trajectory t as shown from the following theorem from the same work letting n to designates the number of iteration to perform the data aggregation section 5 3 if n is o ut then there exists a policy 1 n such that
0	109168	9168	our model car has a mono wide angle camera a servo controlled steering and a thrust motor.first sentence.last sentence.donkey car	our model car has a mono wide angle camera a servo controlled steering and a thrust motor
0	109169	9169	the primary inputs for the training and testing are the 120x160 rgb images captured from the frontal camera classifying the best matching steering angles and the throttling values .first sentence.the agent has performed about total 200 wraps of running on the indoor track in multiple sessions each wrap equaling to capturing about 520 images their corresponding steering angles and throttling values .dataset and features	the primary inputs for the training and testing are the 120x160 rgb images captured from the frontal camera classifying the best matching steering angles and the throttling values 
0	109170	9170	the agent has performed about total 200 wraps of running on the indoor track in multiple sessions each wrap equaling to capturing about 520 images their corresponding steering angles and throttling values .the primary inputs for the training and testing are the 120x160 rgb images captured from the frontal camera classifying the best matching steering angles and the throttling values .the training and validation is performed with split ratio of 0 8 0 1 0 1 between the training developing and validation sets .dataset and features	the agent has performed about total 200 wraps of running on the indoor track in multiple sessions each wrap equaling to capturing about 520 images their corresponding steering angles and throttling values 
0	109171	9171	the training and validation is performed with split ratio of 0 8 0 1 0 1 between the training developing and validation sets .the agent has performed about total 200 wraps of running on the indoor track in multiple sessions each wrap equaling to capturing about 520 images their corresponding steering angles and throttling values .last sentence.dataset and features	the training and validation is performed with split ratio of 0 8 0 1 0 1 between the training developing and validation sets 
1	109172	9172	the the image on the right in the same figure shows an input image super imposed with a masking to show the image segments activating the cnn most where p p s designating the distribution of states visited by the expert .first sentence.last sentence.cnn autopilot	the the image on the right in the same figure shows an input image super imposed with a masking to show the image segments activating the cnn most where p p s designating the distribution of states visited by the expert 
1	109173	9173	in order to improve the baseline autopilot policy i have employed data aggregation also specific to this project s in the iteration i is achieved by the expert manually modifying the actions by its best estimation .first sentence.for a specific example .data aggregation	in order to improve the baseline autopilot policy i have employed data aggregation also specific to this project s in the iteration i is achieved by the expert manually modifying the actions by its best estimation 
0	109174	9174	for a specific example .in order to improve the baseline autopilot policy i have employed data aggregation also specific to this project s in the iteration i is achieved by the expert manually modifying the actions by its best estimation .last sentence.data aggregation	for a specific example 
1	109175	9175	the table in the first row of the table shows the failure rates of each individual policy i without merging all the datasets from each iterations that is not performing d d d i for the next training dataset .first sentence.the first column shows the failure rate of the human driver .experiment results	the table in the first row of the table shows the failure rates of each individual policy i without merging all the datasets from each iterations that is not performing d d d i for the next training dataset 
0	109176	9176	the first column shows the failure rate of the human driver .the table in the first row of the table shows the failure rates of each individual policy i without merging all the datasets from each iterations that is not performing d d d i for the next training dataset .1 is the first policy trained with the training data collected from the human driver .experiment results	the first column shows the failure rate of the human driver 
0	109177	9177	1 is the first policy trained with the training data collected from the human driver .the first column shows the failure rate of the human driver .2 is the first policy which is generated by manually modifying the misbehavior using 1 .experiment results	1 is the first policy trained with the training data collected from the human driver 
1	109178	9178	2 is the first policy which is generated by manually modifying the misbehavior using 1 .1 is the first policy trained with the training data collected from the human driver .3 is the next iteration policy generated from 2 the second row shows the case of the dataset being merged as d d d i .experiment results	2 is the first policy which is generated by manually modifying the misbehavior using 1 
1	109179	9179	3 is the next iteration policy generated from 2 the second row shows the case of the dataset being merged as d d d i .2 is the first policy which is generated by manually modifying the misbehavior using 1 .only the first iteration is conducted stopping early as the agent failed to track from the start with the given policy the also if i had more time i would have tried modeling the policy through a reinforcement learning model other than an imitation learning tried here .experiment results	3 is the next iteration policy generated from 2 the second row shows the case of the dataset being merged as d d d i 
1	109180	9180	only the first iteration is conducted stopping early as the agent failed to track from the start with the given policy the also if i had more time i would have tried modeling the policy through a reinforcement learning model other than an imitation learning tried here .3 is the next iteration policy generated from 2 the second row shows the case of the dataset being merged as d d d i .even though i don t have any quantitative data to support after training the autopilot many times it seems like confirming the fact that the dependency of current state to its past states plays very important role for the agent s robustness section 2 .experiment results	only the first iteration is conducted stopping early as the agent failed to track from the start with the given policy the also if i had more time i would have tried modeling the policy through a reinforcement learning model other than an imitation learning tried here 
1	109181	9181	even though i don t have any quantitative data to support after training the autopilot many times it seems like confirming the fact that the dependency of current state to its past states plays very important role for the agent s robustness section 2 .only the first iteration is conducted stopping early as the agent failed to track from the start with the given policy the also if i had more time i would have tried modeling the policy through a reinforcement learning model other than an imitation learning tried here .launching a real world agent using a reinforcement learning taking it out of the simulated environment will pose very interesting challenging problems 8 references.experiment results	even though i don t have any quantitative data to support after training the autopilot many times it seems like confirming the fact that the dependency of current state to its past states plays very important role for the agent s robustness section 2 
1	109182	9182	launching a real world agent using a reinforcement learning taking it out of the simulated environment will pose very interesting challenging problems 8 references.even though i don t have any quantitative data to support after training the autopilot many times it seems like confirming the fact that the dependency of current state to its past states plays very important role for the agent s robustness section 2 .last sentence.experiment results	launching a real world agent using a reinforcement learning taking it out of the simulated environment will pose very interesting challenging problems 8 references
1	109183	9183	we introduce a novel technique for data augmentation with the goal of improving robustness of semantic segmentation models .first sentence.standard data augmentation methods rely upon augmenting the existing dataset with various transformations of the training samples but do not utilize other existing datasets .abstract	we introduce a novel technique for data augmentation with the goal of improving robustness of semantic segmentation models 
1	109184	9184	standard data augmentation methods rely upon augmenting the existing dataset with various transformations of the training samples but do not utilize other existing datasets .we introduce a novel technique for data augmentation with the goal of improving robustness of semantic segmentation models .we propose a method that draws images from external datasets that are related in content but perhaps stylistically different we perform style normalization on these external datasets to counter differences in style .abstract	standard data augmentation methods rely upon augmenting the existing dataset with various transformations of the training samples but do not utilize other existing datasets 
1	109185	9185	we propose a method that draws images from external datasets that are related in content but perhaps stylistically different we perform style normalization on these external datasets to counter differences in style .standard data augmentation methods rely upon augmenting the existing dataset with various transformations of the training samples but do not utilize other existing datasets .we apply and benchmark our technique on the semantic segmentation task with the deeplabv3 model architecture and the cityscapes dataset leveraging the gta5 dataset for our data augmentation .abstract	we propose a method that draws images from external datasets that are related in content but perhaps stylistically different we perform style normalization on these external datasets to counter differences in style 
1	109186	9186	we apply and benchmark our technique on the semantic segmentation task with the deeplabv3 model architecture and the cityscapes dataset leveraging the gta5 dataset for our data augmentation .we propose a method that draws images from external datasets that are related in content but perhaps stylistically different we perform style normalization on these external datasets to counter differences in style .last sentence.abstract	we apply and benchmark our technique on the semantic segmentation task with the deeplabv3 model architecture and the cityscapes dataset leveraging the gta5 dataset for our data augmentation 
0	109187	9187	the task of semantic segmentation is a key topic in the field of computer vision .first sentence.recent advances in deep learning have yielded increasingly successful models intuitively semantic segmentation should depend only the content of an image and not on the style .introduction	the task of semantic segmentation is a key topic in the field of computer vision 
1	109188	9188	recent advances in deep learning have yielded increasingly successful models intuitively semantic segmentation should depend only the content of an image and not on the style .the task of semantic segmentation is a key topic in the field of computer vision .indeed the style of an image captures domain specific properties while the content is domaininvariant .introduction	recent advances in deep learning have yielded increasingly successful models intuitively semantic segmentation should depend only the content of an image and not on the style 
1	109189	9189	indeed the style of an image captures domain specific properties while the content is domaininvariant .recent advances in deep learning have yielded increasingly successful models intuitively semantic segmentation should depend only the content of an image and not on the style .we choose to focus on the deeplabv3 model flipping.introduction	indeed the style of an image captures domain specific properties while the content is domaininvariant 
0	109190	9190	we choose to focus on the deeplabv3 model flipping.indeed the style of an image captures domain specific properties while the content is domaininvariant .last sentence.introduction	we choose to focus on the deeplabv3 model flipping
0	109191	9191	the cityscapes dataset collects a diverse set of street view images from 50 cities in germany and surrounding countries .first sentence.some examples can be seen in the gta5 dataset consists of screenshots of the open world sandbox game grand theft auto v the game is a particularly apt content domain as it is set in a large metropolitan city where street view style images are possible .datasets	the cityscapes dataset collects a diverse set of street view images from 50 cities in germany and surrounding countries 
1	109192	9192	some examples can be seen in the gta5 dataset consists of screenshots of the open world sandbox game grand theft auto v the game is a particularly apt content domain as it is set in a large metropolitan city where street view style images are possible .the cityscapes dataset collects a diverse set of street view images from 50 cities in germany and surrounding countries .further the gta5 images have pixel level image labels that are compatible with those of cityscapes however they required some additional preprocessing .datasets	some examples can be seen in the gta5 dataset consists of screenshots of the open world sandbox game grand theft auto v the game is a particularly apt content domain as it is set in a large metropolitan city where street view style images are possible 
1	109193	9193	further the gta5 images have pixel level image labels that are compatible with those of cityscapes however they required some additional preprocessing .some examples can be seen in the gta5 dataset consists of screenshots of the open world sandbox game grand theft auto v the game is a particularly apt content domain as it is set in a large metropolitan city where street view style images are possible .the cityscapes and gta5 datasets have a difference in their representations of ground truth .datasets	further the gta5 images have pixel level image labels that are compatible with those of cityscapes however they required some additional preprocessing 
1	109194	9194	the cityscapes and gta5 datasets have a difference in their representations of ground truth .further the gta5 images have pixel level image labels that are compatible with those of cityscapes however they required some additional preprocessing .in particular the cityscapes dataset encodes class labels with a grayscale image where each pixel s grayscale value represents the class label .datasets	the cityscapes and gta5 datasets have a difference in their representations of ground truth 
1	109195	9195	in particular the cityscapes dataset encodes class labels with a grayscale image where each pixel s grayscale value represents the class label .the cityscapes and gta5 datasets have a difference in their representations of ground truth .on the other hand the gta5 dataset encodes class labels with an image where the pixel color represents the class label .datasets	in particular the cityscapes dataset encodes class labels with a grayscale image where each pixel s grayscale value represents the class label 
0	109196	9196	on the other hand the gta5 dataset encodes class labels with an image where the pixel color represents the class label .in particular the cityscapes dataset encodes class labels with a grayscale image where each pixel s grayscale value represents the class label .this difference is displayed below in.datasets	on the other hand the gta5 dataset encodes class labels with an image where the pixel color represents the class label 
0	109197	9197	this difference is displayed below in.on the other hand the gta5 dataset encodes class labels with an image where the pixel color represents the class label .last sentence.datasets	this difference is displayed below in
1	109198	9198	we selected the well known deeplabv3 architecture for semantic segmentation and used a popular pytorch implementation https github com jfzhang95 pytorch deeplab xception .first sentence.deeplabv3 uses a pre trained resnet 101 model as its backbone but adds two additional modules an atrous spacial pyramid pooling module and decoder module designed specifically for the task of semantic segmentation .methods	we selected the well known deeplabv3 architecture for semantic segmentation and used a popular pytorch implementation https github com jfzhang95 pytorch deeplab xception 
1	109199	9199	deeplabv3 uses a pre trained resnet 101 model as its backbone but adds two additional modules an atrous spacial pyramid pooling module and decoder module designed specifically for the task of semantic segmentation .we selected the well known deeplabv3 architecture for semantic segmentation and used a popular pytorch implementation https github com jfzhang95 pytorch deeplab xception .it utilizes cross entropy loss .methods	deeplabv3 uses a pre trained resnet 101 model as its backbone but adds two additional modules an atrous spacial pyramid pooling module and decoder module designed specifically for the task of semantic segmentation 
0	109200	9200	it utilizes cross entropy loss .deeplabv3 uses a pre trained resnet 101 model as its backbone but adds two additional modules an atrous spacial pyramid pooling module and decoder module designed specifically for the task of semantic segmentation .cross entropy loss is defined as follows for a set of classes c and an image i if y i c indicates whether the true label of pixel i is c and i c is the probability computed by our model that pixel i is of class c thenfor style normalization we utilized a recent state of the art image to image translation model called unit introduced in.methods	it utilizes cross entropy loss 
1	109201	9201	cross entropy loss is defined as follows for a set of classes c and an image i if y i c indicates whether the true label of pixel i is c and i c is the probability computed by our model that pixel i is of class c thenfor style normalization we utilized a recent state of the art image to image translation model called unit introduced in.it utilizes cross entropy loss .last sentence.methods	cross entropy loss is defined as follows for a set of classes c and an image i if y i c indicates whether the true label of pixel i is c and i c is the probability computed by our model that pixel i is of class c thenfor style normalization we utilized a recent state of the art image to image translation model called unit introduced in
1	109202	9202	to quantify the effects of our data augmentation technique on the robustness of semantic segmentation models we ran two primary experiments .first sentence.for the first experiment we took a deeplabv3 network pretrained on the pascal voc and semantic boundaries dataset and applied transfer learning with two additional datasets the first dataset consisted of the 587 cityscapes training images and 302 additional gta5 images and the second dataset consisted of the 587 cityscapes training images and the 302 gta5 images mapped to the cityscapes style domain with unit .experiments and results	to quantify the effects of our data augmentation technique on the robustness of semantic segmentation models we ran two primary experiments 
1	109203	9203	for the first experiment we took a deeplabv3 network pretrained on the pascal voc and semantic boundaries dataset and applied transfer learning with two additional datasets the first dataset consisted of the 587 cityscapes training images and 302 additional gta5 images and the second dataset consisted of the 587 cityscapes training images and the 302 gta5 images mapped to the cityscapes style domain with unit .to quantify the effects of our data augmentation technique on the robustness of semantic segmentation models we ran two primary experiments .for the second experiment we trained deeplabv3 from scratch with two datasets the first dataset consisted of the 587 cityscapes training images and 587 additional gta5 images and the second dataset consisted of the 587 cityscapes images and 587 additional gta5 images mapped to the cityscapes style domain with unit the standard benchmark statistic for semantic segmentation is mean intersection over union score miou .experiments and results	for the first experiment we took a deeplabv3 network pretrained on the pascal voc and semantic boundaries dataset and applied transfer learning with two additional datasets the first dataset consisted of the 587 cityscapes training images and 302 additional gta5 images and the second dataset consisted of the 587 cityscapes training images and the 302 gta5 images mapped to the cityscapes style domain with unit 
1	109204	9204	for the second experiment we trained deeplabv3 from scratch with two datasets the first dataset consisted of the 587 cityscapes training images and 587 additional gta5 images and the second dataset consisted of the 587 cityscapes images and 587 additional gta5 images mapped to the cityscapes style domain with unit the standard benchmark statistic for semantic segmentation is mean intersection over union score miou .for the first experiment we took a deeplabv3 network pretrained on the pascal voc and semantic boundaries dataset and applied transfer learning with two additional datasets the first dataset consisted of the 587 cityscapes training images and 302 additional gta5 images and the second dataset consisted of the 587 cityscapes training images and the 302 gta5 images mapped to the cityscapes style domain with unit .intuitively the intersection over union quantifies how accurately a particular model estimates the location of an object relative to a ground truth image by computing the ratio of the number of pixels the model correctly identifies intersection to the total number of pixels representing either the ground truth or object or the model s prediction of the object union .experiments and results	for the second experiment we trained deeplabv3 from scratch with two datasets the first dataset consisted of the 587 cityscapes training images and 587 additional gta5 images and the second dataset consisted of the 587 cityscapes images and 587 additional gta5 images mapped to the cityscapes style domain with unit the standard benchmark statistic for semantic segmentation is mean intersection over union score miou 
1	109205	9205	intuitively the intersection over union quantifies how accurately a particular model estimates the location of an object relative to a ground truth image by computing the ratio of the number of pixels the model correctly identifies intersection to the total number of pixels representing either the ground truth or object or the model s prediction of the object union .for the second experiment we trained deeplabv3 from scratch with two datasets the first dataset consisted of the 587 cityscapes training images and 587 additional gta5 images and the second dataset consisted of the 587 cityscapes images and 587 additional gta5 images mapped to the cityscapes style domain with unit the standard benchmark statistic for semantic segmentation is mean intersection over union score miou .to extend this notion beyond binary classification we introduce the notion of a confusion matrix .experiments and results	intuitively the intersection over union quantifies how accurately a particular model estimates the location of an object relative to a ground truth image by computing the ratio of the number of pixels the model correctly identifies intersection to the total number of pixels representing either the ground truth or object or the model s prediction of the object union 
0	109206	9206	to extend this notion beyond binary classification we introduce the notion of a confusion matrix .intuitively the intersection over union quantifies how accurately a particular model estimates the location of an object relative to a ground truth image by computing the ratio of the number of pixels the model correctly identifies intersection to the total number of pixels representing either the ground truth or object or the model s prediction of the object union .a confusion matrix m is defined such that m ij is the number of pixels whose ground truth label is i that the model classifies as j notice that the diagonal elements m ii represent correctly classified pixels .experiments and results	to extend this notion beyond binary classification we introduce the notion of a confusion matrix 
1	109207	9207	a confusion matrix m is defined such that m ij is the number of pixels whose ground truth label is i that the model classifies as j notice that the diagonal elements m ii represent correctly classified pixels .to extend this notion beyond binary classification we introduce the notion of a confusion matrix .suppose we have a set of class labels c we can then defineas stated above we first used a pretrained deeplabv3 model and applied transfer learning in two ways .experiments and results	a confusion matrix m is defined such that m ij is the number of pixels whose ground truth label is i that the model classifies as j notice that the diagonal elements m ii represent correctly classified pixels 
1	109208	9208	suppose we have a set of class labels c we can then defineas stated above we first used a pretrained deeplabv3 model and applied transfer learning in two ways .a confusion matrix m is defined such that m ij is the number of pixels whose ground truth label is i that the model classifies as j notice that the diagonal elements m ii represent correctly classified pixels .for both models we trained on the first combined dataset of cityscapes and gta5 587 images of each .experiments and results	suppose we have a set of class labels c we can then defineas stated above we first used a pretrained deeplabv3 model and applied transfer learning in two ways 
1	109209	9209	for both models we trained on the first combined dataset of cityscapes and gta5 587 images of each .suppose we have a set of class labels c we can then defineas stated above we first used a pretrained deeplabv3 model and applied transfer learning in two ways .for the baseline model deeplabv3 was trained on this dataset to produce semantic segmentation predictions .experiments and results	for both models we trained on the first combined dataset of cityscapes and gta5 587 images of each 
0	109210	9210	for the baseline model deeplabv3 was trained on this dataset to produce semantic segmentation predictions .for both models we trained on the first combined dataset of cityscapes and gta5 587 images of each .for the unit mapped model we first mapped the gta images in our training dataset to the cityscapes domain using the pretrained unit model .experiments and results	for the baseline model deeplabv3 was trained on this dataset to produce semantic segmentation predictions 
1	109211	9211	for the unit mapped model we first mapped the gta images in our training dataset to the cityscapes domain using the pretrained unit model .for the baseline model deeplabv3 was trained on this dataset to produce semantic segmentation predictions .we then trained deeplabv3 on the cityscapes images and these unit mapped gta images .experiments and results	for the unit mapped model we first mapped the gta images in our training dataset to the cityscapes domain using the pretrained unit model 
0	109212	9212	we then trained deeplabv3 on the cityscapes images and these unit mapped gta images .for the unit mapped model we first mapped the gta images in our training dataset to the cityscapes domain using the pretrained unit model .we also trained deeplab3 from scratch on the second combined dataset of cityscape and gta5 images .experiments and results	we then trained deeplabv3 on the cityscapes images and these unit mapped gta images 
0	109213	9213	we also trained deeplab3 from scratch on the second combined dataset of cityscape and gta5 images .we then trained deeplabv3 on the cityscapes images and these unit mapped gta images .our baseline and unit mapped followed the pipelines in miou scores training from scratch baseline 0 48 unit mapped 0 51 our code is available at the following link https bit ly 2pxnla9 .experiments and results	we also trained deeplab3 from scratch on the second combined dataset of cityscape and gta5 images 
1	109214	9214	our baseline and unit mapped followed the pipelines in miou scores training from scratch baseline 0 48 unit mapped 0 51 our code is available at the following link https bit ly 2pxnla9 .we also trained deeplab3 from scratch on the second combined dataset of cityscape and gta5 images .the downloadable zip file includes our codebase for both the unit and deeplab models .experiments and results	our baseline and unit mapped followed the pipelines in miou scores training from scratch baseline 0 48 unit mapped 0 51 our code is available at the following link https bit ly 2pxnla9 
0	109215	9215	the downloadable zip file includes our codebase for both the unit and deeplab models .our baseline and unit mapped followed the pipelines in miou scores training from scratch baseline 0 48 unit mapped 0 51 our code is available at the following link https bit ly 2pxnla9 .last sentence.experiments and results	the downloadable zip file includes our codebase for both the unit and deeplab models 
0	109216	9216	we find similar performance between baseline and unit mapped for our models trained using transfer learning .first sentence.we hypothesize that the pretrained model may not be the best fit for the street city scenes of the cityscapes and gta datasets .discussion	we find similar performance between baseline and unit mapped for our models trained using transfer learning 
0	109217	9217	we hypothesize that the pretrained model may not be the best fit for the street city scenes of the cityscapes and gta datasets .we find similar performance between baseline and unit mapped for our models trained using transfer learning .the pascal visual object classes voc dataset and the semantic boundaries dataset sbd are different tasks than the semantic segmentation of classes in street scenes we also performed qualitative analyses of our results on this experiment .discussion	we hypothesize that the pretrained model may not be the best fit for the street city scenes of the cityscapes and gta datasets 
1	109218	9218	the pascal visual object classes voc dataset and the semantic boundaries dataset sbd are different tasks than the semantic segmentation of classes in street scenes we also performed qualitative analyses of our results on this experiment .we hypothesize that the pretrained model may not be the best fit for the street city scenes of the cityscapes and gta datasets .we compared the predicted semantic segmentations of our baseline model and the unit mapped model and find that the segmentations are similar with no noticeable differences .discussion	the pascal visual object classes voc dataset and the semantic boundaries dataset sbd are different tasks than the semantic segmentation of classes in street scenes we also performed qualitative analyses of our results on this experiment 
1	109219	9219	we compared the predicted semantic segmentations of our baseline model and the unit mapped model and find that the segmentations are similar with no noticeable differences .the pascal visual object classes voc dataset and the semantic boundaries dataset sbd are different tasks than the semantic segmentation of classes in street scenes we also performed qualitative analyses of our results on this experiment .we hypothesize that training on a larger dataset would yield higher miou scores for our unit mapped model as well as more clear visual differences .discussion	we compared the predicted semantic segmentations of our baseline model and the unit mapped model and find that the segmentations are similar with no noticeable differences 
0	109220	9220	we hypothesize that training on a larger dataset would yield higher miou scores for our unit mapped model as well as more clear visual differences .we compared the predicted semantic segmentations of our baseline model and the unit mapped model and find that the segmentations are similar with no noticeable differences .given our constrained resources and limited compute power we were restricted to a small dataset .discussion	we hypothesize that training on a larger dataset would yield higher miou scores for our unit mapped model as well as more clear visual differences 
0	109221	9221	given our constrained resources and limited compute power we were restricted to a small dataset .we hypothesize that training on a larger dataset would yield higher miou scores for our unit mapped model as well as more clear visual differences .our observed results in the pre trained deeplabv3 experiments reinforce the fact that more data is necessary .discussion	given our constrained resources and limited compute power we were restricted to a small dataset 
0	109222	9222	our observed results in the pre trained deeplabv3 experiments reinforce the fact that more data is necessary .given our constrained resources and limited compute power we were restricted to a small dataset .the comparable performance on the task suggests that neither training regimen could shift the model s weights particularly far from the voc sbd optimum in the parameter space .discussion	our observed results in the pre trained deeplabv3 experiments reinforce the fact that more data is necessary 
0	109223	9223	the comparable performance on the task suggests that neither training regimen could shift the model s weights particularly far from the voc sbd optimum in the parameter space .our observed results in the pre trained deeplabv3 experiments reinforce the fact that more data is necessary .the learned features for the voc task effectively drowned out any subtleties of the street view segmentation task and the effect of our additional images .discussion	the comparable performance on the task suggests that neither training regimen could shift the model s weights particularly far from the voc sbd optimum in the parameter space 
0	109224	9224	the learned features for the voc task effectively drowned out any subtleties of the street view segmentation task and the effect of our additional images .the comparable performance on the task suggests that neither training regimen could shift the model s weights particularly far from the voc sbd optimum in the parameter space .we believe that we would find more significant improvements provided more unit mapped gta images our results from our second round of experiments where we trained deeplabv3 from scratch show some potential .discussion	the learned features for the voc task effectively drowned out any subtleties of the street view segmentation task and the effect of our additional images 
0	109225	9225	we believe that we would find more significant improvements provided more unit mapped gta images our results from our second round of experiments where we trained deeplabv3 from scratch show some potential .the learned features for the voc task effectively drowned out any subtleties of the street view segmentation task and the effect of our additional images .here we find that unit mapped slightly outperformed baseline .discussion	we believe that we would find more significant improvements provided more unit mapped gta images our results from our second round of experiments where we trained deeplabv3 from scratch show some potential 
0	109226	9226	here we find that unit mapped slightly outperformed baseline .we believe that we would find more significant improvements provided more unit mapped gta images our results from our second round of experiments where we trained deeplabv3 from scratch show some potential .the improved miou scores suggest that mapping synthetic data onto the real world domain could potentially improve the robustness of a real world classifier .discussion	here we find that unit mapped slightly outperformed baseline 
1	109227	9227	the improved miou scores suggest that mapping synthetic data onto the real world domain could potentially improve the robustness of a real world classifier .here we find that unit mapped slightly outperformed baseline .last sentence.discussion	the improved miou scores suggest that mapping synthetic data onto the real world domain could potentially improve the robustness of a real world classifier 
0	109228	9228	our results as discussed above do not yield any significant conclusions regarding our novel technique for data augmentation .first sentence.we note again that compute and time restrictions did not allow us to train deeplabv3 with sufficiently many training samples to achieve baseline results as reported in other papers .conclusion and future work	our results as discussed above do not yield any significant conclusions regarding our novel technique for data augmentation 
0	109229	9229	we note again that compute and time restrictions did not allow us to train deeplabv3 with sufficiently many training samples to achieve baseline results as reported in other papers .our results as discussed above do not yield any significant conclusions regarding our novel technique for data augmentation .nonetheless our results yield various promising avenues for future research .conclusion and future work	we note again that compute and time restrictions did not allow us to train deeplabv3 with sufficiently many training samples to achieve baseline results as reported in other papers 
0	109230	9230	nonetheless our results yield various promising avenues for future research .we note again that compute and time restrictions did not allow us to train deeplabv3 with sufficiently many training samples to achieve baseline results as reported in other papers .in particular the superior performance of the deeplabv3 model with our novel technique for data augmentation when trained from scratch in comparison to the model with simply a combined dataset suggests that our technique could be successful if we used more training samples .conclusion and future work	nonetheless our results yield various promising avenues for future research 
1	109231	9231	in particular the superior performance of the deeplabv3 model with our novel technique for data augmentation when trained from scratch in comparison to the model with simply a combined dataset suggests that our technique could be successful if we used more training samples .nonetheless our results yield various promising avenues for future research .another area for future work is exploring the efficacy of our data augmentation approach across other tasks in computer vision .conclusion and future work	in particular the superior performance of the deeplabv3 model with our novel technique for data augmentation when trained from scratch in comparison to the model with simply a combined dataset suggests that our technique could be successful if we used more training samples 
0	109232	9232	another area for future work is exploring the efficacy of our data augmentation approach across other tasks in computer vision .in particular the superior performance of the deeplabv3 model with our novel technique for data augmentation when trained from scratch in comparison to the model with simply a combined dataset suggests that our technique could be successful if we used more training samples .for instance we would like to test our methodology on object detection and localization .conclusion and future work	another area for future work is exploring the efficacy of our data augmentation approach across other tasks in computer vision 
0	109233	9233	for instance we would like to test our methodology on object detection and localization .another area for future work is exploring the efficacy of our data augmentation approach across other tasks in computer vision .last sentence.conclusion and future work	for instance we would like to test our methodology on object detection and localization 
0	109234	9234	there were two main tasks over the course of this project data preprocessing and training deeplabv3 .first sentence.evani worked on training the deeplabv3 model using transfer learning for our initial results .contributions	there were two main tasks over the course of this project data preprocessing and training deeplabv3 
0	109235	9235	evani worked on training the deeplabv3 model using transfer learning for our initial results .there were two main tasks over the course of this project data preprocessing and training deeplabv3 .andrew handled parts of the data preprocessing such as converting gta5 images to the cityscapes style domain with the unit model and contributed to training deeplabv3 for the initial results .contributions	evani worked on training the deeplabv3 model using transfer learning for our initial results 
1	109236	9236	andrew handled parts of the data preprocessing such as converting gta5 images to the cityscapes style domain with the unit model and contributed to training deeplabv3 for the initial results .evani worked on training the deeplabv3 model using transfer learning for our initial results .felix worked on training the deeplabv3 codebase from scratch and some of the data preprocessing such as making the gta5 and cityscapes labels compatible .contributions	andrew handled parts of the data preprocessing such as converting gta5 images to the cityscapes style domain with the unit model and contributed to training deeplabv3 for the initial results 
0	109237	9237	felix worked on training the deeplabv3 codebase from scratch and some of the data preprocessing such as making the gta5 and cityscapes labels compatible .andrew handled parts of the data preprocessing such as converting gta5 images to the cityscapes style domain with the unit model and contributed to training deeplabv3 for the initial results .last sentence.contributions	felix worked on training the deeplabv3 codebase from scratch and some of the data preprocessing such as making the gta5 and cityscapes labels compatible 
1	109238	9238	in this work we implemented and trained an end to end deep neural network songnet to perform real time music genre classification .first sentence.music can be represented in various forms time series decimals spectrum in frequency domain and spectrograms etc .abstract	in this work we implemented and trained an end to end deep neural network songnet to perform real time music genre classification 
0	109239	9239	music can be represented in various forms time series decimals spectrum in frequency domain and spectrograms etc .in this work we implemented and trained an end to end deep neural network songnet to perform real time music genre classification .the spectrogram stands out as the most popular choice since it incorporates time and frequency information .abstract	music can be represented in various forms time series decimals spectrum in frequency domain and spectrograms etc 
0	109240	9240	the spectrogram stands out as the most popular choice since it incorporates time and frequency information .music can be represented in various forms time series decimals spectrum in frequency domain and spectrograms etc .in this project we used the convolutional recurrent neural network c rnn to classify music .abstract	the spectrogram stands out as the most popular choice since it incorporates time and frequency information 
0	109241	9241	in this project we used the convolutional recurrent neural network c rnn to classify music .the spectrogram stands out as the most popular choice since it incorporates time and frequency information .the convolutional network extracts features of spectrogram before feeding them into recurrent network which then performs classification considering both transient and overall characteristics of music .abstract	in this project we used the convolutional recurrent neural network c rnn to classify music 
1	109242	9242	the convolutional network extracts features of spectrogram before feeding them into recurrent network which then performs classification considering both transient and overall characteristics of music .in this project we used the convolutional recurrent neural network c rnn to classify music .taking only raw audio as input the c rnn achieved 65 23 accuracy on fma small dataset beating the best baseline by 41 .abstract	the convolutional network extracts features of spectrogram before feeding them into recurrent network which then performs classification considering both transient and overall characteristics of music 
0	109243	9243	taking only raw audio as input the c rnn achieved 65 23 accuracy on fma small dataset beating the best baseline by 41 .the convolutional network extracts features of spectrogram before feeding them into recurrent network which then performs classification considering both transient and overall characteristics of music .last sentence.abstract	taking only raw audio as input the c rnn achieved 65 23 accuracy on fma small dataset beating the best baseline by 41 
0	109244	9244	with the enormous growth of music released online managing music library manually has become more and more challenging not only for users but also audio streaming service companies such as spotify and itunes .first sentence.fast and accurate music classification is in high demand while it is non trivial for machines to perform the task automatically at human level besides music genre classification is an essential backbone for music recommendation and unknown soundtrack recognition which will benefit music service platforms a lot .introduction	with the enormous growth of music released online managing music library manually has become more and more challenging not only for users but also audio streaming service companies such as spotify and itunes 
0	109245	9245	fast and accurate music classification is in high demand while it is non trivial for machines to perform the task automatically at human level besides music genre classification is an essential backbone for music recommendation and unknown soundtrack recognition which will benefit music service platforms a lot .with the enormous growth of music released online managing music library manually has become more and more challenging not only for users but also audio streaming service companies such as spotify and itunes .building a robust music classifier using machine learning techniques is essential to automate tagging unlabled music and improve users experience of media players and music libraries in recent years convolutional neural networks cnns have brought revolutionary changes to computer vision community.introduction	fast and accurate music classification is in high demand while it is non trivial for machines to perform the task automatically at human level besides music genre classification is an essential backbone for music recommendation and unknown soundtrack recognition which will benefit music service platforms a lot 
1	109246	9246	building a robust music classifier using machine learning techniques is essential to automate tagging unlabled music and improve users experience of media players and music libraries in recent years convolutional neural networks cnns have brought revolutionary changes to computer vision community.fast and accurate music classification is in high demand while it is non trivial for machines to perform the task automatically at human level besides music genre classification is an essential backbone for music recommendation and unknown soundtrack recognition which will benefit music service platforms a lot .last sentence.introduction	building a robust music classifier using machine learning techniques is essential to automate tagging unlabled music and improve users experience of media players and music libraries in recent years convolutional neural networks cnns have brought revolutionary changes to computer vision community
0	109247	9247	music genre classification has been actively studied since the early days of the internet .first sentence.tzanetakis and cook in recent years using audio spectrogram has become mainstream for music genre classification .related work	music genre classification has been actively studied since the early days of the internet 
0	109248	9248	tzanetakis and cook in recent years using audio spectrogram has become mainstream for music genre classification .music genre classification has been actively studied since the early days of the internet .spectrogram encodes time and frequency information of a given music as a whole .related work	tzanetakis and cook in recent years using audio spectrogram has become mainstream for music genre classification 
0	109249	9249	spectrogram encodes time and frequency information of a given music as a whole .tzanetakis and cook in recent years using audio spectrogram has become mainstream for music genre classification .wyse this work aims to train a c rnn model with melspectrogram as the only feature and compare this model with the traditional machine learning classifiers that need to be trained with hand crafted features and metadata .related work	spectrogram encodes time and frequency information of a given music as a whole 
1	109250	9250	wyse this work aims to train a c rnn model with melspectrogram as the only feature and compare this model with the traditional machine learning classifiers that need to be trained with hand crafted features and metadata .spectrogram encodes time and frequency information of a given music as a whole .last sentence.related work	wyse this work aims to train a c rnn model with melspectrogram as the only feature and compare this model with the traditional machine learning classifiers that need to be trained with hand crafted features and metadata 
0	109251	9251	the dataset used for this project is the free music archive fma an interactive library of high quality legal audio downloads direct by wfmu .first sentence.furthermore it provides music s associated information including precomputed features user level metadata etc .free music archive 2 	the dataset used for this project is the free music archive fma an interactive library of high quality legal audio downloads direct by wfmu 
0	109252	9252	furthermore it provides music s associated information including precomputed features user level metadata etc .the dataset used for this project is the free music archive fma an interactive library of high quality legal audio downloads direct by wfmu .to ensure data is balanced among different genres we only use a small subset fma small for the scope of this project .free music archive 2 	furthermore it provides music s associated information including precomputed features user level metadata etc 
0	109253	9253	to ensure data is balanced among different genres we only use a small subset fma small for the scope of this project .furthermore it provides music s associated information including precomputed features user level metadata etc .it con the fma provided fine genre information for each track with built in genre hierarchy which is claimed by the artists themselves .free music archive 2 	to ensure data is balanced among different genres we only use a small subset fma small for the scope of this project 
0	109254	9254	it con the fma provided fine genre information for each track with built in genre hierarchy which is claimed by the artists themselves .to ensure data is balanced among different genres we only use a small subset fma small for the scope of this project .in each of the track table the ids of all the genres indicated by artists are included and the root genres are provided in genre top column the preprocessed dataset is split into 70 training 20 validation 10 test sets respectively .free music archive 2 	it con the fma provided fine genre information for each track with built in genre hierarchy which is claimed by the artists themselves 
0	109255	9255	in each of the track table the ids of all the genres indicated by artists are included and the root genres are provided in genre top column the preprocessed dataset is split into 70 training 20 validation 10 test sets respectively .it con the fma provided fine genre information for each track with built in genre hierarchy which is claimed by the artists themselves .last sentence.free music archive 2 	in each of the track table the ids of all the genres indicated by artists are included and the root genres are provided in genre top column the preprocessed dataset is split into 70 training 20 validation 10 test sets respectively 
0	109256	9256	a popular representation of sound is the spectrogram which captures both time and frequency information .first sentence.in this study we used mel spectrogram as the only input to train our nerual network .features	a popular representation of sound is the spectrogram which captures both time and frequency information 
0	109257	9257	in this study we used mel spectrogram as the only input to train our nerual network .a popular representation of sound is the spectrogram which captures both time and frequency information .a mel spectrogram is a spectrogram transformed to have frequencies in mel scale which basically is a logarithmic scale more naturally representing how human actually senses different sound frequencies .features	in this study we used mel spectrogram as the only input to train our nerual network 
0	109258	9258	a mel spectrogram is a spectrogram transformed to have frequencies in mel scale which basically is a logarithmic scale more naturally representing how human actually senses different sound frequencies .in this study we used mel spectrogram as the only input to train our nerual network .it is simple to implement thanks to librosa aside from the music features extracted by librosa fma also provides music metadata such as release year number of listens composers durations etc .features	a mel spectrogram is a spectrogram transformed to have frequencies in mel scale which basically is a logarithmic scale more naturally representing how human actually senses different sound frequencies 
1	109259	9259	it is simple to implement thanks to librosa aside from the music features extracted by librosa fma also provides music metadata such as release year number of listens composers durations etc .a mel spectrogram is a spectrogram transformed to have frequencies in mel scale which basically is a logarithmic scale more naturally representing how human actually senses different sound frequencies .there are 140 features in total that could be used for training .features	it is simple to implement thanks to librosa aside from the music features extracted by librosa fma also provides music metadata such as release year number of listens composers durations etc 
0	109260	9260	there are 140 features in total that could be used for training .it is simple to implement thanks to librosa aside from the music features extracted by librosa fma also provides music metadata such as release year number of listens composers durations etc .last sentence.features	there are 140 features in total that could be used for training 
0	109261	9261	we trained four traditional classification models on the dataset as baseline classifiers including k nearest neighbors logistic regression multilayer perception and linear support vector machine .first sentence.it was found that baseline models could achieve no higher than 50 accuracy .method 4 1 baseline classifiers	we trained four traditional classification models on the dataset as baseline classifiers including k nearest neighbors logistic regression multilayer perception and linear support vector machine 
0	109262	9262	it was found that baseline models could achieve no higher than 50 accuracy .we trained four traditional classification models on the dataset as baseline classifiers including k nearest neighbors logistic regression multilayer perception and linear support vector machine .since these models were merely used for comparison we adopted the default implementation and parameters in scikit learn library .method 4 1 baseline classifiers	it was found that baseline models could achieve no higher than 50 accuracy 
0	109263	9263	since these models were merely used for comparison we adopted the default implementation and parameters in scikit learn library .it was found that baseline models could achieve no higher than 50 accuracy .the input features include all 140 features provided by fma .method 4 1 baseline classifiers	since these models were merely used for comparison we adopted the default implementation and parameters in scikit learn library 
0	109264	9264	the input features include all 140 features provided by fma .since these models were merely used for comparison we adopted the default implementation and parameters in scikit learn library .last sentence.method 4 1 baseline classifiers	the input features include all 140 features provided by fma 
0	109265	9265	as shown in to start features are extracted from the spectrograms using convolutional layers .first sentence.it is important to point out that the features are translation invariant only in time domain frequencies do matter and needs to be distinguished .songnet architecture	as shown in to start features are extracted from the spectrograms using convolutional layers 
0	109266	9266	it is important to point out that the features are translation invariant only in time domain frequencies do matter and needs to be distinguished .as shown in to start features are extracted from the spectrograms using convolutional layers .thus 2 d convolution seems unsuitable in this case we are interested in changes across time every convolutional layer should look at a small period of time as a whole extract the most valuable information and create a feature map that is still a sequence over time .songnet architecture	it is important to point out that the features are translation invariant only in time domain frequencies do matter and needs to be distinguished 
0	109267	9267	thus 2 d convolution seems unsuitable in this case we are interested in changes across time every convolutional layer should look at a small period of time as a whole extract the most valuable information and create a feature map that is still a sequence over time .it is important to point out that the features are translation invariant only in time domain frequencies do matter and needs to be distinguished .then one dimensional convolutions across the time axis were adopted .songnet architecture	thus 2 d convolution seems unsuitable in this case we are interested in changes across time every convolutional layer should look at a small period of time as a whole extract the most valuable information and create a feature map that is still a sequence over time 
0	109268	9268	then one dimensional convolutions across the time axis were adopted .thus 2 d convolution seems unsuitable in this case we are interested in changes across time every convolutional layer should look at a small period of time as a whole extract the most valuable information and create a feature map that is still a sequence over time .each convolution is followed by relu activation and 1 d max pooling .songnet architecture	then one dimensional convolutions across the time axis were adopted 
0	109269	9269	each convolution is followed by relu activation and 1 d max pooling .then one dimensional convolutions across the time axis were adopted .to regularize the model we added dropout to every convolutional layers the cnn outputs a sequence of features and it is then fed to rnn represented by a time distributed fully connected layer with softmax activation essentially giving us a sequence of 8 dimensional vectors 8 is the number of genres in fma small at each timestep .songnet architecture	each convolution is followed by relu activation and 1 d max pooling 
0	109270	9270	to regularize the model we added dropout to every convolutional layers the cnn outputs a sequence of features and it is then fed to rnn represented by a time distributed fully connected layer with softmax activation essentially giving us a sequence of 8 dimensional vectors 8 is the number of genres in fma small at each timestep .each convolution is followed by relu activation and 1 d max pooling .the rnn part is designed to find both dependencies across short period of time and a long term structure of a song .songnet architecture	to regularize the model we added dropout to every convolutional layers the cnn outputs a sequence of features and it is then fed to rnn represented by a time distributed fully connected layer with softmax activation essentially giving us a sequence of 8 dimensional vectors 8 is the number of genres in fma small at each timestep 
0	109271	9271	the rnn part is designed to find both dependencies across short period of time and a long term structure of a song .to regularize the model we added dropout to every convolutional layers the cnn outputs a sequence of features and it is then fed to rnn represented by a time distributed fully connected layer with softmax activation essentially giving us a sequence of 8 dimensional vectors 8 is the number of genres in fma small at each timestep .these vectors are interpreted as the networks belief of the music genre at the particular point of time i e .songnet architecture	the rnn part is designed to find both dependencies across short period of time and a long term structure of a song 
0	109272	9272	these vectors are interpreted as the networks belief of the music genre at the particular point of time i e .the rnn part is designed to find both dependencies across short period of time and a long term structure of a song .probability distributions .songnet architecture	these vectors are interpreted as the networks belief of the music genre at the particular point of time i e 
0	109273	9273	probability distributions .these vectors are interpreted as the networks belief of the music genre at the particular point of time i e .to reduce the time series of 8 d probability vectors into a single one genre probability distribution we simply take the mean .songnet architecture	probability distributions 
0	109274	9274	to reduce the time series of 8 d probability vectors into a single one genre probability distribution we simply take the mean .probability distributions .it is the most intuitive way to tackle the disproportion problem of inferring music genre per timestep versus just one label for the whole song but it turns out to very effective .songnet architecture	to reduce the time series of 8 d probability vectors into a single one genre probability distribution we simply take the mean 
0	109275	9275	it is the most intuitive way to tackle the disproportion problem of inferring music genre per timestep versus just one label for the whole song but it turns out to very effective .to reduce the time series of 8 d probability vectors into a single one genre probability distribution we simply take the mean .last sentence.songnet architecture	it is the most intuitive way to tackle the disproportion problem of inferring music genre per timestep versus just one label for the whole song but it turns out to very effective 
0	109276	9276	the accuracies of baseline classifiers and songnet are reported in the table below .first sentence.it can be observed that our c rnn model outperforms the best baseline by 41 .performance	the accuracies of baseline classifiers and songnet are reported in the table below 
0	109277	9277	it can be observed that our c rnn model outperforms the best baseline by 41 .the accuracies of baseline classifiers and songnet are reported in the table below .the validation set was used to help us tune hyperparameters of songnet .performance	it can be observed that our c rnn model outperforms the best baseline by 41 
0	109278	9278	the validation set was used to help us tune hyperparameters of songnet .it can be observed that our c rnn model outperforms the best baseline by 41 .during training the learning rate was initially set to 0 001 and further decayed subject to reducelronplateau scheduler .performance	the validation set was used to help us tune hyperparameters of songnet 
0	109279	9279	during training the learning rate was initially set to 0 001 and further decayed subject to reducelronplateau scheduler .the validation set was used to help us tune hyperparameters of songnet .the reported numbers are accuracies with respect to the test set .performance	during training the learning rate was initially set to 0 001 and further decayed subject to reducelronplateau scheduler 
0	109280	9280	the reported numbers are accuracies with respect to the test set .during training the learning rate was initially set to 0 001 and further decayed subject to reducelronplateau scheduler .last sentence.performance	the reported numbers are accuracies with respect to the test set 
0	109281	9281	it is worth mentioning that all of our baseline models were trained and tested with rich features including music metadata year artist etc .first sentence.however in the current c rnn model setting we decided not to incorporate metadata for simpler training setup .model accuracy	it is worth mentioning that all of our baseline models were trained and tested with rich features including music metadata year artist etc 
0	109282	9282	however in the current c rnn model setting we decided not to incorporate metadata for simpler training setup .it is worth mentioning that all of our baseline models were trained and tested with rich features including music metadata year artist etc .the fact that c rnn model still beats the best baseline by a significant amount even without metadata demonstrates the power of deep learning models on classification tasks .model accuracy	however in the current c rnn model setting we decided not to incorporate metadata for simpler training setup 
0	109283	9283	the fact that c rnn model still beats the best baseline by a significant amount even without metadata demonstrates the power of deep learning models on classification tasks .however in the current c rnn model setting we decided not to incorporate metadata for simpler training setup .last sentence.model accuracy	the fact that c rnn model still beats the best baseline by a significant amount even without metadata demonstrates the power of deep learning models on classification tasks 
0	109284	9284	to further interpret the results and guide future work we plotted the confusion matrix.first sentence.last sentence.error analysis	to further interpret the results and guide future work we plotted the confusion matrix
0	109285	9285	in computer vision convolutional layers are used to extract features from images .first sentence.low level kernels can detect edges or corners and higher level kernels can capture more sophisticated structures .kernel clips	in computer vision convolutional layers are used to extract features from images 
0	109286	9286	low level kernels can detect edges or corners and higher level kernels can capture more sophisticated structures .in computer vision convolutional layers are used to extract features from images .in our setting we also expect convolutional layers to do similar things .kernel clips	low level kernels can detect edges or corners and higher level kernels can capture more sophisticated structures 
0	109287	9287	in our setting we also expect convolutional layers to do similar things .low level kernels can detect edges or corners and higher level kernels can capture more sophisticated structures .songnet has 3 convolutional layers so we expect kernels to extract different levels of music genre kernels .kernel clips	in our setting we also expect convolutional layers to do similar things 
1	109288	9288	songnet has 3 convolutional layers so we expect kernels to extract different levels of music genre kernels .in our setting we also expect convolutional layers to do similar things .it would be straightforward and much clearer if kernel numbers are converted to music clips .kernel clips	songnet has 3 convolutional layers so we expect kernels to extract different levels of music genre kernels 
1	109289	9289	it would be straightforward and much clearer if kernel numbers are converted to music clips .songnet has 3 convolutional layers so we expect kernels to extract different levels of music genre kernels .after listening to some of kernels we found that kernel clips in the first convolutional layer are mainly basic beats and elements of music .kernel clips	it would be straightforward and much clearer if kernel numbers are converted to music clips 
0	109290	9290	after listening to some of kernels we found that kernel clips in the first convolutional layer are mainly basic beats and elements of music .it would be straightforward and much clearer if kernel numbers are converted to music clips .the clips from the last convolutional layer however are already human listenable syn thesized music clips of certain genres .kernel clips	after listening to some of kernels we found that kernel clips in the first convolutional layer are mainly basic beats and elements of music 
0	109291	9291	the clips from the last convolutional layer however are already human listenable syn thesized music clips of certain genres .after listening to some of kernels we found that kernel clips in the first convolutional layer are mainly basic beats and elements of music .we demonstrated the kernel clips during the poster session and uploaded them to google drive link for grading purposes .kernel clips	the clips from the last convolutional layer however are already human listenable syn thesized music clips of certain genres 
0	109292	9292	we demonstrated the kernel clips during the poster session and uploaded them to google drive link for grading purposes .the clips from the last convolutional layer however are already human listenable syn thesized music clips of certain genres .last sentence.kernel clips	we demonstrated the kernel clips during the poster session and uploaded them to google drive link for grading purposes 
0	109293	9293	the ultimate goal of songnet is real time genre classification as the soundtrack plays .first sentence.this is the reason why we combined recurrent network with convolutional neural network in our architecture .real time	the ultimate goal of songnet is real time genre classification as the soundtrack plays 
0	109294	9294	this is the reason why we combined recurrent network with convolutional neural network in our architecture .the ultimate goal of songnet is real time genre classification as the soundtrack plays .as discussed in the architecture section for each timestep the model outputs a probability distribution vector among 8 different genres so it enables real time classification .real time	this is the reason why we combined recurrent network with convolutional neural network in our architecture 
1	109295	9295	as discussed in the architecture section for each timestep the model outputs a probability distribution vector among 8 different genres so it enables real time classification .this is the reason why we combined recurrent network with convolutional neural network in our architecture .it is better to show this functionality with a gui .real time	as discussed in the architecture section for each timestep the model outputs a probability distribution vector among 8 different genres so it enables real time classification 
0	109296	9296	it is better to show this functionality with a gui .as discussed in the architecture section for each timestep the model outputs a probability distribution vector among 8 different genres so it enables real time classification .due to limited timeline we did not implement it yet but it could be an interesting extension in future work .real time	it is better to show this functionality with a gui 
0	109297	9297	due to limited timeline we did not implement it yet but it could be an interesting extension in future work .it is better to show this functionality with a gui .last sentence.real time	due to limited timeline we did not implement it yet but it could be an interesting extension in future work 
0	109298	9298	following our discussion above we conclude two possible extensions of current work .first sentence.last sentence.future work	following our discussion above we conclude two possible extensions of current work 
0	109299	9299	to solve the experimental genre issue because it contributes a lot to the loss .first sentence.it is worth trying to incorporate music metadata .to further increase the test accuracy it is essential	to solve the experimental genre issue because it contributes a lot to the loss 
0	109300	9300	it is worth trying to incorporate music metadata .to solve the experimental genre issue because it contributes a lot to the loss .we expect the metadata to help increase the performance because even though this music itself shares some features with other genres such as rock and electronic additional information such as artists and album years will be able to help the model better classify this genre 2 .to further increase the test accuracy it is essential	it is worth trying to incorporate music metadata 
1	109301	9301	we expect the metadata to help increase the performance because even though this music itself shares some features with other genres such as rock and electronic additional information such as artists and album years will be able to help the model better classify this genre 2 .it is worth trying to incorporate music metadata .build a graphical user interface to allow users upload a music clip and then visualize the real time classification .to further increase the test accuracy it is essential	we expect the metadata to help increase the performance because even though this music itself shares some features with other genres such as rock and electronic additional information such as artists and album years will be able to help the model better classify this genre 2 
1	109302	9302	build a graphical user interface to allow users upload a music clip and then visualize the real time classification .we expect the metadata to help increase the performance because even though this music itself shares some features with other genres such as rock and electronic additional information such as artists and album years will be able to help the model better classify this genre 2 .this is fun as well as beneficial for further tuning the model .to further increase the test accuracy it is essential	build a graphical user interface to allow users upload a music clip and then visualize the real time classification 
0	109303	9303	this is fun as well as beneficial for further tuning the model .build a graphical user interface to allow users upload a music clip and then visualize the real time classification .it s fun because users can have a better way of interaction with the model .to further increase the test accuracy it is essential	this is fun as well as beneficial for further tuning the model 
0	109304	9304	it s fun because users can have a better way of interaction with the model .this is fun as well as beneficial for further tuning the model .as users upload more songs we could collect more data to improve the model .to further increase the test accuracy it is essential	it s fun because users can have a better way of interaction with the model 
0	109305	9305	as users upload more songs we could collect more data to improve the model .it s fun because users can have a better way of interaction with the model .last sentence.to further increase the test accuracy it is essential	as users upload more songs we could collect more data to improve the model 
1	109306	9306	we have developed a weakly supervised method for localizing pneumonia on chest x rays .first sentence.our model includes two parts 1 a 10 layer convolutional neural network cnn that predicts the presence of pneumonia and 2 a class activation map cam that localizes the pneumonia manifestation without requiring bounding box labels .abstract	we have developed a weakly supervised method for localizing pneumonia on chest x rays 
1	109307	9307	our model includes two parts 1 a 10 layer convolutional neural network cnn that predicts the presence of pneumonia and 2 a class activation map cam that localizes the pneumonia manifestation without requiring bounding box labels .we have developed a weakly supervised method for localizing pneumonia on chest x rays .by having our weakly supervised approach achieve slightly better performance than a supervised method r cnn we believe that this brings tremendous value in labeling diseases in images that are often unannotated in medical records .abstract	our model includes two parts 1 a 10 layer convolutional neural network cnn that predicts the presence of pneumonia and 2 a class activation map cam that localizes the pneumonia manifestation without requiring bounding box labels 
1	109308	9308	by having our weakly supervised approach achieve slightly better performance than a supervised method r cnn we believe that this brings tremendous value in labeling diseases in images that are often unannotated in medical records .our model includes two parts 1 a 10 layer convolutional neural network cnn that predicts the presence of pneumonia and 2 a class activation map cam that localizes the pneumonia manifestation without requiring bounding box labels .thus our method has the potential to provide care to populations with inadequate access to imaging diagnostic specialists while automate other medical image data sets .abstract	by having our weakly supervised approach achieve slightly better performance than a supervised method r cnn we believe that this brings tremendous value in labeling diseases in images that are often unannotated in medical records 
1	109309	9309	thus our method has the potential to provide care to populations with inadequate access to imaging diagnostic specialists while automate other medical image data sets .by having our weakly supervised approach achieve slightly better performance than a supervised method r cnn we believe that this brings tremendous value in labeling diseases in images that are often unannotated in medical records .last sentence.abstract	thus our method has the potential to provide care to populations with inadequate access to imaging diagnostic specialists while automate other medical image data sets 
1	109310	9310	pneumonia is an inflammatory condition of the lung that is responsible for more than 1 million hospitalizations and 50 000 deaths every year in the united states .first sentence.globally pneumonia is responsible for over 15 of all deaths of children under the age of 5 however chest x ray images generated from hospitals do not specify the precise location of the pneumonia which is a significant challenge when training a machine learning model for this purpose .introduction	pneumonia is an inflammatory condition of the lung that is responsible for more than 1 million hospitalizations and 50 000 deaths every year in the united states 
1	109311	9311	globally pneumonia is responsible for over 15 of all deaths of children under the age of 5 however chest x ray images generated from hospitals do not specify the precise location of the pneumonia which is a significant challenge when training a machine learning model for this purpose .pneumonia is an inflammatory condition of the lung that is responsible for more than 1 million hospitalizations and 50 000 deaths every year in the united states .at most doctors will keep a brief description such as aggregation of lung opacity on the patient s lower right lung .introduction	globally pneumonia is responsible for over 15 of all deaths of children under the age of 5 however chest x ray images generated from hospitals do not specify the precise location of the pneumonia which is a significant challenge when training a machine learning model for this purpose 
1	109312	9312	at most doctors will keep a brief description such as aggregation of lung opacity on the patient s lower right lung .globally pneumonia is responsible for over 15 of all deaths of children under the age of 5 however chest x ray images generated from hospitals do not specify the precise location of the pneumonia which is a significant challenge when training a machine learning model for this purpose .this is because the precise pixel location of lung opacity on the x ray image is only part of the equation for diagnosing and only the final conclusion is recorded in the electrical health record ehr .introduction	at most doctors will keep a brief description such as aggregation of lung opacity on the patient s lower right lung 
1	109313	9313	this is because the precise pixel location of lung opacity on the x ray image is only part of the equation for diagnosing and only the final conclusion is recorded in the electrical health record ehr .at most doctors will keep a brief description such as aggregation of lung opacity on the patient s lower right lung .to developed a machine learning algorithm that predicts pneumonia location using traditional supervised methods requires the 0 precise x y coordinate labels that datasets lack .introduction	this is because the precise pixel location of lung opacity on the x ray image is only part of the equation for diagnosing and only the final conclusion is recorded in the electrical health record ehr 
1	109314	9314	to developed a machine learning algorithm that predicts pneumonia location using traditional supervised methods requires the 0 precise x y coordinate labels that datasets lack .this is because the precise pixel location of lung opacity on the x ray image is only part of the equation for diagnosing and only the final conclusion is recorded in the electrical health record ehr .hence this deficiency in labelled datasets commonly observed in the medical imaging field is the motivation behind our work in this work we tackle this challenge in a novel approach we use a weakly supervised approach to automate localizing pneumonia in chest x rays .introduction	to developed a machine learning algorithm that predicts pneumonia location using traditional supervised methods requires the 0 precise x y coordinate labels that datasets lack 
1	109315	9315	hence this deficiency in labelled datasets commonly observed in the medical imaging field is the motivation behind our work in this work we tackle this challenge in a novel approach we use a weakly supervised approach to automate localizing pneumonia in chest x rays .to developed a machine learning algorithm that predicts pneumonia location using traditional supervised methods requires the 0 precise x y coordinate labels that datasets lack .our model is considered weakly supervised because it only needs the binary labels pneumonia vs no pneumonia during training to estimate a bounding box around the region of the lung opacity .introduction	hence this deficiency in labelled datasets commonly observed in the medical imaging field is the motivation behind our work in this work we tackle this challenge in a novel approach we use a weakly supervised approach to automate localizing pneumonia in chest x rays 
1	109316	9316	our model is considered weakly supervised because it only needs the binary labels pneumonia vs no pneumonia during training to estimate a bounding box around the region of the lung opacity .hence this deficiency in labelled datasets commonly observed in the medical imaging field is the motivation behind our work in this work we tackle this challenge in a novel approach we use a weakly supervised approach to automate localizing pneumonia in chest x rays .at a high level our weakly supervised algorithm works as follows 1 input an x ray image in u net architecture for data augmentation 2 input augmented image and original image in a 10 layer cnn architecture to classify if given image is pneumonia positive and 3 if image is pneumonia positive apply cam to precisely localize the pneumonia aggregation .introduction	our model is considered weakly supervised because it only needs the binary labels pneumonia vs no pneumonia during training to estimate a bounding box around the region of the lung opacity 
1	109317	9317	at a high level our weakly supervised algorithm works as follows 1 input an x ray image in u net architecture for data augmentation 2 input augmented image and original image in a 10 layer cnn architecture to classify if given image is pneumonia positive and 3 if image is pneumonia positive apply cam to precisely localize the pneumonia aggregation .our model is considered weakly supervised because it only needs the binary labels pneumonia vs no pneumonia during training to estimate a bounding box around the region of the lung opacity .last sentence.introduction	at a high level our weakly supervised algorithm works as follows 1 input an x ray image in u net architecture for data augmentation 2 input augmented image and original image in a 10 layer cnn architecture to classify if given image is pneumonia positive and 3 if image is pneumonia positive apply cam to precisely localize the pneumonia aggregation 
1	109318	9318	there have been recent efforts in detecting pneumonia using x ray images .first sentence.for instance the chexnet 10 team modified chestx ray14 s 13 algorithm to increase the accuracy in detecting 14 diseases including pneumonia .related work	there have been recent efforts in detecting pneumonia using x ray images 
1	109319	9319	for instance the chexnet 10 team modified chestx ray14 s 13 algorithm to increase the accuracy in detecting 14 diseases including pneumonia .there have been recent efforts in detecting pneumonia using x ray images .however neither effort localizes using bounding boxes and both use imagenet to pretrain their models .related work	for instance the chexnet 10 team modified chestx ray14 s 13 algorithm to increase the accuracy in detecting 14 diseases including pneumonia 
0	109320	9320	however neither effort localizes using bounding boxes and both use imagenet to pretrain their models .for instance the chexnet 10 team modified chestx ray14 s 13 algorithm to increase the accuracy in detecting 14 diseases including pneumonia .despite the fact that both works achieve high accuracy neither solves the problem of clearly annotating the pneumonia manifestation using bounding boxes in the x ray images .related work	however neither effort localizes using bounding boxes and both use imagenet to pretrain their models 
1	109321	9321	despite the fact that both works achieve high accuracy neither solves the problem of clearly annotating the pneumonia manifestation using bounding boxes in the x ray images .however neither effort localizes using bounding boxes and both use imagenet to pretrain their models .as such we leverage the work of four algorithms in our approach 1 r cnn 6 2 cam 14 3 vgg architecture.related work	despite the fact that both works achieve high accuracy neither solves the problem of clearly annotating the pneumonia manifestation using bounding boxes in the x ray images 
1	109322	9322	as such we leverage the work of four algorithms in our approach 1 r cnn 6 2 cam 14 3 vgg architecture.despite the fact that both works achieve high accuracy neither solves the problem of clearly annotating the pneumonia manifestation using bounding boxes in the x ray images .last sentence.related work	as such we leverage the work of four algorithms in our approach 1 r cnn 6 2 cam 14 3 vgg architecture
0	109323	9323	3 0 1 .first sentence.dataset we acquired our dataset from kaggles rsna pneumonia detection competition.dataset and feature engineering	3 0 1 
0	109324	9324	dataset we acquired our dataset from kaggles rsna pneumonia detection competition.3 0 1 .last sentence.dataset and feature engineering	dataset we acquired our dataset from kaggles rsna pneumonia detection competition
1	109325	9325	each pixel of the images was normalized by subtracting the mean and dividing by the standard deviation at that location .first sentence.we also compressed the original image from 1024 1024 pixels down to 128 128 pixels allowing us to expedite the training of our neural network .feature engineering	each pixel of the images was normalized by subtracting the mean and dividing by the standard deviation at that location 
1	109326	9326	we also compressed the original image from 1024 1024 pixels down to 128 128 pixels allowing us to expedite the training of our neural network .each pixel of the images was normalized by subtracting the mean and dividing by the standard deviation at that location .a u net neural network was used to predict the confidence of each pixel belonging to the lung .feature engineering	we also compressed the original image from 1024 1024 pixels down to 128 128 pixels allowing us to expedite the training of our neural network 
1	109327	9327	a u net neural network was used to predict the confidence of each pixel belonging to the lung .we also compressed the original image from 1024 1024 pixels down to 128 128 pixels allowing us to expedite the training of our neural network .then we segmented the lung by multiplying the original image matrix with the localization matrix figure 2 .feature engineering	a u net neural network was used to predict the confidence of each pixel belonging to the lung 
1	109328	9328	then we segmented the lung by multiplying the original image matrix with the localization matrix figure 2 .a u net neural network was used to predict the confidence of each pixel belonging to the lung .both the original and the segmented images were fed into our model as inputs to provide our model with a hypothesis of lung location .feature engineering	then we segmented the lung by multiplying the original image matrix with the localization matrix figure 2 
1	109329	9329	both the original and the segmented images were fed into our model as inputs to provide our model with a hypothesis of lung location .then we segmented the lung by multiplying the original image matrix with the localization matrix figure 2 .the first part of our work is to build a cnn model that can accurately classify whether a given image is labeled as pneumonia or not .feature engineering	both the original and the segmented images were fed into our model as inputs to provide our model with a hypothesis of lung location 
1	109330	9330	the first part of our work is to build a cnn model that can accurately classify whether a given image is labeled as pneumonia or not .both the original and the segmented images were fed into our model as inputs to provide our model with a hypothesis of lung location .the images that were predicted as pneumonia positive are then fed into our localization model .feature engineering	the first part of our work is to build a cnn model that can accurately classify whether a given image is labeled as pneumonia or not 
1	109331	9331	the images that were predicted as pneumonia positive are then fed into our localization model .the first part of our work is to build a cnn model that can accurately classify whether a given image is labeled as pneumonia or not .last sentence.feature engineering	the images that were predicted as pneumonia positive are then fed into our localization model 
1	109332	9332	since the first part of this project is a supervised classification task svm random forest and logistic regression were used to baseline our classification model .first sentence.these models could not take a matrix of pixels as an input so we flatten the images into one dimensional vectors .baseline	since the first part of this project is a supervised classification task svm random forest and logistic regression were used to baseline our classification model 
1	109333	9333	these models could not take a matrix of pixels as an input so we flatten the images into one dimensional vectors .since the first part of this project is a supervised classification task svm random forest and logistic regression were used to baseline our classification model .last sentence.baseline	these models could not take a matrix of pixels as an input so we flatten the images into one dimensional vectors 
1	109334	9334	our best model contains 10 convolutional layers each with zero padding to keep the size of the original image and we used relu as the activation function .first sentence.the convolution filters are matrix of weights that slides through the original image to pick up patterns for prediction .cnn architecture	our best model contains 10 convolutional layers each with zero padding to keep the size of the original image and we used relu as the activation function 
1	109335	9335	the convolution filters are matrix of weights that slides through the original image to pick up patterns for prediction .our best model contains 10 convolutional layers each with zero padding to keep the size of the original image and we used relu as the activation function .the cam requires our model to only have one fully connected fc layer and a global average pool gap layer before that .cnn architecture	the convolution filters are matrix of weights that slides through the original image to pick up patterns for prediction 
0	109336	9336	the cam requires our model to only have one fully connected fc layer and a global average pool gap layer before that .the convolution filters are matrix of weights that slides through the original image to pick up patterns for prediction .the gap layer takes the average of the output for each of the convolution filters .cnn architecture	the cam requires our model to only have one fully connected fc layer and a global average pool gap layer before that 
0	109337	9337	the gap layer takes the average of the output for each of the convolution filters .the cam requires our model to only have one fully connected fc layer and a global average pool gap layer before that .the fc layer connects the flattened averages to the two classes .cnn architecture	the gap layer takes the average of the output for each of the convolution filters 
0	109338	9338	the fc layer connects the flattened averages to the two classes .the gap layer takes the average of the output for each of the convolution filters .the model was trained with an adam optimizer with 0 0001 learning rate on 20 epochs .cnn architecture	the fc layer connects the flattened averages to the two classes 
1	109339	9339	the model was trained with an adam optimizer with 0 0001 learning rate on 20 epochs .the fc layer connects the flattened averages to the two classes .last sentence.cnn architecture	the model was trained with an adam optimizer with 0 0001 learning rate on 20 epochs 
1	109340	9340	the second part of our project is to build a weakly supervised model that can predict the localization of pneumonia on the positively classified images without the training labels of the locations .first sentence.last sentence.localization	the second part of our project is to build a weakly supervised model that can predict the localization of pneumonia on the positively classified images without the training labels of the locations 
1	109341	9341	to generate region proposals we slide a small network over the convolutional feature map output by the last shared convolutional layer .first sentence.this layer comes from the classifier that is trained on predicting pneumonia .supervised r cnn approach benchmark 	to generate region proposals we slide a small network over the convolutional feature map output by the last shared convolutional layer 
0	109342	9342	this layer comes from the classifier that is trained on predicting pneumonia .to generate region proposals we slide a small network over the convolutional feature map output by the last shared convolutional layer .this small network takes in a small spatial window from the cnn feature map and predicts whether or not these windows contain pneumonia or not pneumonia .supervised r cnn approach benchmark 	this layer comes from the classifier that is trained on predicting pneumonia 
1	109343	9343	this small network takes in a small spatial window from the cnn feature map and predicts whether or not these windows contain pneumonia or not pneumonia .this layer comes from the classifier that is trained on predicting pneumonia .a window is defined as having four coordinates x1 y1 x2 y2 .supervised r cnn approach benchmark 	this small network takes in a small spatial window from the cnn feature map and predicts whether or not these windows contain pneumonia or not pneumonia 
0	109344	9344	a window is defined as having four coordinates x1 y1 x2 y2 .this small network takes in a small spatial window from the cnn feature map and predicts whether or not these windows contain pneumonia or not pneumonia .we only keep the windows that are classified as having pneumonia and by how much these spatial windows overlap with the ground truth labels .supervised r cnn approach benchmark 	a window is defined as having four coordinates x1 y1 x2 y2 
1	109345	9345	we only keep the windows that are classified as having pneumonia and by how much these spatial windows overlap with the ground truth labels .a window is defined as having four coordinates x1 y1 x2 y2 .then for each spatial window the features from the original cnn feature map is mapped back to the cnn feature map from the classifier and these windows are pooled to the same size and are feed to two networks one network to predict class background or pneumonia and another network to predict the coordinates figure 3 b .supervised r cnn approach benchmark 	we only keep the windows that are classified as having pneumonia and by how much these spatial windows overlap with the ground truth labels 
1	109346	9346	then for each spatial window the features from the original cnn feature map is mapped back to the cnn feature map from the classifier and these windows are pooled to the same size and are feed to two networks one network to predict class background or pneumonia and another network to predict the coordinates figure 3 b .we only keep the windows that are classified as having pneumonia and by how much these spatial windows overlap with the ground truth labels .last sentence.supervised r cnn approach benchmark 	then for each spatial window the features from the original cnn feature map is mapped back to the cnn feature map from the classifier and these windows are pooled to the same size and are feed to two networks one network to predict class background or pneumonia and another network to predict the coordinates figure 3 b 
0	109347	9347	our weakly supervised portion of the model consists of the following components figure 3c .first sentence.last sentence.weakly supervised approach	our weakly supervised portion of the model consists of the following components figure 3c 
1	109348	9348	a cam that takes in the output of the final cnn model and the fc layer weights for the pneumonia class neuron and sums up the weighted outputs using the following formula where x is the input image features f k give the output from the last convolution layer given x and w c k is the fully connect weight for the k th filter output to class c in our case class c is the pneumonia class .first sentence.last sentence.cam	a cam that takes in the output of the final cnn model and the fc layer weights for the pneumonia class neuron and sums up the weighted outputs using the following formula where x is the input image features f k give the output from the last convolution layer given x and w c k is the fully connect weight for the k th filter output to class c in our case class c is the pneumonia class 
1	109349	9349	the output from cam was then scaled into a 3 channel rgb heatmap .first sentence.last sentence.heatmap	the output from cam was then scaled into a 3 channel rgb heatmap 
1	109350	9350	to find individual clusters of predictions on the heatmap we applied a depth first search clustering algorithm algorithm 1 on a random non zero pixel on the heatmap and repeated until all non zero pixels are clustered algorithm 1 dfs cluster algorithm class index 0 while still exist non zero pixel without class label do pick a random non zero pixel without class label assign pixel to class index for each neighbor pixel do if if neighbor is also a non zero pixel without class then recursively apply dfs end end end class index 1.first sentence.last sentence.dfs clustering	to find individual clusters of predictions on the heatmap we applied a depth first search clustering algorithm algorithm 1 on a random non zero pixel on the heatmap and repeated until all non zero pixels are clustered algorithm 1 dfs cluster algorithm class index 0 while still exist non zero pixel without class label do pick a random non zero pixel without class label assign pixel to class index for each neighbor pixel do if if neighbor is also a non zero pixel without class then recursively apply dfs end end end class index 1
1	109351	9351	lastly we drew a bounding box around each clusters by finding the minimum and maximum x y coordinates of the clusters and only kept boxes that are within 2 standard deviations of all predictions .first sentence.last sentence.bounding box	lastly we drew a bounding box around each clusters by finding the minimum and maximum x y coordinates of the clusters and only kept boxes that are within 2 standard deviations of all predictions 
1	109352	9352	input features running any localizing model on the original x rays yielded low iou scores .first sentence.we discovered that often time the model will localize matter denser than the lung such as muscle tissue and equipment as pneumonia positive figure 4 right .experimentation	input features running any localizing model on the original x rays yielded low iou scores 
1	109353	9353	we discovered that often time the model will localize matter denser than the lung such as muscle tissue and equipment as pneumonia positive figure 4 right .input features running any localizing model on the original x rays yielded low iou scores .and since a high iou score correspond to a more accurate and tightly fitted localization we experimented with a number of approaches to increase our iou score we started by only feeding in the segmented lungs from u net as the input for our model .experimentation	we discovered that often time the model will localize matter denser than the lung such as muscle tissue and equipment as pneumonia positive figure 4 right 
1	109354	9354	and since a high iou score correspond to a more accurate and tightly fitted localization we experimented with a number of approaches to increase our iou score we started by only feeding in the segmented lungs from u net as the input for our model .we discovered that often time the model will localize matter denser than the lung such as muscle tissue and equipment as pneumonia positive figure 4 right .though initially the classification results were promising the iou score did not improve significantly .experimentation	and since a high iou score correspond to a more accurate and tightly fitted localization we experimented with a number of approaches to increase our iou score we started by only feeding in the segmented lungs from u net as the input for our model 
0	109355	9355	though initially the classification results were promising the iou score did not improve significantly .and since a high iou score correspond to a more accurate and tightly fitted localization we experimented with a number of approaches to increase our iou score we started by only feeding in the segmented lungs from u net as the input for our model .we soon discovered that in instances of sever pneumonia infection where the density of that part of the lung and surrounding tissue were almost identical the u net algorithm segmented out that part of the lung .experimentation	though initially the classification results were promising the iou score did not improve significantly 
1	109356	9356	we soon discovered that in instances of sever pneumonia infection where the density of that part of the lung and surrounding tissue were almost identical the u net algorithm segmented out that part of the lung .though initially the classification results were promising the iou score did not improve significantly .this in turn yielded inaccurate localization results where the algorithm localizes on the healthy part of the lung figure 4 left mid after testing out different combination we found the best results can be achieved when we use both the original and segmented image simultaneously by running them through two channels of the network .experimentation	we soon discovered that in instances of sever pneumonia infection where the density of that part of the lung and surrounding tissue were almost identical the u net algorithm segmented out that part of the lung 
1	109357	9357	this in turn yielded inaccurate localization results where the algorithm localizes on the healthy part of the lung figure 4 left mid after testing out different combination we found the best results can be achieved when we use both the original and segmented image simultaneously by running them through two channels of the network .we soon discovered that in instances of sever pneumonia infection where the density of that part of the lung and surrounding tissue were almost identical the u net algorithm segmented out that part of the lung .we hypothesize that including the segmented healthy part of the lung provides the model with extra information on where the likely locations of the lung opacity .experimentation	this in turn yielded inaccurate localization results where the algorithm localizes on the healthy part of the lung figure 4 left mid after testing out different combination we found the best results can be achieved when we use both the original and segmented image simultaneously by running them through two channels of the network 
1	109358	9358	we hypothesize that including the segmented healthy part of the lung provides the model with extra information on where the likely locations of the lung opacity .this in turn yielded inaccurate localization results where the algorithm localizes on the healthy part of the lung figure 4 left mid after testing out different combination we found the best results can be achieved when we use both the original and segmented image simultaneously by running them through two channels of the network .last sentence.experimentation	we hypothesize that including the segmented healthy part of the lung provides the model with extra information on where the likely locations of the lung opacity 
0	109359	9359	we started our classification model using the vgg16 model which includes 26 layers and 138 357 544 weights .first sentence.it was clear that the large number of weights caused our model to quickly overfit with training accuracy as high as 96 but 72 for validation .model architecture and parameter tuning	we started our classification model using the vgg16 model which includes 26 layers and 138 357 544 weights 
0	109360	9360	it was clear that the large number of weights caused our model to quickly overfit with training accuracy as high as 96 but 72 for validation .we started our classification model using the vgg16 model which includes 26 layers and 138 357 544 weights .we then modified the architecture by removing layer by layer until bias of the model dropped but still were able to make above 90 accuracy on the validation set then we tested different filter sizes of the convolutional layers to improve the classification .model architecture and parameter tuning	it was clear that the large number of weights caused our model to quickly overfit with training accuracy as high as 96 but 72 for validation 
1	109361	9361	we then modified the architecture by removing layer by layer until bias of the model dropped but still were able to make above 90 accuracy on the validation set then we tested different filter sizes of the convolutional layers to improve the classification .it was clear that the large number of weights caused our model to quickly overfit with training accuracy as high as 96 but 72 for validation .our highest validation accuracy was achieved starting with small 3x3 filters and gradually increase the filter size to 16x16 at the last layer .model architecture and parameter tuning	we then modified the architecture by removing layer by layer until bias of the model dropped but still were able to make above 90 accuracy on the validation set then we tested different filter sizes of the convolutional layers to improve the classification 
1	109362	9362	our highest validation accuracy was achieved starting with small 3x3 filters and gradually increase the filter size to 16x16 at the last layer .we then modified the architecture by removing layer by layer until bias of the model dropped but still were able to make above 90 accuracy on the validation set then we tested different filter sizes of the convolutional layers to improve the classification .however a big filter size at the last convolution layer gave us imprecise prediction of the the bounding boxes and caused a sharp decrease in the iou score .model architecture and parameter tuning	our highest validation accuracy was achieved starting with small 3x3 filters and gradually increase the filter size to 16x16 at the last layer 
1	109363	9363	however a big filter size at the last convolution layer gave us imprecise prediction of the the bounding boxes and caused a sharp decrease in the iou score .our highest validation accuracy was achieved starting with small 3x3 filters and gradually increase the filter size to 16x16 at the last layer .therefore we decided to sacrifice some classification accuracy for an increased iou score by keeping all filters to 3x3 finally different optimizers were tested to train our model .model architecture and parameter tuning	however a big filter size at the last convolution layer gave us imprecise prediction of the the bounding boxes and caused a sharp decrease in the iou score 
1	109364	9364	therefore we decided to sacrifice some classification accuracy for an increased iou score by keeping all filters to 3x3 finally different optimizers were tested to train our model .however a big filter size at the last convolution layer gave us imprecise prediction of the the bounding boxes and caused a sharp decrease in the iou score .the standard stochastic gradient descent algorithm trains very slowly and does not converge to above 85 accuracy .model architecture and parameter tuning	therefore we decided to sacrifice some classification accuracy for an increased iou score by keeping all filters to 3x3 finally different optimizers were tested to train our model 
0	109365	9365	the standard stochastic gradient descent algorithm trains very slowly and does not converge to above 85 accuracy .therefore we decided to sacrifice some classification accuracy for an increased iou score by keeping all filters to 3x3 finally different optimizers were tested to train our model .adam and adagrad converge faster but adam achieved a higher validation accuracy .model architecture and parameter tuning	the standard stochastic gradient descent algorithm trains very slowly and does not converge to above 85 accuracy 
0	109366	9366	adam and adagrad converge faster but adam achieved a higher validation accuracy .the standard stochastic gradient descent algorithm trains very slowly and does not converge to above 85 accuracy .we also tested out different learning rates 0 001 0001 0 0001 for each optimizer .model architecture and parameter tuning	adam and adagrad converge faster but adam achieved a higher validation accuracy 
0	109367	9367	we also tested out different learning rates 0 001 0001 0 0001 for each optimizer .adam and adagrad converge faster but adam achieved a higher validation accuracy .learning rate of 0 001 caused the weights to blow up and achieve lower than 50 accuracy .model architecture and parameter tuning	we also tested out different learning rates 0 001 0001 0 0001 for each optimizer 
0	109368	9368	learning rate of 0 001 caused the weights to blow up and achieve lower than 50 accuracy .we also tested out different learning rates 0 001 0001 0 0001 for each optimizer .however a learning rate of 0 0001 with adam optimizer gave us our higher accuracy in the shortest period of time .model architecture and parameter tuning	learning rate of 0 001 caused the weights to blow up and achieve lower than 50 accuracy 
1	109369	9369	however a learning rate of 0 0001 with adam optimizer gave us our higher accuracy in the shortest period of time .learning rate of 0 001 caused the weights to blow up and achieve lower than 50 accuracy .formula 2 the iou formula.model architecture and parameter tuning	however a learning rate of 0 0001 with adam optimizer gave us our higher accuracy in the shortest period of time 
0	109370	9370	formula 2 the iou formula.however a learning rate of 0 0001 with adam optimizer gave us our higher accuracy in the shortest period of time .last sentence.model architecture and parameter tuning	formula 2 the iou formula
0	109371	9371	our cnn significantly outperformed traditional classifiers without over fitting.first sentence.last sentence.discussion	our cnn significantly outperformed traditional classifiers without over fitting
1	109372	9372	based on our result we have shown that our weakly supervised method is able to localize pneumonia slightly better than a supervised method .first sentence.we predict that our model can perform even better if we have the computing power to train on the full images as a lot of information are lost during compression .conclusion	based on our result we have shown that our weakly supervised method is able to localize pneumonia slightly better than a supervised method 
1	109373	9373	we predict that our model can perform even better if we have the computing power to train on the full images as a lot of information are lost during compression .based on our result we have shown that our weakly supervised method is able to localize pneumonia slightly better than a supervised method .we also expect improvements by including more training data or transferring learned models from similar works such as chestxnet .conclusion	we predict that our model can perform even better if we have the computing power to train on the full images as a lot of information are lost during compression 
1	109374	9374	we also expect improvements by including more training data or transferring learned models from similar works such as chestxnet .we predict that our model can perform even better if we have the computing power to train on the full images as a lot of information are lost during compression .if improved to human level performance our weakly supervised model not only can automate pneumonia location annotation and classification tasks but also can be used to automate other medical image datasets .conclusion	we also expect improvements by including more training data or transferring learned models from similar works such as chestxnet 
1	109375	9375	if improved to human level performance our weakly supervised model not only can automate pneumonia location annotation and classification tasks but also can be used to automate other medical image datasets .we also expect improvements by including more training data or transferring learned models from similar works such as chestxnet .last sentence.conclusion	if improved to human level performance our weakly supervised model not only can automate pneumonia location annotation and classification tasks but also can be used to automate other medical image datasets 
0	109376	9376	mars huang came up with project idea and methodology .first sentence.build the cnn classifier and tested different architectures .contribution	mars huang came up with project idea and methodology 
0	109377	9377	build the cnn classifier and tested different architectures .mars huang came up with project idea and methodology .modified the classifier to fit in to class actiation mapping .contribution	build the cnn classifier and tested different architectures 
0	109378	9378	modified the classifier to fit in to class actiation mapping .build the cnn classifier and tested different architectures .implemented cam dfs clustering algorithm .contribution	modified the classifier to fit in to class actiation mapping 
0	109379	9379	implemented cam dfs clustering algorithm .modified the classifier to fit in to class actiation mapping .made functions to draw bounding box calculate iou and feature engineering .contribution	implemented cam dfs clustering algorithm 
0	109380	9380	made functions to draw bounding box calculate iou and feature engineering .implemented cam dfs clustering algorithm .tried to implement em and kmeans to cluster heatmap regions .contribution	made functions to draw bounding box calculate iou and feature engineering 
0	109381	9381	tried to implement em and kmeans to cluster heatmap regions .made functions to draw bounding box calculate iou and feature engineering .attempted to reduce dimentions of the data for baseline by using factor analysis .contribution	tried to implement em and kmeans to cluster heatmap regions 
1	109382	9382	attempted to reduce dimentions of the data for baseline by using factor analysis .tried to implement em and kmeans to cluster heatmap regions .tested all baselines for classification portion of the project and experimentation in the classification and weakly supervised localization .contribution	attempted to reduce dimentions of the data for baseline by using factor analysis 
0	109383	9383	tested all baselines for classification portion of the project and experimentation in the classification and weakly supervised localization .attempted to reduce dimentions of the data for baseline by using factor analysis .created mltoolkit for baselines .contribution	tested all baselines for classification portion of the project and experimentation in the classification and weakly supervised localization 
0	109384	9384	created mltoolkit for baselines .tested all baselines for classification portion of the project and experimentation in the classification and weakly supervised localization .generated all figures major contributed to the paper and poster .contribution	created mltoolkit for baselines 
0	109385	9385	generated all figures major contributed to the paper and poster .created mltoolkit for baselines .set up google cloud medi monam lead in reading literature to gather knowledge in the field .contribution	generated all figures major contributed to the paper and poster 
0	109386	9386	set up google cloud medi monam lead in reading literature to gather knowledge in the field .generated all figures major contributed to the paper and poster .unet segmentation feature engineering .contribution	set up google cloud medi monam lead in reading literature to gather knowledge in the field 
0	109387	9387	unet segmentation feature engineering .set up google cloud medi monam lead in reading literature to gather knowledge in the field .experimented with methods to cluster heatmap islands .contribution	unet segmentation feature engineering 
0	109388	9388	experimented with methods to cluster heatmap islands .unet segmentation feature engineering .experimented with implementation of vgg16 .contribution	experimented with methods to cluster heatmap islands 
0	109389	9389	experimented with implementation of vgg16 .experimented with methods to cluster heatmap islands .contributed to poster and paper .contribution	experimented with implementation of vgg16 
0	109390	9390	contributed to poster and paper .experimented with implementation of vgg16 .printed poster emanuel cortes built the supervised model for classification and localization .contribution	contributed to poster and paper 
0	109391	9391	printed poster emanuel cortes built the supervised model for classification and localization .contributed to poster and paper .implemented a resnet backbone classifier custom region proposal layer roi pooling and a bounding box regressor that is pretrained on the coco dataset and finetuned on kaggles rsna pneumonia detection competition dataset .contribution	printed poster emanuel cortes built the supervised model for classification and localization 
1	109392	9392	implemented a resnet backbone classifier custom region proposal layer roi pooling and a bounding box regressor that is pretrained on the coco dataset and finetuned on kaggles rsna pneumonia detection competition dataset .printed poster emanuel cortes built the supervised model for classification and localization .experimented with feeding other cnn based backbone classifier architectures whose out .contribution	implemented a resnet backbone classifier custom region proposal layer roi pooling and a bounding box regressor that is pretrained on the coco dataset and finetuned on kaggles rsna pneumonia detection competition dataset 
0	109393	9393	experimented with feeding other cnn based backbone classifier architectures whose out .implemented a resnet backbone classifier custom region proposal layer roi pooling and a bounding box regressor that is pretrained on the coco dataset and finetuned on kaggles rsna pneumonia detection competition dataset .last sentence.contribution	experimented with feeding other cnn based backbone classifier architectures whose out 
1	109394	9394	we compared traditional machine learning methods naive bayes linear discriminant analysis knearest neighbors random forest support vector machine and deep learning methods convolutional neural networks in ship detection of satellite images .first sentence.we found that among all traditional methods we have tried random forest gave the best performance 93 accuracy .abstract	we compared traditional machine learning methods naive bayes linear discriminant analysis knearest neighbors random forest support vector machine and deep learning methods convolutional neural networks in ship detection of satellite images 
0	109395	9395	we found that among all traditional methods we have tried random forest gave the best performance 93 accuracy .we compared traditional machine learning methods naive bayes linear discriminant analysis knearest neighbors random forest support vector machine and deep learning methods convolutional neural networks in ship detection of satellite images .among deep learning approaches the simple train from scratch cnn model achieve 94 accuracy which outperforms the pre trained cnn model using transfer learning .abstract	we found that among all traditional methods we have tried random forest gave the best performance 93 accuracy 
1	109396	9396	among deep learning approaches the simple train from scratch cnn model achieve 94 accuracy which outperforms the pre trained cnn model using transfer learning .we found that among all traditional methods we have tried random forest gave the best performance 93 accuracy .last sentence.abstract	among deep learning approaches the simple train from scratch cnn model achieve 94 accuracy which outperforms the pre trained cnn model using transfer learning 
1	109397	9397	the fast growing shipping traffic increases the chances of infractions at sea such as environmentally devastating ship accidents piracy illegal fishing drug trafficking and illegal cargo movement .first sentence.comprehensive maritime monitoring services helps support the maritime industry to increase knowledge anticipate threats trigger alerts and improve efficiency at sea .introduction	the fast growing shipping traffic increases the chances of infractions at sea such as environmentally devastating ship accidents piracy illegal fishing drug trafficking and illegal cargo movement 
1	109398	9398	comprehensive maritime monitoring services helps support the maritime industry to increase knowledge anticipate threats trigger alerts and improve efficiency at sea .the fast growing shipping traffic increases the chances of infractions at sea such as environmentally devastating ship accidents piracy illegal fishing drug trafficking and illegal cargo movement .this challenge origins partly from the airbus ship detection challenge on kaggle .introduction	comprehensive maritime monitoring services helps support the maritime industry to increase knowledge anticipate threats trigger alerts and improve efficiency at sea 
1	109399	9399	this challenge origins partly from the airbus ship detection challenge on kaggle .comprehensive maritime monitoring services helps support the maritime industry to increase knowledge anticipate threats trigger alerts and improve efficiency at sea .we plan to come up with a solution to efficiently detect ship in satellite images .introduction	this challenge origins partly from the airbus ship detection challenge on kaggle 
0	109400	9400	we plan to come up with a solution to efficiently detect ship in satellite images .this challenge origins partly from the airbus ship detection challenge on kaggle .this classification is challenging because boats are really small in the satellite images .introduction	we plan to come up with a solution to efficiently detect ship in satellite images 
0	109401	9401	this classification is challenging because boats are really small in the satellite images .we plan to come up with a solution to efficiently detect ship in satellite images .various scenes including open water wharf buildings and clouds appear in the dataset in this paper we compared traditional methods and deep learning methods in solving the classification problem .introduction	this classification is challenging because boats are really small in the satellite images 
1	109402	9402	various scenes including open water wharf buildings and clouds appear in the dataset in this paper we compared traditional methods and deep learning methods in solving the classification problem .this classification is challenging because boats are really small in the satellite images .for traditional methods we experimented with naive bayes linear discriminant analysis k nearest neighbors random forest and support vector machine model .introduction	various scenes including open water wharf buildings and clouds appear in the dataset in this paper we compared traditional methods and deep learning methods in solving the classification problem 
1	109403	9403	for traditional methods we experimented with naive bayes linear discriminant analysis k nearest neighbors random forest and support vector machine model .various scenes including open water wharf buildings and clouds appear in the dataset in this paper we compared traditional methods and deep learning methods in solving the classification problem .before training these models we did image features extraction by finding global feature descriptors of every image which includes color histogram hu moments haralick texture and histogram of oriented gradients hog .introduction	for traditional methods we experimented with naive bayes linear discriminant analysis k nearest neighbors random forest and support vector machine model 
1	109404	9404	before training these models we did image features extraction by finding global feature descriptors of every image which includes color histogram hu moments haralick texture and histogram of oriented gradients hog .for traditional methods we experimented with naive bayes linear discriminant analysis k nearest neighbors random forest and support vector machine model .we found that feature engineering significantly improved the performance of traditional models .introduction	before training these models we did image features extraction by finding global feature descriptors of every image which includes color histogram hu moments haralick texture and histogram of oriented gradients hog 
0	109405	9405	we found that feature engineering significantly improved the performance of traditional models .before training these models we did image features extraction by finding global feature descriptors of every image which includes color histogram hu moments haralick texture and histogram of oriented gradients hog .we also noticed that for some model certain com for deep learning method we used a pre trained network densenet 169 of imagenet architecture as the baseline network referred as tl cnn .introduction	we found that feature engineering significantly improved the performance of traditional models 
1	109406	9406	we also noticed that for some model certain com for deep learning method we used a pre trained network densenet 169 of imagenet architecture as the baseline network referred as tl cnn .we found that feature engineering significantly improved the performance of traditional models .we then designed a simple cnn model consisting of 4 convolutional layers and 4 maxpooling layers referred as sim cnn without using transfer learning approach .introduction	we also noticed that for some model certain com for deep learning method we used a pre trained network densenet 169 of imagenet architecture as the baseline network referred as tl cnn 
1	109407	9407	we then designed a simple cnn model consisting of 4 convolutional layers and 4 maxpooling layers referred as sim cnn without using transfer learning approach .we also noticed that for some model certain com for deep learning method we used a pre trained network densenet 169 of imagenet architecture as the baseline network referred as tl cnn .we observed that the simple train from scratch cnn model worked better than the pre trained cnn model .introduction	we then designed a simple cnn model consisting of 4 convolutional layers and 4 maxpooling layers referred as sim cnn without using transfer learning approach 
1	109408	9408	we observed that the simple train from scratch cnn model worked better than the pre trained cnn model .we then designed a simple cnn model consisting of 4 convolutional layers and 4 maxpooling layers referred as sim cnn without using transfer learning approach .last sentence.introduction	we observed that the simple train from scratch cnn model worked better than the pre trained cnn model 
0	109409	9409	recently machine learning and artificial intelligence have attracted increasing attention and achieved great success in different areas including computer vision when narrow down to the problem of ship detection there are no research papers studying this problem since it is a recent kaggle challenge problem .first sentence.however there are several attempts made by some kaggle users .related works	recently machine learning and artificial intelligence have attracted increasing attention and achieved great success in different areas including computer vision when narrow down to the problem of ship detection there are no research papers studying this problem since it is a recent kaggle challenge problem 
0	109410	9410	however there are several attempts made by some kaggle users .recently machine learning and artificial intelligence have attracted increasing attention and achieved great success in different areas including computer vision when narrow down to the problem of ship detection there are no research papers studying this problem since it is a recent kaggle challenge problem .for example kevin mader.related works	however there are several attempts made by some kaggle users 
0	109411	9411	for example kevin mader.however there are several attempts made by some kaggle users .last sentence.related works	for example kevin mader
1	109412	9412	we obtained a public dataset provided on the airbus ship detection challenge website.first sentence.last sentence.data description	we obtained a public dataset provided on the airbus ship detection challenge website
1	109413	9413	first of all we re sized the original image to the size of 256 256 3 then we applied different data preprocessing techniques for traditional machine learning algorithms and deep learning algorithms .first sentence.last sentence.data preprocessing	first of all we re sized the original image to the size of 256 256 3 then we applied different data preprocessing techniques for traditional machine learning algorithms and deep learning algorithms 
1	109414	9414	we used hand engineering features extraction methods gogul09 2017 to obtain three different global features for traditional ml algorithms .first sentence.the images were converted to grayscale for hu and ha and to hsv color space for his before extraction as shown in hu moments hu features are used to captured the general shape information .feature extraction for traditional machine learning method	we used hand engineering features extraction methods gogul09 2017 to obtain three different global features for traditional ml algorithms 
1	109415	9415	the images were converted to grayscale for hu and ha and to hsv color space for his before extraction as shown in hu moments hu features are used to captured the general shape information .we used hand engineering features extraction methods gogul09 2017 to obtain three different global features for traditional ml algorithms .hu moment or image moment is a certain particular weighted average moment of the image pixels intensities .feature extraction for traditional machine learning method	the images were converted to grayscale for hu and ha and to hsv color space for his before extraction as shown in hu moments hu features are used to captured the general shape information 
0	109416	9416	hu moment or image moment is a certain particular weighted average moment of the image pixels intensities .the images were converted to grayscale for hu and ha and to hsv color space for his before extraction as shown in hu moments hu features are used to captured the general shape information .simple properties of the image which are found via image moments include area or total intensity centroid and information about its orientation .feature extraction for traditional machine learning method	hu moment or image moment is a certain particular weighted average moment of the image pixels intensities 
1	109417	9417	simple properties of the image which are found via image moments include area or total intensity centroid and information about its orientation .hu moment or image moment is a certain particular weighted average moment of the image pixels intensities .last sentence.feature extraction for traditional machine learning method	simple properties of the image which are found via image moments include area or total intensity centroid and information about its orientation 
1	109418	9418	to enhance the robustness of our cnn model for all the images in the training set we implemented data augmentation method such as rotation shifting adjusting brightness shearing intensity zooming and flipping .first sentence.data augmentation can improve the models ability to generalize and correctly label images with some sort of distortion which can be regarded as adding noise to the data to reduce variance .data augmentation for cnn model	to enhance the robustness of our cnn model for all the images in the training set we implemented data augmentation method such as rotation shifting adjusting brightness shearing intensity zooming and flipping 
1	109419	9419	data augmentation can improve the models ability to generalize and correctly label images with some sort of distortion which can be regarded as adding noise to the data to reduce variance .to enhance the robustness of our cnn model for all the images in the training set we implemented data augmentation method such as rotation shifting adjusting brightness shearing intensity zooming and flipping .last sentence.data augmentation for cnn model	data augmentation can improve the models ability to generalize and correctly label images with some sort of distortion which can be regarded as adding noise to the data to reduce variance 
1	109420	9420	to classify whether an image contains ships or not several standard machine learning algorithms and deep learning algorithms were implemented .first sentence.we compared different approaches to evaluate how different model performed for this specific task .methods	to classify whether an image contains ships or not several standard machine learning algorithms and deep learning algorithms were implemented 
0	109421	9421	we compared different approaches to evaluate how different model performed for this specific task .to classify whether an image contains ships or not several standard machine learning algorithms and deep learning algorithms were implemented .for all the algorithms the feature vector is x x 1 .methods	we compared different approaches to evaluate how different model performed for this specific task 
1	109422	9422	the algorithm finds a linear combination of features that characterizes and separates two classes .first sentence.it assumes that the conditional probability density functions p x y 0 and p x y 0 are both normally distributed with mean and co variance parameters 0 0 and 1 1 .linear discriminant analysis lda 	the algorithm finds a linear combination of features that characterizes and separates two classes 
0	109423	9423	it assumes that the conditional probability density functions p x y 0 and p x y 0 are both normally distributed with mean and co variance parameters 0 0 and 1 1 .the algorithm finds a linear combination of features that characterizes and separates two classes .lda makes simplifying assumption that the co variances are identical 0 1 and the variances have full rank .linear discriminant analysis lda 	it assumes that the conditional probability density functions p x y 0 and p x y 0 are both normally distributed with mean and co variance parameters 0 0 and 1 1 
1	109424	9424	lda makes simplifying assumption that the co variances are identical 0 1 and the variances have full rank .it assumes that the conditional probability density functions p x y 0 and p x y 0 are both normally distributed with mean and co variance parameters 0 0 and 1 1 .after training on data to estimate mean and co variance bayes theorem is applied to predict the probabilities .linear discriminant analysis lda 	lda makes simplifying assumption that the co variances are identical 0 1 and the variances have full rank 
0	109425	9425	after training on data to estimate mean and co variance bayes theorem is applied to predict the probabilities .lda makes simplifying assumption that the co variances are identical 0 1 and the variances have full rank .last sentence.linear discriminant analysis lda 	after training on data to estimate mean and co variance bayes theorem is applied to predict the probabilities 
1	109426	9426	the algorithm classifies an object by a majority vote of its neighbors with the object being assigned to the class that is most common among its 5 nearest neighbors .first sentence.the distance metric uses standard euclidean distance as.k nearest neighbors knn 	the algorithm classifies an object by a majority vote of its neighbors with the object being assigned to the class that is most common among its 5 nearest neighbors 
0	109427	9427	the distance metric uses standard euclidean distance as.the algorithm classifies an object by a majority vote of its neighbors with the object being assigned to the class that is most common among its 5 nearest neighbors .last sentence.k nearest neighbors knn 	the distance metric uses standard euclidean distance as
0	109428	9428	naive bayes is a conditional probability model .first sentence.to determine whether there is a ship in the image given a feature vector x x 1 .naive bayes nb 	naive bayes is a conditional probability model 
0	109429	9429	using .x n .with the naive conditional independent assumption the joint model can be expressedthe algorithm is an ensemble learning method .naive bayes nb 	using 
1	109430	9430	with the naive conditional independent assumption the joint model can be expressedthe algorithm is an ensemble learning method .using .bootstrap samples are selected from the training data and then the model learns classification trees using only some subset of the features at random instead of examining all possible feature splits .naive bayes nb 	with the naive conditional independent assumption the joint model can be expressedthe algorithm is an ensemble learning method 
1	109431	9431	bootstrap samples are selected from the training data and then the model learns classification trees using only some subset of the features at random instead of examining all possible feature splits .with the naive conditional independent assumption the joint model can be expressedthe algorithm is an ensemble learning method .after training prediction is made by taking the majority vote of the learned classification trees .naive bayes nb 	bootstrap samples are selected from the training data and then the model learns classification trees using only some subset of the features at random instead of examining all possible feature splits 
1	109432	9432	after training prediction is made by taking the majority vote of the learned classification trees .bootstrap samples are selected from the training data and then the model learns classification trees using only some subset of the features at random instead of examining all possible feature splits .the depth of tree is limited by 5 and the number of trees is 10 .naive bayes nb 	after training prediction is made by taking the majority vote of the learned classification trees 
0	109433	9433	the depth of tree is limited by 5 and the number of trees is 10 .after training prediction is made by taking the majority vote of the learned classification trees .last sentence.naive bayes nb 	the depth of tree is limited by 5 and the number of trees is 10 
1	109434	9434	the algorithm finds the maximum margin between different classes by determining the weights and bias of the separating hyperplane .first sentence.the soft margin svm classifier minimizes the loss asthe fit time complexity is more than quadratic with the number of samples .support vector machine svm 	the algorithm finds the maximum margin between different classes by determining the weights and bias of the separating hyperplane 
1	109435	9435	the soft margin svm classifier minimizes the loss asthe fit time complexity is more than quadratic with the number of samples .the algorithm finds the maximum margin between different classes by determining the weights and bias of the separating hyperplane .last sentence.support vector machine svm 	the soft margin svm classifier minimizes the loss asthe fit time complexity is more than quadratic with the number of samples 
0	109436	9436	convolutional neural network cnn which is prevailing in the area of computer vision is proved to be extremely powerful in learning effective feature representations from a large number of data .first sentence.it is capable of extracting the underlying structure features of the data which produce better representation than hand crafted features since the learned features adapt better to the tasks at hand .convolutional neural network cnn 	convolutional neural network cnn which is prevailing in the area of computer vision is proved to be extremely powerful in learning effective feature representations from a large number of data 
1	109437	9437	it is capable of extracting the underlying structure features of the data which produce better representation than hand crafted features since the learned features adapt better to the tasks at hand .convolutional neural network cnn which is prevailing in the area of computer vision is proved to be extremely powerful in learning effective feature representations from a large number of data .in our project we experimented two different cnn models .convolutional neural network cnn 	it is capable of extracting the underlying structure features of the data which produce better representation than hand crafted features since the learned features adapt better to the tasks at hand 
0	109438	9438	in our project we experimented two different cnn models .it is capable of extracting the underlying structure features of the data which produce better representation than hand crafted features since the learned features adapt better to the tasks at hand .last sentence.convolutional neural network cnn 	in our project we experimented two different cnn models 
1	109439	9439	the motivation of using a transfer learning technique is because cnns are very good feature extractors this means that you can extract useful attributes from an already trained cnn with its trained weights .first sentence.hence a cnn with pretrained network can provide a reasonable baseline result as mentioned in section 2 we reproduced the so called transfer learning cnn tl cnn as our baseline model .transfer learning cnn tl cnn 	the motivation of using a transfer learning technique is because cnns are very good feature extractors this means that you can extract useful attributes from an already trained cnn with its trained weights 
0	109440	9440	hence a cnn with pretrained network can provide a reasonable baseline result as mentioned in section 2 we reproduced the so called transfer learning cnn tl cnn as our baseline model .the motivation of using a transfer learning technique is because cnns are very good feature extractors this means that you can extract useful attributes from an already trained cnn with its trained weights .we transferred the dense169 .transfer learning cnn tl cnn 	hence a cnn with pretrained network can provide a reasonable baseline result as mentioned in section 2 we reproduced the so called transfer learning cnn tl cnn as our baseline model 
0	109441	9441	we transferred the dense169 .hence a cnn with pretrained network can provide a reasonable baseline result as mentioned in section 2 we reproduced the so called transfer learning cnn tl cnn as our baseline model .last sentence.transfer learning cnn tl cnn 	we transferred the dense169 
1	109442	9442	instead of using a pre trained cnn model we decided to construct a simple cnn model named sim cnn with a few layers and trained it from scratch .first sentence.one might hope that this could improve the performance of tl cnn since its cnn weights were trained specifically by our dataset the.simple cnn sim cnn 	instead of using a pre trained cnn model we decided to construct a simple cnn model named sim cnn with a few layers and trained it from scratch 
1	109443	9443	one might hope that this could improve the performance of tl cnn since its cnn weights were trained specifically by our dataset the.instead of using a pre trained cnn model we decided to construct a simple cnn model named sim cnn with a few layers and trained it from scratch .last sentence.simple cnn sim cnn 	one might hope that this could improve the performance of tl cnn since its cnn weights were trained specifically by our dataset the
1	109444	9444	for the traditional machine learning algorithms discussed in section we applied sklearn package in python to realize each algorithms for the deep learning approach the two cnn models were trained in microsoft azure cloud with gpus .first sentence.cross entropy loss was used .training	for the traditional machine learning algorithms discussed in section we applied sklearn package in python to realize each algorithms for the deep learning approach the two cnn models were trained in microsoft azure cloud with gpus 
0	109445	9445	cross entropy loss was used .for the traditional machine learning algorithms discussed in section we applied sklearn package in python to realize each algorithms for the deep learning approach the two cnn models were trained in microsoft azure cloud with gpus .we trained the model for 30 epochs using mini batch size 64 with batch normalization .training	cross entropy loss was used 
1	109446	9446	we trained the model for 30 epochs using mini batch size 64 with batch normalization .cross entropy loss was used .we applied adam optimizer as well as decaying learning rate to facilitate model training .training	we trained the model for 30 epochs using mini batch size 64 with batch normalization 
1	109447	9447	we applied adam optimizer as well as decaying learning rate to facilitate model training .we trained the model for 30 epochs using mini batch size 64 with batch normalization .the weights of model would not be updated if the loss of development set did not improve after training an epoch .training	we applied adam optimizer as well as decaying learning rate to facilitate model training 
0	109448	9448	the weights of model would not be updated if the loss of development set did not improve after training an epoch .we applied adam optimizer as well as decaying learning rate to facilitate model training .comparing performance with and without feature engineering we found that feature engineering significantly improved performance in general .training	the weights of model would not be updated if the loss of development set did not improve after training an epoch 
0	109449	9449	comparing performance with and without feature engineering we found that feature engineering significantly improved performance in general .the weights of model would not be updated if the loss of development set did not improve after training an epoch .instead of directly training on image pixels information the information extracted from feature engineering amplified the signal of whether an image containing ships especially for nb and svm approaches .training	comparing performance with and without feature engineering we found that feature engineering significantly improved performance in general 
1	109450	9450	instead of directly training on image pixels information the information extracted from feature engineering amplified the signal of whether an image containing ships especially for nb and svm approaches .comparing performance with and without feature engineering we found that feature engineering significantly improved performance in general .however since 0 84 of the images in test set are labeled as no ships among those traditional machine learning method only lda and rf outperformed the accuracy of blindly guesting no ships .training	instead of directly training on image pixels information the information extracted from feature engineering amplified the signal of whether an image containing ships especially for nb and svm approaches 
1	109451	9451	however since 0 84 of the images in test set are labeled as no ships among those traditional machine learning method only lda and rf outperformed the accuracy of blindly guesting no ships .instead of directly training on image pixels information the information extracted from feature engineering amplified the signal of whether an image containing ships especially for nb and svm approaches .in addition we learned that some algorithms give much better performance when working with only certain combination of features .training	however since 0 84 of the images in test set are labeled as no ships among those traditional machine learning method only lda and rf outperformed the accuracy of blindly guesting no ships 
1	109452	9452	in addition we learned that some algorithms give much better performance when working with only certain combination of features .however since 0 84 of the images in test set are labeled as no ships among those traditional machine learning method only lda and rf outperformed the accuracy of blindly guesting no ships .it is suggested that haralick textures information of image is of great importance for nb method improving from 0 42 to 0 75 .training	in addition we learned that some algorithms give much better performance when working with only certain combination of features 
0	109453	9453	it is suggested that haralick textures information of image is of great importance for nb method improving from 0 42 to 0 75 .in addition we learned that some algorithms give much better performance when working with only certain combination of features .last sentence.training	it is suggested that haralick textures information of image is of great importance for nb method improving from 0 42 to 0 75 
1	109454	9454	we applied 10 fold cross validation to each machine learning algorithms and create the corresponding box plot in.first sentence.last sentence.test accuracy	we applied 10 fold cross validation to each machine learning algorithms and create the corresponding box plot in
0	109455	9455	consider our dataset being imbalanced instead of using a threshold of 0 5 we wanted to find one that would take imbalance into consideration as suggested by.first sentence.last sentence.threshold scanning	consider our dataset being imbalanced instead of using a threshold of 0 5 we wanted to find one that would take imbalance into consideration as suggested by
0	109456	9456	among all traditional methods random forest gave the best result 0 93 accuracy with feature engineering .first sentence.as for the cnn model our train from scratch sim cnn model outperforms the baseline tl cnn model based on pre trained densenet 169 network .conclusoin future works	among all traditional methods random forest gave the best result 0 93 accuracy with feature engineering 
1	109457	9457	as for the cnn model our train from scratch sim cnn model outperforms the baseline tl cnn model based on pre trained densenet 169 network .among all traditional methods random forest gave the best result 0 93 accuracy with feature engineering .in the future for traditional machine learning algorithms we plan to improve feature engineering by extracting global features along with local features such as sift surf or dense which could be used along with bag of visual words bovw technique .conclusoin future works	as for the cnn model our train from scratch sim cnn model outperforms the baseline tl cnn model based on pre trained densenet 169 network 
1	109458	9458	in the future for traditional machine learning algorithms we plan to improve feature engineering by extracting global features along with local features such as sift surf or dense which could be used along with bag of visual words bovw technique .as for the cnn model our train from scratch sim cnn model outperforms the baseline tl cnn model based on pre trained densenet 169 network .for deep learning algorithms to achieve better performance we will try implementing different networks e g deeper network to train the classifier .conclusoin future works	in the future for traditional machine learning algorithms we plan to improve feature engineering by extracting global features along with local features such as sift surf or dense which could be used along with bag of visual words bovw technique 
1	109459	9459	for deep learning algorithms to achieve better performance we will try implementing different networks e g deeper network to train the classifier .in the future for traditional machine learning algorithms we plan to improve feature engineering by extracting global features along with local features such as sift surf or dense which could be used along with bag of visual words bovw technique .last but not least it is more challenging but also more interesting to try applying segmentation technique to identify the locations of all ships in a image .conclusoin future works	for deep learning algorithms to achieve better performance we will try implementing different networks e g deeper network to train the classifier 
1	109460	9460	last but not least it is more challenging but also more interesting to try applying segmentation technique to identify the locations of all ships in a image .for deep learning algorithms to achieve better performance we will try implementing different networks e g deeper network to train the classifier .last sentence.conclusoin future works	last but not least it is more challenging but also more interesting to try applying segmentation technique to identify the locations of all ships in a image 
1	109461	9461	all three members of this team work together and contribute equally to this project in data prepossessing algorithm designing model designing model training and report writing please follow project code or https github com cs229shipdetection cs229project airbus ship detection for the project code .first sentence.last sentence.team contribution project code	all three members of this team work together and contribute equally to this project in data prepossessing algorithm designing model designing model training and report writing please follow project code or https github com cs229shipdetection cs229project airbus ship detection for the project code 
0	109462	9462	in the modern era an enormous amount of digital pictures from personal photos to medical images is produced and stored every day .first sentence.it is more and more common to have thousands of photos sitting in our smart phones however what comes with the convenience of recording unforgettable moments is the pain of searching for a specific picture or frame .introduction	in the modern era an enormous amount of digital pictures from personal photos to medical images is produced and stored every day 
0	109463	9463	it is more and more common to have thousands of photos sitting in our smart phones however what comes with the convenience of recording unforgettable moments is the pain of searching for a specific picture or frame .in the modern era an enormous amount of digital pictures from personal photos to medical images is produced and stored every day .how nice it would be to be able to find the desired image just by typing one or few words to describe it .introduction	it is more and more common to have thousands of photos sitting in our smart phones however what comes with the convenience of recording unforgettable moments is the pain of searching for a specific picture or frame 
0	109464	9464	how nice it would be to be able to find the desired image just by typing one or few words to describe it .it is more and more common to have thousands of photos sitting in our smart phones however what comes with the convenience of recording unforgettable moments is the pain of searching for a specific picture or frame .in this context automated captionimage retrieval is becoming an increasingly attracting feature comparable to text search in this project we consider the task of content based image retrieval and propose effective neural network based solutions for that .introduction	how nice it would be to be able to find the desired image just by typing one or few words to describe it 
0	109465	9465	in this context automated captionimage retrieval is becoming an increasingly attracting feature comparable to text search in this project we consider the task of content based image retrieval and propose effective neural network based solutions for that .how nice it would be to be able to find the desired image just by typing one or few words to describe it .specifically the input to our algorithm is a collection of raw images in which the user would like to search and a query sentence meant to describe the desired image .introduction	in this context automated captionimage retrieval is becoming an increasingly attracting feature comparable to text search in this project we consider the task of content based image retrieval and propose effective neural network based solutions for that 
0	109466	9466	specifically the input to our algorithm is a collection of raw images in which the user would like to search and a query sentence meant to describe the desired image .in this context automated captionimage retrieval is becoming an increasingly attracting feature comparable to text search in this project we consider the task of content based image retrieval and propose effective neural network based solutions for that .the output of the algorithm would be a list of top images that we think are relevant to the query sentence .introduction	specifically the input to our algorithm is a collection of raw images in which the user would like to search and a query sentence meant to describe the desired image 
0	109467	9467	the output of the algorithm would be a list of top images that we think are relevant to the query sentence .specifically the input to our algorithm is a collection of raw images in which the user would like to search and a query sentence meant to describe the desired image .in particular we train a recurrent neural network to obtain a representation of the sentence that will be properly aligned with the corresponding image features in a shared highdimensional space .introduction	the output of the algorithm would be a list of top images that we think are relevant to the query sentence 
0	109468	9468	in particular we train a recurrent neural network to obtain a representation of the sentence that will be properly aligned with the corresponding image features in a shared highdimensional space .the output of the algorithm would be a list of top images that we think are relevant to the query sentence .the images are found based on nearest neighborhood search in that shared space the paper is organized as follows first we briefly summarize the most relevant work related to our task then we describe the dataset employed for training and the features of our problem .introduction	in particular we train a recurrent neural network to obtain a representation of the sentence that will be properly aligned with the corresponding image features in a shared highdimensional space 
1	109469	9469	the images are found based on nearest neighborhood search in that shared space the paper is organized as follows first we briefly summarize the most relevant work related to our task then we describe the dataset employed for training and the features of our problem .in particular we train a recurrent neural network to obtain a representation of the sentence that will be properly aligned with the corresponding image features in a shared highdimensional space .subsequently we introduce our models namely a multi response linear regression model and a deep learning method inspired by.introduction	the images are found based on nearest neighborhood search in that shared space the paper is organized as follows first we briefly summarize the most relevant work related to our task then we describe the dataset employed for training and the features of our problem 
0	109470	9470	subsequently we introduce our models namely a multi response linear regression model and a deep learning method inspired by.the images are found based on nearest neighborhood search in that shared space the paper is organized as follows first we briefly summarize the most relevant work related to our task then we describe the dataset employed for training and the features of our problem .last sentence.introduction	subsequently we introduce our models namely a multi response linear regression model and a deep learning method inspired by
0	109471	9471	under the umbrella of multimodal machine learning caption image retrieval has received much attention in recent years .first sentence.one main class of strategies is to learn separate representations for each of the modalities and then coordinate them via some constraint .related work	under the umbrella of multimodal machine learning caption image retrieval has received much attention in recent years 
0	109472	9472	one main class of strategies is to learn separate representations for each of the modalities and then coordinate them via some constraint .under the umbrella of multimodal machine learning caption image retrieval has received much attention in recent years .a natural choice of constraint is similarity either in the sense of cosine distance a different class of constraint considered in more recently there is another line of work that tries to improve retrieval performance with the use of generative models .related work	one main class of strategies is to learn separate representations for each of the modalities and then coordinate them via some constraint 
0	109473	9473	a natural choice of constraint is similarity either in the sense of cosine distance a different class of constraint considered in more recently there is another line of work that tries to improve retrieval performance with the use of generative models .one main class of strategies is to learn separate representations for each of the modalities and then coordinate them via some constraint .in under the hood of most state of the art models the choice of pretrained features embeddings plays an important role .related work	a natural choice of constraint is similarity either in the sense of cosine distance a different class of constraint considered in more recently there is another line of work that tries to improve retrieval performance with the use of generative models 
0	109474	9474	in under the hood of most state of the art models the choice of pretrained features embeddings plays an important role .a natural choice of constraint is similarity either in the sense of cosine distance a different class of constraint considered in more recently there is another line of work that tries to improve retrieval performance with the use of generative models .we use vgg 19.related work	in under the hood of most state of the art models the choice of pretrained features embeddings plays an important role 
0	109475	9475	we use vgg 19.in under the hood of most state of the art models the choice of pretrained features embeddings plays an important role .last sentence.related work	we use vgg 19
1	109476	9476	we train our models using the microsoft coco dataset three teddy bears laying in bed under the covers a group of stuffed animals sitting next to each other in bed a white beige and brown baby bear under a beige white comforter a trio of teddy bears bundled up on a bed three stuffed animals lay in a bed cuddled together to represent images a common choice is to use a pretrained image model as a feature extractor and use the last layer of the forward pass as the representation .first sentence.in the present work we employ the f c7 features of the 19 layer vgg network.dataset and features	we train our models using the microsoft coco dataset three teddy bears laying in bed under the covers a group of stuffed animals sitting next to each other in bed a white beige and brown baby bear under a beige white comforter a trio of teddy bears bundled up on a bed three stuffed animals lay in a bed cuddled together to represent images a common choice is to use a pretrained image model as a feature extractor and use the last layer of the forward pass as the representation 
0	109477	9477	in the present work we employ the f c7 features of the 19 layer vgg network.we train our models using the microsoft coco dataset three teddy bears laying in bed under the covers a group of stuffed animals sitting next to each other in bed a white beige and brown baby bear under a beige white comforter a trio of teddy bears bundled up on a bed three stuffed animals lay in a bed cuddled together to represent images a common choice is to use a pretrained image model as a feature extractor and use the last layer of the forward pass as the representation .last sentence.dataset and features	in the present work we employ the f c7 features of the 19 layer vgg network
0	109478	9478	in this section we describe the methods that we use for this task .first sentence.they include a traditional supervised method based on multiple response linear regression and methods based on neural networks .methods	in this section we describe the methods that we use for this task 
0	109479	9479	they include a traditional supervised method based on multiple response linear regression and methods based on neural networks .in this section we describe the methods that we use for this task .last sentence.methods	they include a traditional supervised method based on multiple response linear regression and methods based on neural networks 
0	109480	9480	in multimodal machine learning a common approach is coordinating the representations of different modalities so that certain similarity among their respective spaces are enforced .first sentence.our task involves texts and images .baseline method	in multimodal machine learning a common approach is coordinating the representations of different modalities so that certain similarity among their respective spaces are enforced 
0	109481	9481	our task involves texts and images .in multimodal machine learning a common approach is coordinating the representations of different modalities so that certain similarity among their respective spaces are enforced .to represent the captions we simply average the glove vectors relative to the words of the sentence though more sophisticated methods exist .baseline method	our task involves texts and images 
0	109482	9482	to represent the captions we simply average the glove vectors relative to the words of the sentence though more sophisticated methods exist .our task involves texts and images .let f glove be the sentence features and f vgg be the image features coming from the vgg network .baseline method	to represent the captions we simply average the glove vectors relative to the words of the sentence though more sophisticated methods exist 
0	109483	9483	let f glove be the sentence features and f vgg be the image features coming from the vgg network .to represent the captions we simply average the glove vectors relative to the words of the sentence though more sophisticated methods exist .in order to encourage similarity between these two different types of representation we would like to find a weight matrix such that this is known as multi response linear regression .baseline method	let f glove be the sentence features and f vgg be the image features coming from the vgg network 
0	109484	9484	in order to encourage similarity between these two different types of representation we would like to find a weight matrix such that this is known as multi response linear regression .let f glove be the sentence features and f vgg be the image features coming from the vgg network .as a generalization of linear regression it has closed form solution or can be solve by stochastic gradient descent when we have a large dataset .baseline method	in order to encourage similarity between these two different types of representation we would like to find a weight matrix such that this is known as multi response linear regression 
0	109485	9485	as a generalization of linear regression it has closed form solution or can be solve by stochastic gradient descent when we have a large dataset .in order to encourage similarity between these two different types of representation we would like to find a weight matrix such that this is known as multi response linear regression .at test time when we are given a caption c t we compute the caption feature vector f glove c t and find the image s closest to that .baseline method	as a generalization of linear regression it has closed form solution or can be solve by stochastic gradient descent when we have a large dataset 
0	109486	9486	at test time when we are given a caption c t we compute the caption feature vector f glove c t and find the image s closest to that .as a generalization of linear regression it has closed form solution or can be solve by stochastic gradient descent when we have a large dataset .last sentence.baseline method	at test time when we are given a caption c t we compute the caption feature vector f glove c t and find the image s closest to that 
0	109487	9487	our method is inspired by where f i and f c are embedding functions for images and captions respectively .first sentence.there is a negative sign since we would like larger s to indicate more similarity .multimodal neural network methods	our method is inspired by where f i and f c are embedding functions for images and captions respectively 
0	109488	9488	there is a negative sign since we would like larger s to indicate more similarity .our method is inspired by where f i and f c are embedding functions for images and captions respectively .for the purpose of contrasting correct and incorrect matches we introduce negative examples that comes handy in the same training batch .multimodal neural network methods	there is a negative sign since we would like larger s to indicate more similarity 
0	109489	9489	for the purpose of contrasting correct and incorrect matches we introduce negative examples that comes handy in the same training batch .there is a negative sign since we would like larger s to indicate more similarity .the cost can thus be expressed aswhere c i is the true caption image pair c and i refer to incorrect captions and images for the selected pair .multimodal neural network methods	for the purpose of contrasting correct and incorrect matches we introduce negative examples that comes handy in the same training batch 
0	109490	9490	the cost can thus be expressed aswhere c i is the true caption image pair c and i refer to incorrect captions and images for the selected pair .for the purpose of contrasting correct and incorrect matches we introduce negative examples that comes handy in the same training batch .therefore the cost function enforces positive i e .multimodal neural network methods	the cost can thus be expressed aswhere c i is the true caption image pair c and i refer to incorrect captions and images for the selected pair 
0	109491	9491	therefore the cost function enforces positive i e .the cost can thus be expressed aswhere c i is the true caption image pair c and i refer to incorrect captions and images for the selected pair .correct examples to have zeropenalty and negative i e .multimodal neural network methods	therefore the cost function enforces positive i e 
0	109492	9492	correct examples to have zeropenalty and negative i e .therefore the cost function enforces positive i e .incorrect examples to have penalty greater than a margin feature extraction for both modalities is similar to the baseline .multimodal neural network methods	correct examples to have zeropenalty and negative i e 
0	109493	9493	incorrect examples to have penalty greater than a margin feature extraction for both modalities is similar to the baseline .correct examples to have zeropenalty and negative i e .the embedding function f i is obtained by opportunely weighting the outcome before the output layer of the vgg network and f c takes the last state of a recurrent neural network rnn with gated recurrent unit gru activation functions where w i is a n 4096 matrix of weights to be trained and n is the number of features of the embedding space .multimodal neural network methods	incorrect examples to have penalty greater than a margin feature extraction for both modalities is similar to the baseline 
0	109494	9494	the embedding function f i is obtained by opportunely weighting the outcome before the output layer of the vgg network and f c takes the last state of a recurrent neural network rnn with gated recurrent unit gru activation functions where w i is a n 4096 matrix of weights to be trained and n is the number of features of the embedding space .incorrect examples to have penalty greater than a margin feature extraction for both modalities is similar to the baseline .the embedding function f c is now the outcome of a recurrent neural network rnn with gated recurrent unit gru activation functions figure 2 recurrent neural network to process captions we observe that in in addition we explore the usage of a different modules in the rnn namely long short term memory lstm and different rnn architectures such as stacked bidirectional rnns .multimodal neural network methods	the embedding function f i is obtained by opportunely weighting the outcome before the output layer of the vgg network and f c takes the last state of a recurrent neural network rnn with gated recurrent unit gru activation functions where w i is a n 4096 matrix of weights to be trained and n is the number of features of the embedding space 
1	109495	9495	the embedding function f c is now the outcome of a recurrent neural network rnn with gated recurrent unit gru activation functions figure 2 recurrent neural network to process captions we observe that in in addition we explore the usage of a different modules in the rnn namely long short term memory lstm and different rnn architectures such as stacked bidirectional rnns .the embedding function f i is obtained by opportunely weighting the outcome before the output layer of the vgg network and f c takes the last state of a recurrent neural network rnn with gated recurrent unit gru activation functions where w i is a n 4096 matrix of weights to be trained and n is the number of features of the embedding space .last sentence.multimodal neural network methods	the embedding function f c is now the outcome of a recurrent neural network rnn with gated recurrent unit gru activation functions figure 2 recurrent neural network to process captions we observe that in in addition we explore the usage of a different modules in the rnn namely long short term memory lstm and different rnn architectures such as stacked bidirectional rnns 
0	109496	9496	metric the metric we use in this project is recall k r k .first sentence.given a list of predicted rankings r i 1 i m for m images based on their corresponding input captions we definewe should notice that this metric also depends on the size of the image database .experiments	metric the metric we use in this project is recall k r k 
0	109497	9497	given a list of predicted rankings r i 1 i m for m images based on their corresponding input captions we definewe should notice that this metric also depends on the size of the image database .metric the metric we use in this project is recall k r k .for example searching over a one million image database is clearly harder than a one hundred database .experiments	given a list of predicted rankings r i 1 i m for m images based on their corresponding input captions we definewe should notice that this metric also depends on the size of the image database 
0	109498	9498	for example searching over a one million image database is clearly harder than a one hundred database .given a list of predicted rankings r i 1 i m for m images based on their corresponding input captions we definewe should notice that this metric also depends on the size of the image database .in this project we focus on the size of 1k images .experiments	for example searching over a one million image database is clearly harder than a one hundred database 
0	109499	9499	in this project we focus on the size of 1k images .for example searching over a one million image database is clearly harder than a one hundred database .in addition we will also look at some conditional metrics such as the length of the caption to better understand the results hyperparameters we started with a set of hyperparameters suggested in results in we also compare the baseline method and the neural network solution through some real examples .experiments	in this project we focus on the size of 1k images 
