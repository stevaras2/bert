Label	id1	id2	clipping	sentence
1	145	9501	as a baseline measure we incorporated a standard logistic regression model with a loss function of the form shown in 1 to make the model more robust against overfitting l2 regularization was incorporated with the c parameter inversely related to the strength of regularization in order to pick the optimal value for c the model was independently trained over a range of c values 	as a baseline measure we incorporated a standard logistic regression model with a loss function of the form shown in 1 
1	145	9502	as a baseline measure we incorporated a standard logistic regression model with a loss function of the form shown in 1 to make the model more robust against overfitting l2 regularization was incorporated with the c parameter inversely related to the strength of regularization in order to pick the optimal value for c the model was independently trained over a range of c values 	to make the model more robust against overfitting l2 regularization was incorporated with the c parameter inversely related to the strength of regularization 
1	145	9503	as a baseline measure we incorporated a standard logistic regression model with a loss function of the form shown in 1 to make the model more robust against overfitting l2 regularization was incorporated with the c parameter inversely related to the strength of regularization in order to pick the optimal value for c the model was independently trained over a range of c values 	in order to pick the optimal value for c the model was independently trained over a range of c values 
1	145	9504	as a baseline measure we incorporated a standard logistic regression model with a loss function of the form shown in 1 to make the model more robust against overfitting l2 regularization was incorporated with the c parameter inversely related to the strength of regularization in order to pick the optimal value for c the model was independently trained over a range of c values 	the c value that produced the highest accuracy on the validation set was selected and tested on the test set for each training phase 5 fold cross validation was incorporated in order to reduce the variance in a trained model 
1	145	9505	as a baseline measure we incorporated a standard logistic regression model with a loss function of the form shown in 1 to make the model more robust against overfitting l2 regularization was incorporated with the c parameter inversely related to the strength of regularization in order to pick the optimal value for c the model was independently trained over a range of c values 	stochastic average gradient sag descent was selected as the solver to use for training as it generally provides fast convergence for large feature sets such as ours
1	145	9506	we wanted to also create a non linear classifier in the hopes that it can outperform the linear logistic regression model so a svm was a natural choice as it is fairly easy to implement with few parameters to tune we created our svm s by solving the following primal problem and the decision function is defined as through training an svm model can create multiple hyperplanes to split the training set into its labeled categories this is done through the use of kernels that transform the input data into a higher dimension so that the data can then be linearly separated 	we wanted to also create a non linear classifier in the hopes that it can outperform the linear logistic regression model so a svm was a natural choice as it is fairly easy to implement with few parameters to tune 
1	145	9507	we wanted to also create a non linear classifier in the hopes that it can outperform the linear logistic regression model so a svm was a natural choice as it is fairly easy to implement with few parameters to tune we created our svm s by solving the following primal problem and the decision function is defined as through training an svm model can create multiple hyperplanes to split the training set into its labeled categories this is done through the use of kernels that transform the input data into a higher dimension so that the data can then be linearly separated 	we created our svm s by solving the following primal problem and the decision function is defined as through training an svm model can create multiple hyperplanes to split the training set into its labeled categories 
1	145	9508	we wanted to also create a non linear classifier in the hopes that it can outperform the linear logistic regression model so a svm was a natural choice as it is fairly easy to implement with few parameters to tune we created our svm s by solving the following primal problem and the decision function is defined as through training an svm model can create multiple hyperplanes to split the training set into its labeled categories this is done through the use of kernels that transform the input data into a higher dimension so that the data can then be linearly separated 	this is done through the use of kernels that transform the input data into a higher dimension so that the data can then be linearly separated 
1	145	9509	we wanted to also create a non linear classifier in the hopes that it can outperform the linear logistic regression model so a svm was a natural choice as it is fairly easy to implement with few parameters to tune we created our svm s by solving the following primal problem and the decision function is defined as through training an svm model can create multiple hyperplanes to split the training set into its labeled categories this is done through the use of kernels that transform the input data into a higher dimension so that the data can then be linearly separated 	part of the advantage for svm s is that only a fraction represented by n in eqn 3 of the original training set has to be retrained for creating the hyperplane during predictions 
1	145	9510	we wanted to also create a non linear classifier in the hopes that it can outperform the linear logistic regression model so a svm was a natural choice as it is fairly easy to implement with few parameters to tune we created our svm s by solving the following primal problem and the decision function is defined as through training an svm model can create multiple hyperplanes to split the training set into its labeled categories this is done through the use of kernels that transform the input data into a higher dimension so that the data can then be linearly separated 	another advantage is that various kernel functions can easily be tested and selected for the one that best fits a particular application 
1	145	9511	we wanted to also create a non linear classifier in the hopes that it can outperform the linear logistic regression model so a svm was a natural choice as it is fairly easy to implement with few parameters to tune we created our svm s by solving the following primal problem and the decision function is defined as through training an svm model can create multiple hyperplanes to split the training set into its labeled categories this is done through the use of kernels that transform the input data into a higher dimension so that the data can then be linearly separated 	thus to select the optimal kernel function along with the c parameter specifying the softmargin size a grid search was performed over three standard kernel functions polynomial rbf and linear with a range of c values for each 
1	145	9512	based on our literature review we believed that deep learning techniques should work well for this classification problem we therefore implemented a multilayer perceptron architecture for multi class classification a multilayer perceptron architecture is a fully connected feedforward neural network with one input layer one or more hidden layers and one output layer 	based on our literature review we believed that deep learning techniques should work well for this classification problem 
1	145	9513	based on our literature review we believed that deep learning techniques should work well for this classification problem we therefore implemented a multilayer perceptron architecture for multi class classification a multilayer perceptron architecture is a fully connected feedforward neural network with one input layer one or more hidden layers and one output layer 	we therefore implemented a multilayer perceptron architecture for multi class classification 
1	145	9514	based on our literature review we believed that deep learning techniques should work well for this classification problem we therefore implemented a multilayer perceptron architecture for multi class classification a multilayer perceptron architecture is a fully connected feedforward neural network with one input layer one or more hidden layers and one output layer 	a multilayer perceptron architecture is a fully connected feedforward neural network with one input layer one or more hidden layers and one output layer 
0	145	9515	based on our literature review we believed that deep learning techniques should work well for this classification problem we therefore implemented a multilayer perceptron architecture for multi class classification a multilayer perceptron architecture is a fully connected feedforward neural network with one input layer one or more hidden layers and one output layer 	formally the mlp can be considered a function f r n r k where n is the number of input features and k is the number of classes 
0	145	9516	based on our literature review we believed that deep learning techniques should work well for this classification problem we therefore implemented a multilayer perceptron architecture for multi class classification a multilayer perceptron architecture is a fully connected feedforward neural network with one input layer one or more hidden layers and one output layer 	each hidden layer can be formalized as f r a r b where a is the input size and b is the output size 
1	145	9517	based on our literature review we believed that deep learning techniques should work well for this classification problem we therefore implemented a multilayer perceptron architecture for multi class classification a multilayer perceptron architecture is a fully connected feedforward neural network with one input layer one or more hidden layers and one output layer 	in matrix notation it would be where x is the input vector w c is the weight matrix associated with layer c and b c is the bias vector associated with layer c and a c is the activation function associated with layer c we use softmax as the final activation so each prediction is size kx1 where k is the number of classes 
0	145	9518	based on our literature review we believed that deep learning techniques should work well for this classification problem we therefore implemented a multilayer perceptron architecture for multi class classification a multilayer perceptron architecture is a fully connected feedforward neural network with one input layer one or more hidden layers and one output layer 	the classifier predicts a score for each class instead of simply producing a single class label 
0	145	9519	based on our literature review we believed that deep learning techniques should work well for this classification problem we therefore implemented a multilayer perceptron architecture for multi class classification a multilayer perceptron architecture is a fully connected feedforward neural network with one input layer one or more hidden layers and one output layer 	this can give a sense of how close we are to the correct label 
1	145	9520	based on our literature review we believed that deep learning techniques should work well for this classification problem we therefore implemented a multilayer perceptron architecture for multi class classification a multilayer perceptron architecture is a fully connected feedforward neural network with one input layer one or more hidden layers and one output layer 	we converted the true labels to use one hot encoding that is we took an mx1 array and made it mxk where m is the number of datapoints 
1	145	9521	based on our literature review we believed that deep learning techniques should work well for this classification problem we therefore implemented a multilayer perceptron architecture for multi class classification a multilayer perceptron architecture is a fully connected feedforward neural network with one input layer one or more hidden layers and one output layer 	we initially found that making the model deeper produced worse results but increasing the number of neurons per layer improved the accuracy 
0	145	9522	based on our literature review we believed that deep learning techniques should work well for this classification problem we therefore implemented a multilayer perceptron architecture for multi class classification a multilayer perceptron architecture is a fully connected feedforward neural network with one input layer one or more hidden layers and one output layer 	our final architecture is layer1 relu layer2 relu layer3 softmax 
1	145	9523	based on our literature review we believed that deep learning techniques should work well for this classification problem we therefore implemented a multilayer perceptron architecture for multi class classification a multilayer perceptron architecture is a fully connected feedforward neural network with one input layer one or more hidden layers and one output layer 	the layers have weight sizes of layer1 n 512 layer2 512 k 
1	145	9524	based on our literature review we believed that deep learning techniques should work well for this classification problem we therefore implemented a multilayer perceptron architecture for multi class classification a multilayer perceptron architecture is a fully connected feedforward neural network with one input layer one or more hidden layers and one output layer 	in order to generalize better we apply dropout for each hidden layer this helps combat overfitting by suppressing each node with 50 probability 
0	145	9525	based on our literature review we believed that deep learning techniques should work well for this classification problem we therefore implemented a multilayer perceptron architecture for multi class classification a multilayer perceptron architecture is a fully connected feedforward neural network with one input layer one or more hidden layers and one output layer 	we tried different gradient descent rules with the best result coming from the sgd optimizer 
1	145	9526	based on our literature review we believed that deep learning techniques should work well for this classification problem we therefore implemented a multilayer perceptron architecture for multi class classification a multilayer perceptron architecture is a fully connected feedforward neural network with one input layer one or more hidden layers and one output layer 	to evaluate loss we use categorical cross entropy which is as follows we use this in conjunction with softmax activation for the final layer which gives us a probability or confidence for each prediction 
1	145	9527	based on our literature review we believed that deep learning techniques should work well for this classification problem we therefore implemented a multilayer perceptron architecture for multi class classification a multilayer perceptron architecture is a fully connected feedforward neural network with one input layer one or more hidden layers and one output layer 	softmax output for the i th element is as follows this is advantageous because we have multiple classes instead of simple binary classification 
0	145	9528	based on our literature review we believed that deep learning techniques should work well for this classification problem we therefore implemented a multilayer perceptron architecture for multi class classification a multilayer perceptron architecture is a fully connected feedforward neural network with one input layer one or more hidden layers and one output layer 	we want the loss to give a sense of the degree of error for each category instead of a simple yes or no 
0	145	9529	based on our literature review we believed that deep learning techniques should work well for this classification problem we therefore implemented a multilayer perceptron architecture for multi class classification a multilayer perceptron architecture is a fully connected feedforward neural network with one input layer one or more hidden layers and one output layer 	for example assume we have 3 classes and the first class is the correct label for this time step 
0	145	9530	based on our literature review we believed that deep learning techniques should work well for this classification problem we therefore implemented a multilayer perceptron architecture for multi class classification a multilayer perceptron architecture is a fully connected feedforward neural network with one input layer one or more hidden layers and one output layer 	consider two example outputs 0 98 0 01 0 01 and 0 51 0 48 0 01 
1	145	9531	based on our literature review we believed that deep learning techniques should work well for this classification problem we therefore implemented a multilayer perceptron architecture for multi class classification a multilayer perceptron architecture is a fully connected feedforward neural network with one input layer one or more hidden layers and one output layer 	the first output is clearly superior to the second because it has a higher confidence for the correct class however both will predict the first class 
1	145	9532	based on our literature review we believed that deep learning techniques should work well for this classification problem we therefore implemented a multilayer perceptron architecture for multi class classification a multilayer perceptron architecture is a fully connected feedforward neural network with one input layer one or more hidden layers and one output layer 	if we use something like simple error rate for our loss we are not able to capture the confidence of our predictions 
1	145	9533	based on our literature review we believed that deep learning techniques should work well for this classification problem we therefore implemented a multilayer perceptron architecture for multi class classification a multilayer perceptron architecture is a fully connected feedforward neural network with one input layer one or more hidden layers and one output layer 	using softmax activation with cross entropy loss helps us capture this difference 
0	145	9534	based on our literature review we believed that deep learning techniques should work well for this classification problem we therefore implemented a multilayer perceptron architecture for multi class classification a multilayer perceptron architecture is a fully connected feedforward neural network with one input layer one or more hidden layers and one output layer 	our final architecture is shown in
0	145	9535	decision trees are a useful method for multi class classification for nonlinear feature sets decision trees perform greedy splits on the each feature of the data at a specific threshold in order to choose a split a decision tree seeks to maximize the difference between the loss of the parent node and the sum of the losses of the child nodes 	decision trees are a useful method for multi class classification for nonlinear feature sets 
0	145	9536	decision trees are a useful method for multi class classification for nonlinear feature sets decision trees perform greedy splits on the each feature of the data at a specific threshold in order to choose a split a decision tree seeks to maximize the difference between the loss of the parent node and the sum of the losses of the child nodes 	decision trees perform greedy splits on the each feature of the data at a specific threshold 
1	145	9537	decision trees are a useful method for multi class classification for nonlinear feature sets decision trees perform greedy splits on the each feature of the data at a specific threshold in order to choose a split a decision tree seeks to maximize the difference between the loss of the parent node and the sum of the losses of the child nodes 	in order to choose a split a decision tree seeks to maximize the difference between the loss of the parent node and the sum of the losses of the child nodes 
1	145	9538	decision trees are a useful method for multi class classification for nonlinear feature sets decision trees perform greedy splits on the each feature of the data at a specific threshold in order to choose a split a decision tree seeks to maximize the difference between the loss of the parent node and the sum of the losses of the child nodes 	the specific loss function we used was the gini loss shown below where p mk is the proportion of examples in class k present in region r m and q m is the proportion of examples in r m from tree t with t different r m regions there are multiple methods of regularizing or preventing overfitting for decision trees including setting a minimum size of leaf terminal nodes and setting a maximum tree depth setting a maximum number of nodes 3 random forest another ensemble method for decision trees for the purpose of improving prediction accuracy is random forest 
1	145	9539	decision trees are a useful method for multi class classification for nonlinear feature sets decision trees perform greedy splits on the each feature of the data at a specific threshold in order to choose a split a decision tree seeks to maximize the difference between the loss of the parent node and the sum of the losses of the child nodes 	random forest is a form of bagging bootstrap aggregation which involves sampling with replacement from the original population for the purpose of reducing variance at the expense of an increase in bias increased computational cost and decreased interpretability of the trees 
1	145	9540	decision trees are a useful method for multi class classification for nonlinear feature sets decision trees perform greedy splits on the each feature of the data at a specific threshold in order to choose a split a decision tree seeks to maximize the difference between the loss of the parent node and the sum of the losses of the child nodes 	for a random forest a large number of decision trees are generated and the bias is further reduced by decorrelating the trees by only considering a subset of the total number of features at each split in the decision tree
1	145	9541	as discussed previously we were interested in classification performance on both the full set of features and performance when using a reduced set of features we tried several feature combinations and discovered that we could get decent performance when using just the hand imu plus heart rate sensor the hand imu performed better than any individual imu 	as discussed previously we were interested in classification performance on both the full set of features and performance when using a reduced set of features 
1	145	9542	as discussed previously we were interested in classification performance on both the full set of features and performance when using a reduced set of features we tried several feature combinations and discovered that we could get decent performance when using just the hand imu plus heart rate sensor the hand imu performed better than any individual imu 	we tried several feature combinations and discovered that we could get decent performance when using just the hand imu plus heart rate sensor 
0	145	9543	as discussed previously we were interested in classification performance on both the full set of features and performance when using a reduced set of features we tried several feature combinations and discovered that we could get decent performance when using just the hand imu plus heart rate sensor the hand imu performed better than any individual imu 	the hand imu performed better than any individual imu 
0	145	9544	as discussed previously we were interested in classification performance on both the full set of features and performance when using a reduced set of features we tried several feature combinations and discovered that we could get decent performance when using just the hand imu plus heart rate sensor the hand imu performed better than any individual imu 	this is a positive result as we are particularly interested in applications where a user is holding a phone or wearing a smart watch 
1	145	9545	as discussed previously we were interested in classification performance on both the full set of features and performance when using a reduced set of features we tried several feature combinations and discovered that we could get decent performance when using just the hand imu plus heart rate sensor the hand imu performed better than any individual imu 	we will refer to the hand imu plus heart rate data as the reduced or limited feature set 
0	145	9546	as discussed previously we were interested in classification performance on both the full set of features and performance when using a reduced set of features we tried several feature combinations and discovered that we could get decent performance when using just the hand imu plus heart rate sensor the hand imu performed better than any individual imu 	our primary evaluation metric for all models was classification accuracy 
0	145	9547	as discussed previously we were interested in classification performance on both the full set of features and performance when using a reduced set of features we tried several feature combinations and discovered that we could get decent performance when using just the hand imu plus heart rate sensor the hand imu performed better than any individual imu 	this is simply the count of correctly classified data points divided by the count of classifications attempted 
0	145	9548	as discussed previously we were interested in classification performance on both the full set of features and performance when using a reduced set of features we tried several feature combinations and discovered that we could get decent performance when using just the hand imu plus heart rate sensor the hand imu performed better than any individual imu 	it is as follows where y is our set of predicted labels and y is the set of true labels 
0	145	9549	as discussed previously we were interested in classification performance on both the full set of features and performance when using a reduced set of features we tried several feature combinations and discovered that we could get decent performance when using just the hand imu plus heart rate sensor the hand imu performed better than any individual imu 	we used various loss functions as described in the subsection for each technique 
0	145	9550	as discussed previously we were interested in classification performance on both the full set of features and performance when using a reduced set of features we tried several feature combinations and discovered that we could get decent performance when using just the hand imu plus heart rate sensor the hand imu performed better than any individual imu 	the full results are shown below in
0	145	9551	through 5 fold cross validation in	through 5 fold cross validation in
1	145	9552	in order to tune the maximum depth hyperparameter of the decision tree we used scikit learn s validation curve function to perform 5 fold cross validation the training and crossvalidation curves for the limited feature set is shown in the left side of 4 below as a function of maximum tree depth and similar curves were produced for the full feature set it is typical in practice to use the one standard error rule when using cross validation to choose the simplest model whose accuracy or error is within one standard deviation of the best performing model 2 boosted decision trees for boosted decision trees we used the default learning rate and manually tuned the maximum depth of the base decision tree estimators and the number of trees since the base estimator of boosting should be a weak learner and since the strong learner from the results from ordinary decision trees had a maximum depth of 15 we decided to limit our weak learners to having a maximum depth of less than or equal to 10 	in order to tune the maximum depth hyperparameter of the decision tree we used scikit learn s validation curve function to perform 5 fold cross validation 
1	145	9553	in order to tune the maximum depth hyperparameter of the decision tree we used scikit learn s validation curve function to perform 5 fold cross validation the training and crossvalidation curves for the limited feature set is shown in the left side of 4 below as a function of maximum tree depth and similar curves were produced for the full feature set it is typical in practice to use the one standard error rule when using cross validation to choose the simplest model whose accuracy or error is within one standard deviation of the best performing model 2 boosted decision trees for boosted decision trees we used the default learning rate and manually tuned the maximum depth of the base decision tree estimators and the number of trees since the base estimator of boosting should be a weak learner and since the strong learner from the results from ordinary decision trees had a maximum depth of 15 we decided to limit our weak learners to having a maximum depth of less than or equal to 10 	the training and crossvalidation curves for the limited feature set is shown in the left side of 4 below as a function of maximum tree depth and similar curves were produced for the full feature set it is typical in practice to use the one standard error rule when using cross validation to choose the simplest model whose accuracy or error is within one standard deviation of the best performing model 2 boosted decision trees for boosted decision trees we used the default learning rate and manually tuned the maximum depth of the base decision tree estimators and the number of trees 
1	145	9554	in order to tune the maximum depth hyperparameter of the decision tree we used scikit learn s validation curve function to perform 5 fold cross validation the training and crossvalidation curves for the limited feature set is shown in the left side of 4 below as a function of maximum tree depth and similar curves were produced for the full feature set it is typical in practice to use the one standard error rule when using cross validation to choose the simplest model whose accuracy or error is within one standard deviation of the best performing model 2 boosted decision trees for boosted decision trees we used the default learning rate and manually tuned the maximum depth of the base decision tree estimators and the number of trees since the base estimator of boosting should be a weak learner and since the strong learner from the results from ordinary decision trees had a maximum depth of 15 we decided to limit our weak learners to having a maximum depth of less than or equal to 10 	since the base estimator of boosting should be a weak learner and since the strong learner from the results from ordinary decision trees had a maximum depth of 15 we decided to limit our weak learners to having a maximum depth of less than or equal to 10 
1	145	9555	in order to tune the maximum depth hyperparameter of the decision tree we used scikit learn s validation curve function to perform 5 fold cross validation the training and crossvalidation curves for the limited feature set is shown in the left side of 4 below as a function of maximum tree depth and similar curves were produced for the full feature set it is typical in practice to use the one standard error rule when using cross validation to choose the simplest model whose accuracy or error is within one standard deviation of the best performing model 2 boosted decision trees for boosted decision trees we used the default learning rate and manually tuned the maximum depth of the base decision tree estimators and the number of trees since the base estimator of boosting should be a weak learner and since the strong learner from the results from ordinary decision trees had a maximum depth of 15 we decided to limit our weak learners to having a maximum depth of less than or equal to 10 	for both the limited and full feature sets we created tables that presented the training and validation accuracy values for different combinations of maximum depth of the weak learners from 1 10 and number of trees 50 100 250 or 500 
1	145	9556	in order to tune the maximum depth hyperparameter of the decision tree we used scikit learn s validation curve function to perform 5 fold cross validation the training and crossvalidation curves for the limited feature set is shown in the left side of 4 below as a function of maximum tree depth and similar curves were produced for the full feature set it is typical in practice to use the one standard error rule when using cross validation to choose the simplest model whose accuracy or error is within one standard deviation of the best performing model 2 boosted decision trees for boosted decision trees we used the default learning rate and manually tuned the maximum depth of the base decision tree estimators and the number of trees since the base estimator of boosting should be a weak learner and since the strong learner from the results from ordinary decision trees had a maximum depth of 15 we decided to limit our weak learners to having a maximum depth of less than or equal to 10 	using the one standard error rule from the optimal validation performance we chose 500 trees of maxdepth 10 for the limited feature set and 250 trees of maxdepth 9 
1	145	9557	in order to tune the maximum depth hyperparameter of the decision tree we used scikit learn s validation curve function to perform 5 fold cross validation the training and crossvalidation curves for the limited feature set is shown in the left side of 4 below as a function of maximum tree depth and similar curves were produced for the full feature set it is typical in practice to use the one standard error rule when using cross validation to choose the simplest model whose accuracy or error is within one standard deviation of the best performing model 2 boosted decision trees for boosted decision trees we used the default learning rate and manually tuned the maximum depth of the base decision tree estimators and the number of trees since the base estimator of boosting should be a weak learner and since the strong learner from the results from ordinary decision trees had a maximum depth of 15 we decided to limit our weak learners to having a maximum depth of less than or equal to 10 	using these models the limited feature set scored an accuracy of 0 940 on the test set and the full feature set scored an accuracy of 0 985 on the test set 
0	145	9558	in order to tune the maximum depth hyperparameter of the decision tree we used scikit learn s validation curve function to perform 5 fold cross validation the training and crossvalidation curves for the limited feature set is shown in the left side of 4 below as a function of maximum tree depth and similar curves were produced for the full feature set it is typical in practice to use the one standard error rule when using cross validation to choose the simplest model whose accuracy or error is within one standard deviation of the best performing model 2 boosted decision trees for boosted decision trees we used the default learning rate and manually tuned the maximum depth of the base decision tree estimators and the number of trees since the base estimator of boosting should be a weak learner and since the strong learner from the results from ordinary decision trees had a maximum depth of 15 we decided to limit our weak learners to having a maximum depth of less than or equal to 10 	a confusion matrix for the limited feature set is shown below in 5
1	145	9559	we decided to include 100 trees in our random forest model because 100 was a well performing trade off of time and accuracy for random forests increasing the number of trees will only serve to decrease the variance and will not increase the likelihood of overfitting we used the default option of only considering a random subset of the square root of the total number of features for each split in order to tune the maximum depth for each decision tree in the random forest we again used scikit learn s validation curve function to perform 5 fold cross validation 	we decided to include 100 trees in our random forest model because 100 was a well performing trade off of time and accuracy for random forests increasing the number of trees will only serve to decrease the variance and will not increase the likelihood of overfitting 
1	145	9560	we decided to include 100 trees in our random forest model because 100 was a well performing trade off of time and accuracy for random forests increasing the number of trees will only serve to decrease the variance and will not increase the likelihood of overfitting we used the default option of only considering a random subset of the square root of the total number of features for each split in order to tune the maximum depth for each decision tree in the random forest we again used scikit learn s validation curve function to perform 5 fold cross validation 	we used the default option of only considering a random subset of the square root of the total number of features for each split 
1	145	9561	we decided to include 100 trees in our random forest model because 100 was a well performing trade off of time and accuracy for random forests increasing the number of trees will only serve to decrease the variance and will not increase the likelihood of overfitting we used the default option of only considering a random subset of the square root of the total number of features for each split in order to tune the maximum depth for each decision tree in the random forest we again used scikit learn s validation curve function to perform 5 fold cross validation 	in order to tune the maximum depth for each decision tree in the random forest we again used scikit learn s validation curve function to perform 5 fold cross validation 
1	145	9562	we decided to include 100 trees in our random forest model because 100 was a well performing trade off of time and accuracy for random forests increasing the number of trees will only serve to decrease the variance and will not increase the likelihood of overfitting we used the default option of only considering a random subset of the square root of the total number of features for each split in order to tune the maximum depth for each decision tree in the random forest we again used scikit learn s validation curve function to perform 5 fold cross validation 	the training and cross validation curves for the limited feature set and the full feature set as a function of maximum tree depth in the random forest were created using a similar approach to the cross validation for ordinary decision trees 
1	145	9563	we decided to include 100 trees in our random forest model because 100 was a well performing trade off of time and accuracy for random forests increasing the number of trees will only serve to decrease the variance and will not increase the likelihood of overfitting we used the default option of only considering a random subset of the square root of the total number of features for each split in order to tune the maximum depth for each decision tree in the random forest we again used scikit learn s validation curve function to perform 5 fold cross validation 	using the one standard error rule for the validation accuracy the results suggest that for both the limited and full feature sets a maximum depth of 20 should be used 
0	145	9564	we decided to include 100 trees in our random forest model because 100 was a well performing trade off of time and accuracy for random forests increasing the number of trees will only serve to decrease the variance and will not increase the likelihood of overfitting we used the default option of only considering a random subset of the square root of the total number of features for each split in order to tune the maximum depth for each decision tree in the random forest we again used scikit learn s validation curve function to perform 5 fold cross validation 	using a maximum depth of 20 the limited featureset achieved a test accuracy of 0 937 and the full featureset achieved a test accuracy of 0 980 
0	145	9565	we decided to include 100 trees in our random forest model because 100 was a well performing trade off of time and accuracy for random forests increasing the number of trees will only serve to decrease the variance and will not increase the likelihood of overfitting we used the default option of only considering a random subset of the square root of the total number of features for each split in order to tune the maximum depth for each decision tree in the random forest we again used scikit learn s validation curve function to perform 5 fold cross validation 	from the confusion matrix for this model the most frequent misclassifications were between vacuum cleaning and ironing ascending and descending stairs vacuum cleaning and ascending descending stairs 
1	145	9566	we decided to include 100 trees in our random forest model because 100 was a well performing trade off of time and accuracy for random forests increasing the number of trees will only serve to decrease the variance and will not increase the likelihood of overfitting we used the default option of only considering a random subset of the square root of the total number of features for each split in order to tune the maximum depth for each decision tree in the random forest we again used scikit learn s validation curve function to perform 5 fold cross validation 	these are exactly the common misclassifications found in boosting and are expected because of the relative similarity in hand motions and heart rate between these activities 
1	145	9567	the multilayer perceptron neural network was able to achieve high classification accuracy when trained on the entire dataset it consistently achieved training accuracies greater than 98 and test accuracies greater than 95 the best model we trained produced a test accuracy of 98 1 	the multilayer perceptron neural network was able to achieve high classification accuracy 
1	145	9568	the multilayer perceptron neural network was able to achieve high classification accuracy when trained on the entire dataset it consistently achieved training accuracies greater than 98 and test accuracies greater than 95 the best model we trained produced a test accuracy of 98 1 	when trained on the entire dataset it consistently achieved training accuracies greater than 98 and test accuracies greater than 95 
0	145	9569	the multilayer perceptron neural network was able to achieve high classification accuracy when trained on the entire dataset it consistently achieved training accuracies greater than 98 and test accuracies greater than 95 the best model we trained produced a test accuracy of 98 1 	the best model we trained produced a test accuracy of 98 1 
1	145	9570	the multilayer perceptron neural network was able to achieve high classification accuracy when trained on the entire dataset it consistently achieved training accuracies greater than 98 and test accuracies greater than 95 the best model we trained produced a test accuracy of 98 1 	when trained on the reduced feature set consisting of only the hand imu and the heart rate sensor it achieved 81 4 test accuracy 
0	145	9571	the multilayer perceptron neural network was able to achieve high classification accuracy when trained on the entire dataset it consistently achieved training accuracies greater than 98 and test accuracies greater than 95 the best model we trained produced a test accuracy of 98 1 	we noted a trend in accuracy vs hidden layer size 
1	145	9572	the multilayer perceptron neural network was able to achieve high classification accuracy when trained on the entire dataset it consistently achieved training accuracies greater than 98 and test accuracies greater than 95 the best model we trained produced a test accuracy of 98 1 	increasing the size of each layer number of neurons improved performance while increasing the depth number of hidden layers degraded performance 
1	145	9573	the multilayer perceptron neural network was able to achieve high classification accuracy when trained on the entire dataset it consistently achieved training accuracies greater than 98 and test accuracies greater than 95 the best model we trained produced a test accuracy of 98 1 	we did not notice a significant difference in performance when using relu vs other activation functions however we did find that our model converged faster when using softmax for the final activation function in conjunction with categorical cross entropy loss 
0	145	9574	the multilayer perceptron neural network was able to achieve high classification accuracy when trained on the entire dataset it consistently achieved training accuracies greater than 98 and test accuracies greater than 95 the best model we trained produced a test accuracy of 98 1 	as expected reducing the dropout rate tended to improve training accuracy but reducing it too much caused a degradation in test accuracy 
1	145	9575	the multilayer perceptron neural network was able to achieve high classification accuracy when trained on the entire dataset it consistently achieved training accuracies greater than 98 and test accuracies greater than 95 the best model we trained produced a test accuracy of 98 1 	in this project we were limited in both time and compute and we believe we can improve accuracy given more of both 
0	145	9576	the multilayer perceptron neural network was able to achieve high classification accuracy when trained on the entire dataset it consistently achieved training accuracies greater than 98 and test accuracies greater than 95 the best model we trained produced a test accuracy of 98 1 	we can improve performance by training for more epochs 
0	145	9577	the multilayer perceptron neural network was able to achieve high classification accuracy when trained on the entire dataset it consistently achieved training accuracies greater than 98 and test accuracies greater than 95 the best model we trained produced a test accuracy of 98 1 	loss continued to decrease at the end of our training indicating performance was still improving when training finished 
1	145	9578	the multilayer perceptron neural network was able to achieve high classification accuracy when trained on the entire dataset it consistently achieved training accuracies greater than 98 and test accuracies greater than 95 the best model we trained produced a test accuracy of 98 1 	we could also further increase the number of neurons per hidden layer at the cost of a larger model with slower training time 
0	145	9579	unsurprisingly logistic regression performed the worst having the advantage of extremely low memory usage and speed for predictions it can still be a viable method in lowcompute devices like microcontrollers svm s on the other hand provide great results in accuracy but may not be viable solutions for microcontrollers because of the large number of support vectors required to store and do operations on for predicting 	unsurprisingly logistic regression performed the worst 
1	145	9580	unsurprisingly logistic regression performed the worst having the advantage of extremely low memory usage and speed for predictions it can still be a viable method in lowcompute devices like microcontrollers svm s on the other hand provide great results in accuracy but may not be viable solutions for microcontrollers because of the large number of support vectors required to store and do operations on for predicting 	having the advantage of extremely low memory usage and speed for predictions it can still be a viable method in lowcompute devices like microcontrollers 
1	145	9581	unsurprisingly logistic regression performed the worst having the advantage of extremely low memory usage and speed for predictions it can still be a viable method in lowcompute devices like microcontrollers svm s on the other hand provide great results in accuracy but may not be viable solutions for microcontrollers because of the large number of support vectors required to store and do operations on for predicting 	svm s on the other hand provide great results in accuracy but may not be viable solutions for microcontrollers because of the large number of support vectors required to store and do operations on for predicting 
1	145	9582	unsurprisingly logistic regression performed the worst having the advantage of extremely low memory usage and speed for predictions it can still be a viable method in lowcompute devices like microcontrollers svm s on the other hand provide great results in accuracy but may not be viable solutions for microcontrollers because of the large number of support vectors required to store and do operations on for predicting 	for decision tree methods as expected ensembling methods improved test performance over ordinary decision trees for both the full and limited feature sets 
1	145	9583	unsurprisingly logistic regression performed the worst having the advantage of extremely low memory usage and speed for predictions it can still be a viable method in lowcompute devices like microcontrollers svm s on the other hand provide great results in accuracy but may not be viable solutions for microcontrollers because of the large number of support vectors required to store and do operations on for predicting 	boosted decision trees performed slightly better than random forest on both feature sets which is promising because it is generally less computationally intensive and thus is a good candidate for a model to actually deploy in a smart device 
0	145	9584	unsurprisingly logistic regression performed the worst having the advantage of extremely low memory usage and speed for predictions it can still be a viable method in lowcompute devices like microcontrollers svm s on the other hand provide great results in accuracy but may not be viable solutions for microcontrollers because of the large number of support vectors required to store and do operations on for predicting 	we would also be interested in exploring different types of deep learning architectures 
1	145	9585	unsurprisingly logistic regression performed the worst having the advantage of extremely low memory usage and speed for predictions it can still be a viable method in lowcompute devices like microcontrollers svm s on the other hand provide great results in accuracy but may not be viable solutions for microcontrollers because of the large number of support vectors required to store and do operations on for predicting 	we considered using rnn s recurrent neural networks but our feature set had a relatively large number of features per time step and the activities did not involve more than a few actions so it was not necessary to take history into account when classifying a single time step 
1	145	9586	unsurprisingly logistic regression performed the worst having the advantage of extremely low memory usage and speed for predictions it can still be a viable method in lowcompute devices like microcontrollers svm s on the other hand provide great results in accuracy but may not be viable solutions for microcontrollers because of the large number of support vectors required to store and do operations on for predicting 	as such simple feed forward neural nets were sufficient for this problem 
1	145	9587	unsurprisingly logistic regression performed the worst having the advantage of extremely low memory usage and speed for predictions it can still be a viable method in lowcompute devices like microcontrollers svm s on the other hand provide great results in accuracy but may not be viable solutions for microcontrollers because of the large number of support vectors required to store and do operations on for predicting 	however we would like to explore cnn s convolutional neural networks which could potentially give similar or improved performance while using substantially less memory 
1	145	9588	unsurprisingly logistic regression performed the worst having the advantage of extremely low memory usage and speed for predictions it can still be a viable method in lowcompute devices like microcontrollers svm s on the other hand provide great results in accuracy but may not be viable solutions for microcontrollers because of the large number of support vectors required to store and do operations on for predicting 	in general the limited feature set performed only slightly worse than the full feature set on all of the methods which is a promising result for actual deployment in smart devices 
1	145	9589	unsurprisingly logistic regression performed the worst having the advantage of extremely low memory usage and speed for predictions it can still be a viable method in lowcompute devices like microcontrollers svm s on the other hand provide great results in accuracy but may not be viable solutions for microcontrollers because of the large number of support vectors required to store and do operations on for predicting 	in the future we would like to test these models using real imu s 
1	145	9590	unsurprisingly logistic regression performed the worst having the advantage of extremely low memory usage and speed for predictions it can still be a viable method in lowcompute devices like microcontrollers svm s on the other hand provide great results in accuracy but may not be viable solutions for microcontrollers because of the large number of support vectors required to store and do operations on for predicting 	in particular we would want to see if a low compute embedded device could perform classifications with neural nets or svm s in real time in addition to computationally cheaper methods such as decision trees 
0	145	9591	all code used in this project can be found at https github com aristosathens human a ctivity c lassif ier contributions aristos zach and navjot all contributed equally to this project aristos focused on deep learning navjot focused on logistic regression and svm and zach focused on trees ordinary decision trees boosting and random forests all three members worked on data preprocessing analysis and writing this report 	all code used in this project can be found at https github com aristosathens human a ctivity c lassif ier contributions aristos zach and navjot all contributed equally to this project 
1	145	9592	all code used in this project can be found at https github com aristosathens human a ctivity c lassif ier contributions aristos zach and navjot all contributed equally to this project aristos focused on deep learning navjot focused on logistic regression and svm and zach focused on trees ordinary decision trees boosting and random forests all three members worked on data preprocessing analysis and writing this report 	aristos focused on deep learning navjot focused on logistic regression and svm and zach focused on trees ordinary decision trees boosting and random forests 
0	145	9593	all code used in this project can be found at https github com aristosathens human a ctivity c lassif ier contributions aristos zach and navjot all contributed equally to this project aristos focused on deep learning navjot focused on logistic regression and svm and zach focused on trees ordinary decision trees boosting and random forests all three members worked on data preprocessing analysis and writing this report 	all three members worked on data preprocessing analysis and writing this report 
1	146	9594	in this paper i describe a real time image processing pipeline for fruit fly videos that can detect the position oriention sex and for male flies wing angles the machine learning algorithms used include a decision tree linear and logistic regressions and principal component analysis the histogram of oriented gradients 2 descriptor is used as well to generate features 	in this paper i describe a real time image processing pipeline for fruit fly videos that can detect the position oriention sex and for male flies wing angles 
0	146	9595	in this paper i describe a real time image processing pipeline for fruit fly videos that can detect the position oriention sex and for male flies wing angles the machine learning algorithms used include a decision tree linear and logistic regressions and principal component analysis the histogram of oriented gradients 2 descriptor is used as well to generate features 	the machine learning algorithms used include a decision tree linear and logistic regressions and principal component analysis 
0	146	9596	in this paper i describe a real time image processing pipeline for fruit fly videos that can detect the position oriention sex and for male flies wing angles the machine learning algorithms used include a decision tree linear and logistic regressions and principal component analysis the histogram of oriented gradients 2 descriptor is used as well to generate features 	the histogram of oriented gradients 2 descriptor is used as well to generate features 
0	146	9597	in this paper i describe a real time image processing pipeline for fruit fly videos that can detect the position oriention sex and for male flies wing angles the machine learning algorithms used include a decision tree linear and logistic regressions and principal component analysis the histogram of oriented gradients 2 descriptor is used as well to generate features 	ultimately i achieved a processing throughput of 84 frames per second on 1530x1530 grayscale frames without gpu acceleration and demonstrated high accuracy across several metrics 
0	146	9598	fruit flies are often used in neurobiology experiments because they can be genetically modified using well established tools and because it is feasible to do in vivo whole brain imaging of them in addition to other reasons in these experiments it is often desirable to measure the behavior of each fly by recording its position and orientation over time along with its leg and wing positions automated tools for doing this kind of video analysis exist but are usually too slow to run in real time this is problematic for two reasons first real time operation is a prerequisite for running closed loop behavior experiments 	fruit flies are often used in neurobiology experiments because they can be genetically modified using well established tools and because it is feasible to do in vivo whole brain imaging of them in addition to other reasons 
1	146	9599	fruit flies are often used in neurobiology experiments because they can be genetically modified using well established tools and because it is feasible to do in vivo whole brain imaging of them in addition to other reasons in these experiments it is often desirable to measure the behavior of each fly by recording its position and orientation over time along with its leg and wing positions automated tools for doing this kind of video analysis exist but are usually too slow to run in real time this is problematic for two reasons first real time operation is a prerequisite for running closed loop behavior experiments 	in these experiments it is often desirable to measure the behavior of each fly by recording its position and orientation over time along with its leg and wing positions automated tools for doing this kind of video analysis exist but are usually too slow to run in real time 
0	146	9600	fruit flies are often used in neurobiology experiments because they can be genetically modified using well established tools and because it is feasible to do in vivo whole brain imaging of them in addition to other reasons in these experiments it is often desirable to measure the behavior of each fly by recording its position and orientation over time along with its leg and wing positions automated tools for doing this kind of video analysis exist but are usually too slow to run in real time this is problematic for two reasons first real time operation is a prerequisite for running closed loop behavior experiments 	this is problematic for two reasons first real time operation is a prerequisite for running closed loop behavior experiments 
1	146	9601	fruit flies are often used in neurobiology experiments because they can be genetically modified using well established tools and because it is feasible to do in vivo whole brain imaging of them in addition to other reasons in these experiments it is often desirable to measure the behavior of each fly by recording its position and orientation over time along with its leg and wing positions automated tools for doing this kind of video analysis exist but are usually too slow to run in real time this is problematic for two reasons first real time operation is a prerequisite for running closed loop behavior experiments 	second in an experiment where flies are recorded continuously over a long period e g a circadian rhythm study video processing will become the bottleneck for experimental throughput unless it runs in real time or faster to address these issues i sought to develop a tool for the realtime video analysis of fruit flies 
0	146	9602	fruit flies are often used in neurobiology experiments because they can be genetically modified using well established tools and because it is feasible to do in vivo whole brain imaging of them in addition to other reasons in these experiments it is often desirable to measure the behavior of each fly by recording its position and orientation over time along with its leg and wing positions automated tools for doing this kind of video analysis exist but are usually too slow to run in real time this is problematic for two reasons first real time operation is a prerequisite for running closed loop behavior experiments 	in this project i chose to focus on a specific experiment being developed in prof tom clandinin s lab at stanford to study the courtship interaction between one male and one female fly 
0	146	9603	fruit flies are often used in neurobiology experiments because they can be genetically modified using well established tools and because it is feasible to do in vivo whole brain imaging of them in addition to other reasons in these experiments it is often desirable to measure the behavior of each fly by recording its position and orientation over time along with its leg and wing positions automated tools for doing this kind of video analysis exist but are usually too slow to run in real time this is problematic for two reasons first real time operation is a prerequisite for running closed loop behavior experiments 	as a result the input to my algorithm is a grayscale video of the two flies in this paper i ll start off by describing some existing tools for fly video analysis section 2 and will then move on to describe the dataset i worked with section 3 
0	146	9604	fruit flies are often used in neurobiology experiments because they can be genetically modified using well established tools and because it is feasible to do in vivo whole brain imaging of them in addition to other reasons in these experiments it is often desirable to measure the behavior of each fly by recording its position and orientation over time along with its leg and wing positions automated tools for doing this kind of video analysis exist but are usually too slow to run in real time this is problematic for two reasons first real time operation is a prerequisite for running closed loop behavior experiments 	next i ll describe the algorithm i developed which consists of four distinct processing steps using machine learning section 4 
0	146	9605	fruit flies are often used in neurobiology experiments because they can be genetically modified using well established tools and because it is feasible to do in vivo whole brain imaging of them in addition to other reasons in these experiments it is often desirable to measure the behavior of each fly by recording its position and orientation over time along with its leg and wing positions automated tools for doing this kind of video analysis exist but are usually too slow to run in real time this is problematic for two reasons first real time operation is a prerequisite for running closed loop behavior experiments 	finally i ll wrap up the experimental results section 6 and conclusion section 7 
0	146	9606	fruit flies are often used in neurobiology experiments because they can be genetically modified using well established tools and because it is feasible to do in vivo whole brain imaging of them in addition to other reasons in these experiments it is often desirable to measure the behavior of each fly by recording its position and orientation over time along with its leg and wing positions automated tools for doing this kind of video analysis exist but are usually too slow to run in real time this is problematic for two reasons first real time operation is a prerequisite for running closed loop behavior experiments 	the source code and dataset for this project are available on github at https github 
0	146	9607	fruit flies are often used in neurobiology experiments because they can be genetically modified using well established tools and because it is feasible to do in vivo whole brain imaging of them in addition to other reasons in these experiments it is often desirable to measure the behavior of each fly by recording its position and orientation over time along with its leg and wing positions automated tools for doing this kind of video analysis exist but are usually too slow to run in real time this is problematic for two reasons first real time operation is a prerequisite for running closed loop behavior experiments 	com sgherbst cs229 project git 
1	146	9608	the tool considered a gold standard of sorts for automated fruit fly video analysis is called flytracker in another project finally in the past year two different approaches to fly video analysis using deep neural networks were published deeplabcut in this project i sought to combine various aspects of these previous studies on one hand i wanted to develop an algorithm that could run in real time on large frames 1530x1530 without gpu acceleration and i wanted training to be fast to allow for more experimentation hence i needed to further simplify the machine learning models as compared to deeplabcut and leap 	the tool considered a gold standard of sorts for automated fruit fly video analysis is called flytracker in another project finally in the past year two different approaches to fly video analysis using deep neural networks were published deeplabcut in this project i sought to combine various aspects of these previous studies 
0	146	9609	the tool considered a gold standard of sorts for automated fruit fly video analysis is called flytracker in another project finally in the past year two different approaches to fly video analysis using deep neural networks were published deeplabcut in this project i sought to combine various aspects of these previous studies on one hand i wanted to develop an algorithm that could run in real time on large frames 1530x1530 without gpu acceleration and i wanted training to be fast to allow for more experimentation hence i needed to further simplify the machine learning models as compared to deeplabcut and leap 	on one hand i wanted to develop an algorithm that could run in real time on large frames 1530x1530 without gpu acceleration and i wanted training to be fast to allow for more experimentation 
0	146	9610	the tool considered a gold standard of sorts for automated fruit fly video analysis is called flytracker in another project finally in the past year two different approaches to fly video analysis using deep neural networks were published deeplabcut in this project i sought to combine various aspects of these previous studies on one hand i wanted to develop an algorithm that could run in real time on large frames 1530x1530 without gpu acceleration and i wanted training to be fast to allow for more experimentation hence i needed to further simplify the machine learning models as compared to deeplabcut and leap 	hence i needed to further simplify the machine learning models as compared to deeplabcut and leap 
0	146	9611	the tool considered a gold standard of sorts for automated fruit fly video analysis is called flytracker in another project finally in the past year two different approaches to fly video analysis using deep neural networks were published deeplabcut in this project i sought to combine various aspects of these previous studies on one hand i wanted to develop an algorithm that could run in real time on large frames 1530x1530 without gpu acceleration and i wanted training to be fast to allow for more experimentation hence i needed to further simplify the machine learning models as compared to deeplabcut and leap 	but i still wanted to apply supervised learning to take advantage of labeled data departing from the hand crafted image processing rules of flytracker 
0	146	9612	the source data for this project was a 15 minute grayscale video of the interaction between one male fly and one female fly the video was furnished by dr ryan york of prof clandinin s lab as an example of the kind of footage that will be produced by the experimental rig they are developing i did a bit of initial preprocessing to crop the video to a 1530x1530 frame that exactly contained the circular well in which the flies were placed working from the cropped video i then hand annotated 326 frames using labelme additional preprocessing and data augmentation was used throughout the image processing pipeline and these steps will be covered in the next section 	the source data for this project was a 15 minute grayscale video of the interaction between one male fly and one female fly 
0	146	9613	the source data for this project was a 15 minute grayscale video of the interaction between one male fly and one female fly the video was furnished by dr ryan york of prof clandinin s lab as an example of the kind of footage that will be produced by the experimental rig they are developing i did a bit of initial preprocessing to crop the video to a 1530x1530 frame that exactly contained the circular well in which the flies were placed working from the cropped video i then hand annotated 326 frames using labelme additional preprocessing and data augmentation was used throughout the image processing pipeline and these steps will be covered in the next section 	the video was furnished by dr ryan york of prof clandinin s lab as an example of the kind of footage that will be produced by the experimental rig they are developing 
1	146	9614	the source data for this project was a 15 minute grayscale video of the interaction between one male fly and one female fly the video was furnished by dr ryan york of prof clandinin s lab as an example of the kind of footage that will be produced by the experimental rig they are developing i did a bit of initial preprocessing to crop the video to a 1530x1530 frame that exactly contained the circular well in which the flies were placed working from the cropped video i then hand annotated 326 frames using labelme additional preprocessing and data augmentation was used throughout the image processing pipeline and these steps will be covered in the next section 	i did a bit of initial preprocessing to crop the video to a 1530x1530 frame that exactly contained the circular well in which the flies were placed working from the cropped video i then hand annotated 326 frames using labelme additional preprocessing and data augmentation was used throughout the image processing pipeline and these steps will be covered in the next section 
0	146	9615	as shown in the pipeline described below was implemented in python using the packages scikit learn	as shown in the pipeline described below was implemented in python using the packages scikit learn
0	146	9616	if there are two contours with one fly each the next stage of the image processing pipeline determines which is the male fly and which is the female fly this classification is done jointly that is the classifier is a given a list of the two contours and asked whether that list is ordered male female or female male in general this is fairly straightforward since female fruit flies are larger than male fruit flies 	if there are two contours with one fly each the next stage of the image processing pipeline determines which is the male fly and which is the female fly 
0	146	9617	if there are two contours with one fly each the next stage of the image processing pipeline determines which is the male fly and which is the female fly this classification is done jointly that is the classifier is a given a list of the two contours and asked whether that list is ordered male female or female male in general this is fairly straightforward since female fruit flies are larger than male fruit flies 	this classification is done jointly that is the classifier is a given a list of the two contours and asked whether that list is ordered male female or female male 
0	146	9618	if there are two contours with one fly each the next stage of the image processing pipeline determines which is the male fly and which is the female fly this classification is done jointly that is the classifier is a given a list of the two contours and asked whether that list is ordered male female or female male in general this is fairly straightforward since female fruit flies are larger than male fruit flies 	in general this is fairly straightforward since female fruit flies are larger than male fruit flies 
1	146	9619	if there are two contours with one fly each the next stage of the image processing pipeline determines which is the male fly and which is the female fly this classification is done jointly that is the classifier is a given a list of the two contours and asked whether that list is ordered male female or female male in general this is fairly straightforward since female fruit flies are larger than male fruit flies 	but in some cases such as when the flies are climbing along the walls making the distinction can be a bit trickier there are four input features for this pipeline stage namely the area and aspect ratio of both contours the latter determined via image moment analysis in this case a rescaling step is needed before training the logistic regression because contour areas and aspects ratios are of vastly different scales so the above update rule would otherwise perform quite poorly 
0	146	9620	if there are two contours with one fly each the next stage of the image processing pipeline determines which is the male fly and which is the female fly this classification is done jointly that is the classifier is a given a list of the two contours and asked whether that list is ordered male female or female male in general this is fairly straightforward since female fruit flies are larger than male fruit flies 	to further improve the quality of training data augmentation was applied by swapping the order of the two contours and their labels for each example 
1	146	9621	in the third pipeline stage the 0 360 orientation of both flies is determined and this is done in a way that reduces the machine learning task to a binary classification as pre processing the image is first masked to everything expect the body of one fly after which point the orientation of the fly is determined using image	in the third pipeline stage the 0 360 orientation of both flies is determined and this is done in a way that reduces the machine learning task to a binary classification 
0	146	9622	in the third pipeline stage the 0 360 orientation of both flies is determined and this is done in a way that reduces the machine learning task to a binary classification as pre processing the image is first masked to everything expect the body of one fly after which point the orientation of the fly is determined using image	as pre processing the image is first masked to everything expect the body of one fly after which point the orientation of the fly is determined using image
0	146	9623	read frame from video threshold image and extract contours classify each contour as containing 0 1 or 2 flies if there are two one fly contours identify which is the male fly and which is the female fly for both flies determine the 0 360 orientation of the fly body for the male fly determine the 0 90 angle of each wing with respect to the major axis of its body 	read frame from video 
0	146	9624	read frame from video threshold image and extract contours classify each contour as containing 0 1 or 2 flies if there are two one fly contours identify which is the male fly and which is the female fly for both flies determine the 0 360 orientation of the fly body for the male fly determine the 0 90 angle of each wing with respect to the major axis of its body 	threshold image and extract contours 
1	146	9625	read frame from video threshold image and extract contours classify each contour as containing 0 1 or 2 flies if there are two one fly contours identify which is the male fly and which is the female fly for both flies determine the 0 360 orientation of the fly body for the male fly determine the 0 90 angle of each wing with respect to the major axis of its body 	classify each contour as containing 0 1 or 2 flies if there are two one fly contours identify which is the male fly and which is the female fly for both flies determine the 0 360 orientation of the fly body for the male fly determine the 0 90 angle of each wing with respect to the major axis of its body 
0	146	9626	read frame from video threshold image and extract contours classify each contour as containing 0 1 or 2 flies if there are two one fly contours identify which is the male fly and which is the female fly for both flies determine the 0 360 orientation of the fly body for the male fly determine the 0 90 angle of each wing with respect to the major axis of its body 	moments where pq is central image moment of the masked image defined by the summation where x is the center of mass of the image and f x y is the intensity at pixel x y 
0	146	9627	read frame from video threshold image and extract contours classify each contour as containing 0 1 or 2 flies if there are two one fly contours identify which is the male fly and which is the female fly for both flies determine the 0 360 orientation of the fly body for the male fly determine the 0 90 angle of each wing with respect to the major axis of its body 	unfortunately the orientation angle produced using this approach has a sign ambiguity it cannot discern whether an object is facing forwards or backwards 
1	146	9628	read frame from video threshold image and extract contours classify each contour as containing 0 1 or 2 flies if there are two one fly contours identify which is the male fly and which is the female fly for both flies determine the 0 360 orientation of the fly body for the male fly determine the 0 90 angle of each wing with respect to the major axis of its body 	as a result i needed to develop a machine learning algorithm to decide if the orientation computed via image moments should be corrected by adding 180 
1	146	9629	read frame from video threshold image and extract contours classify each contour as containing 0 1 or 2 flies if there are two one fly contours identify which is the male fly and which is the female fly for both flies determine the 0 360 orientation of the fly body for the male fly determine the 0 90 angle of each wing with respect to the major axis of its body 	as shown in briefly the hog descriptor after computing the hog descriptor for a fly image the descriptor is projected onto a basis of 15 principal components or in other words the eigenvectors corresponding to the 15 largest eigenvectors of i x i x i t where x i is the hog descriptor of the ith training example as the final step of this pipeline stage the dimensionally reduced hog descriptor is fed into a logistic regression 2 
0	146	9630	read frame from video threshold image and extract contours classify each contour as containing 0 1 or 2 flies if there are two one fly contours identify which is the male fly and which is the female fly for both flies determine the 0 360 orientation of the fly body for the male fly determine the 0 90 angle of each wing with respect to the major axis of its body 	somewhat surprisingly as shown in
0	146	9631	in the final pipeline stage the angles of the right and left wings of the male fly are determined similarly to the previous stage preprocessing is used to construct a region of interest roi containing just the male fly and its wings that roi can then be orientated in an upright direction using the results of the preceding pipeline stage 	in the final pipeline stage the angles of the right and left wings of the male fly are determined 
1	146	9632	in the final pipeline stage the angles of the right and left wings of the male fly are determined similarly to the previous stage preprocessing is used to construct a region of interest roi containing just the male fly and its wings that roi can then be orientated in an upright direction using the results of the preceding pipeline stage 	similarly to the previous stage preprocessing is used to construct a region of interest roi containing just the male fly and its wings 
0	146	9633	in the final pipeline stage the angles of the right and left wings of the male fly are determined similarly to the previous stage preprocessing is used to construct a region of interest roi containing just the male fly and its wings that roi can then be orientated in an upright direction using the results of the preceding pipeline stage 	that roi can then be orientated in an upright direction using the results of the preceding pipeline stage 
1	146	9634	in the final pipeline stage the angles of the right and left wings of the male fly are determined similarly to the previous stage preprocessing is used to construct a region of interest roi containing just the male fly and its wings that roi can then be orientated in an upright direction using the results of the preceding pipeline stage 	the preprocessing uses median blurring adaptive thresholding and erosion to preserve fly wings while removing fly legs in my approach the wing angles are determined separately by dividing the image of the male fly into two halves one for each wing 
0	146	9635	in the final pipeline stage the angles of the right and left wings of the male fly are determined similarly to the previous stage preprocessing is used to construct a region of interest roi containing just the male fly and its wings that roi can then be orientated in an upright direction using the results of the preceding pipeline stage 	as shown in the final step in this pipeline stage is a linear regression on the reduced dimensionality hog descriptors 
0	146	9636	in the final pipeline stage the angles of the right and left wings of the male fly are determined similarly to the previous stage preprocessing is used to construct a region of interest roi containing just the male fly and its wings that roi can then be orientated in an upright direction using the results of the preceding pipeline stage 	briefly linear regression seeks to minimize the mean squared error between predicted and given labels 
0	146	9637	in the final pipeline stage the angles of the right and left wings of the male fly are determined similarly to the previous stage preprocessing is used to construct a region of interest roi containing just the male fly and its wings that roi can then be orientated in an upright direction using the results of the preceding pipeline stage 	the optimal parameters can be computed directly by the equation x t x 1 x t y where x contains the features of all training examples and y is a vector of their labels 
0	146	9638	in the final pipeline stage the angles of the right and left wings of the male fly are determined similarly to the previous stage preprocessing is used to construct a region of interest roi containing just the male fly and its wings that roi can then be orientated in an upright direction using the results of the preceding pipeline stage 	after computing the predicted label given features x is simply t x 
0	146	9639	for all four pipeline stages error was evaluated by retaining one third of the dataset for testing this test set was not used at any point during training the first three stages of the pipeline were classifiers so their test error is reported simply as misclassification error as seen in 2sgmxtq 	for all four pipeline stages error was evaluated by retaining one third of the dataset for testing this test set was not used at any point during training 
0	146	9640	for all four pipeline stages error was evaluated by retaining one third of the dataset for testing this test set was not used at any point during training the first three stages of the pipeline were classifiers so their test error is reported simply as misclassification error as seen in 2sgmxtq 	the first three stages of the pipeline were classifiers so their test error is reported simply as misclassification error 
0	146	9641	for all four pipeline stages error was evaluated by retaining one third of the dataset for testing this test set was not used at any point during training the first three stages of the pipeline were classifiers so their test error is reported simply as misclassification error as seen in 2sgmxtq 	as seen in 2sgmxtq 
0	146	9642	for all four pipeline stages error was evaluated by retaining one third of the dataset for testing this test set was not used at any point during training the first three stages of the pipeline were classifiers so their test error is reported simply as misclassification error as seen in 2sgmxtq 	the wing angles over time are also plotted in
1	146	9643	in this report i described a real time image processing pipeline intended for neurobiology experiments that takes as input a grayscale video of a pair of fruit flies and produces as output an estimate of the position orientation and sex of each fly in addition to the wing angles for the male fly the pipeline had four distinct stages employing machine learning techniques including a decision tree logistic regresion linear regression and pca for both classification and regression tasks i demonstrated good accuracy on 1530x1530 frames at a throughput of 84 fps without gpu acceleration in the orientation and wingangle stages hog was used in conjunction with pca to produce input features for a trained model 	in this report i described a real time image processing pipeline intended for neurobiology experiments that takes as input a grayscale video of a pair of fruit flies and produces as output an estimate of the position orientation and sex of each fly in addition to the wing angles for the male fly 
0	146	9644	in this report i described a real time image processing pipeline intended for neurobiology experiments that takes as input a grayscale video of a pair of fruit flies and produces as output an estimate of the position orientation and sex of each fly in addition to the wing angles for the male fly the pipeline had four distinct stages employing machine learning techniques including a decision tree logistic regresion linear regression and pca for both classification and regression tasks i demonstrated good accuracy on 1530x1530 frames at a throughput of 84 fps without gpu acceleration in the orientation and wingangle stages hog was used in conjunction with pca to produce input features for a trained model 	the pipeline had four distinct stages employing machine learning techniques including a decision tree logistic regresion linear regression and pca 
1	146	9645	in this report i described a real time image processing pipeline intended for neurobiology experiments that takes as input a grayscale video of a pair of fruit flies and produces as output an estimate of the position orientation and sex of each fly in addition to the wing angles for the male fly the pipeline had four distinct stages employing machine learning techniques including a decision tree logistic regresion linear regression and pca for both classification and regression tasks i demonstrated good accuracy on 1530x1530 frames at a throughput of 84 fps without gpu acceleration in the orientation and wingangle stages hog was used in conjunction with pca to produce input features for a trained model 	for both classification and regression tasks i demonstrated good accuracy on 1530x1530 frames at a throughput of 84 fps without gpu acceleration in the orientation and wingangle stages hog was used in conjunction with pca to produce input features for a trained model 
0	146	9646	in this report i described a real time image processing pipeline intended for neurobiology experiments that takes as input a grayscale video of a pair of fruit flies and produces as output an estimate of the position orientation and sex of each fly in addition to the wing angles for the male fly the pipeline had four distinct stages employing machine learning techniques including a decision tree logistic regresion linear regression and pca for both classification and regression tasks i demonstrated good accuracy on 1530x1530 frames at a throughput of 84 fps without gpu acceleration in the orientation and wingangle stages hog was used in conjunction with pca to produce input features for a trained model 	in both cases i was surprised how well this worked even when just one or two principal components were used 
1	146	9647	in this report i described a real time image processing pipeline intended for neurobiology experiments that takes as input a grayscale video of a pair of fruit flies and produces as output an estimate of the position orientation and sex of each fly in addition to the wing angles for the male fly the pipeline had four distinct stages employing machine learning techniques including a decision tree logistic regresion linear regression and pca for both classification and regression tasks i demonstrated good accuracy on 1530x1530 frames at a throughput of 84 fps without gpu acceleration in the orientation and wingangle stages hog was used in conjunction with pca to produce input features for a trained model 	i think one reason this approach was successful was that i designed the preprocessing stages in a way that increased hog variance due to the variable of interest orientation or wing angle while decreasing hog variance due to other variables fly legs the other wing the other fly background roughness etc 
0	146	9648	in this report i described a real time image processing pipeline intended for neurobiology experiments that takes as input a grayscale video of a pair of fruit flies and produces as output an estimate of the position orientation and sex of each fly in addition to the wing angles for the male fly the pipeline had four distinct stages employing machine learning techniques including a decision tree logistic regresion linear regression and pca for both classification and regression tasks i demonstrated good accuracy on 1530x1530 frames at a throughput of 84 fps without gpu acceleration in the orientation and wingangle stages hog was used in conjunction with pca to produce input features for a trained model 	this in turn was a useful lesson about the role of preprocessing and feature selection in machine learning in the future there are a number of possible directions to explore 
1	146	9649	in this report i described a real time image processing pipeline intended for neurobiology experiments that takes as input a grayscale video of a pair of fruit flies and produces as output an estimate of the position orientation and sex of each fly in addition to the wing angles for the male fly the pipeline had four distinct stages employing machine learning techniques including a decision tree logistic regresion linear regression and pca for both classification and regression tasks i demonstrated good accuracy on 1530x1530 frames at a throughput of 84 fps without gpu acceleration in the orientation and wingangle stages hog was used in conjunction with pca to produce input features for a trained model 	first i could try measuring leg positions from the video preliminary experiments suggest that legs tips are selected fairly reliably with a keypoint detector such as sift
0	147	9650	amazon fulfillment centers are bustling hubs of innovation that allow amazon to deliver millions of products to over 100 countries worldwide these products are randomly placed in bins which are carried by robots occasionally items are misplaced while being handled resulting in a mismatch the recorded bin inventory versus its actual content the paper describes methods to predict the number of items in a bin thus detecting any inventory variance 	amazon fulfillment centers are bustling hubs of innovation that allow amazon to deliver millions of products to over 100 countries worldwide 
1	147	9651	amazon fulfillment centers are bustling hubs of innovation that allow amazon to deliver millions of products to over 100 countries worldwide these products are randomly placed in bins which are carried by robots occasionally items are misplaced while being handled resulting in a mismatch the recorded bin inventory versus its actual content the paper describes methods to predict the number of items in a bin thus detecting any inventory variance 	these products are randomly placed in bins which are carried by robots occasionally items are misplaced while being handled resulting in a mismatch the recorded bin inventory versus its actual content 
1	147	9652	amazon fulfillment centers are bustling hubs of innovation that allow amazon to deliver millions of products to over 100 countries worldwide these products are randomly placed in bins which are carried by robots occasionally items are misplaced while being handled resulting in a mismatch the recorded bin inventory versus its actual content the paper describes methods to predict the number of items in a bin thus detecting any inventory variance 	the paper describes methods to predict the number of items in a bin thus detecting any inventory variance 
1	147	9653	amazon fulfillment centers are bustling hubs of innovation that allow amazon to deliver millions of products to over 100 countries worldwide these products are randomly placed in bins which are carried by robots occasionally items are misplaced while being handled resulting in a mismatch the recorded bin inventory versus its actual content the paper describes methods to predict the number of items in a bin thus detecting any inventory variance 	by correcting variance upon detection amazon will better serve its customers specifically the input to our model is a raw color photo of the products in a bin 
1	147	9654	amazon fulfillment centers are bustling hubs of innovation that allow amazon to deliver millions of products to over 100 countries worldwide these products are randomly placed in bins which are carried by robots occasionally items are misplaced while being handled resulting in a mismatch the recorded bin inventory versus its actual content the paper describes methods to predict the number of items in a bin thus detecting any inventory variance 	to find the best solution to the inventory mismatch problem amazon published the bin image dataset which is detailed in section ii the output of a model is the bin s predicted quantity the number of products in the image while we started with linear methods the quest for model performance lead us to non linear algorithms and ultimately to convolutional deep learning 
0	147	9655	amazon fulfillment centers are bustling hubs of innovation that allow amazon to deliver millions of products to over 100 countries worldwide these products are randomly placed in bins which are carried by robots occasionally items are misplaced while being handled resulting in a mismatch the recorded bin inventory versus its actual content the paper describes methods to predict the number of items in a bin thus detecting any inventory variance 	section iii summarizes each algorithms applied 
0	147	9656	amazon fulfillment centers are bustling hubs of innovation that allow amazon to deliver millions of products to over 100 countries worldwide these products are randomly placed in bins which are carried by robots occasionally items are misplaced while being handled resulting in a mismatch the recorded bin inventory versus its actual content the paper describes methods to predict the number of items in a bin thus detecting any inventory variance 	they include logistic regression and classification trees summarized in section iii a and section iii b support vector machines linear kernel polynomial kernel radial kernel 
0	147	9657	amazon fulfillment centers are bustling hubs of innovation that allow amazon to deliver millions of products to over 100 countries worldwide these products are randomly placed in bins which are carried by robots occasionally items are misplaced while being handled resulting in a mismatch the recorded bin inventory versus its actual content the paper describes methods to predict the number of items in a bin thus detecting any inventory variance 	the algorithms are summarized in section iii c convolutional neural network resnet cross entropy loss function with learning rate optimizations 
0	147	9658	amazon fulfillment centers are bustling hubs of innovation that allow amazon to deliver millions of products to over 100 countries worldwide these products are randomly placed in bins which are carried by robots occasionally items are misplaced while being handled resulting in a mismatch the recorded bin inventory versus its actual content the paper describes methods to predict the number of items in a bin thus detecting any inventory variance 	the algorithm summary in section iii d section iv summarizes performance resuts 
0	147	9659	amazon fulfillment centers are bustling hubs of innovation that allow amazon to deliver millions of products to over 100 countries worldwide these products are randomly placed in bins which are carried by robots occasionally items are misplaced while being handled resulting in a mismatch the recorded bin inventory versus its actual content the paper describes methods to predict the number of items in a bin thus detecting any inventory variance 	with convolutional neural networks we were able to achieve an overall accuracy exceeding 56 
0	147	9660	amazon fulfillment centers are bustling hubs of innovation that allow amazon to deliver millions of products to over 100 countries worldwide these products are randomly placed in bins which are carried by robots occasionally items are misplaced while being handled resulting in a mismatch the recorded bin inventory versus its actual content the paper describes methods to predict the number of items in a bin thus detecting any inventory variance 	this is over 60 better than with support vector machines 
0	147	9661	amazon fulfillment centers are bustling hubs of innovation that allow amazon to deliver millions of products to over 100 countries worldwide these products are randomly placed in bins which are carried by robots occasionally items are misplaced while being handled resulting in a mismatch the recorded bin inventory versus its actual content the paper describes methods to predict the number of items in a bin thus detecting any inventory variance 	for the latter we attained accuracy of over 30 
1	147	9662	amazon fulfillment centers are bustling hubs of innovation that allow amazon to deliver millions of products to over 100 countries worldwide these products are randomly placed in bins which are carried by robots occasionally items are misplaced while being handled resulting in a mismatch the recorded bin inventory versus its actual content the paper describes methods to predict the number of items in a bin thus detecting any inventory variance 	we operated on a reduced dataset for bins that contained up to 5 products for a random baseline probability of 16 6 we are among the first to publish results on amazon s bin image dataset 
0	147	9663	amazon fulfillment centers are bustling hubs of innovation that allow amazon to deliver millions of products to over 100 countries worldwide these products are randomly placed in bins which are carried by robots occasionally items are misplaced while being handled resulting in a mismatch the recorded bin inventory versus its actual content the paper describes methods to predict the number of items in a bin thus detecting any inventory variance 	prior art by eunbyung park of the university of north carolina at chapel hill
0	147	9664	the data set contains 535 234 images which contain 459 476 different product skews of different shapes and sizes each image metadata tuple corresponds to a bin with products the metadata includes the actual count of objects in the bin which is used as the label to train our model we worked with the subset of 150k images each containing with up to five products 	the data set contains 535 234 images which contain 459 476 different product skews of different shapes and sizes 
0	147	9665	the data set contains 535 234 images which contain 459 476 different product skews of different shapes and sizes each image metadata tuple corresponds to a bin with products the metadata includes the actual count of objects in the bin which is used as the label to train our model we worked with the subset of 150k images each containing with up to five products 	each image metadata tuple corresponds to a bin with products 
1	147	9666	the data set contains 535 234 images which contain 459 476 different product skews of different shapes and sizes each image metadata tuple corresponds to a bin with products the metadata includes the actual count of objects in the bin which is used as the label to train our model we worked with the subset of 150k images each containing with up to five products 	the metadata includes the actual count of objects in the bin which is used as the label to train our model we worked with the subset of 150k images each containing with up to five products 
0	147	9667	the data set contains 535 234 images which contain 459 476 different product skews of different shapes and sizes each image metadata tuple corresponds to a bin with products the metadata includes the actual count of objects in the bin which is used as the label to train our model we worked with the subset of 150k images each containing with up to five products 	we split this as follows 70 training 20 validation and 10 test 
1	147	9668	before model training images were normalized 1 re sized to 224x224 pixels 2 tansformed for zero mean and unit variance 3 for convolutional models the dataset was augmented with horizontal flips of every image	before model training images were normalized 1 re sized to 224x224 pixels 2 tansformed for zero mean and unit variance 3 for convolutional models the dataset was augmented with horizontal flips of every image
0	147	9669	we explored the blob features extraction blobs are bright on dark or dark on bright regions in an image all the bins in which items are placed are similar and if items are present in the bin the idea is to make an attempt to create features assuming items in the bin are relatively bright on the backgrounds 	we explored the blob features extraction 
0	147	9670	we explored the blob features extraction blobs are bright on dark or dark on bright regions in an image all the bins in which items are placed are similar and if items are present in the bin the idea is to make an attempt to create features assuming items in the bin are relatively bright on the backgrounds 	blobs are bright on dark or dark on bright regions in an image 
1	147	9671	we explored the blob features extraction blobs are bright on dark or dark on bright regions in an image all the bins in which items are placed are similar and if items are present in the bin the idea is to make an attempt to create features assuming items in the bin are relatively bright on the backgrounds 	all the bins in which items are placed are similar and if items are present in the bin the idea is to make an attempt to create features assuming items in the bin are relatively bright on the backgrounds 
0	147	9672	we explored the blob features extraction blobs are bright on dark or dark on bright regions in an image all the bins in which items are placed are similar and if items are present in the bin the idea is to make an attempt to create features assuming items in the bin are relatively bright on the backgrounds 	we used laplacian of gaussian approach it computes the laplacian of images with successively increasing standard deviation and stacks them up in a cube 
0	147	9673	we explored the blob features extraction blobs are bright on dark or dark on bright regions in an image all the bins in which items are placed are similar and if items are present in the bin the idea is to make an attempt to create features assuming items in the bin are relatively bright on the backgrounds 	blobs are local maximas in this cube 
0	147	9674	we explored the blob features extraction blobs are bright on dark or dark on bright regions in an image all the bins in which items are placed are similar and if items are present in the bin the idea is to make an attempt to create features assuming items in the bin are relatively bright on the backgrounds 	note the yellow circles in
0	147	9675	the probability that each observation is classified to each class is defined as a logistic function as followthe observation is assigned to the class in which it has highest probability 	the probability that each observation is classified to each class is defined as a logistic function as followthe observation is assigned to the class in which it has highest probability 
0	147	9676	in a classification tree each observation belongs to the most commonly occurring class of training observations in the region to which it belongs we use the gini index of which measures total variance across the k classes as the loss function gini index is defined by	in a classification tree each observation belongs to the most commonly occurring class of training observations in the region to which it belongs 
0	147	9677	in a classification tree each observation belongs to the most commonly occurring class of training observations in the region to which it belongs we use the gini index of which measures total variance across the k classes as the loss function gini index is defined by	we use the gini index of which measures total variance across the k classes as the loss function 
0	147	9678	in a classification tree each observation belongs to the most commonly occurring class of training observations in the region to which it belongs we use the gini index of which measures total variance across the k classes as the loss function gini index is defined by	gini index is defined by
0	147	9679	the support vector machine svm is an extension of the support vector classifier that results from enlarging the feature space in a specific way using kernels feature space is enlarged in order to accommodate a non linear boundary between the classes the kernel approach enable an efficient computational approach for svm 	the support vector machine svm is an extension of the support vector classifier that results from enlarging the feature space in a specific way using kernels 
0	147	9680	the support vector machine svm is an extension of the support vector classifier that results from enlarging the feature space in a specific way using kernels feature space is enlarged in order to accommodate a non linear boundary between the classes the kernel approach enable an efficient computational approach for svm 	feature space is enlarged in order to accommodate a non linear boundary between the classes 
0	147	9681	the support vector machine svm is an extension of the support vector classifier that results from enlarging the feature space in a specific way using kernels feature space is enlarged in order to accommodate a non linear boundary between the classes the kernel approach enable an efficient computational approach for svm 	the kernel approach enable an efficient computational approach for svm 
0	147	9682	the support vector machine svm is an extension of the support vector classifier that results from enlarging the feature space in a specific way using kernels feature space is enlarged in order to accommodate a non linear boundary between the classes the kernel approach enable an efficient computational approach for svm 	we have attempted several kernel types as follow polynomial kernel
1	147	9683	resnet 1 classifier and loss function softmax layer and cross entropy loss cel function were used since we are solving multi class classification problem2 learning rate finder learning rate determines the step size of the update and is one of the key hyper parameters to training a network for some of our experiments we set the learning rate based on an approach introduced in the paper cyclical learning rates for training neural networks smith and leslie n	resnet 1 classifier and loss function softmax layer and cross entropy loss cel function were used since we are solving multi class classification problem2 learning rate finder learning rate determines the step size of the update and is one of the key hyper parameters to training a network 
0	147	9684	resnet 1 classifier and loss function softmax layer and cross entropy loss cel function were used since we are solving multi class classification problem2 learning rate finder learning rate determines the step size of the update and is one of the key hyper parameters to training a network for some of our experiments we set the learning rate based on an approach introduced in the paper cyclical learning rates for training neural networks smith and leslie n	for some of our experiments we set the learning rate based on an approach introduced in the paper cyclical learning rates for training neural networks smith and leslie n
0	147	9685	the performance of the best methods as well as the rationale leading to identifying them is outlined below given the nature of the data set we expect the decision boundary to be highly non linear 	the performance of the best methods as well as the rationale leading to identifying them is outlined below given the nature of the data set we expect the decision boundary to be highly non linear 
0	147	9686	several multi class classifiers were explored with raw pixel data 	several multi class classifiers were explored with raw pixel data 
0	147	9687	next with the intention of arriving at a more useful description of an image than raw pixel data a number of feature extraction algorithms were attempted with respect to histogram of oriented gradients hog evaluation suggests that the images in the data set do not have enough dominant gradients thus identifying products in a bin is difficult 	next with the intention of arriving at a more useful description of an image than raw pixel data a number of feature extraction algorithms were attempted 
1	147	9688	next with the intention of arriving at a more useful description of an image than raw pixel data a number of feature extraction algorithms were attempted with respect to histogram of oriented gradients hog evaluation suggests that the images in the data set do not have enough dominant gradients thus identifying products in a bin is difficult 	with respect to histogram of oriented gradients hog evaluation suggests that the images in the data set do not have enough dominant gradients 
0	147	9689	next with the intention of arriving at a more useful description of an image than raw pixel data a number of feature extraction algorithms were attempted with respect to histogram of oriented gradients hog evaluation suggests that the images in the data set do not have enough dominant gradients thus identifying products in a bin is difficult 	thus identifying products in a bin is difficult 
0	147	9690	next with the intention of arriving at a more useful description of an image than raw pixel data a number of feature extraction algorithms were attempted with respect to histogram of oriented gradients hog evaluation suggests that the images in the data set do not have enough dominant gradients thus identifying products in a bin is difficult 	in part this may be due to amazon s usage of tape to cover products in a bin 
0	147	9691	next with the intention of arriving at a more useful description of an image than raw pixel data a number of feature extraction algorithms were attempted with respect to histogram of oriented gradients hog evaluation suggests that the images in the data set do not have enough dominant gradients thus identifying products in a bin is difficult 	for many images the tape occludes the products causing a significant information loss 
0	147	9692	first manually cleaning the data to remove such images would enhance learning second with the adam optimizer we have seen that performance increases with a larger dataset we are intrigued by the possibility that photos be taken by from different angles 	first manually cleaning the data to remove such images would enhance learning 
0	147	9693	first manually cleaning the data to remove such images would enhance learning second with the adam optimizer we have seen that performance increases with a larger dataset we are intrigued by the possibility that photos be taken by from different angles 	second with the adam optimizer we have seen that performance increases with a larger dataset 
0	147	9694	first manually cleaning the data to remove such images would enhance learning second with the adam optimizer we have seen that performance increases with a larger dataset we are intrigued by the possibility that photos be taken by from different angles 	we are intrigued by the possibility that photos be taken by from different angles 
0	147	9695	first manually cleaning the data to remove such images would enhance learning second with the adam optimizer we have seen that performance increases with a larger dataset we are intrigued by the possibility that photos be taken by from different angles 	and that metadata connect the content of a bin over time so belief state may be tracked 
0	147	9696	first manually cleaning the data to remove such images would enhance learning second with the adam optimizer we have seen that performance increases with a larger dataset we are intrigued by the possibility that photos be taken by from different angles 	third with over 450k product skews images are bound to violate our assumption that they re drawn from a single distribution 
0	147	9697	first manually cleaning the data to remove such images would enhance learning second with the adam optimizer we have seen that performance increases with a larger dataset we are intrigued by the possibility that photos be taken by from different angles 	thus forming ensemble models with different architectures and learning approaches may achieve higher accuracy 
0	147	9698	first manually cleaning the data to remove such images would enhance learning second with the adam optimizer we have seen that performance increases with a larger dataset we are intrigued by the possibility that photos be taken by from different angles 	the project s repository is https github com onenow aiinventory reconciliation vii 
0	147	9699	first manually cleaning the data to remove such images would enhance learning second with the adam optimizer we have seen that performance increases with a larger dataset we are intrigued by the possibility that photos be taken by from different angles 	contributions pablo s primary contribution was on support vector machines sravan s on convolutional neural networks and nutchapols across the board 
0	147	9700	the authors are grateful to professor andrew ng for his masterly transmission of machine learning to us 	the authors are grateful to professor andrew ng for his masterly transmission of machine learning to us 
0	147	9701	pablo rodriguez bertorello leads next generation data engineering at cadreon a maketing technology platform company previously he was cto of airfox which completed a successful initial coin offering he is the co inventor of cloud platform company acquired by oracle 	pablo rodriguez bertorello leads next generation data engineering at cadreon a maketing technology platform company 
0	147	9702	pablo rodriguez bertorello leads next generation data engineering at cadreon a maketing technology platform company previously he was cto of airfox which completed a successful initial coin offering he is the co inventor of cloud platform company acquired by oracle 	previously he was cto of airfox which completed a successful initial coin offering 
0	147	9703	pablo rodriguez bertorello leads next generation data engineering at cadreon a maketing technology platform company previously he was cto of airfox which completed a successful initial coin offering he is the co inventor of cloud platform company acquired by oracle 	he is the co inventor of cloud platform company acquired by oracle 
0	147	9704	pablo rodriguez bertorello leads next generation data engineering at cadreon a maketing technology platform company previously he was cto of airfox which completed a successful initial coin offering he is the co inventor of cloud platform company acquired by oracle 	and the original designer of the data bus for intel s itanium processor 
0	147	9705	pablo rodriguez bertorello leads next generation data engineering at cadreon a maketing technology platform company previously he was cto of airfox which completed a successful initial coin offering he is the co inventor of cloud platform company acquired by oracle 	pablo has been issued over a dozen patents sravan sripada works at amazon 
0	147	9706	pablo rodriguez bertorello leads next generation data engineering at cadreon a maketing technology platform company previously he was cto of airfox which completed a successful initial coin offering he is the co inventor of cloud platform company acquired by oracle 	he is interested in applying artificial intelligence techniques to solve problems in retail cloud computing and voice controlled devices nutchapol dendumrongsup is a master s student at the institute for computational and mathematical engineering and deapartment of energy resources engineering at stanford 
0	147	9707	pablo rodriguez bertorello leads next generation data engineering at cadreon a maketing technology platform company previously he was cto of airfox which completed a successful initial coin offering he is the co inventor of cloud platform company acquired by oracle 	he is interested in the application of machine learning in the energy industry and the traditional reservoir simulation in oil and gas industry 
0	148	9708	office hours at stanford are typically subject to significant variance in student demand to tackle this problem we predict student demand at any office hours on an hourly basis using data scraped from queuestatus carta and course syllabi we conducted experiments using regression on fully connected nns univariate and multivariate lstms and compared with an ensemble of multimodal classification models such as random forests and svms 	office hours at stanford are typically subject to significant variance in student demand 
1	148	9709	office hours at stanford are typically subject to significant variance in student demand to tackle this problem we predict student demand at any office hours on an hourly basis using data scraped from queuestatus carta and course syllabi we conducted experiments using regression on fully connected nns univariate and multivariate lstms and compared with an ensemble of multimodal classification models such as random forests and svms 	to tackle this problem we predict student demand at any office hours on an hourly basis using data scraped from queuestatus carta and course syllabi 
1	148	9710	office hours at stanford are typically subject to significant variance in student demand to tackle this problem we predict student demand at any office hours on an hourly basis using data scraped from queuestatus carta and course syllabi we conducted experiments using regression on fully connected nns univariate and multivariate lstms and compared with an ensemble of multimodal classification models such as random forests and svms 	we conducted experiments using regression on fully connected nns univariate and multivariate lstms and compared with an ensemble of multimodal classification models such as random forests and svms 
1	148	9711	office hours at stanford are typically subject to significant variance in student demand to tackle this problem we predict student demand at any office hours on an hourly basis using data scraped from queuestatus carta and course syllabi we conducted experiments using regression on fully connected nns univariate and multivariate lstms and compared with an ensemble of multimodal classification models such as random forests and svms 	we compared different losses such as mse mae huber and our own sqhuber against normalized inputs and evaluate on student demand with and without smoothing 
0	148	9712	office hours at stanford are typically subject to significant variance in student demand to tackle this problem we predict student demand at any office hours on an hourly basis using data scraped from queuestatus carta and course syllabi we conducted experiments using regression on fully connected nns univariate and multivariate lstms and compared with an ensemble of multimodal classification models such as random forests and svms 	results show that our models predict demand well on held out test quarters both in seen and unseen courses 
0	148	9713	office hours at stanford are typically subject to significant variance in student demand to tackle this problem we predict student demand at any office hours on an hourly basis using data scraped from queuestatus carta and course syllabi we conducted experiments using regression on fully connected nns univariate and multivariate lstms and compared with an ensemble of multimodal classification models such as random forests and svms 	our model could thus be a useful reference for both new and existing courses 
0	148	9714	among cs students at stanford the experience of queueing at office hours ohs is practically universal office hours is an important part of any class allowing students to get valuable one on one help unfortunately student demand is prone to high variance resulting in sometimes students waiting hours before receiving help or conversely teaching assistants tas waiting hours before any students arrive 	among cs students at stanford the experience of queueing at office hours ohs is practically universal 
0	148	9715	among cs students at stanford the experience of queueing at office hours ohs is practically universal office hours is an important part of any class allowing students to get valuable one on one help unfortunately student demand is prone to high variance resulting in sometimes students waiting hours before receiving help or conversely teaching assistants tas waiting hours before any students arrive 	office hours is an important part of any class allowing students to get valuable one on one help 
1	148	9716	among cs students at stanford the experience of queueing at office hours ohs is practically universal office hours is an important part of any class allowing students to get valuable one on one help unfortunately student demand is prone to high variance resulting in sometimes students waiting hours before receiving help or conversely teaching assistants tas waiting hours before any students arrive 	unfortunately student demand is prone to high variance resulting in sometimes students waiting hours before receiving help or conversely teaching assistants tas waiting hours before any students arrive 
0	148	9717	among cs students at stanford the experience of queueing at office hours ohs is practically universal office hours is an important part of any class allowing students to get valuable one on one help unfortunately student demand is prone to high variance resulting in sometimes students waiting hours before receiving help or conversely teaching assistants tas waiting hours before any students arrive 	in particular periods of overcrowding are a source of stress for both students and tas and are among the most commonly cited sources of negative experience on carta 
1	148	9718	among cs students at stanford the experience of queueing at office hours ohs is practically universal office hours is an important part of any class allowing students to get valuable one on one help unfortunately student demand is prone to high variance resulting in sometimes students waiting hours before receiving help or conversely teaching assistants tas waiting hours before any students arrive 	thus improvements in oh scheduling could significantly improve overall course experience for all parties however as with all logistical decision making at universities there are significant complexities in the process 
0	148	9719	among cs students at stanford the experience of queueing at office hours ohs is practically universal office hours is an important part of any class allowing students to get valuable one on one help unfortunately student demand is prone to high variance resulting in sometimes students waiting hours before receiving help or conversely teaching assistants tas waiting hours before any students arrive 	our project addresses the arguably most variable component of the input predicting peaks of student demand 
1	148	9720	among cs students at stanford the experience of queueing at office hours ohs is practically universal office hours is an important part of any class allowing students to get valuable one on one help unfortunately student demand is prone to high variance resulting in sometimes students waiting hours before receiving help or conversely teaching assistants tas waiting hours before any students arrive 	using hourly oh data scraped from queuestatus course information from carta and major dates from class syllabi we trained a fully connected neural network model that predicts the hourly load influx for any given course and quarter 
0	148	9721	among cs students at stanford the experience of queueing at office hours ohs is practically universal office hours is an important part of any class allowing students to get valuable one on one help unfortunately student demand is prone to high variance resulting in sometimes students waiting hours before receiving help or conversely teaching assistants tas waiting hours before any students arrive 	we define the load influx as the average serve time for the day times the number of student signups 
0	148	9722	among cs students at stanford the experience of queueing at office hours ohs is practically universal office hours is an important part of any class allowing students to get valuable one on one help unfortunately student demand is prone to high variance resulting in sometimes students waiting hours before receiving help or conversely teaching assistants tas waiting hours before any students arrive 	conceptually this is the aggregate ta time needed to satisfy all student demand over some period 
0	148	9723	among cs students at stanford the experience of queueing at office hours ohs is practically universal office hours is an important part of any class allowing students to get valuable one on one help unfortunately student demand is prone to high variance resulting in sometimes students waiting hours before receiving help or conversely teaching assistants tas waiting hours before any students arrive 	note in terms of dataset and big picture goals this is a shared project between cs229 and cs221 
1	148	9724	among cs students at stanford the experience of queueing at office hours ohs is practically universal office hours is an important part of any class allowing students to get valuable one on one help unfortunately student demand is prone to high variance resulting in sometimes students waiting hours before receiving help or conversely teaching assistants tas waiting hours before any students arrive 	for cs229 we focused on a more theoretical approach in predicting load influx by designing and evaluating new loss functions catered towards data with high variance and fluctuations 
0	148	9725	among cs students at stanford the experience of queueing at office hours ohs is practically universal office hours is an important part of any class allowing students to get valuable one on one help unfortunately student demand is prone to high variance resulting in sometimes students waiting hours before receiving help or conversely teaching assistants tas waiting hours before any students arrive 	we also combine an ensemble of approaches to fine tune our prediction by using signal processing practices as well as experiment with multimodal classification using svms and random forest models 
1	148	9726	among cs students at stanford the experience of queueing at office hours ohs is practically universal office hours is an important part of any class allowing students to get valuable one on one help unfortunately student demand is prone to high variance resulting in sometimes students waiting hours before receiving help or conversely teaching assistants tas waiting hours before any students arrive 	for cs221 we focus on assigning tas to the surge timings using modified gibbs sampling and em algorithms as well as lstm prediction models approaches of a cs229 project that had a similar goal 
0	148	9727	among cs students at stanford the experience of queueing at office hours ohs is practically universal office hours is an important part of any class allowing students to get valuable one on one help unfortunately student demand is prone to high variance resulting in sometimes students waiting hours before receiving help or conversely teaching assistants tas waiting hours before any students arrive 	al used custom feature extractors to predict wait times at the lair cs106 office hours 
0	148	9728	to obtain data we set up a pipeline that scrapes hourly office hours from queuestatus through customized json parsing we were able to obtain a combined 17 quarters of data across 7 prominent cs classes after preprocessing to remove all entries with zero serves and signups we ended with 4672 hours or just under 200 straight days worth of oh data 	to obtain data we set up a pipeline that scrapes hourly office hours from queuestatus 
0	148	9729	to obtain data we set up a pipeline that scrapes hourly office hours from queuestatus through customized json parsing we were able to obtain a combined 17 quarters of data across 7 prominent cs classes after preprocessing to remove all entries with zero serves and signups we ended with 4672 hours or just under 200 straight days worth of oh data 	through customized json parsing we were able to obtain a combined 17 quarters of data across 7 prominent cs classes 
0	148	9730	to obtain data we set up a pipeline that scrapes hourly office hours from queuestatus through customized json parsing we were able to obtain a combined 17 quarters of data across 7 prominent cs classes after preprocessing to remove all entries with zero serves and signups we ended with 4672 hours or just under 200 straight days worth of oh data 	after preprocessing to remove all entries with zero serves and signups we ended with 4672 hours or just under 200 straight days worth of oh data 
0	148	9731	to obtain data we set up a pipeline that scrapes hourly office hours from queuestatus through customized json parsing we were able to obtain a combined 17 quarters of data across 7 prominent cs classes after preprocessing to remove all entries with zero serves and signups we ended with 4672 hours or just under 200 straight days worth of oh data 	a summary is shown below 
1	148	9732	to obtain data we set up a pipeline that scrapes hourly office hours from queuestatus through customized json parsing we were able to obtain a combined 17 quarters of data across 7 prominent cs classes after preprocessing to remove all entries with zero serves and signups we ended with 4672 hours or just under 200 straight days worth of oh data 	we experimented with a plethora of features to augment our dataset with and decided on the following predictors based on a combination of logic and significant correlation with load influx 
0	148	9733	to obtain data we set up a pipeline that scrapes hourly office hours from queuestatus through customized json parsing we were able to obtain a combined 17 quarters of data across 7 prominent cs classes after preprocessing to remove all entries with zero serves and signups we ended with 4672 hours or just under 200 straight days worth of oh data 	on a per class basis we used number of enrolled students instructor rating and proportion of freshman graduate phd students enrolled 
0	148	9734	to obtain data we set up a pipeline that scrapes hourly office hours from queuestatus through customized json parsing we were able to obtain a combined 17 quarters of data across 7 prominent cs classes after preprocessing to remove all entries with zero serves and signups we ended with 4672 hours or just under 200 straight days worth of oh data 	on a per hour day basis we used days until next assignment due days after previous assignment due days until an exam hour of day weekdays 
1	148	9735	to obtain data we set up a pipeline that scrapes hourly office hours from queuestatus through customized json parsing we were able to obtain a combined 17 quarters of data across 7 prominent cs classes after preprocessing to remove all entries with zero serves and signups we ended with 4672 hours or just under 200 straight days worth of oh data 	for the hourly daily features validation testing found that one hot bucket encodings were more effective for predictions 
0	148	9736	to obtain data we set up a pipeline that scrapes hourly office hours from queuestatus through customized json parsing we were able to obtain a combined 17 quarters of data across 7 prominent cs classes after preprocessing to remove all entries with zero serves and signups we ended with 4672 hours or just under 200 straight days worth of oh data 	day differences were bucketed in ranges of 10 to 5 4 to 3 2 to 1 and 0 
0	148	9737	to obtain data we set up a pipeline that scrapes hourly office hours from queuestatus through customized json parsing we were able to obtain a combined 17 quarters of data across 7 prominent cs classes after preprocessing to remove all entries with zero serves and signups we ended with 4672 hours or just under 200 straight days worth of oh data 	hour of day was evenly bucketed into morning noon afternoon and evening 
0	148	9738	to obtain data we set up a pipeline that scrapes hourly office hours from queuestatus through customized json parsing we were able to obtain a combined 17 quarters of data across 7 prominent cs classes after preprocessing to remove all entries with zero serves and signups we ended with 4672 hours or just under 200 straight days worth of oh data 	each entry corresponds to one hour of oh and every entry in the same course quarter shares the same course quarter features 
1	148	9739	to obtain data we set up a pipeline that scrapes hourly office hours from queuestatus through customized json parsing we were able to obtain a combined 17 quarters of data across 7 prominent cs classes after preprocessing to remove all entries with zero serves and signups we ended with 4672 hours or just under 200 straight days worth of oh data 	as discussed later we also experimented with log transformations as our ultimate goal is to predict entire unseen quarters we separated our training validation test sets by entire quarters 
0	148	9740	to obtain data we set up a pipeline that scrapes hourly office hours from queuestatus through customized json parsing we were able to obtain a combined 17 quarters of data across 7 prominent cs classes after preprocessing to remove all entries with zero serves and signups we ended with 4672 hours or just under 200 straight days worth of oh data 	due to our limited sample size we use k fold cross validation to tune hyperparameters where k is our number of quarters 
0	148	9741	to obtain data we set up a pipeline that scrapes hourly office hours from queuestatus through customized json parsing we were able to obtain a combined 17 quarters of data across 7 prominent cs classes after preprocessing to remove all entries with zero serves and signups we ended with 4672 hours or just under 200 straight days worth of oh data 	our test set consisted of 4 total classes cs110 spring 2018 and cs107 spring 2017 as unseen quarters of classes we trained on and cs224n winter 2018 and cs231n spring 2018 as entirely unseen courses 
1	148	9742	to obtain data we set up a pipeline that scrapes hourly office hours from queuestatus through customized json parsing we were able to obtain a combined 17 quarters of data across 7 prominent cs classes after preprocessing to remove all entries with zero serves and signups we ended with 4672 hours or just under 200 straight days worth of oh data 	our training set thus consisted of the remaining classes totaling 13 quarters of data between 5 unique classes we note that after training models to predict load influx on these datasets we do not predict hourly student demand for tas as is ideal 
0	148	9743	to obtain data we set up a pipeline that scrapes hourly office hours from queuestatus through customized json parsing we were able to obtain a combined 17 quarters of data across 7 prominent cs classes after preprocessing to remove all entries with zero serves and signups we ended with 4672 hours or just under 200 straight days worth of oh data 	rather we predict hourly student demand for tas given that office hours is held 
0	148	9744	to obtain data we set up a pipeline that scrapes hourly office hours from queuestatus through customized json parsing we were able to obtain a combined 17 quarters of data across 7 prominent cs classes after preprocessing to remove all entries with zero serves and signups we ended with 4672 hours or just under 200 straight days worth of oh data 	we determined that current ta assignments are uncorrelated with time of day p 0 63 cor test in r and typically scheduled throughout active hours 
0	148	9745	to obtain data we set up a pipeline that scrapes hourly office hours from queuestatus through customized json parsing we were able to obtain a combined 17 quarters of data across 7 prominent cs classes after preprocessing to remove all entries with zero serves and signups we ended with 4672 hours or just under 200 straight days worth of oh data 	therefore we assume that the status quo scheduling of office hours is frequent and unbiased enough such that real student demand is proportional to the student demand given office hours is held 
1	148	9746	we first implemented multimodal classification models as baselines where instead of using equidepth buckets we divided the minimum and maximum load influx into 7 logarithmic time buckets using svms with radial kernel and random forests with 1000 estimators we obtained an initial baseline with accuracy 0 422 and 0 359 respectively with the confusion matrix as shown below we see that even with fine tuning of hyperparameters the classification models have decent performance but with large skew and variance in predicting high load influxes which could be possibly due to class imbalance in different buckets when on a log scale 	we first implemented multimodal classification models as baselines where instead of using equidepth buckets we divided the minimum and maximum load influx into 7 logarithmic time buckets 
1	148	9747	we first implemented multimodal classification models as baselines where instead of using equidepth buckets we divided the minimum and maximum load influx into 7 logarithmic time buckets using svms with radial kernel and random forests with 1000 estimators we obtained an initial baseline with accuracy 0 422 and 0 359 respectively with the confusion matrix as shown below we see that even with fine tuning of hyperparameters the classification models have decent performance but with large skew and variance in predicting high load influxes which could be possibly due to class imbalance in different buckets when on a log scale 	using svms with radial kernel and random forests with 1000 estimators we obtained an initial baseline with accuracy 0 422 and 0 359 respectively with the confusion matrix as shown below 
1	148	9748	we first implemented multimodal classification models as baselines where instead of using equidepth buckets we divided the minimum and maximum load influx into 7 logarithmic time buckets using svms with radial kernel and random forests with 1000 estimators we obtained an initial baseline with accuracy 0 422 and 0 359 respectively with the confusion matrix as shown below we see that even with fine tuning of hyperparameters the classification models have decent performance but with large skew and variance in predicting high load influxes which could be possibly due to class imbalance in different buckets when on a log scale 	we see that even with fine tuning of hyperparameters the classification models have decent performance but with large skew and variance in predicting high load influxes which could be possibly due to class imbalance in different buckets when on a log scale 
1	148	9749	we first implemented multimodal classification models as baselines where instead of using equidepth buckets we divided the minimum and maximum load influx into 7 logarithmic time buckets using svms with radial kernel and random forests with 1000 estimators we obtained an initial baseline with accuracy 0 422 and 0 359 respectively with the confusion matrix as shown below we see that even with fine tuning of hyperparameters the classification models have decent performance but with large skew and variance in predicting high load influxes which could be possibly due to class imbalance in different buckets when on a log scale 	we thus choose to focus on regression next to predict the spikes of load influx in different hours 
1	148	9750	we also set up baselines by training fully connected networks and lstms for regression tasks the fcn which approximates functions with non linearities and multiple layers that activate depending on weights mapped to higher dimension across stacked layers has input size 30 with our 30 features with 2 hidden layers of size 15 and 4 respectively followed by a single output final layer each hidden layer uses a relu activation function with a linear activation for the output layer 	we also set up baselines by training fully connected networks and lstms for regression tasks 
1	148	9751	we also set up baselines by training fully connected networks and lstms for regression tasks the fcn which approximates functions with non linearities and multiple layers that activate depending on weights mapped to higher dimension across stacked layers has input size 30 with our 30 features with 2 hidden layers of size 15 and 4 respectively followed by a single output final layer each hidden layer uses a relu activation function with a linear activation for the output layer 	the fcn which approximates functions with non linearities and multiple layers that activate depending on weights mapped to higher dimension across stacked layers has input size 30 with our 30 features with 2 hidden layers of size 15 and 4 respectively followed by a single output final layer 
0	148	9752	we also set up baselines by training fully connected networks and lstms for regression tasks the fcn which approximates functions with non linearities and multiple layers that activate depending on weights mapped to higher dimension across stacked layers has input size 30 with our 30 features with 2 hidden layers of size 15 and 4 respectively followed by a single output final layer each hidden layer uses a relu activation function with a linear activation for the output layer 	each hidden layer uses a relu activation function with a linear activation for the output layer 
1	148	9753	we also set up baselines by training fully connected networks and lstms for regression tasks the fcn which approximates functions with non linearities and multiple layers that activate depending on weights mapped to higher dimension across stacked layers has input size 30 with our 30 features with 2 hidden layers of size 15 and 4 respectively followed by a single output final layer each hidden layer uses a relu activation function with a linear activation for the output layer 	we also experimented with 3 4 hidden layers which led to overfitting even with normalization techniques that performed worse on the validation set lstms long short term memory is a form of recurrent neural network focused in 221 report 
0	148	9754	we also set up baselines by training fully connected networks and lstms for regression tasks the fcn which approximates functions with non linearities and multiple layers that activate depending on weights mapped to higher dimension across stacked layers has input size 30 with our 30 features with 2 hidden layers of size 15 and 4 respectively followed by a single output final layer each hidden layer uses a relu activation function with a linear activation for the output layer 	it addresses vanishing gradients while factoring in previous states with a recurrence formula at each time step which makes it suitable for temporal data 
0	148	9755	we also set up baselines by training fully connected networks and lstms for regression tasks the fcn which approximates functions with non linearities and multiple layers that activate depending on weights mapped to higher dimension across stacked layers has input size 30 with our 30 features with 2 hidden layers of size 15 and 4 respectively followed by a single output final layer each hidden layer uses a relu activation function with a linear activation for the output layer 	we used two lstm cells in autoregressive lstm with window size of 16 and each output was fed back as part of the next window 
1	148	9756	we also set up baselines by training fully connected networks and lstms for regression tasks the fcn which approximates functions with non linearities and multiple layers that activate depending on weights mapped to higher dimension across stacked layers has input size 30 with our 30 features with 2 hidden layers of size 15 and 4 respectively followed by a single output final layer each hidden layer uses a relu activation function with a linear activation for the output layer 	all input features were normalized in a range 0 1 for every experiment and all baseline models were compiled with adam optimizer with early stopping to prevent overfitting 
1	148	9757	we also set up baselines by training fully connected networks and lstms for regression tasks the fcn which approximates functions with non linearities and multiple layers that activate depending on weights mapped to higher dimension across stacked layers has input size 30 with our 30 features with 2 hidden layers of size 15 and 4 respectively followed by a single output final layer each hidden layer uses a relu activation function with a linear activation for the output layer 	due to insufficient data we face high variance in training lstms with the initial baselines reported below therefore we choose to continue work on the fully connected network fcn 
1	148	9758	we also set up baselines by training fully connected networks and lstms for regression tasks the fcn which approximates functions with non linearities and multiple layers that activate depending on weights mapped to higher dimension across stacked layers has input size 30 with our 30 features with 2 hidden layers of size 15 and 4 respectively followed by a single output final layer each hidden layer uses a relu activation function with a linear activation for the output layer 	however in our fcn we notice our predictions for load influx throughout the quarter suffer from a consistent offset from the mean of the distribution 
1	148	9759	we also set up baselines by training fully connected networks and lstms for regression tasks the fcn which approximates functions with non linearities and multiple layers that activate depending on weights mapped to higher dimension across stacked layers has input size 30 with our 30 features with 2 hidden layers of size 15 and 4 respectively followed by a single output final layer each hidden layer uses a relu activation function with a linear activation for the output layer 	upon inspection we suspect that the large amount of outliers may have caused the bias due to their huge penalties while minimizing the l2 norm loss function 
0	148	9760	we also set up baselines by training fully connected networks and lstms for regression tasks the fcn which approximates functions with non linearities and multiple layers that activate depending on weights mapped to higher dimension across stacked layers has input size 30 with our 30 features with 2 hidden layers of size 15 and 4 respectively followed by a single output final layer each hidden layer uses a relu activation function with a linear activation for the output layer 	thus we seek a new loss function that doesn t penalize outliers as heavily 
0	148	9761	we also set up baselines by training fully connected networks and lstms for regression tasks the fcn which approximates functions with non linearities and multiple layers that activate depending on weights mapped to higher dimension across stacked layers has input size 30 with our 30 features with 2 hidden layers of size 15 and 4 respectively followed by a single output final layer each hidden layer uses a relu activation function with a linear activation for the output layer 	the huber loss is particularly useful for this since it scales linearly outside a specified domain we compare this traditional loss function with a novel loss function we designed for the purposes of experimentation the sqhuber loss 
0	148	9762	we also set up baselines by training fully connected networks and lstms for regression tasks the fcn which approximates functions with non linearities and multiple layers that activate depending on weights mapped to higher dimension across stacked layers has input size 30 with our 30 features with 2 hidden layers of size 15 and 4 respectively followed by a single output final layer each hidden layer uses a relu activation function with a linear activation for the output layer 	the sqhuber loss is defined as the sqhuber loss is piece wise continuous and scales proportional to the square root of the residual for values above a specified domain 
0	148	9763	we also set up baselines by training fully connected networks and lstms for regression tasks the fcn which approximates functions with non linearities and multiple layers that activate depending on weights mapped to higher dimension across stacked layers has input size 30 with our 30 features with 2 hidden layers of size 15 and 4 respectively followed by a single output final layer each hidden layer uses a relu activation function with a linear activation for the output layer 	thus it is even more robust to significant amounts of outliers 
0	148	9764	the load influx is an erratic function large fluctuations or spikes in the load are difficult to predict without overfitting the model thus transforming the training labels actual load influx before training may be fruitful we attempted two methods to transform our data for better predictions 1 	the load influx is an erratic function 
1	148	9765	the load influx is an erratic function large fluctuations or spikes in the load are difficult to predict without overfitting the model thus transforming the training labels actual load influx before training may be fruitful we attempted two methods to transform our data for better predictions 1 	large fluctuations or spikes in the load are difficult to predict without overfitting the model thus transforming the training labels actual load influx before training may be fruitful 
0	148	9766	the load influx is an erratic function large fluctuations or spikes in the load are difficult to predict without overfitting the model thus transforming the training labels actual load influx before training may be fruitful we attempted two methods to transform our data for better predictions 1 	we attempted two methods to transform our data for better predictions 1 
0	148	9767	the load influx is an erratic function large fluctuations or spikes in the load are difficult to predict without overfitting the model thus transforming the training labels actual load influx before training may be fruitful we attempted two methods to transform our data for better predictions 1 	hanning window a 1 d convolution with a hanning window 
0	148	9768	the load influx is an erratic function large fluctuations or spikes in the load are difficult to predict without overfitting the model thus transforming the training labels actual load influx before training may be fruitful we attempted two methods to transform our data for better predictions 1 	this reduces spikes and thus potential to overfit 
1	148	9769	during validation tests we find the root mean squared error rmse to be heavily dependent on the course and quarter used thus to reduce variance in rmse obtained from validation tests we used leave one out cross validation for our validation studies where we stochastically choose a course quarter pair as the validation set and use the remainder of the joint training validation sets for training this process is repeated for k 8 iterations with the validation rmse the mean of the results 	during validation tests we find the root mean squared error rmse to be heavily dependent on the course and quarter used 
1	148	9770	during validation tests we find the root mean squared error rmse to be heavily dependent on the course and quarter used thus to reduce variance in rmse obtained from validation tests we used leave one out cross validation for our validation studies where we stochastically choose a course quarter pair as the validation set and use the remainder of the joint training validation sets for training this process is repeated for k 8 iterations with the validation rmse the mean of the results 	thus to reduce variance in rmse obtained from validation tests we used leave one out cross validation for our validation studies where we stochastically choose a course quarter pair as the validation set and use the remainder of the joint training validation sets for training 
0	148	9771	during validation tests we find the root mean squared error rmse to be heavily dependent on the course and quarter used thus to reduce variance in rmse obtained from validation tests we used leave one out cross validation for our validation studies where we stochastically choose a course quarter pair as the validation set and use the remainder of the joint training validation sets for training this process is repeated for k 8 iterations with the validation rmse the mean of the results 	this process is repeated for k 8 iterations with the validation rmse the mean of the results 
1	148	9772	during validation tests we find the root mean squared error rmse to be heavily dependent on the course and quarter used thus to reduce variance in rmse obtained from validation tests we used leave one out cross validation for our validation studies where we stochastically choose a course quarter pair as the validation set and use the remainder of the joint training validation sets for training this process is repeated for k 8 iterations with the validation rmse the mean of the results 	note that for each of the k iterations the validation set was stochastically chosen and isolated from the training data with the parameters of the model reset between iterations 
0	148	9773	during validation tests we find the root mean squared error rmse to be heavily dependent on the course and quarter used thus to reduce variance in rmse obtained from validation tests we used leave one out cross validation for our validation studies where we stochastically choose a course quarter pair as the validation set and use the remainder of the joint training validation sets for training this process is repeated for k 8 iterations with the validation rmse the mean of the results 	the results for cross validation between models are tabulated in final evaluation on test set 
0	148	9774	during validation tests we find the root mean squared error rmse to be heavily dependent on the course and quarter used thus to reduce variance in rmse obtained from validation tests we used leave one out cross validation for our validation studies where we stochastically choose a course quarter pair as the validation set and use the remainder of the joint training validation sets for training this process is repeated for k 8 iterations with the validation rmse the mean of the results 	we obtained an avg 
0	148	9775	during validation tests we find the root mean squared error rmse to be heavily dependent on the course and quarter used thus to reduce variance in rmse obtained from validation tests we used leave one out cross validation for our validation studies where we stochastically choose a course quarter pair as the validation set and use the remainder of the joint training validation sets for training this process is repeated for k 8 iterations with the validation rmse the mean of the results 	rmse of 124 466 for our set of seen courses in an unseen quarter and 106 478 for our set of unseen courses in unseen quarters 
0	148	9776	during validation tests we find the root mean squared error rmse to be heavily dependent on the course and quarter used thus to reduce variance in rmse obtained from validation tests we used leave one out cross validation for our validation studies where we stochastically choose a course quarter pair as the validation set and use the remainder of the joint training validation sets for training this process is repeated for k 8 iterations with the validation rmse the mean of the results 	furthermore similar to
0	148	9777	overall our project provides the first general use model for predicting student demand at stanford cs office hours using hourly queuestatus data and course information we were able to generate realistic predictions for office hours load in a wide range of cs classes ultimately out of several tried our best model was our fully connected neural network using mean absolute error and trained on log transformed load influx 	overall our project provides the first general use model for predicting student demand at stanford cs office hours 
1	148	9778	overall our project provides the first general use model for predicting student demand at stanford cs office hours using hourly queuestatus data and course information we were able to generate realistic predictions for office hours load in a wide range of cs classes ultimately out of several tried our best model was our fully connected neural network using mean absolute error and trained on log transformed load influx 	using hourly queuestatus data and course information we were able to generate realistic predictions for office hours load in a wide range of cs classes 
0	148	9779	overall our project provides the first general use model for predicting student demand at stanford cs office hours using hourly queuestatus data and course information we were able to generate realistic predictions for office hours load in a wide range of cs classes ultimately out of several tried our best model was our fully connected neural network using mean absolute error and trained on log transformed load influx 	ultimately out of several tried our best model was our fully connected neural network using mean absolute error and trained on log transformed load influx 
1	148	9780	overall our project provides the first general use model for predicting student demand at stanford cs office hours using hourly queuestatus data and course information we were able to generate realistic predictions for office hours load in a wide range of cs classes ultimately out of several tried our best model was our fully connected neural network using mean absolute error and trained on log transformed load influx 	although a slightly different model using our custom sqhuber loss gave marginally lower rmse it failed to retain spike information due to perhaps too much outlier penalty 
0	148	9781	overall our project provides the first general use model for predicting student demand at stanford cs office hours using hourly queuestatus data and course information we were able to generate realistic predictions for office hours load in a wide range of cs classes ultimately out of several tried our best model was our fully connected neural network using mean absolute error and trained on log transformed load influx 	our rmse indicates that the model is off by an average of 2 hours students in testing 
1	148	9782	overall our project provides the first general use model for predicting student demand at stanford cs office hours using hourly queuestatus data and course information we were able to generate realistic predictions for office hours load in a wide range of cs classes ultimately out of several tried our best model was our fully connected neural network using mean absolute error and trained on log transformed load influx 	empirically we see this is a mostly a result of slightly misplaced and or incorrectly heighted spikes since our final log model makes predictions that are then exponentiated it often predicts the locations of spikes correctly but fails to capture exact magnitude 
0	148	9783	overall our project provides the first general use model for predicting student demand at stanford cs office hours using hourly queuestatus data and course information we were able to generate realistic predictions for office hours load in a wide range of cs classes ultimately out of several tried our best model was our fully connected neural network using mean absolute error and trained on log transformed load influx 	thus although our system may not be able to predict exact student demand it can still serve as a valuable guideline regarding when to expect relative peaks 
1	148	9784	overall our project provides the first general use model for predicting student demand at stanford cs office hours using hourly queuestatus data and course information we were able to generate realistic predictions for office hours load in a wide range of cs classes ultimately out of several tried our best model was our fully connected neural network using mean absolute error and trained on log transformed load influx 	furthermore we constructed a basic gui in r that given basic course information generates oh hourly load influx for the whole quarter within a minute demoed during poster session 
0	148	9785	overall our project provides the first general use model for predicting student demand at stanford cs office hours using hourly queuestatus data and course information we were able to generate realistic predictions for office hours load in a wide range of cs classes ultimately out of several tried our best model was our fully connected neural network using mean absolute error and trained on log transformed load influx 	so far chris piech has expressed interest in using our model next spring 
0	148	9786	overall our project provides the first general use model for predicting student demand at stanford cs office hours using hourly queuestatus data and course information we were able to generate realistic predictions for office hours load in a wide range of cs classes ultimately out of several tried our best model was our fully connected neural network using mean absolute error and trained on log transformed load influx 	given more time we would like to extend our predictions to more classes and perhaps even other universities using queuestatus 
1	149	9787	with the rising popularity of peer to peer lending platforms in recent years investors now have easy access to this alternative investment asset class by lending money to individual borrowers through platforms such as lendingclub prosper marketplace and upstart or to small businesses through funding circle the process starts with borrowers submitting loan applications to the platform which performs credit reviews and either approves or denies each application the platform also uses a proprietary model to determine the interest rate of approved loans based on the credit worthiness of borrowers approved loans are then listed on the platform for investor funding 	with the rising popularity of peer to peer lending platforms in recent years investors now have easy access to this alternative investment asset class by lending money to individual borrowers through platforms such as lendingclub prosper marketplace and upstart or to small businesses through funding circle the process starts with borrowers submitting loan applications to the platform which performs credit reviews and either approves or denies each application 
1	149	9788	with the rising popularity of peer to peer lending platforms in recent years investors now have easy access to this alternative investment asset class by lending money to individual borrowers through platforms such as lendingclub prosper marketplace and upstart or to small businesses through funding circle the process starts with borrowers submitting loan applications to the platform which performs credit reviews and either approves or denies each application the platform also uses a proprietary model to determine the interest rate of approved loans based on the credit worthiness of borrowers approved loans are then listed on the platform for investor funding 	the platform also uses a proprietary model to determine the interest rate of approved loans based on the credit worthiness of borrowers 
0	149	9789	with the rising popularity of peer to peer lending platforms in recent years investors now have easy access to this alternative investment asset class by lending money to individual borrowers through platforms such as lendingclub prosper marketplace and upstart or to small businesses through funding circle the process starts with borrowers submitting loan applications to the platform which performs credit reviews and either approves or denies each application the platform also uses a proprietary model to determine the interest rate of approved loans based on the credit worthiness of borrowers approved loans are then listed on the platform for investor funding 	approved loans are then listed on the platform for investor funding 
0	149	9790	with the rising popularity of peer to peer lending platforms in recent years investors now have easy access to this alternative investment asset class by lending money to individual borrowers through platforms such as lendingclub prosper marketplace and upstart or to small businesses through funding circle the process starts with borrowers submitting loan applications to the platform which performs credit reviews and either approves or denies each application the platform also uses a proprietary model to determine the interest rate of approved loans based on the credit worthiness of borrowers approved loans are then listed on the platform for investor funding 	investors usually want to diversify their portfolio by only investing a small amount e g 
0	149	9791	with the rising popularity of peer to peer lending platforms in recent years investors now have easy access to this alternative investment asset class by lending money to individual borrowers through platforms such as lendingclub prosper marketplace and upstart or to small businesses through funding circle the process starts with borrowers submitting loan applications to the platform which performs credit reviews and either approves or denies each application the platform also uses a proprietary model to determine the interest rate of approved loans based on the credit worthiness of borrowers approved loans are then listed on the platform for investor funding 	 25 in each loan 
1	149	9792	with the rising popularity of peer to peer lending platforms in recent years investors now have easy access to this alternative investment asset class by lending money to individual borrowers through platforms such as lendingclub prosper marketplace and upstart or to small businesses through funding circle the process starts with borrowers submitting loan applications to the platform which performs credit reviews and either approves or denies each application the platform also uses a proprietary model to determine the interest rate of approved loans based on the credit worthiness of borrowers approved loans are then listed on the platform for investor funding 	hence it is desirable for investors to be able to independently evaluate the credit risk of a large number of listed loans quickly and invest in those with lower perceived risks this motivates us to build machine learned classification and regression models that can quantify the credit risk with a lendingclub historical loan dataset 
1	149	9793	with the rising popularity of peer to peer lending platforms in recent years investors now have easy access to this alternative investment asset class by lending money to individual borrowers through platforms such as lendingclub prosper marketplace and upstart or to small businesses through funding circle the process starts with borrowers submitting loan applications to the platform which performs credit reviews and either approves or denies each application the platform also uses a proprietary model to determine the interest rate of approved loans based on the credit worthiness of borrowers approved loans are then listed on the platform for investor funding 	specifically we build and evaluate classifiers that predict whether a given loan will be fully paid by the borrower as well as regressors that predict the annualized net return from investment in a given loan 
1	149	9794	with the rising popularity of peer to peer lending platforms in recent years investors now have easy access to this alternative investment asset class by lending money to individual borrowers through platforms such as lendingclub prosper marketplace and upstart or to small businesses through funding circle the process starts with borrowers submitting loan applications to the platform which performs credit reviews and either approves or denies each application the platform also uses a proprietary model to determine the interest rate of approved loans based on the credit worthiness of borrowers approved loans are then listed on the platform for investor funding 	finally we simulate and evaluate a simple loan selection strategy by investing in loans that pass a certain regressor prediction threshold 
0	149	9795	there have been many studies on classification models predicting lendingclub loan default chang et al tsai et al 	there have been many studies on classification models predicting lendingclub loan default 
0	149	9796	there have been many studies on classification models predicting lendingclub loan default chang et al tsai et al 	chang et al 
0	149	9797	there have been many studies on classification models predicting lendingclub loan default chang et al tsai et al 	tsai et al 
1	149	9798	there have been many studies on classification models predicting lendingclub loan default chang et al tsai et al 	in addition to classification models that predict loan default gutierrez and mathieson pujun et al 
1	149	9799	we worked with public dataset published by lending club	we worked with public dataset published by lending club
1	149	9800	columns with empty values for most of the rows as well as columns with the same values across all rows are dropped in order to have a cleaner dataset free form text columns are also dropped because we posited that these fields would have more noise and are better tackled at a later stage when we have better understanding of the problem for features with missing values they are categorized into three cases and treated differently mean set zero set and max set for mean set fields we took the average of the non empty values 	columns with empty values for most of the rows as well as columns with the same values across all rows are dropped in order to have a cleaner dataset 
1	149	9801	columns with empty values for most of the rows as well as columns with the same values across all rows are dropped in order to have a cleaner dataset free form text columns are also dropped because we posited that these fields would have more noise and are better tackled at a later stage when we have better understanding of the problem for features with missing values they are categorized into three cases and treated differently mean set zero set and max set for mean set fields we took the average of the non empty values 	free form text columns are also dropped because we posited that these fields would have more noise and are better tackled at a later stage when we have better understanding of the problem for features with missing values they are categorized into three cases and treated differently mean set zero set and max set 
0	149	9802	columns with empty values for most of the rows as well as columns with the same values across all rows are dropped in order to have a cleaner dataset free form text columns are also dropped because we posited that these fields would have more noise and are better tackled at a later stage when we have better understanding of the problem for features with missing values they are categorized into three cases and treated differently mean set zero set and max set for mean set fields we took the average of the non empty values 	for mean set fields we took the average of the non empty values 
1	149	9803	columns with empty values for most of the rows as well as columns with the same values across all rows are dropped in order to have a cleaner dataset free form text columns are also dropped because we posited that these fields would have more noise and are better tackled at a later stage when we have better understanding of the problem for features with missing values they are categorized into three cases and treated differently mean set zero set and max set for mean set fields we took the average of the non empty values 	one such example is debt to income ratio dti borrowers with lower dti likely have lower risks compared to those with higher dtis 
1	149	9804	columns with empty values for most of the rows as well as columns with the same values across all rows are dropped in order to have a cleaner dataset free form text columns are also dropped because we posited that these fields would have more noise and are better tackled at a later stage when we have better understanding of the problem for features with missing values they are categorized into three cases and treated differently mean set zero set and max set for mean set fields we took the average of the non empty values 	for loan applicants missing dti information it is unreasonable to reward them by assigning zero dti hence taking average is a good starting point 
1	149	9805	columns with empty values for most of the rows as well as columns with the same values across all rows are dropped in order to have a cleaner dataset free form text columns are also dropped because we posited that these fields would have more noise and are better tackled at a later stage when we have better understanding of the problem for features with missing values they are categorized into three cases and treated differently mean set zero set and max set for mean set fields we took the average of the non empty values 	in the case of max set missing values are replaced with a constant factor multiplied with the maximum value in that column 
0	149	9806	columns with empty values for most of the rows as well as columns with the same values across all rows are dropped in order to have a cleaner dataset free form text columns are also dropped because we posited that these fields would have more noise and are better tackled at a later stage when we have better understanding of the problem for features with missing values they are categorized into three cases and treated differently mean set zero set and max set for mean set fields we took the average of the non empty values 	for instance if the data for the number of months since last delinquency is missing it would be unfair to punish the applicants by assigning zero for missing data 
1	149	9807	columns with empty values for most of the rows as well as columns with the same values across all rows are dropped in order to have a cleaner dataset free form text columns are also dropped because we posited that these fields would have more noise and are better tackled at a later stage when we have better understanding of the problem for features with missing values they are categorized into three cases and treated differently mean set zero set and max set for mean set fields we took the average of the non empty values 	finally zeros are given for zero set which we believe would be a neutral replacement for the missing data categorical features such as obfuscated zipcode e g 
0	149	9808	columns with empty values for most of the rows as well as columns with the same values across all rows are dropped in order to have a cleaner dataset free form text columns are also dropped because we posited that these fields would have more noise and are better tackled at a later stage when we have better understanding of the problem for features with missing values they are categorized into three cases and treated differently mean set zero set and max set for mean set fields we took the average of the non empty values 	 940xx are replaced with their one hot representations 
0	149	9809	columns with empty values for most of the rows as well as columns with the same values across all rows are dropped in order to have a cleaner dataset free form text columns are also dropped because we posited that these fields would have more noise and are better tackled at a later stage when we have better understanding of the problem for features with missing values they are categorized into three cases and treated differently mean set zero set and max set for mean set fields we took the average of the non empty values 	features with date values are converted into the number of days since epoch 
1	149	9810	columns with empty values for most of the rows as well as columns with the same values across all rows are dropped in order to have a cleaner dataset free form text columns are also dropped because we posited that these fields would have more noise and are better tackled at a later stage when we have better understanding of the problem for features with missing values they are categorized into three cases and treated differently mean set zero set and max set for mean set fields we took the average of the non empty values 	normalization is then performed at the end on all features so they have zero mean and one standard deviation after the above preprocessing we ended up with 1 097 features 
1	149	9811	columns with empty values for most of the rows as well as columns with the same values across all rows are dropped in order to have a cleaner dataset free form text columns are also dropped because we posited that these fields would have more noise and are better tackled at a later stage when we have better understanding of the problem for features with missing values they are categorized into three cases and treated differently mean set zero set and max set for mean set fields we took the average of the non empty values 	we then ran pca on the dataset with the hope to further reduce feature size 
1	149	9812	columns with empty values for most of the rows as well as columns with the same values across all rows are dropped in order to have a cleaner dataset free form text columns are also dropped because we posited that these fields would have more noise and are better tackled at a later stage when we have better understanding of the problem for features with missing values they are categorized into three cases and treated differently mean set zero set and max set for mean set fields we took the average of the non empty values 	unfortunately the 95 variance threshold corresponds to around 900 features which is close to 95 of the total number of features and therefore means that we cannot significantly reduce the feature size without sacrificing variances see
1	149	9813	for classification model both default and charged off are assigned label 0 and fully paid is assigned label 1 for regression model we use annualized return rate calculated from loan amount total payment made by the borrower and the time interval between loan initiation and the date of last payment 	for classification model both default and charged off are assigned label 0 and fully paid is assigned label 1 
1	149	9814	for classification model both default and charged off are assigned label 0 and fully paid is assigned label 1 for regression model we use annualized return rate calculated from loan amount total payment made by the borrower and the time interval between loan initiation and the date of last payment 	for regression model we use annualized return rate calculated from loan amount total payment made by the borrower and the time interval between loan initiation and the date of last payment 
1	149	9815	our classification goal is to predict which class the loan belongs to either default or fully paid in the following sections we will share and discuss our experiments using logistic regression neutral networks and random forest for classification problem for metrics to evaluate classification performance we use confusion matrix whose columns represent predicted values and rows represent true values 	our classification goal is to predict which class the loan belongs to either default or fully paid 
1	149	9816	our classification goal is to predict which class the loan belongs to either default or fully paid in the following sections we will share and discuss our experiments using logistic regression neutral networks and random forest for classification problem for metrics to evaluate classification performance we use confusion matrix whose columns represent predicted values and rows represent true values 	in the following sections we will share and discuss our experiments using logistic regression neutral networks and random forest for classification problem 
1	149	9817	our classification goal is to predict which class the loan belongs to either default or fully paid in the following sections we will share and discuss our experiments using logistic regression neutral networks and random forest for classification problem for metrics to evaluate classification performance we use confusion matrix whose columns represent predicted values and rows represent true values 	for metrics to evaluate classification performance we use confusion matrix whose columns represent predicted values and rows represent true values 
1	149	9818	our classification goal is to predict which class the loan belongs to either default or fully paid in the following sections we will share and discuss our experiments using logistic regression neutral networks and random forest for classification problem for metrics to evaluate classification performance we use confusion matrix whose columns represent predicted values and rows represent true values 	we also measure precision recall f1 score the harmonic mean of precision and recall and weighted average as defined to derive optimal parameters the model iteratively updates weights by minimizing the negative log likelihood with l2 regularizationto tackle the class imbalance problem only 19 of our dataset are negative examples we used balanced weight for class labels which is inversely proportional to class frequencies in the input data n samples total n classes label count after running logistic regression with the above setting for a maximum of 1000 iterations we arrived at the following results as we can see logistic regression is doing fairly well compared to naive models that blindly predict positive for all examples or randomly guess positive and negative with 50 chance 
0	149	9819	our classification goal is to predict which class the loan belongs to either default or fully paid in the following sections we will share and discuss our experiments using logistic regression neutral networks and random forest for classification problem for metrics to evaluate classification performance we use confusion matrix whose columns represent predicted values and rows represent true values 	thanks to l2 regularization we did not observe overfitting issues 
0	149	9820	our classification goal is to predict which class the loan belongs to either default or fully paid in the following sections we will share and discuss our experiments using logistic regression neutral networks and random forest for classification problem for metrics to evaluate classification performance we use confusion matrix whose columns represent predicted values and rows represent true values 	one thing that we noticed and would like to improve upon is the precision and recall for negative class 
1	149	9821	our classification goal is to predict which class the loan belongs to either default or fully paid in the following sections we will share and discuss our experiments using logistic regression neutral networks and random forest for classification problem for metrics to evaluate classification performance we use confusion matrix whose columns represent predicted values and rows represent true values 	although we used balanced class weights to offset data imbalance the prediction precision is only slightly better than randomly guessing 
1	149	9822	our classification goal is to predict which class the loan belongs to either default or fully paid in the following sections we will share and discuss our experiments using logistic regression neutral networks and random forest for classification problem for metrics to evaluate classification performance we use confusion matrix whose columns represent predicted values and rows represent true values 	therefore we suspect there may be non linear relationships in the dataset that is not learned by logistic regression which leads to our exploration with neural network next 
0	149	9823	we constructed a fully connected neural network with 4 hidden layers of shape j the final output of the network uses cross entropy log loss as loss function to arrive at optimal parameters the model iteratively updates weights within each layer using gradient descentbased solver with a mini batch size of 200 learning rate of 0 001 and l2 regularization penalty of 0 0001 we obtained the following results training set result the model has high variance and is suffering from overfitting compared with the logistic regression model this neural network model achieves a better weighted precision at the expense of weighted recall and the difference between precision and recall is less polarized compared to that of the logistic regression 	we constructed a fully connected neural network with 4 hidden layers of shape j 
1	149	9824	we constructed a fully connected neural network with 4 hidden layers of shape j the final output of the network uses cross entropy log loss as loss function to arrive at optimal parameters the model iteratively updates weights within each layer using gradient descentbased solver with a mini batch size of 200 learning rate of 0 001 and l2 regularization penalty of 0 0001 we obtained the following results training set result the model has high variance and is suffering from overfitting compared with the logistic regression model this neural network model achieves a better weighted precision at the expense of weighted recall and the difference between precision and recall is less polarized compared to that of the logistic regression 	the final output of the network uses cross entropy log loss as loss function to arrive at optimal parameters the model iteratively updates weights within each layer using gradient descentbased solver with a mini batch size of 200 learning rate of 0 001 and l2 regularization penalty of 0 0001 we obtained the following results training set result the model has high variance and is suffering from overfitting 
1	149	9825	we constructed a fully connected neural network with 4 hidden layers of shape j the final output of the network uses cross entropy log loss as loss function to arrive at optimal parameters the model iteratively updates weights within each layer using gradient descentbased solver with a mini batch size of 200 learning rate of 0 001 and l2 regularization penalty of 0 0001 we obtained the following results training set result the model has high variance and is suffering from overfitting compared with the logistic regression model this neural network model achieves a better weighted precision at the expense of weighted recall and the difference between precision and recall is less polarized compared to that of the logistic regression 	compared with the logistic regression model this neural network model achieves a better weighted precision at the expense of weighted recall and the difference between precision and recall is less polarized compared to that of the logistic regression 
1	149	9826	random forest classifier is one of the tree ensemble methods that make decision splits using a random subset of features and combine the output of multiple weak classifiers to derive a strong classifier of lower variance at the cost of higher bias we started off our venture into random forest with 200 trees using gini loss 1 1 j 0 p 2 j decision splits are based on at most 50 features to reduce variance after training we reached the following result although the performance is on par with neural network and logistic regression random forest s overfitting problem is much more prominent than any other models even after restricting the maximum number of features considered for decision splits to 50 	random forest classifier is one of the tree ensemble methods that make decision splits using a random subset of features and combine the output of multiple weak classifiers to derive a strong classifier of lower variance at the cost of higher bias we started off our venture into random forest with 200 trees using gini loss 1 1 j 0 p 2 j 
0	149	9827	random forest classifier is one of the tree ensemble methods that make decision splits using a random subset of features and combine the output of multiple weak classifiers to derive a strong classifier of lower variance at the cost of higher bias we started off our venture into random forest with 200 trees using gini loss 1 1 j 0 p 2 j decision splits are based on at most 50 features to reduce variance after training we reached the following result although the performance is on par with neural network and logistic regression random forest s overfitting problem is much more prominent than any other models even after restricting the maximum number of features considered for decision splits to 50 	decision splits are based on at most 50 features to reduce variance 
1	149	9828	random forest classifier is one of the tree ensemble methods that make decision splits using a random subset of features and combine the output of multiple weak classifiers to derive a strong classifier of lower variance at the cost of higher bias we started off our venture into random forest with 200 trees using gini loss 1 1 j 0 p 2 j decision splits are based on at most 50 features to reduce variance after training we reached the following result although the performance is on par with neural network and logistic regression random forest s overfitting problem is much more prominent than any other models even after restricting the maximum number of features considered for decision splits to 50 	after training we reached the following result although the performance is on par with neural network and logistic regression random forest s overfitting problem is much more prominent than any other models even after restricting the maximum number of features considered for decision splits to 50 
1	149	9829	based on our explorations with logistic regression neural network and random forest we are able to achieve weighted average of 0 89 for both precision and recall more specifically our classification results appear to be better than the works done by the previous project	based on our explorations with logistic regression neural network and random forest we are able to achieve weighted average of 0 89 for both precision and recall 
0	149	9830	based on our explorations with logistic regression neural network and random forest we are able to achieve weighted average of 0 89 for both precision and recall more specifically our classification results appear to be better than the works done by the previous project	more specifically our classification results appear to be better than the works done by the previous project
1	149	9831	we strive to predict the investment return if we were to invest in a given loan our goal is to build regression models that predict the net annualized return nar of a given loan in a way similar to how lendingclub calculates nar for investors where x la is the loan amount x t p is total payment made by the borrower and d is the number of days between loan funding and date of last payment we evaluate regression models in terms of mean square error mse and coefficient of determination r 2 is the mean of the true labels the coefficient of determination tells us how much variability of the true nars can be explained by the model 	we strive to predict the investment return if we were to invest in a given loan 
1	149	9832	we strive to predict the investment return if we were to invest in a given loan our goal is to build regression models that predict the net annualized return nar of a given loan in a way similar to how lendingclub calculates nar for investors where x la is the loan amount x t p is total payment made by the borrower and d is the number of days between loan funding and date of last payment we evaluate regression models in terms of mean square error mse and coefficient of determination r 2 is the mean of the true labels the coefficient of determination tells us how much variability of the true nars can be explained by the model 	our goal is to build regression models that predict the net annualized return nar of a given loan in a way similar to how lendingclub calculates nar for investors where x la is the loan amount x t p is total payment made by the borrower and d is the number of days between loan funding and date of last payment we evaluate regression models in terms of mean square error mse and coefficient of determination r 2 is the mean of the true labels 
1	149	9833	we strive to predict the investment return if we were to invest in a given loan our goal is to build regression models that predict the net annualized return nar of a given loan in a way similar to how lendingclub calculates nar for investors where x la is the loan amount x t p is total payment made by the borrower and d is the number of days between loan funding and date of last payment we evaluate regression models in terms of mean square error mse and coefficient of determination r 2 is the mean of the true labels the coefficient of determination tells us how much variability of the true nars can be explained by the model 	the coefficient of determination tells us how much variability of the true nars can be explained by the model 
1	149	9834	the goal of linear regression is to find a linear hyperplane that minimizes the ordinary least squares specifically it finds parameters that minimizes	the goal of linear regression is to find a linear hyperplane that minimizes the ordinary least squares 
0	149	9835	the goal of linear regression is to find a linear hyperplane that minimizes the ordinary least squares specifically it finds parameters that minimizes	specifically it finds parameters that minimizes
1	149	9836	split mse r 2 train 0 040 0 243 test 5 014 9 494 10 22the extremely skewed mse and r 2 values on the test set clearly indicate a high variance problem of the model which overfits the training examples to rectify this we employ l2 regularization in our next model 	split mse r 2 train 0 040 0 243 test 5 014 9 494 10 22the extremely skewed mse and r 2 values on the test set clearly indicate a high variance problem of the model which overfits the training examples 
1	149	9837	split mse r 2 train 0 040 0 243 test 5 014 9 494 10 22the extremely skewed mse and r 2 values on the test set clearly indicate a high variance problem of the model which overfits the training examples to rectify this we employ l2 regularization in our next model 	to rectify this we employ l2 regularization in our next model 
1	149	9838	ridge regression adds an l2 regularization term to the cost function of linear regressionbut otherwise works the same way as linear regression performance of ridge regression with 1 split mse r 2 train 0 040 0 243 test 0 040 0 238as expected l2 regularization mitigated the problem of overfitting giving similar metrics for both train and test sets r 2 0 24 means that 24 of the nar s variability can be explained by the ridge regression model we next try nonlinear models to further decrease mse and increase r 2 	ridge regression adds an l2 regularization term to the cost function of linear regressionbut otherwise works the same way as linear regression performance of ridge regression with 1 split mse r 2 train 0 040 0 243 test 0 040 0 238as expected l2 regularization mitigated the problem of overfitting giving similar metrics for both train and test sets 
1	149	9839	ridge regression adds an l2 regularization term to the cost function of linear regressionbut otherwise works the same way as linear regression performance of ridge regression with 1 split mse r 2 train 0 040 0 243 test 0 040 0 238as expected l2 regularization mitigated the problem of overfitting giving similar metrics for both train and test sets r 2 0 24 means that 24 of the nar s variability can be explained by the ridge regression model we next try nonlinear models to further decrease mse and increase r 2 	r 2 0 24 means that 24 of the nar s variability can be explained by the ridge regression model 
1	149	9840	ridge regression adds an l2 regularization term to the cost function of linear regressionbut otherwise works the same way as linear regression performance of ridge regression with 1 split mse r 2 train 0 040 0 243 test 0 040 0 238as expected l2 regularization mitigated the problem of overfitting giving similar metrics for both train and test sets r 2 0 24 means that 24 of the nar s variability can be explained by the ridge regression model we next try nonlinear models to further decrease mse and increase r 2 	we next try nonlinear models to further decrease mse and increase r 2 
1	149	9841	the fully connected neural network regression model is very similar to the classifier described earlier in section v b the only difference is that all neurons use the relu activation function f x max 0 x and the neural network tries to minimize the squared loss on the training set we used the adam stochastic gradient based optimizer we see that the neural network regressor performs much better than ridge regression thanks to its ability to model non linear relationships 	the fully connected neural network regression model is very similar to the classifier described earlier in section v b 
1	149	9842	the fully connected neural network regression model is very similar to the classifier described earlier in section v b the only difference is that all neurons use the relu activation function f x max 0 x and the neural network tries to minimize the squared loss on the training set we used the adam stochastic gradient based optimizer we see that the neural network regressor performs much better than ridge regression thanks to its ability to model non linear relationships 	the only difference is that all neurons use the relu activation function f x max 0 x and the neural network tries to minimize the squared loss on the training set we used the adam stochastic gradient based optimizer we see that the neural network regressor performs much better than ridge regression thanks to its ability to model non linear relationships 
1	149	9843	a decision tree regression model infers decision rules from example features by finding a feature split for each non leaf node that maximizes the variance reduction as measured by mse the mean of leaf node example labels is the output of the decision tree regressor decision trees tend to overfit especially when the tree is deep and leaf nodes comprise too few examples limiting the maximum depth or the minimum leaf node examples not only reduces overfitting but also speeds up training significantly as random forest model builds numerous decision trees before taking the average of their predictions specifically random forest regressor repeatedly builds decision trees on a bootstrap sample drawn from the training set and considers a random subset of features as candidates when finding an optimal split 	a decision tree regression model infers decision rules from example features by finding a feature split for each non leaf node that maximizes the variance reduction as measured by mse 
1	149	9844	a decision tree regression model infers decision rules from example features by finding a feature split for each non leaf node that maximizes the variance reduction as measured by mse the mean of leaf node example labels is the output of the decision tree regressor decision trees tend to overfit especially when the tree is deep and leaf nodes comprise too few examples limiting the maximum depth or the minimum leaf node examples not only reduces overfitting but also speeds up training significantly as random forest model builds numerous decision trees before taking the average of their predictions specifically random forest regressor repeatedly builds decision trees on a bootstrap sample drawn from the training set and considers a random subset of features as candidates when finding an optimal split 	the mean of leaf node example labels is the output of the decision tree regressor decision trees tend to overfit especially when the tree is deep and leaf nodes comprise too few examples 
1	149	9845	a decision tree regression model infers decision rules from example features by finding a feature split for each non leaf node that maximizes the variance reduction as measured by mse the mean of leaf node example labels is the output of the decision tree regressor decision trees tend to overfit especially when the tree is deep and leaf nodes comprise too few examples limiting the maximum depth or the minimum leaf node examples not only reduces overfitting but also speeds up training significantly as random forest model builds numerous decision trees before taking the average of their predictions specifically random forest regressor repeatedly builds decision trees on a bootstrap sample drawn from the training set and considers a random subset of features as candidates when finding an optimal split 	limiting the maximum depth or the minimum leaf node examples not only reduces overfitting but also speeds up training significantly as random forest model builds numerous decision trees before taking the average of their predictions specifically random forest regressor repeatedly builds decision trees on a bootstrap sample drawn from the training set and considers a random subset of features as candidates when finding an optimal split 
1	149	9846	a decision tree regression model infers decision rules from example features by finding a feature split for each non leaf node that maximizes the variance reduction as measured by mse the mean of leaf node example labels is the output of the decision tree regressor decision trees tend to overfit especially when the tree is deep and leaf nodes comprise too few examples limiting the maximum depth or the minimum leaf node examples not only reduces overfitting but also speeds up training significantly as random forest model builds numerous decision trees before taking the average of their predictions specifically random forest regressor repeatedly builds decision trees on a bootstrap sample drawn from the training set and considers a random subset of features as candidates when finding an optimal split 	from these results we see that as we allow the decision trees to grow deeper bias increases while variance decreases 
1	149	9847	a decision tree regression model infers decision rules from example features by finding a feature split for each non leaf node that maximizes the variance reduction as measured by mse the mean of leaf node example labels is the output of the decision tree regressor decision trees tend to overfit especially when the tree is deep and leaf nodes comprise too few examples limiting the maximum depth or the minimum leaf node examples not only reduces overfitting but also speeds up training significantly as random forest model builds numerous decision trees before taking the average of their predictions specifically random forest regressor repeatedly builds decision trees on a bootstrap sample drawn from the training set and considers a random subset of features as candidates when finding an optimal split 	the performance of random forest regressor beats both ridge regression and neural network likely due to the fact that decision trees are able to capture very nuanced and nonlinear relationships 
1	149	9848	our best random forest regressor achieves a root mse of 0 036 0 19 on the test set which implies that the predicted nar is estimated to differ from the true nar by 0 19 while this may appear very large at first glance the model can actually be very useful in formulating a loan selection strategy loan defaults usually happen soon after loan funding and the chance of default decreases as more payment is made 	our best random forest regressor achieves a root mse of 0 036 0 19 on the test set which implies that the predicted nar is estimated to differ from the true nar by 0 19 
1	149	9849	our best random forest regressor achieves a root mse of 0 036 0 19 on the test set which implies that the predicted nar is estimated to differ from the true nar by 0 19 while this may appear very large at first glance the model can actually be very useful in formulating a loan selection strategy loan defaults usually happen soon after loan funding and the chance of default decreases as more payment is made 	while this may appear very large at first glance the model can actually be very useful in formulating a loan selection strategy 
1	149	9850	our best random forest regressor achieves a root mse of 0 036 0 19 on the test set which implies that the predicted nar is estimated to differ from the true nar by 0 19 while this may appear very large at first glance the model can actually be very useful in formulating a loan selection strategy loan defaults usually happen soon after loan funding and the chance of default decreases as more payment is made 	loan defaults usually happen soon after loan funding and the chance of default decreases as more payment is made 
1	149	9851	our best random forest regressor achieves a root mse of 0 036 0 19 on the test set which implies that the predicted nar is estimated to differ from the true nar by 0 19 while this may appear very large at first glance the model can actually be very useful in formulating a loan selection strategy loan defaults usually happen soon after loan funding and the chance of default decreases as more payment is made 	as a result most true nars of defaulted loans are well below 0 5 so the model can still very accurately tell us that investing in loans like these likely result in losses in light of this we experimented with the strategy of investing in loans with model nar predictions higher than a reasonable threshold m 0 
1	149	9852	our best random forest regressor achieves a root mse of 0 036 0 19 on the test set which implies that the predicted nar is estimated to differ from the true nar by 0 19 while this may appear very large at first glance the model can actually be very useful in formulating a loan selection strategy loan defaults usually happen soon after loan funding and the chance of default decreases as more payment is made 	intuitively the threshold m can serve as a parameter investors can tune according to their investment account size the bigger m is the more stringent the loan selection is so less amount of money can be invested but hopefully the annualized return will be higher due to investing in loans more selectively in order to determine a reasonable range of values for m we rank the training set examples by model predictions from high to low 
1	149	9853	our best random forest regressor achieves a root mse of 0 036 0 19 on the test set which implies that the predicted nar is estimated to differ from the true nar by 0 19 while this may appear very large at first glance the model can actually be very useful in formulating a loan selection strategy loan defaults usually happen soon after loan funding and the chance of default decreases as more payment is made 	for a specific threshold m 0 132 on both training and test set the strategy yields an annualized return of 15 with 1 7 loans picked and invested 
1	149	9854	comparing our models with those from related work ours have better precision recall and are more practical in terms of enabling implementable investment strategies in the case of classification models random forest achieved 0 89 weighted average precision and recall but it is also important to note that the random forest and neural network models do have higher variance than desired and have space for improvement 	comparing our models with those from related work ours have better precision recall and are more practical in terms of enabling implementable investment strategies 
1	149	9855	comparing our models with those from related work ours have better precision recall and are more practical in terms of enabling implementable investment strategies in the case of classification models random forest achieved 0 89 weighted average precision and recall but it is also important to note that the random forest and neural network models do have higher variance than desired and have space for improvement 	in the case of classification models random forest achieved 0 89 weighted average precision and recall 
1	149	9856	comparing our models with those from related work ours have better precision recall and are more practical in terms of enabling implementable investment strategies in the case of classification models random forest achieved 0 89 weighted average precision and recall but it is also important to note that the random forest and neural network models do have higher variance than desired and have space for improvement 	but it is also important to note that the random forest and neural network models do have higher variance than desired and have space for improvement 
1	149	9857	comparing our models with those from related work ours have better precision recall and are more practical in terms of enabling implementable investment strategies in the case of classification models random forest achieved 0 89 weighted average precision and recall but it is also important to note that the random forest and neural network models do have higher variance than desired and have space for improvement 	for the regression counterpart random forest is able to attain 0 315 coefficient of determination and to deliver predictions that lead to a profitable and actionable loan selection strategy in the sense that the return rate is higher than s p 500 s 10 annualized return for the past 90 years
1	149	9858	we obtained a regression prediction threshold based on the training set and simulated the strategy on the test set both sets comprise loans initiated within the same periods we worked with a 70 training and 30 test split for simplicity in this project the absence of a development set didn t afford us much opportunity to tune the hyperparameters of our models such as the number of decision trees to use in random forest models and the number of hidden layers and neurons of each layer in neural network models 	we obtained a regression prediction threshold based on the training set and simulated the strategy on the test set 
1	149	9859	we obtained a regression prediction threshold based on the training set and simulated the strategy on the test set both sets comprise loans initiated within the same periods we worked with a 70 training and 30 test split for simplicity in this project the absence of a development set didn t afford us much opportunity to tune the hyperparameters of our models such as the number of decision trees to use in random forest models and the number of hidden layers and neurons of each layer in neural network models 	both sets comprise loans initiated within the same periods we worked with a 70 training and 30 test split for simplicity in this project 
1	149	9860	we obtained a regression prediction threshold based on the training set and simulated the strategy on the test set both sets comprise loans initiated within the same periods we worked with a 70 training and 30 test split for simplicity in this project the absence of a development set didn t afford us much opportunity to tune the hyperparameters of our models such as the number of decision trees to use in random forest models and the number of hidden layers and neurons of each layer in neural network models 	the absence of a development set didn t afford us much opportunity to tune the hyperparameters of our models such as the number of decision trees to use in random forest models and the number of hidden layers and neurons of each layer in neural network models 
1	149	9861	we obtained a regression prediction threshold based on the training set and simulated the strategy on the test set both sets comprise loans initiated within the same periods we worked with a 70 training and 30 test split for simplicity in this project the absence of a development set didn t afford us much opportunity to tune the hyperparameters of our models such as the number of decision trees to use in random forest models and the number of hidden layers and neurons of each layer in neural network models 	having a small development set would enable us to tune some hyper parameters quickly to help improve model performance metrics there are definitely factors that contribute to default not captured by features in our dataset 
1	149	9862	we obtained a regression prediction threshold based on the training set and simulated the strategy on the test set both sets comprise loans initiated within the same periods we worked with a 70 training and 30 test split for simplicity in this project the absence of a development set didn t afford us much opportunity to tune the hyperparameters of our models such as the number of decision trees to use in random forest models and the number of hidden layers and neurons of each layer in neural network models 	we can add external features such as macroeconomic metrics that have been historically correlated to bond default rate 
1	149	9863	we obtained a regression prediction threshold based on the training set and simulated the strategy on the test set both sets comprise loans initiated within the same periods we worked with a 70 training and 30 test split for simplicity in this project the absence of a development set didn t afford us much opportunity to tune the hyperparameters of our models such as the number of decision trees to use in random forest models and the number of hidden layers and neurons of each layer in neural network models 	for categorical features like employment title we can join them with signals such as average income by industry similar to what chang et al 
0	149	9864	we obtained a regression prediction threshold based on the training set and simulated the strategy on the test set both sets comprise loans initiated within the same periods we worked with a 70 training and 30 test split for simplicity in this project the absence of a development set didn t afford us much opportunity to tune the hyperparameters of our models such as the number of decision trees to use in random forest models and the number of hidden layers and neurons of each layer in neural network models 	we can also make better use of existing features in the lendingclub dataset 
0	149	9865	we obtained a regression prediction threshold based on the training set and simulated the strategy on the test set both sets comprise loans initiated within the same periods we worked with a 70 training and 30 test split for simplicity in this project the absence of a development set didn t afford us much opportunity to tune the hyperparameters of our models such as the number of decision trees to use in random forest models and the number of hidden layers and neurons of each layer in neural network models 	one example is loan description which the borrower enters at the time of loan application 
1	149	9866	we obtained a regression prediction threshold based on the training set and simulated the strategy on the test set both sets comprise loans initiated within the same periods we worked with a 70 training and 30 test split for simplicity in this project the absence of a development set didn t afford us much opportunity to tune the hyperparameters of our models such as the number of decision trees to use in random forest models and the number of hidden layers and neurons of each layer in neural network models 	instead of dropping such freeform features we can try applying some statistical natural language processing techniques such as tf idf as chang et al 
0	149	9867	we obtained a regression prediction threshold based on the training set and simulated the strategy on the test set both sets comprise loans initiated within the same periods we worked with a 70 training and 30 test split for simplicity in this project the absence of a development set didn t afford us much opportunity to tune the hyperparameters of our models such as the number of decision trees to use in random forest models and the number of hidden layers and neurons of each layer in neural network models 	finally we notice that lendingclub also publishes declined loan datasets
0	149	9868	the two of us paired up on all components of this project including dataset cleaning feature engineering model formulation evaluation and the write up of this report and the poster codebase https goo gl sxf1rm	the two of us paired up on all components of this project including dataset cleaning feature engineering model formulation evaluation and the write up of this report and the poster codebase https goo gl sxf1rm
1	150	9869	mild traumatic brain injury mtbi more commonly known as concussion has become a serious health concern with recent increase in media coverage on the long term health issues of professional athletes and military personnel acute symptoms include dizziness confusion and personality changes which can remain for days or even years after injury according to the cdc contact sports such as football are one of the leading causes of mtbi in these sports mtbi is diagnosed by a sideline clinician through subjective evaluation of symptoms and neurological testing 	mild traumatic brain injury mtbi more commonly known as concussion has become a serious health concern with recent increase in media coverage on the long term health issues of professional athletes and military personnel 
1	150	9870	mild traumatic brain injury mtbi more commonly known as concussion has become a serious health concern with recent increase in media coverage on the long term health issues of professional athletes and military personnel acute symptoms include dizziness confusion and personality changes which can remain for days or even years after injury according to the cdc contact sports such as football are one of the leading causes of mtbi in these sports mtbi is diagnosed by a sideline clinician through subjective evaluation of symptoms and neurological testing 	acute symptoms include dizziness confusion and personality changes which can remain for days or even years after injury according to the cdc contact sports such as football are one of the leading causes of mtbi 
1	150	9871	mild traumatic brain injury mtbi more commonly known as concussion has become a serious health concern with recent increase in media coverage on the long term health issues of professional athletes and military personnel acute symptoms include dizziness confusion and personality changes which can remain for days or even years after injury according to the cdc contact sports such as football are one of the leading causes of mtbi in these sports mtbi is diagnosed by a sideline clinician through subjective evaluation of symptoms and neurological testing 	in these sports mtbi is diagnosed by a sideline clinician through subjective evaluation of symptoms and neurological testing 
1	150	9872	mild traumatic brain injury mtbi more commonly known as concussion has become a serious health concern with recent increase in media coverage on the long term health issues of professional athletes and military personnel acute symptoms include dizziness confusion and personality changes which can remain for days or even years after injury according to the cdc contact sports such as football are one of the leading causes of mtbi in these sports mtbi is diagnosed by a sideline clinician through subjective evaluation of symptoms and neurological testing 	because of the large variance of symptoms within different individuals and the pressure of athletes to return to play mtbi can often be missed by these tests in this project our goal is to train a neural network which will automatically extract relevant features to classify between real impacts and false positives 
0	150	9873	mild traumatic brain injury mtbi more commonly known as concussion has become a serious health concern with recent increase in media coverage on the long term health issues of professional athletes and military personnel acute symptoms include dizziness confusion and personality changes which can remain for days or even years after injury according to the cdc contact sports such as football are one of the leading causes of mtbi in these sports mtbi is diagnosed by a sideline clinician through subjective evaluation of symptoms and neurological testing 	the input to our algorithm is mouthguard time series data 
1	150	9874	currently there are a number of sensor systems used for measuring head impact kinematics in contact sports many of these systems use a simple linear acceleration threshold for differentiating impacts and non impacts however this leaves the device prone to a large number of false positives many companies and research groups are developing proprietary algorithms for detecting impacts but little has been published validating their accuracy to the best of our knowledge only one study has attempted to use a neural network algorithm for detecting head impacts and non impacts from kinematic sensor data this study used a simple net with a single fully connected layer and only achieved 47 specificity and 88 sensitivity on their dataset of soccer athletes	currently there are a number of sensor systems used for measuring head impact kinematics in contact sports 
1	150	9875	currently there are a number of sensor systems used for measuring head impact kinematics in contact sports many of these systems use a simple linear acceleration threshold for differentiating impacts and non impacts however this leaves the device prone to a large number of false positives many companies and research groups are developing proprietary algorithms for detecting impacts but little has been published validating their accuracy to the best of our knowledge only one study has attempted to use a neural network algorithm for detecting head impacts and non impacts from kinematic sensor data this study used a simple net with a single fully connected layer and only achieved 47 specificity and 88 sensitivity on their dataset of soccer athletes	many of these systems use a simple linear acceleration threshold for differentiating impacts and non impacts however this leaves the device prone to a large number of false positives 
1	150	9876	currently there are a number of sensor systems used for measuring head impact kinematics in contact sports many of these systems use a simple linear acceleration threshold for differentiating impacts and non impacts however this leaves the device prone to a large number of false positives many companies and research groups are developing proprietary algorithms for detecting impacts but little has been published validating their accuracy to the best of our knowledge only one study has attempted to use a neural network algorithm for detecting head impacts and non impacts from kinematic sensor data this study used a simple net with a single fully connected layer and only achieved 47 specificity and 88 sensitivity on their dataset of soccer athletes	many companies and research groups are developing proprietary algorithms for detecting impacts but little has been published validating their accuracy to the best of our knowledge only one study has attempted to use a neural network algorithm for detecting head impacts and non impacts from kinematic sensor data this study used a simple net with a single fully connected layer and only achieved 47 specificity and 88 sensitivity on their dataset of soccer athletes
0	150	9877	our dataset is 527 examples of which half are labeled real or true impact and the other half are labeled as false impacts the dataset was obtained by instrumenting stanford football athletes over the fall 2017 season with the camarillo lab instrumented mouthguard to obtain the ground truth labels for the dataset videos of each game and practice time synced with the mouthguards were analyzed according to the methodology outlined by kuo et al we split our dataset up into 70 for training and 15 for both evaluation and testing for when leveraging k fold cross validation and 70 for training and 30 for evaluation for when training our selected architecture 	our dataset is 527 examples of which half are labeled real or true impact and the other half are labeled as false impacts 
0	150	9878	our dataset is 527 examples of which half are labeled real or true impact and the other half are labeled as false impacts the dataset was obtained by instrumenting stanford football athletes over the fall 2017 season with the camarillo lab instrumented mouthguard to obtain the ground truth labels for the dataset videos of each game and practice time synced with the mouthguards were analyzed according to the methodology outlined by kuo et al we split our dataset up into 70 for training and 15 for both evaluation and testing for when leveraging k fold cross validation and 70 for training and 30 for evaluation for when training our selected architecture 	the dataset was obtained by instrumenting stanford football athletes over the fall 2017 season with the camarillo lab instrumented mouthguard 
1	150	9879	our dataset is 527 examples of which half are labeled real or true impact and the other half are labeled as false impacts the dataset was obtained by instrumenting stanford football athletes over the fall 2017 season with the camarillo lab instrumented mouthguard to obtain the ground truth labels for the dataset videos of each game and practice time synced with the mouthguards were analyzed according to the methodology outlined by kuo et al we split our dataset up into 70 for training and 15 for both evaluation and testing for when leveraging k fold cross validation and 70 for training and 30 for evaluation for when training our selected architecture 	to obtain the ground truth labels for the dataset videos of each game and practice time synced with the mouthguards were analyzed according to the methodology outlined by kuo et al we split our dataset up into 70 for training and 15 for both evaluation and testing for when leveraging k fold cross validation and 70 for training and 30 for evaluation for when training our selected architecture 
0	150	9880	our dataset is 527 examples of which half are labeled real or true impact and the other half are labeled as false impacts the dataset was obtained by instrumenting stanford football athletes over the fall 2017 season with the camarillo lab instrumented mouthguard to obtain the ground truth labels for the dataset videos of each game and practice time synced with the mouthguards were analyzed according to the methodology outlined by kuo et al we split our dataset up into 70 for training and 15 for both evaluation and testing for when leveraging k fold cross validation and 70 for training and 30 for evaluation for when training our selected architecture 	each example has dimension 199x6 comprised of 6 time traces of length 199 200 ms 
0	150	9881	our dataset is 527 examples of which half are labeled real or true impact and the other half are labeled as false impacts the dataset was obtained by instrumenting stanford football athletes over the fall 2017 season with the camarillo lab instrumented mouthguard to obtain the ground truth labels for the dataset videos of each game and practice time synced with the mouthguards were analyzed according to the methodology outlined by kuo et al we split our dataset up into 70 for training and 15 for both evaluation and testing for when leveraging k fold cross validation and 70 for training and 30 for evaluation for when training our selected architecture 	the six time traces are the linear acceleration at the head center of gravity in the x y and z axes and angular velocity of the head in the x y and z anatomical planes 
0	150	9882	our dataset is 527 examples of which half are labeled real or true impact and the other half are labeled as false impacts the dataset was obtained by instrumenting stanford football athletes over the fall 2017 season with the camarillo lab instrumented mouthguard to obtain the ground truth labels for the dataset videos of each game and practice time synced with the mouthguards were analyzed according to the methodology outlined by kuo et al we split our dataset up into 70 for training and 15 for both evaluation and testing for when leveraging k fold cross validation and 70 for training and 30 for evaluation for when training our selected architecture 	the data was sampled with a time step of 1000 hz with 50 ms recorded pre trigger and 150 ms post trigger for 299 data points 
0	150	9883	our dataset is 527 examples of which half are labeled real or true impact and the other half are labeled as false impacts the dataset was obtained by instrumenting stanford football athletes over the fall 2017 season with the camarillo lab instrumented mouthguard to obtain the ground truth labels for the dataset videos of each game and practice time synced with the mouthguards were analyzed according to the methodology outlined by kuo et al we split our dataset up into 70 for training and 15 for both evaluation and testing for when leveraging k fold cross validation and 70 for training and 30 for evaluation for when training our selected architecture 	data was pre processed using standardization by subtracting out the mean of each sensor s values and dividing by the standard deviation 
0	150	9884	a convolutional neural network is a class of deep neural networks comprised of convolutional layers in convolutional layers each sensor measurement is convolved with a weighting function w in the case of a 1d input to a 1d convolutional layer the i th product element can be found as follows where b is the bias term d is the filter width and w d a re each of the filters in the case where the input to the 1d convolution is a multi channeled such as our application where we are stacking six input signals the output of the convolution each channel is added to give the following where s is the number of input channels in our case six 	a convolutional neural network is a class of deep neural networks comprised of convolutional layers 
0	150	9885	a convolutional neural network is a class of deep neural networks comprised of convolutional layers in convolutional layers each sensor measurement is convolved with a weighting function w in the case of a 1d input to a 1d convolutional layer the i th product element can be found as follows where b is the bias term d is the filter width and w d a re each of the filters in the case where the input to the 1d convolution is a multi channeled such as our application where we are stacking six input signals the output of the convolution each channel is added to give the following where s is the number of input channels in our case six 	in convolutional layers each sensor measurement is convolved with a weighting function w in the case of a 1d input to a 1d convolutional layer the i th product element can be found as follows where b is the bias term d is the filter width and w d a re each of the filters 
0	150	9886	a convolutional neural network is a class of deep neural networks comprised of convolutional layers in convolutional layers each sensor measurement is convolved with a weighting function w in the case of a 1d input to a 1d convolutional layer the i th product element can be found as follows where b is the bias term d is the filter width and w d a re each of the filters in the case where the input to the 1d convolution is a multi channeled such as our application where we are stacking six input signals the output of the convolution each channel is added to give the following where s is the number of input channels in our case six 	in the case where the input to the 1d convolution is a multi channeled such as our application where we are stacking six input signals the output of the convolution each channel is added to give the following where s is the number of input channels in our case six 
0	150	9887	a convolutional neural network is a class of deep neural networks comprised of convolutional layers in convolutional layers each sensor measurement is convolved with a weighting function w in the case of a 1d input to a 1d convolutional layer the i th product element can be found as follows where b is the bias term d is the filter width and w d a re each of the filters in the case where the input to the 1d convolution is a multi channeled such as our application where we are stacking six input signals the output of the convolution each channel is added to give the following where s is the number of input channels in our case six 	the output of a 1d convolutional layer is a single vector 
0	150	9888	a convolutional neural network is a class of deep neural networks comprised of convolutional layers in convolutional layers each sensor measurement is convolved with a weighting function w in the case of a 1d input to a 1d convolutional layer the i th product element can be found as follows where b is the bias term d is the filter width and w d a re each of the filters in the case where the input to the 1d convolution is a multi channeled such as our application where we are stacking six input signals the output of the convolution each channel is added to give the following where s is the number of input channels in our case six 	in 2d convolutional layers this process is repeated in two dimensions providing a two dimensional output 
0	150	9889	a convolutional neural network is a class of deep neural networks comprised of convolutional layers in convolutional layers each sensor measurement is convolved with a weighting function w in the case of a 1d input to a 1d convolutional layer the i th product element can be found as follows where b is the bias term d is the filter width and w d a re each of the filters in the case where the input to the 1d convolution is a multi channeled such as our application where we are stacking six input signals the output of the convolution each channel is added to give the following where s is the number of input channels in our case six 	convolutional neural networks commonly have pooling operations which combine outputs of neuron clusters at one layer into a single neuron in the next layer 
0	150	9890	a convolutional neural network is a class of deep neural networks comprised of convolutional layers in convolutional layers each sensor measurement is convolved with a weighting function w in the case of a 1d input to a 1d convolutional layer the i th product element can be found as follows where b is the bias term d is the filter width and w d a re each of the filters in the case where the input to the 1d convolution is a multi channeled such as our application where we are stacking six input signals the output of the convolution each channel is added to give the following where s is the number of input channels in our case six 	max pooling layers use the maximum value from a specified cluster of neurons while average pooling uses the average of a specified cluster 
1	150	9891	a convolutional neural network is a class of deep neural networks comprised of convolutional layers in convolutional layers each sensor measurement is convolved with a weighting function w in the case of a 1d input to a 1d convolutional layer the i th product element can be found as follows where b is the bias term d is the filter width and w d a re each of the filters in the case where the input to the 1d convolution is a multi channeled such as our application where we are stacking six input signals the output of the convolution each channel is added to give the following where s is the number of input channels in our case six 	further dropout layers can be added to help prevent overfitting a dropout layer will randomly ignore a certain percent of the layer interconnections during training we investigated multiple different convolutional neural network architectures using keras and tensorflow written in python specifically we developed both sequential models and recursive network models 
1	150	9892	a convolutional neural network is a class of deep neural networks comprised of convolutional layers in convolutional layers each sensor measurement is convolved with a weighting function w in the case of a 1d input to a 1d convolutional layer the i th product element can be found as follows where b is the bias term d is the filter width and w d a re each of the filters in the case where the input to the 1d convolution is a multi channeled such as our application where we are stacking six input signals the output of the convolution each channel is added to give the following where s is the number of input channels in our case six 	in investigating proper model architecture we utilized the k fold cross validation technique k 10 as we knew that 527 examples is not a very large amount and gathering more data was not feasible within the scope of this project 
1	150	9893	a convolutional neural network is a class of deep neural networks comprised of convolutional layers in convolutional layers each sensor measurement is convolved with a weighting function w in the case of a 1d input to a 1d convolutional layer the i th product element can be found as follows where b is the bias term d is the filter width and w d a re each of the filters in the case where the input to the 1d convolution is a multi channeled such as our application where we are stacking six input signals the output of the convolution each channel is added to give the following where s is the number of input channels in our case six 	in training all of our networks the number of epochs was increased indefinitely until five consecutive epochs did not result in an improved evaluation binary cross entropy loss 
1	150	9894	a convolutional neural network is a class of deep neural networks comprised of convolutional layers in convolutional layers each sensor measurement is convolved with a weighting function w in the case of a 1d input to a 1d convolutional layer the i th product element can be found as follows where b is the bias term d is the filter width and w d a re each of the filters in the case where the input to the 1d convolution is a multi channeled such as our application where we are stacking six input signals the output of the convolution each channel is added to give the following where s is the number of input channels in our case six 	following completion of training the model at the end of the epoch with the lowest evaluation loss was saved and used for analysis we developed and compared two primary architectures 
0	150	9895	a convolutional neural network is a class of deep neural networks comprised of convolutional layers in convolutional layers each sensor measurement is convolved with a weighting function w in the case of a 1d input to a 1d convolutional layer the i th product element can be found as follows where b is the bias term d is the filter width and w d a re each of the filters in the case where the input to the 1d convolution is a multi channeled such as our application where we are stacking six input signals the output of the convolution each channel is added to give the following where s is the number of input channels in our case six 	the recursivenet model has the most convolutional layers as seen in
0	150	9896	in all testing the metric we optimized for was accuracy but we also performed tests on precision specificity and sensitivity the equations for the metrics are described below where tp is true positive tn is true negative fp is false positive and fn is false negative using our baseline hyperparameters in preliminary testing we found that the hiknet had comparable accuracy to recursivenet at a much lower computational cost 	in all testing the metric we optimized for was accuracy but we also performed tests on precision specificity and sensitivity 
0	150	9897	in all testing the metric we optimized for was accuracy but we also performed tests on precision specificity and sensitivity the equations for the metrics are described below where tp is true positive tn is true negative fp is false positive and fn is false negative using our baseline hyperparameters in preliminary testing we found that the hiknet had comparable accuracy to recursivenet at a much lower computational cost 	the equations for the metrics are described below where tp is true positive tn is true negative fp is false positive and fn is false negative 
0	150	9898	in all testing the metric we optimized for was accuracy but we also performed tests on precision specificity and sensitivity the equations for the metrics are described below where tp is true positive tn is true negative fp is false positive and fn is false negative using our baseline hyperparameters in preliminary testing we found that the hiknet had comparable accuracy to recursivenet at a much lower computational cost 	 using our baseline hyperparameters in preliminary testing we found that the hiknet had comparable accuracy to recursivenet at a much lower computational cost 
1	150	9899	in all testing the metric we optimized for was accuracy but we also performed tests on precision specificity and sensitivity the equations for the metrics are described below where tp is true positive tn is true negative fp is false positive and fn is false negative using our baseline hyperparameters in preliminary testing we found that the hiknet had comparable accuracy to recursivenet at a much lower computational cost 	thus we focused our hyperparameter tuning on the hiknet architecture we tuned the final hiknet using a greedy optimization scheme for number of 1d conv layers 2d conv layers and type of final layer 
1	150	9900	in all testing the metric we optimized for was accuracy but we also performed tests on precision specificity and sensitivity the equations for the metrics are described below where tp is true positive tn is true negative fp is false positive and fn is false negative using our baseline hyperparameters in preliminary testing we found that the hiknet had comparable accuracy to recursivenet at a much lower computational cost 	because our parameters were initialized to random values and convergence is highly dependent on weight initialization we also did a parameter sweep to find the optimal filter size kernel width and dropout threshold 
0	150	9901	in all testing the metric we optimized for was accuracy but we also performed tests on precision specificity and sensitivity the equations for the metrics are described below where tp is true positive tn is true negative fp is false positive and fn is false negative using our baseline hyperparameters in preliminary testing we found that the hiknet had comparable accuracy to recursivenet at a much lower computational cost 	the filter size was changed between the values of 15 and 200 the kernel width was between 0 and 50 and the dropout threshold was swept between 0 and 0 6 
1	150	9902	in all testing the metric we optimized for was accuracy but we also performed tests on precision specificity and sensitivity the equations for the metrics are described below where tp is true positive tn is true negative fp is false positive and fn is false negative using our baseline hyperparameters in preliminary testing we found that the hiknet had comparable accuracy to recursivenet at a much lower computational cost 	the optimal dropout threshold was found to be 0 4 kernel width was 15 and filter number was 150 
1	150	9903	in all testing the metric we optimized for was accuracy but we also performed tests on precision specificity and sensitivity the equations for the metrics are described below where tp is true positive tn is true negative fp is false positive and fn is false negative using our baseline hyperparameters in preliminary testing we found that the hiknet had comparable accuracy to recursivenet at a much lower computational cost 	we found that the optimal kernel width and dropout threshold for hiknet was the same for the perceptionnet the final performance metrics are summarized in
1	150	9904	conclusion future workin conclusion a low parameter neural network performed very well and achieved better performance metrics than the existing svm classifier trained on the same mouthguard time series data set we created two deep convolutional neural networks one that was recursive and one that was sequential and although both performed similarly we chose the hiknet because it had fewer parameters and a higher confidence in its ability to generalize to other kinematic time series datasets than the recursivenet based on our literature search we used a greedy optimization scheme to build the architecture of hiknet and did a parameter sweep to find the optimal filter size kernel width and dropout percent as an immediate next step we can apply our neural networks to a larger mouthguard dataset as more data is collected in the camarillo lab to further tune parameters 	conclusion future workin conclusion a low parameter neural network performed very well and achieved better performance metrics than the existing svm classifier trained on the same mouthguard time series data set 
1	150	9905	conclusion future workin conclusion a low parameter neural network performed very well and achieved better performance metrics than the existing svm classifier trained on the same mouthguard time series data set we created two deep convolutional neural networks one that was recursive and one that was sequential and although both performed similarly we chose the hiknet because it had fewer parameters and a higher confidence in its ability to generalize to other kinematic time series datasets than the recursivenet based on our literature search we used a greedy optimization scheme to build the architecture of hiknet and did a parameter sweep to find the optimal filter size kernel width and dropout percent as an immediate next step we can apply our neural networks to a larger mouthguard dataset as more data is collected in the camarillo lab to further tune parameters 	we created two deep convolutional neural networks one that was recursive and one that was sequential and although both performed similarly we chose the hiknet because it had fewer parameters and a higher confidence in its ability to generalize to other kinematic time series datasets than the recursivenet based on our literature search 
1	150	9906	conclusion future workin conclusion a low parameter neural network performed very well and achieved better performance metrics than the existing svm classifier trained on the same mouthguard time series data set we created two deep convolutional neural networks one that was recursive and one that was sequential and although both performed similarly we chose the hiknet because it had fewer parameters and a higher confidence in its ability to generalize to other kinematic time series datasets than the recursivenet based on our literature search we used a greedy optimization scheme to build the architecture of hiknet and did a parameter sweep to find the optimal filter size kernel width and dropout percent as an immediate next step we can apply our neural networks to a larger mouthguard dataset as more data is collected in the camarillo lab to further tune parameters 	we used a greedy optimization scheme to build the architecture of hiknet and did a parameter sweep to find the optimal filter size kernel width and dropout percent as an immediate next step we can apply our neural networks to a larger mouthguard dataset as more data is collected in the camarillo lab to further tune parameters 
1	150	9907	conclusion future workin conclusion a low parameter neural network performed very well and achieved better performance metrics than the existing svm classifier trained on the same mouthguard time series data set we created two deep convolutional neural networks one that was recursive and one that was sequential and although both performed similarly we chose the hiknet because it had fewer parameters and a higher confidence in its ability to generalize to other kinematic time series datasets than the recursivenet based on our literature search we used a greedy optimization scheme to build the architecture of hiknet and did a parameter sweep to find the optimal filter size kernel width and dropout percent as an immediate next step we can apply our neural networks to a larger mouthguard dataset as more data is collected in the camarillo lab to further tune parameters 	in future work we can use the same mouthguard and video impact footage to create a dataset with more specific labels i e 
0	150	9908	conclusion future workin conclusion a low parameter neural network performed very well and achieved better performance metrics than the existing svm classifier trained on the same mouthguard time series data set we created two deep convolutional neural networks one that was recursive and one that was sequential and although both performed similarly we chose the hiknet because it had fewer parameters and a higher confidence in its ability to generalize to other kinematic time series datasets than the recursivenet based on our literature search we used a greedy optimization scheme to build the architecture of hiknet and did a parameter sweep to find the optimal filter size kernel width and dropout percent as an immediate next step we can apply our neural networks to a larger mouthguard dataset as more data is collected in the camarillo lab to further tune parameters 	where the impact was located on the head body impact or no impact 
1	150	9909	conclusion future workin conclusion a low parameter neural network performed very well and achieved better performance metrics than the existing svm classifier trained on the same mouthguard time series data set we created two deep convolutional neural networks one that was recursive and one that was sequential and although both performed similarly we chose the hiknet because it had fewer parameters and a higher confidence in its ability to generalize to other kinematic time series datasets than the recursivenet based on our literature search we used a greedy optimization scheme to build the architecture of hiknet and did a parameter sweep to find the optimal filter size kernel width and dropout percent as an immediate next step we can apply our neural networks to a larger mouthguard dataset as more data is collected in the camarillo lab to further tune parameters 	using this data we could create a softmax classifier to predict whether an impact occurred and where it occurred on the head and body 
1	150	9910	conclusion future workin conclusion a low parameter neural network performed very well and achieved better performance metrics than the existing svm classifier trained on the same mouthguard time series data set we created two deep convolutional neural networks one that was recursive and one that was sequential and although both performed similarly we chose the hiknet because it had fewer parameters and a higher confidence in its ability to generalize to other kinematic time series datasets than the recursivenet based on our literature search we used a greedy optimization scheme to build the architecture of hiknet and did a parameter sweep to find the optimal filter size kernel width and dropout percent as an immediate next step we can apply our neural networks to a larger mouthguard dataset as more data is collected in the camarillo lab to further tune parameters 	lastly once more concussion data is obtained we could create a neural network that could detect whether an impact occurred and predicts if the impact resulted in a concussion or not 
0	150	9911	conclusion future workin conclusion a low parameter neural network performed very well and achieved better performance metrics than the existing svm classifier trained on the same mouthguard time series data set we created two deep convolutional neural networks one that was recursive and one that was sequential and although both performed similarly we chose the hiknet because it had fewer parameters and a higher confidence in its ability to generalize to other kinematic time series datasets than the recursivenet based on our literature search we used a greedy optimization scheme to build the architecture of hiknet and did a parameter sweep to find the optimal filter size kernel width and dropout percent as an immediate next step we can apply our neural networks to a larger mouthguard dataset as more data is collected in the camarillo lab to further tune parameters 	this would require additional data beyond just the mouthguard data such as clinical diagnoses and medical records 
1	150	9912	conclusion future workin conclusion a low parameter neural network performed very well and achieved better performance metrics than the existing svm classifier trained on the same mouthguard time series data set we created two deep convolutional neural networks one that was recursive and one that was sequential and although both performed similarly we chose the hiknet because it had fewer parameters and a higher confidence in its ability to generalize to other kinematic time series datasets than the recursivenet based on our literature search we used a greedy optimization scheme to build the architecture of hiknet and did a parameter sweep to find the optimal filter size kernel width and dropout percent as an immediate next step we can apply our neural networks to a larger mouthguard dataset as more data is collected in the camarillo lab to further tune parameters 	the ultimate goal would be to have a device that could instantly tell if an impact resulted in concussion although it may take years to obtain the dataset needed to train this classifier the performance of our network architecture gives promise that this could be possible using a similar methodology as put forth in this work 
1	150	9913	michael fanton developed hiknet neural network architecture in keras set up architecture optimization helped with statistical analyses provided background information nicholas gaudio lead the insight into keras and the model architecture setup created the recursivenet setup auto epoch stopping and saved the best epoch model conducted experiments to find the optimal filter width and filter number alissa ling preprocessed data wrote the k fold function optimized the dropout threshold lead the final poster wrote first draft of sections 9 	michael fanton developed hiknet neural network architecture in keras set up architecture optimization helped with statistical analyses provided background information nicholas gaudio lead the insight into keras and the model architecture setup created the recursivenet setup auto epoch stopping and saved the best epoch model conducted experiments to find the optimal filter width and filter number 
1	150	9914	michael fanton developed hiknet neural network architecture in keras set up architecture optimization helped with statistical analyses provided background information nicholas gaudio lead the insight into keras and the model architecture setup created the recursivenet setup auto epoch stopping and saved the best epoch model conducted experiments to find the optimal filter width and filter number alissa ling preprocessed data wrote the k fold function optimized the dropout threshold lead the final poster wrote first draft of sections 9 	alissa ling preprocessed data wrote the k fold function optimized the dropout threshold lead the final poster wrote first draft of sections 9 
0	151	9915	with the recent failure of senate bill sb 827 in california pressure is higher than ever on state politicians to better understand and respond to the increasing unaffordability of california s urban centers designed to issue more housing construction permits in high opportunity areas sb 827 was ironically crippled by its failure to explicitly acknowledge the possible gentrification externalities of new housing construction because of the astronomical and increasing cost of housing more californians live in poverty than in any other state when cost of living is accounted for one tool that academics use to design thoughtful housing policy is the gentrification early warning system i 	with the recent failure of senate bill sb 827 in california pressure is higher than ever on state politicians to better understand and respond to the increasing unaffordability of california s urban centers 
0	151	9916	with the recent failure of senate bill sb 827 in california pressure is higher than ever on state politicians to better understand and respond to the increasing unaffordability of california s urban centers designed to issue more housing construction permits in high opportunity areas sb 827 was ironically crippled by its failure to explicitly acknowledge the possible gentrification externalities of new housing construction because of the astronomical and increasing cost of housing more californians live in poverty than in any other state when cost of living is accounted for one tool that academics use to design thoughtful housing policy is the gentrification early warning system i 	designed to issue more housing construction permits in high opportunity areas sb 827 was ironically crippled by its failure to explicitly acknowledge the possible gentrification externalities of new housing construction 
1	151	9917	with the recent failure of senate bill sb 827 in california pressure is higher than ever on state politicians to better understand and respond to the increasing unaffordability of california s urban centers designed to issue more housing construction permits in high opportunity areas sb 827 was ironically crippled by its failure to explicitly acknowledge the possible gentrification externalities of new housing construction because of the astronomical and increasing cost of housing more californians live in poverty than in any other state when cost of living is accounted for one tool that academics use to design thoughtful housing policy is the gentrification early warning system i 	because of the astronomical and increasing cost of housing more californians live in poverty than in any other state when cost of living is accounted for one tool that academics use to design thoughtful housing policy is the gentrification early warning system i 
1	151	9918	with the recent failure of senate bill sb 827 in california pressure is higher than ever on state politicians to better understand and respond to the increasing unaffordability of california s urban centers designed to issue more housing construction permits in high opportunity areas sb 827 was ironically crippled by its failure to explicitly acknowledge the possible gentrification externalities of new housing construction because of the astronomical and increasing cost of housing more californians live in poverty than in any other state when cost of living is accounted for one tool that academics use to design thoughtful housing policy is the gentrification early warning system i 	using california wide census data to classify emergent gentrification and to understand the leading indicators of gentrification through feature selection ii 
1	151	9919	with the recent failure of senate bill sb 827 in california pressure is higher than ever on state politicians to better understand and respond to the increasing unaffordability of california s urban centers designed to issue more housing construction permits in high opportunity areas sb 827 was ironically crippled by its failure to explicitly acknowledge the possible gentrification externalities of new housing construction because of the astronomical and increasing cost of housing more californians live in poverty than in any other state when cost of living is accounted for one tool that academics use to design thoughtful housing policy is the gentrification early warning system i 	and modelling the state s housing market as an interconnected network to test an economic theory of how gentrification spreads specifically we use machine learning techniques primarily non parametric models such as random forests and gradient boosting to ascertain the leading indicators of gentrification at the census tract level in california 
1	151	9920	with the recent failure of senate bill sb 827 in california pressure is higher than ever on state politicians to better understand and respond to the increasing unaffordability of california s urban centers designed to issue more housing construction permits in high opportunity areas sb 827 was ironically crippled by its failure to explicitly acknowledge the possible gentrification externalities of new housing construction because of the astronomical and increasing cost of housing more californians live in poverty than in any other state when cost of living is accounted for one tool that academics use to design thoughtful housing policy is the gentrification early warning system i 	we formulate the problem as binary classification over a five year time horizon using custom designed responses to proxy for whether gentrification was observed in a community over the prediction period 
1	151	9921	we source data from american factfinder aff a public information tool produced by the united states census prior research describes gentrification in terms of either rising costs of living or displacement of the poor as income distributions shift towards affluence because gentrification occurs over a long time horizon we split the feature set around a pivot year of 2012 we compute the responses using the data from years 2012 2016 with the data from 2010 and 2011 used as features in the above formulation t 2012 and t 2016 splitting the data to forecast gentrification over a long time horizon comports with previous research to model the second response we use an imputed measurement of the inter year intra tract change in the income distribution of the tract see for each tract we compute the hellinger distance between the observed income distribution and a baseline in which all residents are perfectly affluent with probability 1 tracts with low hellinger distances tend to be high income tracts with high hellinger distances tend to be low income 	we source data from american factfinder aff a public information tool produced by the united states census prior research describes gentrification in terms of either rising costs of living or displacement of the poor as income distributions shift towards affluence because gentrification occurs over a long time horizon we split the feature set around a pivot year of 2012 we compute the responses using the data from years 2012 2016 with the data from 2010 and 2011 used as features in the above formulation t 2012 and t 2016 
1	151	9922	we source data from american factfinder aff a public information tool produced by the united states census prior research describes gentrification in terms of either rising costs of living or displacement of the poor as income distributions shift towards affluence because gentrification occurs over a long time horizon we split the feature set around a pivot year of 2012 we compute the responses using the data from years 2012 2016 with the data from 2010 and 2011 used as features in the above formulation t 2012 and t 2016 splitting the data to forecast gentrification over a long time horizon comports with previous research to model the second response we use an imputed measurement of the inter year intra tract change in the income distribution of the tract see for each tract we compute the hellinger distance between the observed income distribution and a baseline in which all residents are perfectly affluent with probability 1 tracts with low hellinger distances tend to be high income tracts with high hellinger distances tend to be low income 	splitting the data to forecast gentrification over a long time horizon comports with previous research to model the second response we use an imputed measurement of the inter year intra tract change in the income distribution of the tract see for each tract we compute the hellinger distance between the observed income distribution and a baseline in which all residents are perfectly affluent with probability 1 
0	151	9923	we source data from american factfinder aff a public information tool produced by the united states census prior research describes gentrification in terms of either rising costs of living or displacement of the poor as income distributions shift towards affluence because gentrification occurs over a long time horizon we split the feature set around a pivot year of 2012 we compute the responses using the data from years 2012 2016 with the data from 2010 and 2011 used as features in the above formulation t 2012 and t 2016 splitting the data to forecast gentrification over a long time horizon comports with previous research to model the second response we use an imputed measurement of the inter year intra tract change in the income distribution of the tract see for each tract we compute the hellinger distance between the observed income distribution and a baseline in which all residents are perfectly affluent with probability 1 tracts with low hellinger distances tend to be high income tracts with high hellinger distances tend to be low income 	tracts with low hellinger distances tend to be high income tracts with high hellinger distances tend to be low income 
0	151	9924	we source data from american factfinder aff a public information tool produced by the united states census prior research describes gentrification in terms of either rising costs of living or displacement of the poor as income distributions shift towards affluence because gentrification occurs over a long time horizon we split the feature set around a pivot year of 2012 we compute the responses using the data from years 2012 2016 with the data from 2010 and 2011 used as features in the above formulation t 2012 and t 2016 splitting the data to forecast gentrification over a long time horizon comports with previous research to model the second response we use an imputed measurement of the inter year intra tract change in the income distribution of the tract see for each tract we compute the hellinger distance between the observed income distribution and a baseline in which all residents are perfectly affluent with probability 1 tracts with low hellinger distances tend to be high income tracts with high hellinger distances tend to be low income 	finally we compute the response by taking the differences of these hellinger distances for each tract between 2012 the pivot year and 2016 
0	151	9925	we source data from american factfinder aff a public information tool produced by the united states census prior research describes gentrification in terms of either rising costs of living or displacement of the poor as income distributions shift towards affluence because gentrification occurs over a long time horizon we split the feature set around a pivot year of 2012 we compute the responses using the data from years 2012 2016 with the data from 2010 and 2011 used as features in the above formulation t 2012 and t 2016 splitting the data to forecast gentrification over a long time horizon comports with previous research to model the second response we use an imputed measurement of the inter year intra tract change in the income distribution of the tract see for each tract we compute the hellinger distance between the observed income distribution and a baseline in which all residents are perfectly affluent with probability 1 tracts with low hellinger distances tend to be high income tracts with high hellinger distances tend to be low income 	a tract that becomes more affluent gentrifies from 2012 to 2016 has a negative difference and vice versa for a tract that becomes more low income 
0	151	9926	we source data from american factfinder aff a public information tool produced by the united states census prior research describes gentrification in terms of either rising costs of living or displacement of the poor as income distributions shift towards affluence because gentrification occurs over a long time horizon we split the feature set around a pivot year of 2012 we compute the responses using the data from years 2012 2016 with the data from 2010 and 2011 used as features in the above formulation t 2012 and t 2016 splitting the data to forecast gentrification over a long time horizon comports with previous research to model the second response we use an imputed measurement of the inter year intra tract change in the income distribution of the tract see for each tract we compute the hellinger distance between the observed income distribution and a baseline in which all residents are perfectly affluent with probability 1 tracts with low hellinger distances tend to be high income tracts with high hellinger distances tend to be low income 	we rescale the responses so that they are bounded between 0 and 100 and positive differences signal gentrification 
1	151	9927	we source data from american factfinder aff a public information tool produced by the united states census prior research describes gentrification in terms of either rising costs of living or displacement of the poor as income distributions shift towards affluence because gentrification occurs over a long time horizon we split the feature set around a pivot year of 2012 we compute the responses using the data from years 2012 2016 with the data from 2010 and 2011 used as features in the above formulation t 2012 and t 2016 splitting the data to forecast gentrification over a long time horizon comports with previous research to model the second response we use an imputed measurement of the inter year intra tract change in the income distribution of the tract see for each tract we compute the hellinger distance between the observed income distribution and a baseline in which all residents are perfectly affluent with probability 1 tracts with low hellinger distances tend to be high income tracts with high hellinger distances tend to be low income 	finally we relabel each response 1 gentrification occurred or 0 gentrification did not occur for both the monthly cost of housing and income distribution shift responses we characterize each census tract using a vector of roughly 150 features assembled from tables s2502 s2503 b25085 and dp03 in aff 
1	151	9928	we source data from american factfinder aff a public information tool produced by the united states census prior research describes gentrification in terms of either rising costs of living or displacement of the poor as income distributions shift towards affluence because gentrification occurs over a long time horizon we split the feature set around a pivot year of 2012 we compute the responses using the data from years 2012 2016 with the data from 2010 and 2011 used as features in the above formulation t 2012 and t 2016 splitting the data to forecast gentrification over a long time horizon comports with previous research to model the second response we use an imputed measurement of the inter year intra tract change in the income distribution of the tract see for each tract we compute the hellinger distance between the observed income distribution and a baseline in which all residents are perfectly affluent with probability 1 tracts with low hellinger distances tend to be high income tracts with high hellinger distances tend to be low income 	these include tracts demographic and economic characteristics such as employment by industry ethnic and racial composition level education and more additionally we engineer four features based on the theory of spatial equilibrium proposed in prior work on endogenous gentrification here y j denotes each response computed between the pre pivot years 2010 and 2011 
1	151	9929	we source data from american factfinder aff a public information tool produced by the united states census prior research describes gentrification in terms of either rising costs of living or displacement of the poor as income distributions shift towards affluence because gentrification occurs over a long time horizon we split the feature set around a pivot year of 2012 we compute the responses using the data from years 2012 2016 with the data from 2010 and 2011 used as features in the above formulation t 2012 and t 2016 splitting the data to forecast gentrification over a long time horizon comports with previous research to model the second response we use an imputed measurement of the inter year intra tract change in the income distribution of the tract see for each tract we compute the hellinger distance between the observed income distribution and a baseline in which all residents are perfectly affluent with probability 1 tracts with low hellinger distances tend to be high income tracts with high hellinger distances tend to be low income 	likewise for each response we compute the local moran s i statistic a measure of spatial clustering where z k is the deviation of the response of interest from the mean across all n tracts in the training sample computed between 2010 and 2011 the observation period 
0	151	9930	we source data from american factfinder aff a public information tool produced by the united states census prior research describes gentrification in terms of either rising costs of living or displacement of the poor as income distributions shift towards affluence because gentrification occurs over a long time horizon we split the feature set around a pivot year of 2012 we compute the responses using the data from years 2012 2016 with the data from 2010 and 2011 used as features in the above formulation t 2012 and t 2016 splitting the data to forecast gentrification over a long time horizon comports with previous research to model the second response we use an imputed measurement of the inter year intra tract change in the income distribution of the tract see for each tract we compute the hellinger distance between the observed income distribution and a baseline in which all residents are perfectly affluent with probability 1 tracts with low hellinger distances tend to be high income tracts with high hellinger distances tend to be low income 	we do not use time invariant features describing the geography of the census tracts 
0	151	9931	we source data from american factfinder aff a public information tool produced by the united states census prior research describes gentrification in terms of either rising costs of living or displacement of the poor as income distributions shift towards affluence because gentrification occurs over a long time horizon we split the feature set around a pivot year of 2012 we compute the responses using the data from years 2012 2016 with the data from 2010 and 2011 used as features in the above formulation t 2012 and t 2016 splitting the data to forecast gentrification over a long time horizon comports with previous research to model the second response we use an imputed measurement of the inter year intra tract change in the income distribution of the tract see for each tract we compute the hellinger distance between the observed income distribution and a baseline in which all residents are perfectly affluent with probability 1 tracts with low hellinger distances tend to be high income tracts with high hellinger distances tend to be low income 	these ought not add much explanatory power to a model that forecasts gentrification by time 
0	151	9932	we source data from american factfinder aff a public information tool produced by the united states census prior research describes gentrification in terms of either rising costs of living or displacement of the poor as income distributions shift towards affluence because gentrification occurs over a long time horizon we split the feature set around a pivot year of 2012 we compute the responses using the data from years 2012 2016 with the data from 2010 and 2011 used as features in the above formulation t 2012 and t 2016 splitting the data to forecast gentrification over a long time horizon comports with previous research to model the second response we use an imputed measurement of the inter year intra tract change in the income distribution of the tract see for each tract we compute the hellinger distance between the observed income distribution and a baseline in which all residents are perfectly affluent with probability 1 tracts with low hellinger distances tend to be high income tracts with high hellinger distances tend to be low income 	likewise we do not add network topological features from e g 
0	151	9933	we source data from american factfinder aff a public information tool produced by the united states census prior research describes gentrification in terms of either rising costs of living or displacement of the poor as income distributions shift towards affluence because gentrification occurs over a long time horizon we split the feature set around a pivot year of 2012 we compute the responses using the data from years 2012 2016 with the data from 2010 and 2011 used as features in the above formulation t 2012 and t 2016 splitting the data to forecast gentrification over a long time horizon comports with previous research to model the second response we use an imputed measurement of the inter year intra tract change in the income distribution of the tract see for each tract we compute the hellinger distance between the observed income distribution and a baseline in which all residents are perfectly affluent with probability 1 tracts with low hellinger distances tend to be high income tracts with high hellinger distances tend to be low income 	overall the data consist of 8 056 observations for each of california s census tracts with one dropped due to missing data 
0	151	9934	we source data from american factfinder aff a public information tool produced by the united states census prior research describes gentrification in terms of either rising costs of living or displacement of the poor as income distributions shift towards affluence because gentrification occurs over a long time horizon we split the feature set around a pivot year of 2012 we compute the responses using the data from years 2012 2016 with the data from 2010 and 2011 used as features in the above formulation t 2012 and t 2016 splitting the data to forecast gentrification over a long time horizon comports with previous research to model the second response we use an imputed measurement of the inter year intra tract change in the income distribution of the tract see for each tract we compute the hellinger distance between the observed income distribution and a baseline in which all residents are perfectly affluent with probability 1 tracts with low hellinger distances tend to be high income tracts with high hellinger distances tend to be low income 	surprisingly a priori we observed the classes to be roughly balanced for both responses suggesting that there still exist pockets of affordability in the state 
0	151	9935	we source data from american factfinder aff a public information tool produced by the united states census prior research describes gentrification in terms of either rising costs of living or displacement of the poor as income distributions shift towards affluence because gentrification occurs over a long time horizon we split the feature set around a pivot year of 2012 we compute the responses using the data from years 2012 2016 with the data from 2010 and 2011 used as features in the above formulation t 2012 and t 2016 splitting the data to forecast gentrification over a long time horizon comports with previous research to model the second response we use an imputed measurement of the inter year intra tract change in the income distribution of the tract see for each tract we compute the hellinger distance between the observed income distribution and a baseline in which all residents are perfectly affluent with probability 1 tracts with low hellinger distances tend to be high income tracts with high hellinger distances tend to be low income 	we split the data into a training set comprising 90 7 262 of the observations and validation and test sets comprising 5 397 respectively 
1	151	9936	we applied four machine learning methods to each classification problem defining gentrification as the change in monthly cost and as the shift in income distribution we used a random forest classifier a gradient boosting model xgboost an 1 penalized logistic regression and an ensemble approach that classified census tracts according to a majority vote of the aforementioned three models random forests are a variant of bagged decision trees a random forest classifier grows a substantial number of independent classification trees each of which minimizes the gini impurity of its leaf nodes through recursive binary splitting gini impurity measures how often a randomly chosen observation in the node would be mislabelled if it were assigned a random label according to the distribution of responses in the node as classification trees grown on the same set of bootstrapped data tend to be highly correlated the random forest algorithm decorrelates the trees by constraining each split in each tree to be on a random subsample of features in the feature space gradient boosting is an ensemble technique using classification trees in which trees are grown sequentially as opposed to simultaneously in random forests 	we applied four machine learning methods to each classification problem defining gentrification as the change in monthly cost and as the shift in income distribution 
1	151	9937	we applied four machine learning methods to each classification problem defining gentrification as the change in monthly cost and as the shift in income distribution we used a random forest classifier a gradient boosting model xgboost an 1 penalized logistic regression and an ensemble approach that classified census tracts according to a majority vote of the aforementioned three models random forests are a variant of bagged decision trees a random forest classifier grows a substantial number of independent classification trees each of which minimizes the gini impurity of its leaf nodes through recursive binary splitting gini impurity measures how often a randomly chosen observation in the node would be mislabelled if it were assigned a random label according to the distribution of responses in the node as classification trees grown on the same set of bootstrapped data tend to be highly correlated the random forest algorithm decorrelates the trees by constraining each split in each tree to be on a random subsample of features in the feature space gradient boosting is an ensemble technique using classification trees in which trees are grown sequentially as opposed to simultaneously in random forests 	we used a random forest classifier a gradient boosting model xgboost an 1 penalized logistic regression and an ensemble approach that classified census tracts according to a majority vote of the aforementioned three models random forests are a variant of bagged decision trees a random forest classifier grows a substantial number of independent classification trees each of which minimizes the gini impurity of its leaf nodes through recursive binary splitting gini impurity measures how often a randomly chosen observation in the node would be mislabelled if it were assigned a random label according to the distribution of responses in the node 
1	151	9938	we applied four machine learning methods to each classification problem defining gentrification as the change in monthly cost and as the shift in income distribution we used a random forest classifier a gradient boosting model xgboost an 1 penalized logistic regression and an ensemble approach that classified census tracts according to a majority vote of the aforementioned three models random forests are a variant of bagged decision trees a random forest classifier grows a substantial number of independent classification trees each of which minimizes the gini impurity of its leaf nodes through recursive binary splitting gini impurity measures how often a randomly chosen observation in the node would be mislabelled if it were assigned a random label according to the distribution of responses in the node as classification trees grown on the same set of bootstrapped data tend to be highly correlated the random forest algorithm decorrelates the trees by constraining each split in each tree to be on a random subsample of features in the feature space gradient boosting is an ensemble technique using classification trees in which trees are grown sequentially as opposed to simultaneously in random forests 	as classification trees grown on the same set of bootstrapped data tend to be highly correlated the random forest algorithm decorrelates the trees by constraining each split in each tree to be on a random subsample of features in the feature space gradient boosting is an ensemble technique using classification trees in which trees are grown sequentially as opposed to simultaneously in random forests 
0	151	9939	we applied four machine learning methods to each classification problem defining gentrification as the change in monthly cost and as the shift in income distribution we used a random forest classifier a gradient boosting model xgboost an 1 penalized logistic regression and an ensemble approach that classified census tracts according to a majority vote of the aforementioned three models random forests are a variant of bagged decision trees a random forest classifier grows a substantial number of independent classification trees each of which minimizes the gini impurity of its leaf nodes through recursive binary splitting gini impurity measures how often a randomly chosen observation in the node would be mislabelled if it were assigned a random label according to the distribution of responses in the node as classification trees grown on the same set of bootstrapped data tend to be highly correlated the random forest algorithm decorrelates the trees by constraining each split in each tree to be on a random subsample of features in the feature space gradient boosting is an ensemble technique using classification trees in which trees are grown sequentially as opposed to simultaneously in random forests 	later trees are grown to minimize the errors made by their predecessors 
0	151	9940	we applied four machine learning methods to each classification problem defining gentrification as the change in monthly cost and as the shift in income distribution we used a random forest classifier a gradient boosting model xgboost an 1 penalized logistic regression and an ensemble approach that classified census tracts according to a majority vote of the aforementioned three models random forests are a variant of bagged decision trees a random forest classifier grows a substantial number of independent classification trees each of which minimizes the gini impurity of its leaf nodes through recursive binary splitting gini impurity measures how often a randomly chosen observation in the node would be mislabelled if it were assigned a random label according to the distribution of responses in the node as classification trees grown on the same set of bootstrapped data tend to be highly correlated the random forest algorithm decorrelates the trees by constraining each split in each tree to be on a random subsample of features in the feature space gradient boosting is an ensemble technique using classification trees in which trees are grown sequentially as opposed to simultaneously in random forests 	each subsequent tree learns from the mistakes made earlier in training 
0	151	9941	we applied four machine learning methods to each classification problem defining gentrification as the change in monthly cost and as the shift in income distribution we used a random forest classifier a gradient boosting model xgboost an 1 penalized logistic regression and an ensemble approach that classified census tracts according to a majority vote of the aforementioned three models random forests are a variant of bagged decision trees a random forest classifier grows a substantial number of independent classification trees each of which minimizes the gini impurity of its leaf nodes through recursive binary splitting gini impurity measures how often a randomly chosen observation in the node would be mislabelled if it were assigned a random label according to the distribution of responses in the node as classification trees grown on the same set of bootstrapped data tend to be highly correlated the random forest algorithm decorrelates the trees by constraining each split in each tree to be on a random subsample of features in the feature space gradient boosting is an ensemble technique using classification trees in which trees are grown sequentially as opposed to simultaneously in random forests 	xgboost a popular implementation of gradient boosting which enables regularization of the trees minimizes the loss function where i are the predicted class each f k is a decision tree and is a regularization function of the number of leaves in each tree and the weights of those leaves for the random forest estimator we tuned n the number of trees and p the number of features in the random split set at every split 
1	151	9942	we applied four machine learning methods to each classification problem defining gentrification as the change in monthly cost and as the shift in income distribution we used a random forest classifier a gradient boosting model xgboost an 1 penalized logistic regression and an ensemble approach that classified census tracts according to a majority vote of the aforementioned three models random forests are a variant of bagged decision trees a random forest classifier grows a substantial number of independent classification trees each of which minimizes the gini impurity of its leaf nodes through recursive binary splitting gini impurity measures how often a randomly chosen observation in the node would be mislabelled if it were assigned a random label according to the distribution of responses in the node as classification trees grown on the same set of bootstrapped data tend to be highly correlated the random forest algorithm decorrelates the trees by constraining each split in each tree to be on a random subsample of features in the feature space gradient boosting is an ensemble technique using classification trees in which trees are grown sequentially as opposed to simultaneously in random forests 	for the xgboost estimator we tuned the learning rate the tree depth d on each tree and the regularization parameter our final unitary model was the only parametric estimator 1 penalized logistic regression commonly known as the lasso 
0	151	9943	we applied four machine learning methods to each classification problem defining gentrification as the change in monthly cost and as the shift in income distribution we used a random forest classifier a gradient boosting model xgboost an 1 penalized logistic regression and an ensemble approach that classified census tracts according to a majority vote of the aforementioned three models random forests are a variant of bagged decision trees a random forest classifier grows a substantial number of independent classification trees each of which minimizes the gini impurity of its leaf nodes through recursive binary splitting gini impurity measures how often a randomly chosen observation in the node would be mislabelled if it were assigned a random label according to the distribution of responses in the node as classification trees grown on the same set of bootstrapped data tend to be highly correlated the random forest algorithm decorrelates the trees by constraining each split in each tree to be on a random subsample of features in the feature space gradient boosting is an ensemble technique using classification trees in which trees are grown sequentially as opposed to simultaneously in random forests 	the lasso estimator is a variation on linear regression that logit transforms the responses to estimate logistic regression models p r y i 1 as logistic in the features where l is the logistic loss function 
0	151	9944	we applied four machine learning methods to each classification problem defining gentrification as the change in monthly cost and as the shift in income distribution we used a random forest classifier a gradient boosting model xgboost an 1 penalized logistic regression and an ensemble approach that classified census tracts according to a majority vote of the aforementioned three models random forests are a variant of bagged decision trees a random forest classifier grows a substantial number of independent classification trees each of which minimizes the gini impurity of its leaf nodes through recursive binary splitting gini impurity measures how often a randomly chosen observation in the node would be mislabelled if it were assigned a random label according to the distribution of responses in the node as classification trees grown on the same set of bootstrapped data tend to be highly correlated the random forest algorithm decorrelates the trees by constraining each split in each tree to be on a random subsample of features in the feature space gradient boosting is an ensemble technique using classification trees in which trees are grown sequentially as opposed to simultaneously in random forests 	because the lasso penalizes parameter coefficients in absolute value it implicitly performs feature selection as features with little predictive power have their parameter coefficients driven to zero 
0	151	9945	we applied four machine learning methods to each classification problem defining gentrification as the change in monthly cost and as the shift in income distribution we used a random forest classifier a gradient boosting model xgboost an 1 penalized logistic regression and an ensemble approach that classified census tracts according to a majority vote of the aforementioned three models random forests are a variant of bagged decision trees a random forest classifier grows a substantial number of independent classification trees each of which minimizes the gini impurity of its leaf nodes through recursive binary splitting gini impurity measures how often a randomly chosen observation in the node would be mislabelled if it were assigned a random label according to the distribution of responses in the node as classification trees grown on the same set of bootstrapped data tend to be highly correlated the random forest algorithm decorrelates the trees by constraining each split in each tree to be on a random subsample of features in the feature space gradient boosting is an ensemble technique using classification trees in which trees are grown sequentially as opposed to simultaneously in random forests 	for the lasso estimator we tuned the regularization parameter c we tuned all hyperparameters via two stage grid search 
0	151	9946	we applied four machine learning methods to each classification problem defining gentrification as the change in monthly cost and as the shift in income distribution we used a random forest classifier a gradient boosting model xgboost an 1 penalized logistic regression and an ensemble approach that classified census tracts according to a majority vote of the aforementioned three models random forests are a variant of bagged decision trees a random forest classifier grows a substantial number of independent classification trees each of which minimizes the gini impurity of its leaf nodes through recursive binary splitting gini impurity measures how often a randomly chosen observation in the node would be mislabelled if it were assigned a random label according to the distribution of responses in the node as classification trees grown on the same set of bootstrapped data tend to be highly correlated the random forest algorithm decorrelates the trees by constraining each split in each tree to be on a random subsample of features in the feature space gradient boosting is an ensemble technique using classification trees in which trees are grown sequentially as opposed to simultaneously in random forests 	first we drew test hyperparameters uniformly from a representative interval around the model implementations default parameters in
1	151	9947	parameter the high value of and low value of c found by grid search on the validation set suggest that models that perform poorly may be vulnerable to overfitting especially given the high feature dimensionality 	parameter the high value of and low value of c found by grid search on the validation set suggest that models that perform poorly may be vulnerable to overfitting especially given the high feature dimensionality 
0	151	9948	we evaluated each classifier on each of the two responses using accuracy precision and recall while accuracy measures the proportion of test set class assignments that match the true labels precision and recall provide granular insight into classification errors recall quantifies the proportion of positive classes instantiations of gentrification that were correctly captured by the classifier precision quantifies the prediction accuracy solely among the samples that were predicted to be in the positive class 	we evaluated each classifier on each of the two responses using accuracy precision and recall 
0	151	9949	we evaluated each classifier on each of the two responses using accuracy precision and recall while accuracy measures the proportion of test set class assignments that match the true labels precision and recall provide granular insight into classification errors recall quantifies the proportion of positive classes instantiations of gentrification that were correctly captured by the classifier precision quantifies the prediction accuracy solely among the samples that were predicted to be in the positive class 	while accuracy measures the proportion of test set class assignments that match the true labels precision and recall provide granular insight into classification errors 
1	151	9950	we evaluated each classifier on each of the two responses using accuracy precision and recall while accuracy measures the proportion of test set class assignments that match the true labels precision and recall provide granular insight into classification errors recall quantifies the proportion of positive classes instantiations of gentrification that were correctly captured by the classifier precision quantifies the prediction accuracy solely among the samples that were predicted to be in the positive class 	recall quantifies the proportion of positive classes instantiations of gentrification that were correctly captured by the classifier precision quantifies the prediction accuracy solely among the samples that were predicted to be in the positive class 
0	151	9951	we evaluated each classifier on each of the two responses using accuracy precision and recall while accuracy measures the proportion of test set class assignments that match the true labels precision and recall provide granular insight into classification errors recall quantifies the proportion of positive classes instantiations of gentrification that were correctly captured by the classifier precision quantifies the prediction accuracy solely among the samples that were predicted to be in the positive class 	precision is commonly used when the cost of false positives is high such as when there may be resources wasted in a misdirected policy response 
0	151	9952	we evaluated each classifier on each of the two responses using accuracy precision and recall while accuracy measures the proportion of test set class assignments that match the true labels precision and recall provide granular insight into classification errors recall quantifies the proportion of positive classes instantiations of gentrification that were correctly captured by the classifier precision quantifies the prediction accuracy solely among the samples that were predicted to be in the positive class 	recall is commonly used when the cost of false negatives is high such as when families are being displaced 
1	151	9953	we evaluated each classifier on each of the two responses using accuracy precision and recall while accuracy measures the proportion of test set class assignments that match the true labels precision and recall provide granular insight into classification errors recall quantifies the proportion of positive classes instantiations of gentrification that were correctly captured by the classifier precision quantifies the prediction accuracy solely among the samples that were predicted to be in the positive class 	while no one metric dominates in importance in this domain precision and recall illuminate why the performance of all classifiers on the task of classifying tracts according to their change in income distribution during the prediction period was so poor 
1	151	9954	we evaluated each classifier on each of the two responses using accuracy precision and recall while accuracy measures the proportion of test set class assignments that match the true labels precision and recall provide granular insight into classification errors recall quantifies the proportion of positive classes instantiations of gentrification that were correctly captured by the classifier precision quantifies the prediction accuracy solely among the samples that were predicted to be in the positive class 	all four classifiers outperformed the no information classifier in predicting whether a tract would gentrify as defined by a rise in the monthly cost of housing see by contrast no model outperformed the no information classifier in predicting whether a tract would gentrify based on its income distribution 
0	151	9955	we evaluated each classifier on each of the two responses using accuracy precision and recall while accuracy measures the proportion of test set class assignments that match the true labels precision and recall provide granular insight into classification errors recall quantifies the proportion of positive classes instantiations of gentrification that were correctly captured by the classifier precision quantifies the prediction accuracy solely among the samples that were predicted to be in the positive class 	this is not surprising given how uncorrelated these responses were with 0 06 
1	151	9956	we evaluated each classifier on each of the two responses using accuracy precision and recall while accuracy measures the proportion of test set class assignments that match the true labels precision and recall provide granular insight into classification errors recall quantifies the proportion of positive classes instantiations of gentrification that were correctly captured by the classifier precision quantifies the prediction accuracy solely among the samples that were predicted to be in the positive class 	the high recalls and relatively low precisions reported by the random forest logit model and voting classifier suggest a plausible explanation that all three were overly trigger happy in labelling tracts as positive instantiations of the response leading to high counts of true positive labelings and few false negatives boosting recall as well as high counts of false positive labelings damping precision 
1	151	9957	we evaluated each classifier on each of the two responses using accuracy precision and recall while accuracy measures the proportion of test set class assignments that match the true labels precision and recall provide granular insight into classification errors recall quantifies the proportion of positive classes instantiations of gentrification that were correctly captured by the classifier precision quantifies the prediction accuracy solely among the samples that were predicted to be in the positive class 	the confusion matrix for the random forest estimator the best model on this problemindicates that the estimator guessed positive 86 of the time an overwhelming majority given that the classes were balanced in the training and test sets see
1	151	9958	in this research we develop a classifier to predict whether gentrification will occur in a california census tract with 65 accuracy we defined gentrification as an increase in the inflation adjusted monthly cost of housing and observed experimentally that other definitions such as ones based on localities income distributions yielded noisy results using public data non parametric ensemble models such as random forests and xgboost outperformed parametric models which may have overfit the training data 	in this research we develop a classifier to predict whether gentrification will occur in a california census tract with 65 accuracy 
1	151	9959	in this research we develop a classifier to predict whether gentrification will occur in a california census tract with 65 accuracy we defined gentrification as an increase in the inflation adjusted monthly cost of housing and observed experimentally that other definitions such as ones based on localities income distributions yielded noisy results using public data non parametric ensemble models such as random forests and xgboost outperformed parametric models which may have overfit the training data 	we defined gentrification as an increase in the inflation adjusted monthly cost of housing and observed experimentally that other definitions such as ones based on localities income distributions yielded noisy results using public data 
0	151	9960	in this research we develop a classifier to predict whether gentrification will occur in a california census tract with 65 accuracy we defined gentrification as an increase in the inflation adjusted monthly cost of housing and observed experimentally that other definitions such as ones based on localities income distributions yielded noisy results using public data non parametric ensemble models such as random forests and xgboost outperformed parametric models which may have overfit the training data 	non parametric ensemble models such as random forests and xgboost outperformed parametric models which may have overfit the training data 
1	151	9961	in this research we develop a classifier to predict whether gentrification will occur in a california census tract with 65 accuracy we defined gentrification as an increase in the inflation adjusted monthly cost of housing and observed experimentally that other definitions such as ones based on localities income distributions yielded noisy results using public data non parametric ensemble models such as random forests and xgboost outperformed parametric models which may have overfit the training data 	furthermore engineered features describing the spatial characteristics of each census tract proved most consequential lending credence to the theory that housing markets in spatial disequilibrium precede gentrification further work might refine the spatially engineered features by e g 
0	151	9962	in this research we develop a classifier to predict whether gentrification will occur in a california census tract with 65 accuracy we defined gentrification as an increase in the inflation adjusted monthly cost of housing and observed experimentally that other definitions such as ones based on localities income distributions yielded noisy results using public data non parametric ensemble models such as random forests and xgboost outperformed parametric models which may have overfit the training data 	weighting the network adjacency matrix so that the i jth entry denotes inverse intercentroid distance instead of adjacency 
1	151	9963	in this research we develop a classifier to predict whether gentrification will occur in a california census tract with 65 accuracy we defined gentrification as an increase in the inflation adjusted monthly cost of housing and observed experimentally that other definitions such as ones based on localities income distributions yielded noisy results using public data non parametric ensemble models such as random forests and xgboost outperformed parametric models which may have overfit the training data 	alternatively further work might focus on better defining gentrification by quantifying displacement of families or collapsing the bins of the income distribution response to increase the signal in the data 
0	151	9964	in this research we develop a classifier to predict whether gentrification will occur in a california census tract with 65 accuracy we defined gentrification as an increase in the inflation adjusted monthly cost of housing and observed experimentally that other definitions such as ones based on localities income distributions yielded noisy results using public data non parametric ensemble models such as random forests and xgboost outperformed parametric models which may have overfit the training data 	finally causal work could ascertain the drivers of gentrification as opposed to simply leading indicators 
0	151	9965	in this research we develop a classifier to predict whether gentrification will occur in a california census tract with 65 accuracy we defined gentrification as an increase in the inflation adjusted monthly cost of housing and observed experimentally that other definitions such as ones based on localities income distributions yielded noisy results using public data non parametric ensemble models such as random forests and xgboost outperformed parametric models which may have overfit the training data 	accurately forecasting gentrification continues to be a pressing problem for california policymakers 
0	151	9966	all code written for this project can be found here 	all code written for this project can be found here 
0	152	9967	stock market predictions lend themselves well to a machine learning framework due to their quantitative nature a supervised learning model to predict stock movement direction can combine technical information and qualitative sentiment through news encoded into fixed length real vectors we attempt a large range of models both to encode qualitative sentiment information into features and to make a final up or down prediction on the direction of a particular stock given encoded news and technical features 	stock market predictions lend themselves well to a machine learning framework due to their quantitative nature 
1	152	9968	stock market predictions lend themselves well to a machine learning framework due to their quantitative nature a supervised learning model to predict stock movement direction can combine technical information and qualitative sentiment through news encoded into fixed length real vectors we attempt a large range of models both to encode qualitative sentiment information into features and to make a final up or down prediction on the direction of a particular stock given encoded news and technical features 	a supervised learning model to predict stock movement direction can combine technical information and qualitative sentiment through news encoded into fixed length real vectors 
1	152	9969	stock market predictions lend themselves well to a machine learning framework due to their quantitative nature a supervised learning model to predict stock movement direction can combine technical information and qualitative sentiment through news encoded into fixed length real vectors we attempt a large range of models both to encode qualitative sentiment information into features and to make a final up or down prediction on the direction of a particular stock given encoded news and technical features 	we attempt a large range of models both to encode qualitative sentiment information into features and to make a final up or down prediction on the direction of a particular stock given encoded news and technical features 
1	152	9970	stock market predictions lend themselves well to a machine learning framework due to their quantitative nature a supervised learning model to predict stock movement direction can combine technical information and qualitative sentiment through news encoded into fixed length real vectors we attempt a large range of models both to encode qualitative sentiment information into features and to make a final up or down prediction on the direction of a particular stock given encoded news and technical features 	we find that a universal sentence encoder combined with svms achieves encouraging results on our data 
0	152	9971	stock market predictions have been a pivotal and controversial subject in the field of finance some theorists believe in the efficient market hypothesis that stock prices reflect all current information and thus think that the stock market is inherently unpredictable others have attempted to predict the market through fundamental analysis technical analysis and more recently machine learning 	stock market predictions have been a pivotal and controversial subject in the field of finance 
0	152	9972	stock market predictions have been a pivotal and controversial subject in the field of finance some theorists believe in the efficient market hypothesis that stock prices reflect all current information and thus think that the stock market is inherently unpredictable others have attempted to predict the market through fundamental analysis technical analysis and more recently machine learning 	some theorists believe in the efficient market hypothesis that stock prices reflect all current information and thus think that the stock market is inherently unpredictable 
0	152	9973	stock market predictions have been a pivotal and controversial subject in the field of finance some theorists believe in the efficient market hypothesis that stock prices reflect all current information and thus think that the stock market is inherently unpredictable others have attempted to predict the market through fundamental analysis technical analysis and more recently machine learning 	others have attempted to predict the market through fundamental analysis technical analysis and more recently machine learning 
0	152	9974	stock market predictions have been a pivotal and controversial subject in the field of finance some theorists believe in the efficient market hypothesis that stock prices reflect all current information and thus think that the stock market is inherently unpredictable others have attempted to predict the market through fundamental analysis technical analysis and more recently machine learning 	a technique such as machine learning may lend itself well to such an application because of the fundamentally quantitative nature of the stock market 
0	152	9975	stock market predictions have been a pivotal and controversial subject in the field of finance some theorists believe in the efficient market hypothesis that stock prices reflect all current information and thus think that the stock market is inherently unpredictable others have attempted to predict the market through fundamental analysis technical analysis and more recently machine learning 	current machine learning models have focused on technical analyses or sentiment as a single feature 
1	152	9976	stock market predictions have been a pivotal and controversial subject in the field of finance some theorists believe in the efficient market hypothesis that stock prices reflect all current information and thus think that the stock market is inherently unpredictable others have attempted to predict the market through fundamental analysis technical analysis and more recently machine learning 	but since the stock market is also heavily dependent on market sentiment and fundamental company information which cannot be captured with a simple numeric indicator we decided to create a machine learning model that takes in both stock financial data and news information which we encode into a fixed length vector 
0	152	9977	stock market predictions have been a pivotal and controversial subject in the field of finance some theorists believe in the efficient market hypothesis that stock prices reflect all current information and thus think that the stock market is inherently unpredictable others have attempted to predict the market through fundamental analysis technical analysis and more recently machine learning 	our model tries to predict stock direction using a variety of techniques including svms and neural networks 
1	152	9978	stock market predictions have been a pivotal and controversial subject in the field of finance some theorists believe in the efficient market hypothesis that stock prices reflect all current information and thus think that the stock market is inherently unpredictable others have attempted to predict the market through fundamental analysis technical analysis and more recently machine learning 	by creating a machine learning model that combines the approaches of technical analysis and fundamental analysis we hope our model can paint a better picture of the overall market 
0	152	9979	sentiment analysis and machine learning for stock predictions is an active research area existing work to predict stock movement direction using sentiment analysis includes dictionary based correlation finding methods and sentiment mood detection algorithms several papers such as nagar and hahsler	sentiment analysis and machine learning for stock predictions is an active research area 
0	152	9980	sentiment analysis and machine learning for stock predictions is an active research area existing work to predict stock movement direction using sentiment analysis includes dictionary based correlation finding methods and sentiment mood detection algorithms several papers such as nagar and hahsler	existing work to predict stock movement direction using sentiment analysis includes dictionary based correlation finding methods and sentiment mood detection algorithms 
0	152	9981	sentiment analysis and machine learning for stock predictions is an active research area existing work to predict stock movement direction using sentiment analysis includes dictionary based correlation finding methods and sentiment mood detection algorithms several papers such as nagar and hahsler	several papers such as nagar and hahsler
0	152	9982	our dataset is composed of trading macro technical and news data related to 20 nasdaq companies from 2013 to 2017 we used the yahoo finance api to extract trading related information on each stock ticker including price and volume on a daily basis we also extracted overarching macro data including quarterly gdp cpi and daily libor from the fed website 	our dataset is composed of trading macro technical and news data related to 20 nasdaq companies from 2013 to 2017 
0	152	9983	our dataset is composed of trading macro technical and news data related to 20 nasdaq companies from 2013 to 2017 we used the yahoo finance api to extract trading related information on each stock ticker including price and volume on a daily basis we also extracted overarching macro data including quarterly gdp cpi and daily libor from the fed website 	we used the yahoo finance api to extract trading related information on each stock ticker including price and volume on a daily basis 
0	152	9984	our dataset is composed of trading macro technical and news data related to 20 nasdaq companies from 2013 to 2017 we used the yahoo finance api to extract trading related information on each stock ticker including price and volume on a daily basis we also extracted overarching macro data including quarterly gdp cpi and daily libor from the fed website 	we also extracted overarching macro data including quarterly gdp cpi and daily libor from the fed website 
1	152	9985	our dataset is composed of trading macro technical and news data related to 20 nasdaq companies from 2013 to 2017 we used the yahoo finance api to extract trading related information on each stock ticker including price and volume on a daily basis we also extracted overarching macro data including quarterly gdp cpi and daily libor from the fed website 	in addition we computed technical indicators including cci rsi and evm from trading data 
0	152	9986	our dataset is composed of trading macro technical and news data related to 20 nasdaq companies from 2013 to 2017 we used the yahoo finance api to extract trading related information on each stock ticker including price and volume on a daily basis we also extracted overarching macro data including quarterly gdp cpi and daily libor from the fed website 	finally we scraped daily news headlines and snippets for each ticker from new york times and google news 
0	152	9987	we used a few approaches to merge and preprocess the data to match quarterly macro data e g gdp with other daily data we assumed the macro features to be constant throughout the same quarter we processed the news data using text representation and sentiment analysis models which will be discussed in detail in section 4 before merging it to the full data set 	we used a few approaches to merge and preprocess the data 
0	152	9988	we used a few approaches to merge and preprocess the data to match quarterly macro data e g gdp with other daily data we assumed the macro features to be constant throughout the same quarter we processed the news data using text representation and sentiment analysis models which will be discussed in detail in section 4 before merging it to the full data set 	to match quarterly macro data e g gdp with other daily data we assumed the macro features to be constant throughout the same quarter 
1	152	9989	we used a few approaches to merge and preprocess the data to match quarterly macro data e g gdp with other daily data we assumed the macro features to be constant throughout the same quarter we processed the news data using text representation and sentiment analysis models which will be discussed in detail in section 4 before merging it to the full data set 	we processed the news data using text representation and sentiment analysis models which will be discussed in detail in section 4 before merging it to the full data set 
1	152	9990	we used a few approaches to merge and preprocess the data to match quarterly macro data e g gdp with other daily data we assumed the macro features to be constant throughout the same quarter we processed the news data using text representation and sentiment analysis models which will be discussed in detail in section 4 before merging it to the full data set 	for tickers which have multiple news for certain dates we averaged the sentiment encoded vectors for google news and used the top 1 news for new york times because new york times ranks top articles 
0	152	9991	we used a few approaches to merge and preprocess the data to match quarterly macro data e g gdp with other daily data we assumed the macro features to be constant throughout the same quarter we processed the news data using text representation and sentiment analysis models which will be discussed in detail in section 4 before merging it to the full data set 	for tickers which don t have news articles on certain dates we replaced the missing value with the latest available news 
0	152	9992	we used a few approaches to merge and preprocess the data to match quarterly macro data e g gdp with other daily data we assumed the macro features to be constant throughout the same quarter we processed the news data using text representation and sentiment analysis models which will be discussed in detail in section 4 before merging it to the full data set 	we choose not to normalize the data to avoid destroying correlations of the sparse matrix 
0	152	9993	we used a few approaches to merge and preprocess the data to match quarterly macro data e g gdp with other daily data we assumed the macro features to be constant throughout the same quarter we processed the news data using text representation and sentiment analysis models which will be discussed in detail in section 4 before merging it to the full data set 	furthermore we classified the 1 day next day stock movement into a binary label y where y 1 if adj 
0	152	9994	we used a few approaches to merge and preprocess the data to match quarterly macro data e g gdp with other daily data we assumed the macro features to be constant throughout the same quarter we processed the news data using text representation and sentiment analysis models which will be discussed in detail in section 4 before merging it to the full data set 	close price last adj 
0	152	9995	we used a few approaches to merge and preprocess the data to match quarterly macro data e g gdp with other daily data we assumed the macro features to be constant throughout the same quarter we processed the news data using text representation and sentiment analysis models which will be discussed in detail in section 4 before merging it to the full data set 	close price and y 0 if adj 
0	152	9996	we used a few approaches to merge and preprocess the data to match quarterly macro data e g gdp with other daily data we assumed the macro features to be constant throughout the same quarter we processed the news data using text representation and sentiment analysis models which will be discussed in detail in section 4 before merging it to the full data set 	close price last adj 
1	152	9997	we used a few approaches to merge and preprocess the data to match quarterly macro data e g gdp with other daily data we assumed the macro features to be constant throughout the same quarter we processed the news data using text representation and sentiment analysis models which will be discussed in detail in section 4 before merging it to the full data set 	finally we built two datasets using news from new york times and google respectively each of which contains 24k entries and 70 features 
0	152	9998	we used a few approaches to merge and preprocess the data to match quarterly macro data e g gdp with other daily data we assumed the macro features to be constant throughout the same quarter we processed the news data using text representation and sentiment analysis models which will be discussed in detail in section 4 before merging it to the full data set 	we split all samples before 2017 into the training set and hold out the rest as test set 
0	152	9999	we plotted label y on the first two principal components of news data the plot reveals the complicated nature of the features implying that high dimension classifiers are required for the dataset 	we plotted label y on the first two principal components of news data 
0	152	10000	we plotted label y on the first two principal components of news data the plot reveals the complicated nature of the features implying that high dimension classifiers are required for the dataset 	the plot reveals the complicated nature of the features implying that high dimension classifiers are required for the dataset 
0	152	10001	in general the problem is a supervised learning problem i e we are predicting the next day movement of the stock by taking in the trading information about the stock and the information from the ticker specific daily news the task can be split into two parts namely to represent the news as a fixed length real scalar or vector and to use the news together with trading information technical indicators and macro data to make the prediction in order to capture the semantic information of the text and represent it in a vector space we eventually decided to use a google universal sentence encode use as the encoder section 4 2 	in general the problem is a supervised learning problem i e we are predicting the next day movement of the stock by taking in the trading information about the stock and the information from the ticker specific daily news 
1	152	10002	in general the problem is a supervised learning problem i e we are predicting the next day movement of the stock by taking in the trading information about the stock and the information from the ticker specific daily news the task can be split into two parts namely to represent the news as a fixed length real scalar or vector and to use the news together with trading information technical indicators and macro data to make the prediction in order to capture the semantic information of the text and represent it in a vector space we eventually decided to use a google universal sentence encode use as the encoder section 4 2 	the task can be split into two parts namely to represent the news as a fixed length real scalar or vector and to use the news together with trading information technical indicators and macro data to make the prediction 
1	152	10003	in general the problem is a supervised learning problem i e we are predicting the next day movement of the stock by taking in the trading information about the stock and the information from the ticker specific daily news the task can be split into two parts namely to represent the news as a fixed length real scalar or vector and to use the news together with trading information technical indicators and macro data to make the prediction in order to capture the semantic information of the text and represent it in a vector space we eventually decided to use a google universal sentence encode use as the encoder section 4 2 	in order to capture the semantic information of the text and represent it in a vector space we eventually decided to use a google universal sentence encode use as the encoder section 4 2 
1	152	10004	in general the problem is a supervised learning problem i e we are predicting the next day movement of the stock by taking in the trading information about the stock and the information from the ticker specific daily news the task can be split into two parts namely to represent the news as a fixed length real scalar or vector and to use the news together with trading information technical indicators and macro data to make the prediction in order to capture the semantic information of the text and represent it in a vector space we eventually decided to use a google universal sentence encode use as the encoder section 4 2 	in terms of the stock prediction model which is trained to take in all of the technical and encoded features to make the prediction we used logistic regression random forest svm and a variety of neural networks section 4 3 
1	152	10005	the text representation model is required to convert news sentences and snippets to real value fixed length scalar or vector representations that can be used in the stock movement prediction model the goal is to have the model best capture fine grained semantic and syntactic information one method we tried was to directly extract a sentiment signal scalar from a given sentence vector 	the text representation model is required to convert news sentences and snippets to real value fixed length scalar or vector representations that can be used in the stock movement prediction model 
0	152	10006	the text representation model is required to convert news sentences and snippets to real value fixed length scalar or vector representations that can be used in the stock movement prediction model the goal is to have the model best capture fine grained semantic and syntactic information one method we tried was to directly extract a sentiment signal scalar from a given sentence vector 	the goal is to have the model best capture fine grained semantic and syntactic information 
0	152	10007	the text representation model is required to convert news sentences and snippets to real value fixed length scalar or vector representations that can be used in the stock movement prediction model the goal is to have the model best capture fine grained semantic and syntactic information one method we tried was to directly extract a sentiment signal scalar from a given sentence vector 	one method we tried was to directly extract a sentiment signal scalar from a given sentence vector 
1	152	10008	the text representation model is required to convert news sentences and snippets to real value fixed length scalar or vector representations that can be used in the stock movement prediction model the goal is to have the model best capture fine grained semantic and syntactic information one method we tried was to directly extract a sentiment signal scalar from a given sentence vector 	dictionary based approaches use statistical information of word occurence count to extract useful information 
1	152	10009	the text representation model is required to convert news sentences and snippets to real value fixed length scalar or vector representations that can be used in the stock movement prediction model the goal is to have the model best capture fine grained semantic and syntactic information one method we tried was to directly extract a sentiment signal scalar from a given sentence vector 	we used a sentimentanalysis package from r with a financial dictionary similar to the one mentioned in related work to get a scalar sentiment score for each of our sentences 
1	152	10010	the text representation model is required to convert news sentences and snippets to real value fixed length scalar or vector representations that can be used in the stock movement prediction model the goal is to have the model best capture fine grained semantic and syntactic information one method we tried was to directly extract a sentiment signal scalar from a given sentence vector 	we also tried using a pre trained sentiment lstm model which was trained using labeled data from cnn news 5 to extract the sentiment from the headline and snippet text 
1	152	10011	the text representation model is required to convert news sentences and snippets to real value fixed length scalar or vector representations that can be used in the stock movement prediction model the goal is to have the model best capture fine grained semantic and syntactic information one method we tried was to directly extract a sentiment signal scalar from a given sentence vector 	however neither of the methods mentioned above achieved a reasonable accuracy in making the overall prediction and a plausible reason is that the high level sentiment information is not sufficient in representing the text 
0	152	10012	the text representation model is required to convert news sentences and snippets to real value fixed length scalar or vector representations that can be used in the stock movement prediction model the goal is to have the model best capture fine grained semantic and syntactic information one method we tried was to directly extract a sentiment signal scalar from a given sentence vector 	thus we used sentence embeddings to produce a vector space with a more meaningful substructure to represent the news and fed the entire vector embedding into our classification model 
1	152	10013	the text representation model is required to convert news sentences and snippets to real value fixed length scalar or vector representations that can be used in the stock movement prediction model the goal is to have the model best capture fine grained semantic and syntactic information one method we tried was to directly extract a sentiment signal scalar from a given sentence vector 	recent methods of finding fixed length real vector representations of words and sentences have succeeded in a wide range of tasks from sentiment analysis to question and answering 
0	152	10014	the text representation model is required to convert news sentences and snippets to real value fixed length scalar or vector representations that can be used in the stock movement prediction model the goal is to have the model best capture fine grained semantic and syntactic information one method we tried was to directly extract a sentiment signal scalar from a given sentence vector 	these models can be broadly divided into word encoding methods and sentence encoding methods 
1	152	10015	the text representation model is required to convert news sentences and snippets to real value fixed length scalar or vector representations that can be used in the stock movement prediction model the goal is to have the model best capture fine grained semantic and syntactic information one method we tried was to directly extract a sentiment signal scalar from a given sentence vector 	to evaluate each of these models to choose one for us to use we took several sentences and compared the results of the encoding to see if the encoders captured the similarities and differences between sentences 
0	152	10016	the text representation model is required to convert news sentences and snippets to real value fixed length scalar or vector representations that can be used in the stock movement prediction model the goal is to have the model best capture fine grained semantic and syntactic information one method we tried was to directly extract a sentiment signal scalar from a given sentence vector 	word encoding strategies include word2vec elmo glove and fasttext 
1	152	10017	the text representation model is required to convert news sentences and snippets to real value fixed length scalar or vector representations that can be used in the stock movement prediction model the goal is to have the model best capture fine grained semantic and syntactic information one method we tried was to directly extract a sentiment signal scalar from a given sentence vector 	these models use the bag of words technique which detect how often words appear in similar context of other words to get a vector representation of each word though the fasttext actually goes character by character 
1	152	10018	the text representation model is required to convert news sentences and snippets to real value fixed length scalar or vector representations that can be used in the stock movement prediction model the goal is to have the model best capture fine grained semantic and syntactic information one method we tried was to directly extract a sentiment signal scalar from a given sentence vector 	we noticed that one problem with using one of these word encoding strategies on our sentences is that it does not consider the words of the sentence together and we are unsure about how to composite the words to the sentence 
1	152	10019	the text representation model is required to convert news sentences and snippets to real value fixed length scalar or vector representations that can be used in the stock movement prediction model the goal is to have the model best capture fine grained semantic and syntactic information one method we tried was to directly extract a sentiment signal scalar from a given sentence vector 	thus we decided to choose a method that encoded entire sentences such as skip thoughts similar to word2vec but with sentences instead infersent looks at pairs of sentences or a universal sentence encoder 
1	152	10020	the text representation model is required to convert news sentences and snippets to real value fixed length scalar or vector representations that can be used in the stock movement prediction model the goal is to have the model best capture fine grained semantic and syntactic information one method we tried was to directly extract a sentiment signal scalar from a given sentence vector 	the use consists of a deep average network dan structure although this structure also takes an average of words there are layers of dropout that allow important words to be highlighted 
1	152	10021	the text representation model is required to convert news sentences and snippets to real value fixed length scalar or vector representations that can be used in the stock movement prediction model the goal is to have the model best capture fine grained semantic and syntactic information one method we tried was to directly extract a sentiment signal scalar from a given sentence vector 	there was also another variant of the use that used a transformer module a novel neural network architecture based on a self attention mechanism of context this method achieves the best in detecting sentence similarities however we found this technique to be too slow on our data 
1	152	10022	the text representation model is required to convert news sentences and snippets to real value fixed length scalar or vector representations that can be used in the stock movement prediction model the goal is to have the model best capture fine grained semantic and syntactic information one method we tried was to directly extract a sentiment signal scalar from a given sentence vector 	eventually we decided to use the pre trained google dan use as our sentence representation because of its ability to detect features in a large range of sentence types including our news large pretrained corpus and dropout technique 
0	152	10023	the text representation model is required to convert news sentences and snippets to real value fixed length scalar or vector representations that can be used in the stock movement prediction model the goal is to have the model best capture fine grained semantic and syntactic information one method we tried was to directly extract a sentiment signal scalar from a given sentence vector 	the pca is based off of all of the seen vectors in the training set and the principal components stay the same for the test set 
1	152	10024	logistic regression is used as a baseline to map the features to stock movement random forest which constructs a multitude of decision trees at training time and outputs the class that is the majority vote of the individual leaves is widely used for classification and regression we tuned depth of the tree and leaf size to regularize the model support vector machines are mentioned in previous research fully connected neural networks are composed by interconnected neurons whose activation functions capture the non linear relationship between the variables and the binary result 	logistic regression is used as a baseline to map the features to stock movement random forest which constructs a multitude of decision trees at training time and outputs the class that is the majority vote of the individual leaves is widely used for classification and regression 
0	152	10025	logistic regression is used as a baseline to map the features to stock movement random forest which constructs a multitude of decision trees at training time and outputs the class that is the majority vote of the individual leaves is widely used for classification and regression we tuned depth of the tree and leaf size to regularize the model support vector machines are mentioned in previous research fully connected neural networks are composed by interconnected neurons whose activation functions capture the non linear relationship between the variables and the binary result 	we tuned depth of the tree and leaf size to regularize the model 
0	152	10026	logistic regression is used as a baseline to map the features to stock movement random forest which constructs a multitude of decision trees at training time and outputs the class that is the majority vote of the individual leaves is widely used for classification and regression we tuned depth of the tree and leaf size to regularize the model support vector machines are mentioned in previous research fully connected neural networks are composed by interconnected neurons whose activation functions capture the non linear relationship between the variables and the binary result 	support vector machines are mentioned in previous research fully connected neural networks are composed by interconnected neurons whose activation functions capture the non linear relationship between the variables and the binary result 
1	152	10027	logistic regression is used as a baseline to map the features to stock movement random forest which constructs a multitude of decision trees at training time and outputs the class that is the majority vote of the individual leaves is widely used for classification and regression we tuned depth of the tree and leaf size to regularize the model support vector machines are mentioned in previous research fully connected neural networks are composed by interconnected neurons whose activation functions capture the non linear relationship between the variables and the binary result 	we tuned the model on different parameters and the best performance model structure consists of two hidden layers 50 2 or 10 10 with relu activation and a learning rate of 1e 3 although it did vary based on dataset convolutional neural networks have been widely used for image processing 
1	152	10028	logistic regression is used as a baseline to map the features to stock movement random forest which constructs a multitude of decision trees at training time and outputs the class that is the majority vote of the individual leaves is widely used for classification and regression we tuned depth of the tree and leaf size to regularize the model support vector machines are mentioned in previous research fully connected neural networks are composed by interconnected neurons whose activation functions capture the non linear relationship between the variables and the binary result 	we thought it might be effective to do convolutions over the sentence embeddings because of their structure however we also acknowledge that because of the pca and the way the google use dan works the adjacent features may not be relevant to each other 
0	152	10029	logistic regression is used as a baseline to map the features to stock movement random forest which constructs a multitude of decision trees at training time and outputs the class that is the majority vote of the individual leaves is widely used for classification and regression we tuned depth of the tree and leaf size to regularize the model support vector machines are mentioned in previous research fully connected neural networks are composed by interconnected neurons whose activation functions capture the non linear relationship between the variables and the binary result 	two 1d conv layers each followed by a pooling layer are included before the final fully connected layer 
1	152	10030	logistic regression is used as a baseline to map the features to stock movement random forest which constructs a multitude of decision trees at training time and outputs the class that is the majority vote of the individual leaves is widely used for classification and regression we tuned depth of the tree and leaf size to regularize the model support vector machines are mentioned in previous research fully connected neural networks are composed by interconnected neurons whose activation functions capture the non linear relationship between the variables and the binary result 	we picked the learning rate with which the model converges most effectively 1e 3 recurrent neural networks are proven to be effective in dealing with sequential data with the output being dependent on the previous computations 
0	152	10031	logistic regression is used as a baseline to map the features to stock movement random forest which constructs a multitude of decision trees at training time and outputs the class that is the majority vote of the individual leaves is widely used for classification and regression we tuned depth of the tree and leaf size to regularize the model support vector machines are mentioned in previous research fully connected neural networks are composed by interconnected neurons whose activation functions capture the non linear relationship between the variables and the binary result 	we are training separate rnns for each ticker 
0	152	10032	logistic regression is used as a baseline to map the features to stock movement random forest which constructs a multitude of decision trees at training time and outputs the class that is the majority vote of the individual leaves is widely used for classification and regression we tuned depth of the tree and leaf size to regularize the model support vector machines are mentioned in previous research fully connected neural networks are composed by interconnected neurons whose activation functions capture the non linear relationship between the variables and the binary result 	the learning rate is 1e 4 
1	152	10033	to examine the model stability we trained each model on the two datasets using ny times and google news separately with a learning rate mentioned in 4 3 respectively the mini batch size selected was the largest possible value to fit into cpu for rnn the mini batch is all data for each ticker we evaluate the model using mainly test accuracy 	to examine the model stability we trained each model on the two datasets using ny times and google news separately with a learning rate mentioned in 4 3 respectively 
1	152	10034	to examine the model stability we trained each model on the two datasets using ny times and google news separately with a learning rate mentioned in 4 3 respectively the mini batch size selected was the largest possible value to fit into cpu for rnn the mini batch is all data for each ticker we evaluate the model using mainly test accuracy 	the mini batch size selected was the largest possible value to fit into cpu for rnn the mini batch is all data for each ticker 
0	152	10035	to examine the model stability we trained each model on the two datasets using ny times and google news separately with a learning rate mentioned in 4 3 respectively the mini batch size selected was the largest possible value to fit into cpu for rnn the mini batch is all data for each ticker we evaluate the model using mainly test accuracy 	we evaluate the model using mainly test accuracy 
1	152	10036	to examine the model stability we trained each model on the two datasets using ny times and google news separately with a learning rate mentioned in 4 3 respectively the mini batch size selected was the largest possible value to fit into cpu for rnn the mini batch is all data for each ticker we evaluate the model using mainly test accuracy 	meanwhile we also monitor the f 1 score to ensure balanced performance on both 0 and 1 labels 
1	152	10037	to examine the model stability we trained each model on the two datasets using ny times and google news separately with a learning rate mentioned in 4 3 respectively the mini batch size selected was the largest possible value to fit into cpu for rnn the mini batch is all data for each ticker we evaluate the model using mainly test accuracy 	as shown in table 1 svm with rbf kernel is the best performing model on both datasets 
0	152	10038	to examine the model stability we trained each model on the two datasets using ny times and google news separately with a learning rate mentioned in 4 3 respectively the mini batch size selected was the largest possible value to fit into cpu for rnn the mini batch is all data for each ticker we evaluate the model using mainly test accuracy 	neural network and cnn also achieved decent performance 
0	152	10039	to examine the model stability we trained each model on the two datasets using ny times and google news separately with a learning rate mentioned in 4 3 respectively the mini batch size selected was the largest possible value to fit into cpu for rnn the mini batch is all data for each ticker we evaluate the model using mainly test accuracy 	however results from logistic regression and random forest are not satisfactory 
1	152	10040	best model svm with rbf kernel is able to project the features onto a high dimensional space and effectively separate the labels we tuned the cost parameter to prevent overfitting precision and recall rates of the best performing models on google news and ny times are shown in table 2 	best model svm with rbf kernel is able to project the features onto a high dimensional space and effectively separate the labels 
0	152	10041	best model svm with rbf kernel is able to project the features onto a high dimensional space and effectively separate the labels we tuned the cost parameter to prevent overfitting precision and recall rates of the best performing models on google news and ny times are shown in table 2 	we tuned the cost parameter to prevent overfitting 
0	152	10042	best model svm with rbf kernel is able to project the features onto a high dimensional space and effectively separate the labels we tuned the cost parameter to prevent overfitting precision and recall rates of the best performing models on google news and ny times are shown in table 2 	precision and recall rates of the best performing models on google news and ny times are shown in table 2 
1	152	10043	best model svm with rbf kernel is able to project the features onto a high dimensional space and effectively separate the labels we tuned the cost parameter to prevent overfitting precision and recall rates of the best performing models on google news and ny times are shown in table 2 	although we attempted to achieve a balanced performance on 0 and 1 labels the selected model still outputs a relatively imbalanced confusion matrix 
1	152	10044	best model svm with rbf kernel is able to project the features onto a high dimensional space and effectively separate the labels we tuned the cost parameter to prevent overfitting precision and recall rates of the best performing models on google news and ny times are shown in table 2 	we believe that such issue is raised by our loss function which is designed to maximize the overall accuracy but not to ensure the performance on both labels 
1	152	10045	best model svm with rbf kernel is able to project the features onto a high dimensional space and effectively separate the labels we tuned the cost parameter to prevent overfitting precision and recall rates of the best performing models on google news and ny times are shown in table 2 	bias data visualization reveals that our dataset is not separable in a low dimension space which explains why random forest and logistic regression with simple structure are not working well variance random forest shows the overfitting problem even after regularization 
1	152	10046	best model svm with rbf kernel is able to project the features onto a high dimensional space and effectively separate the labels we tuned the cost parameter to prevent overfitting precision and recall rates of the best performing models on google news and ny times are shown in table 2 	our dataset contains features which might be positively or negatively correlated with each other e g vectors representing news headlines 
0	152	10047	best model svm with rbf kernel is able to project the features onto a high dimensional space and effectively separate the labels we tuned the cost parameter to prevent overfitting precision and recall rates of the best performing models on google news and ny times are shown in table 2 	selecting a subset of such features may not be able to reduce the variance efficiently 
1	152	10048	best model svm with rbf kernel is able to project the features onto a high dimensional space and effectively separate the labels we tuned the cost parameter to prevent overfitting precision and recall rates of the best performing models on google news and ny times are shown in table 2 	stability as mentioned before for the rnn in order to capture the time series nature of each stock we split the dataset by ticker before running the model which in turn shrunk the data size 
0	152	10049	best model svm with rbf kernel is able to project the features onto a high dimensional space and effectively separate the labels we tuned the cost parameter to prevent overfitting precision and recall rates of the best performing models on google news and ny times are shown in table 2 	additionally some of the tickers had relatively sparse unique news data 
0	152	10050	best model svm with rbf kernel is able to project the features onto a high dimensional space and effectively separate the labels we tuned the cost parameter to prevent overfitting precision and recall rates of the best performing models on google news and ny times are shown in table 2 	furthermore it is probable that the deep structure of the model caused the gradient update to be inefficient 
0	152	10051	best model svm with rbf kernel is able to project the features onto a high dimensional space and effectively separate the labels we tuned the cost parameter to prevent overfitting precision and recall rates of the best performing models on google news and ny times are shown in table 2 	we also found that the rnn is very sensitive to the initialization of the hidden state which shed some light on the inefficacy of back propagation 
0	152	10052	best model svm with rbf kernel is able to project the features onto a high dimensional space and effectively separate the labels we tuned the cost parameter to prevent overfitting precision and recall rates of the best performing models on google news and ny times are shown in table 2 	to fix this we might change the structure of hidden state or use a different activation function 
1	152	10053	best model svm with rbf kernel is able to project the features onto a high dimensional space and effectively separate the labels we tuned the cost parameter to prevent overfitting precision and recall rates of the best performing models on google news and ny times are shown in table 2 	these are some possible reasons the rnn outputs a high proportion of 1s or 0s on some of the subsets and cannot be used as a stable model for future predictions to gain better understanding of the model performance we plotted the true and predicted stock movement of facebook in 2017 as follows where the same color on the same day indicates correct predictions 
1	152	10054	best model svm with rbf kernel is able to project the features onto a high dimensional space and effectively separate the labels we tuned the cost parameter to prevent overfitting precision and recall rates of the best performing models on google news and ny times are shown in table 2 	examining the predictions closely we found that the best performing model svm is more able to detect major up downs than smaller changes 
1	152	10055	best model svm with rbf kernel is able to project the features onto a high dimensional space and effectively separate the labels we tuned the cost parameter to prevent overfitting precision and recall rates of the best performing models on google news and ny times are shown in table 2 	6 conclusion future workin conclusion we think stock specific news might help in predicting next day stock movement 
0	152	10056	best model svm with rbf kernel is able to project the features onto a high dimensional space and effectively separate the labels we tuned the cost parameter to prevent overfitting precision and recall rates of the best performing models on google news and ny times are shown in table 2 	however it is hard to turn such informational edge into a profitable trading strategy given that we are merely predicting ups and downs 
0	152	10057	best model svm with rbf kernel is able to project the features onto a high dimensional space and effectively separate the labels we tuned the cost parameter to prevent overfitting precision and recall rates of the best performing models on google news and ny times are shown in table 2 	in addition our model seems to be more able to detect major movements than smaller ones 
1	152	10058	best model svm with rbf kernel is able to project the features onto a high dimensional space and effectively separate the labels we tuned the cost parameter to prevent overfitting precision and recall rates of the best performing models on google news and ny times are shown in table 2 	we believe the following steps can be taken to improve model performance in the future customized loss function we think achieving high accuracy and balanced performance on 1 and 0 labels are both important in stock movement prediction 
0	152	10059	best model svm with rbf kernel is able to project the features onto a high dimensional space and effectively separate the labels we tuned the cost parameter to prevent overfitting precision and recall rates of the best performing models on google news and ny times are shown in table 2 	however the second goal was not built into the loss function of our models 
1	152	10060	best model svm with rbf kernel is able to project the features onto a high dimensional space and effectively separate the labels we tuned the cost parameter to prevent overfitting precision and recall rates of the best performing models on google news and ny times are shown in table 2 	as the next step we can customize the loss function e g as binary cross entropy to obtain a more balanced performance enhance data quality to make the project usable in real life we built the dataset using news we scraped from the internet 
0	152	10061	best model svm with rbf kernel is able to project the features onto a high dimensional space and effectively separate the labels we tuned the cost parameter to prevent overfitting precision and recall rates of the best performing models on google news and ny times are shown in table 2 	such data might include irrelevant or inaccurate news which increases noise 
0	152	10062	best model svm with rbf kernel is able to project the features onto a high dimensional space and effectively separate the labels we tuned the cost parameter to prevent overfitting precision and recall rates of the best performing models on google news and ny times are shown in table 2 	in the future we think adding more cleaning techniques and including models to detect unhelpful news may help 
0	152	10063	our team spent 50 percent of our time on collecting and preprocessing data 20 percent on text representation and 30 percent price movement modelling and debugging given the challenging nature of our topic three of us worked closely during the whole process chris contributed primarily to collecting the trading data working on sentiment signal modelling using text representations and applying the models to new york times data 	our team spent 50 percent of our time on collecting and preprocessing data 20 percent on text representation and 30 percent price movement modelling and debugging 
0	152	10064	our team spent 50 percent of our time on collecting and preprocessing data 20 percent on text representation and 30 percent price movement modelling and debugging given the challenging nature of our topic three of us worked closely during the whole process chris contributed primarily to collecting the trading data working on sentiment signal modelling using text representations and applying the models to new york times data 	given the challenging nature of our topic three of us worked closely during the whole process 
0	152	10065	our team spent 50 percent of our time on collecting and preprocessing data 20 percent on text representation and 30 percent price movement modelling and debugging given the challenging nature of our topic three of us worked closely during the whole process chris contributed primarily to collecting the trading data working on sentiment signal modelling using text representations and applying the models to new york times data 	chris contributed primarily to collecting the trading data working on sentiment signal modelling using text representations and applying the models to new york times data 
0	152	10066	our team spent 50 percent of our time on collecting and preprocessing data 20 percent on text representation and 30 percent price movement modelling and debugging given the challenging nature of our topic three of us worked closely during the whole process chris contributed primarily to collecting the trading data working on sentiment signal modelling using text representations and applying the models to new york times data 	yilun contributed primarily to collecting sentiment data and testing and debugging the rnn and cnn models 
1	152	10067	our team spent 50 percent of our time on collecting and preprocessing data 20 percent on text representation and 30 percent price movement modelling and debugging given the challenging nature of our topic three of us worked closely during the whole process chris contributed primarily to collecting the trading data working on sentiment signal modelling using text representations and applying the models to new york times data 	iris contributed primarily to collecting sentiment and trading data data preprocessing and applying the models to google news data 
0	152	10068	our team spent 50 percent of our time on collecting and preprocessing data 20 percent on text representation and 30 percent price movement modelling and debugging given the challenging nature of our topic three of us worked closely during the whole process chris contributed primarily to collecting the trading data working on sentiment signal modelling using text representations and applying the models to new york times data 	we would like to thank the entire cs 229 teaching staff including our mentor atharva parulekar for providing invaluable feedback thorughout the course of the project 8 references bibliography
1	153	10069	the finance industry has been revolutionized by the increased availability of data the rise in computing power and the popularization of machine learning algorithms according to the wall street journal 2017b quantitative hedge funds represented 27 of total trading activity in 2017 rivaling the 29 that represents all individual investors most of these institutions are applying a machine learning approach to investing despite this boom in data driven strategies the literature that analyzes machine learning methods in financial forecasting is very limited with most papers focusing on stock return prediction 	the finance industry has been revolutionized by the increased availability of data the rise in computing power and the popularization of machine learning algorithms 
0	153	10070	the finance industry has been revolutionized by the increased availability of data the rise in computing power and the popularization of machine learning algorithms according to the wall street journal 2017b quantitative hedge funds represented 27 of total trading activity in 2017 rivaling the 29 that represents all individual investors most of these institutions are applying a machine learning approach to investing despite this boom in data driven strategies the literature that analyzes machine learning methods in financial forecasting is very limited with most papers focusing on stock return prediction 	according to the wall street journal 2017b quantitative hedge funds represented 27 of total trading activity in 2017 rivaling the 29 that represents all individual investors 
1	153	10071	the finance industry has been revolutionized by the increased availability of data the rise in computing power and the popularization of machine learning algorithms according to the wall street journal 2017b quantitative hedge funds represented 27 of total trading activity in 2017 rivaling the 29 that represents all individual investors most of these institutions are applying a machine learning approach to investing despite this boom in data driven strategies the literature that analyzes machine learning methods in financial forecasting is very limited with most papers focusing on stock return prediction 	most of these institutions are applying a machine learning approach to investing despite this boom in data driven strategies the literature that analyzes machine learning methods in financial forecasting is very limited with most papers focusing on stock return prediction 
1	153	10072	the finance industry has been revolutionized by the increased availability of data the rise in computing power and the popularization of machine learning algorithms according to the wall street journal 2017b quantitative hedge funds represented 27 of total trading activity in 2017 rivaling the 29 that represents all individual investors most of these institutions are applying a machine learning approach to investing despite this boom in data driven strategies the literature that analyzes machine learning methods in financial forecasting is very limited with most papers focusing on stock return prediction 	the objective of this paper is to produce directional fx forecasts that are able to yield profitable investment strategies 
1	153	10073	the finance industry has been revolutionized by the increased availability of data the rise in computing power and the popularization of machine learning algorithms according to the wall street journal 2017b quantitative hedge funds represented 27 of total trading activity in 2017 rivaling the 29 that represents all individual investors most of these institutions are applying a machine learning approach to investing despite this boom in data driven strategies the literature that analyzes machine learning methods in financial forecasting is very limited with most papers focusing on stock return prediction 	hence we approach the problem from two perspectives 1 classification of long short signals 2 point forecasts of fx levels that translate into long short signals these frameworks allow us to exploit different machine learning methodologies to solve a single problem designing a profitable fx strategy based on ml generated forecasts 
1	153	10074	machine learning methods have long been used in stock return prediction for instance variations of principal component analysis an unsupervised learning technique have been applied by nevertheless despite the large adoption of machine learning in stock return forecasting ml applications in fx prediction have been widely ignored by the literature few exceptions are available 	machine learning methods have long been used in stock return prediction 
1	153	10075	machine learning methods have long been used in stock return prediction for instance variations of principal component analysis an unsupervised learning technique have been applied by nevertheless despite the large adoption of machine learning in stock return forecasting ml applications in fx prediction have been widely ignored by the literature few exceptions are available 	for instance variations of principal component analysis an unsupervised learning technique have been applied by nevertheless despite the large adoption of machine learning in stock return forecasting ml applications in fx prediction have been widely ignored by the literature 
0	153	10076	machine learning methods have long been used in stock return prediction for instance variations of principal component analysis an unsupervised learning technique have been applied by nevertheless despite the large adoption of machine learning in stock return forecasting ml applications in fx prediction have been widely ignored by the literature few exceptions are available 	few exceptions are available 
1	153	10077	machine learning methods have long been used in stock return prediction for instance variations of principal component analysis an unsupervised learning technique have been applied by nevertheless despite the large adoption of machine learning in stock return forecasting ml applications in fx prediction have been widely ignored by the literature few exceptions are available 	the main contribution of this paper is the assessment of the statistical and economic performance of ml generated directional forecasts iii 
1	153	10078	machine learning methods have long been used in stock return prediction for instance variations of principal component analysis an unsupervised learning technique have been applied by nevertheless despite the large adoption of machine learning in stock return forecasting ml applications in fx prediction have been widely ignored by the literature few exceptions are available 	datasets we make use of two different datasets to explore the forecasting power of two types of variables market and fundamentals 
1	153	10079	machine learning methods have long been used in stock return prediction for instance variations of principal component analysis an unsupervised learning technique have been applied by nevertheless despite the large adoption of machine learning in stock return forecasting ml applications in fx prediction have been widely ignored by the literature few exceptions are available 	we define a market variable as an indicator with daily to weekly frequency that has a close relationship with traded securities 
1	153	10080	machine learning methods have long been used in stock return prediction for instance variations of principal component analysis an unsupervised learning technique have been applied by nevertheless despite the large adoption of machine learning in stock return forecasting ml applications in fx prediction have been widely ignored by the literature few exceptions are available 	on the other hand we define a fundamental variable as an indicator with monthly frequency that is closely related to the macroeconomy finally we limit the scope of our project to forecasting the usdmxn which is the exchange rate between the us dollar usd and the mexican peso mxn expressed in mxn per usd 
0	153	10081	machine learning methods have long been used in stock return prediction for instance variations of principal component analysis an unsupervised learning technique have been applied by nevertheless despite the large adoption of machine learning in stock return forecasting ml applications in fx prediction have been widely ignored by the literature few exceptions are available 	however the exercise can be generalized to other currencies 
1	153	10082	machine learning methods have long been used in stock return prediction for instance variations of principal component analysis an unsupervised learning technique have been applied by nevertheless despite the large adoption of machine learning in stock return forecasting ml applications in fx prediction have been widely ignored by the literature few exceptions are available 	all data was retrieved either from bloomberg the global financial dataset or the federal reserve bank 
0	153	10083	we obtained the weekly closing price of the usdmxn currency pair which we use as our target variable in addition we consider 25 features across both mexico and the united states a summary is shown in	we obtained the weekly closing price of the usdmxn currency pair which we use as our target variable 
0	153	10084	we obtained the weekly closing price of the usdmxn currency pair which we use as our target variable in addition we consider 25 features across both mexico and the united states a summary is shown in	in addition we consider 25 features across both mexico and the united states 
0	153	10085	we obtained the weekly closing price of the usdmxn currency pair which we use as our target variable in addition we consider 25 features across both mexico and the united states a summary is shown in	a summary is shown in
1	153	10086	the fundamental variables data uses the monthly closing price of the usdmxn currency pair as our target variable we use 27 features that describe the macroeconomic conditions of both the us and mexico between march 1990 and october 2018 the additional features that are considered in this dataset are detailed in	the fundamental variables data uses the monthly closing price of the usdmxn currency pair as our target variable 
0	153	10087	the fundamental variables data uses the monthly closing price of the usdmxn currency pair as our target variable we use 27 features that describe the macroeconomic conditions of both the us and mexico between march 1990 and october 2018 the additional features that are considered in this dataset are detailed in	we use 27 features that describe the macroeconomic conditions of both the us and mexico between march 1990 and october 2018 
0	153	10088	the fundamental variables data uses the monthly closing price of the usdmxn currency pair as our target variable we use 27 features that describe the macroeconomic conditions of both the us and mexico between march 1990 and october 2018 the additional features that are considered in this dataset are detailed in	the additional features that are considered in this dataset are detailed in
0	153	10089	almost all data processing is identical in both datasets we first split the data into 60 train set 20 validation set and 20 test set these subsets are taken sequentially in order to keep the time series nature of the data and to guarantee our algorithms train exclusively on past data to translate our problem into a classification problem we introduce the signal t variable which we set to 1 if the usdmxn was higher tomorrow than today 	almost all data processing is identical in both datasets 
0	153	10090	almost all data processing is identical in both datasets we first split the data into 60 train set 20 validation set and 20 test set these subsets are taken sequentially in order to keep the time series nature of the data and to guarantee our algorithms train exclusively on past data to translate our problem into a classification problem we introduce the signal t variable which we set to 1 if the usdmxn was higher tomorrow than today 	we first split the data into 60 train set 20 validation set and 20 test set 
1	153	10091	almost all data processing is identical in both datasets we first split the data into 60 train set 20 validation set and 20 test set these subsets are taken sequentially in order to keep the time series nature of the data and to guarantee our algorithms train exclusively on past data to translate our problem into a classification problem we introduce the signal t variable which we set to 1 if the usdmxn was higher tomorrow than today 	these subsets are taken sequentially in order to keep the time series nature of the data and to guarantee our algorithms train exclusively on past data to translate our problem into a classification problem we introduce the signal t variable which we set to 1 if the usdmxn was higher tomorrow than today 
1	153	10092	almost all data processing is identical in both datasets we first split the data into 60 train set 20 validation set and 20 test set these subsets are taken sequentially in order to keep the time series nature of the data and to guarantee our algorithms train exclusively on past data to translate our problem into a classification problem we introduce the signal t variable which we set to 1 if the usdmxn was higher tomorrow than today 	this is we also perform data processing on the features 
1	153	10093	almost all data processing is identical in both datasets we first split the data into 60 train set 20 validation set and 20 test set these subsets are taken sequentially in order to keep the time series nature of the data and to guarantee our algorithms train exclusively on past data to translate our problem into a classification problem we introduce the signal t variable which we set to 1 if the usdmxn was higher tomorrow than today 	in particular we standardize using the mean and standard deviation of the training set for every covariate for the fundamentals dataset covariates are lagged by an additional period 
0	153	10094	almost all data processing is identical in both datasets we first split the data into 60 train set 20 validation set and 20 test set these subsets are taken sequentially in order to keep the time series nature of the data and to guarantee our algorithms train exclusively on past data to translate our problem into a classification problem we introduce the signal t variable which we set to 1 if the usdmxn was higher tomorrow than today 	this is done to approximate the fact that it is extremely rare to obtain real time macroeconomic data 
0	153	10095	almost all data processing is identical in both datasets we first split the data into 60 train set 20 validation set and 20 test set these subsets are taken sequentially in order to keep the time series nature of the data and to guarantee our algorithms train exclusively on past data to translate our problem into a classification problem we introduce the signal t variable which we set to 1 if the usdmxn was higher tomorrow than today 	by lagging the features by one month we ensure we are not peeking into the future by including unpublished data 
1	153	10096	first we perform binary classification on the signal t variable we constructed in the data processing step this essentially transforms what initially is a continuous variable problem into a classification task on a second exercise we use ml algorithms to construct point forecasts for our raw continuous target variable usdmxn t we then construct an estimated long short signal by computing both strategies yield a binary signal output that we can execute as a trading strategy 	first we perform binary classification on the signal t variable we constructed in the data processing step 
1	153	10097	first we perform binary classification on the signal t variable we constructed in the data processing step this essentially transforms what initially is a continuous variable problem into a classification task on a second exercise we use ml algorithms to construct point forecasts for our raw continuous target variable usdmxn t we then construct an estimated long short signal by computing both strategies yield a binary signal output that we can execute as a trading strategy 	this essentially transforms what initially is a continuous variable problem into a classification task on a second exercise we use ml algorithms to construct point forecasts for our raw continuous target variable usdmxn t 
1	153	10098	first we perform binary classification on the signal t variable we constructed in the data processing step this essentially transforms what initially is a continuous variable problem into a classification task on a second exercise we use ml algorithms to construct point forecasts for our raw continuous target variable usdmxn t we then construct an estimated long short signal by computing both strategies yield a binary signal output that we can execute as a trading strategy 	we then construct an estimated long short signal by computing both strategies yield a binary signal output that we can execute as a trading strategy 
0	153	10099	the performance of different machine learning algorithms is tested for each framework in particular we considered 1 logistic linear regression we use logistic and linear regression as our benchmark models 2 regularized logistic linear regression we consider l 1 and l 2 regularization applied to logistic and linear regression this allows to reduce overfitting in the validation set 	the performance of different machine learning algorithms is tested for each framework 
1	153	10100	the performance of different machine learning algorithms is tested for each framework in particular we considered 1 logistic linear regression we use logistic and linear regression as our benchmark models 2 regularized logistic linear regression we consider l 1 and l 2 regularization applied to logistic and linear regression this allows to reduce overfitting in the validation set 	in particular we considered 1 logistic linear regression we use logistic and linear regression as our benchmark models 2 regularized logistic linear regression we consider l 1 and l 2 regularization applied to logistic and linear regression 
0	153	10101	the performance of different machine learning algorithms is tested for each framework in particular we considered 1 logistic linear regression we use logistic and linear regression as our benchmark models 2 regularized logistic linear regression we consider l 1 and l 2 regularization applied to logistic and linear regression this allows to reduce overfitting in the validation set 	this allows to reduce overfitting in the validation set 
1	153	10102	the performance of different machine learning algorithms is tested for each framework in particular we considered 1 logistic linear regression we use logistic and linear regression as our benchmark models 2 regularized logistic linear regression we consider l 1 and l 2 regularization applied to logistic and linear regression this allows to reduce overfitting in the validation set 	the hyperparameter which penalizes large coefficients is tuned using the validation set accuracy 3 support vector machines regression svm svr it is highly likely that fitting fx dynamics requires a non linear boundary 
1	153	10103	the performance of different machine learning algorithms is tested for each framework in particular we considered 1 logistic linear regression we use logistic and linear regression as our benchmark models 2 regularized logistic linear regression we consider l 1 and l 2 regularization applied to logistic and linear regression this allows to reduce overfitting in the validation set 	svm svr with a gaussian kernel provide the flexibility to generate a non linear boundary as a result of the infinite dimensional feature vector generated by the kernel 
1	153	10104	tree based models allow us to capture complex interactions between the variables unlike random forests which require bootstrapping gbc allows us to keep the time series structure of the data while considering non linearities it is important to notice that gbc and gbr is just considered for the market variables dataset due to the division of work between the authors see section ix 	tree based models allow us to capture complex interactions between the variables 
1	153	10105	tree based models allow us to capture complex interactions between the variables unlike random forests which require bootstrapping gbc allows us to keep the time series structure of the data while considering non linearities it is important to notice that gbc and gbr is just considered for the market variables dataset due to the division of work between the authors see section ix 	unlike random forests which require bootstrapping gbc allows us to keep the time series structure of the data while considering non linearities 
0	153	10106	tree based models allow us to capture complex interactions between the variables unlike random forests which require bootstrapping gbc allows us to keep the time series structure of the data while considering non linearities it is important to notice that gbc and gbr is just considered for the market variables dataset due to the division of work between the authors see section ix 	it is important to notice that gbc and gbr is just considered for the market variables dataset due to the division of work between the authors see section ix 
0	153	10107	neural networks can model complex relationships between input features which could improve the forecasting performance we consider fullyconnected networks the architecture is shown in our choice for loss depends on the framework 	neural networks can model complex relationships between input features which could improve the forecasting performance 
0	153	10108	neural networks can model complex relationships between input features which could improve the forecasting performance we consider fullyconnected networks the architecture is shown in our choice for loss depends on the framework 	we consider fullyconnected networks 
1	153	10109	neural networks can model complex relationships between input features which could improve the forecasting performance we consider fullyconnected networks the architecture is shown in our choice for loss depends on the framework 	the architecture is shown in our choice for loss depends on the framework 
1	153	10110	neural networks can model complex relationships between input features which could improve the forecasting performance we consider fullyconnected networks the architecture is shown in our choice for loss depends on the framework 	we select logistic loss for classification and mean squared error for the continuous target variable problem 
1	153	10111	neural networks can model complex relationships between input features which could improve the forecasting performance we consider fullyconnected networks the architecture is shown in our choice for loss depends on the framework 	we choose the proper activations in the same fashion sigmoid is used for classification while relu is used for the continuous target variable 
0	153	10112	neural networks can model complex relationships between input features which could improve the forecasting performance we consider fullyconnected networks the architecture is shown in our choice for loss depends on the framework 	finally we use dropout or activation regularization to avoid overfitting 
1	153	10113	all model parameters are tuned using the validation set we use accuracy as our performance evaluation in the binary classification model and mean squared error in the continuous target variable model the resulting parameters are detailed in there is however an important caveat when interpreting the results 	all model parameters are tuned using the validation set 
1	153	10114	all model parameters are tuned using the validation set we use accuracy as our performance evaluation in the binary classification model and mean squared error in the continuous target variable model the resulting parameters are detailed in there is however an important caveat when interpreting the results 	we use accuracy as our performance evaluation in the binary classification model and mean squared error in the continuous target variable model 
1	153	10115	all model parameters are tuned using the validation set we use accuracy as our performance evaluation in the binary classification model and mean squared error in the continuous target variable model the resulting parameters are detailed in there is however an important caveat when interpreting the results 	the resulting parameters are detailed in there is however an important caveat when interpreting the results 
1	153	10116	all model parameters are tuned using the validation set we use accuracy as our performance evaluation in the binary classification model and mean squared error in the continuous target variable model the resulting parameters are detailed in there is however an important caveat when interpreting the results 	being a measurement of the fraction of predictions that we can correctly forecast accuracy does not differentiate between true positives and true negatives 
1	153	10117	all model parameters are tuned using the validation set we use accuracy as our performance evaluation in the binary classification model and mean squared error in the continuous target variable model the resulting parameters are detailed in there is however an important caveat when interpreting the results 	a successful trading strategy should exploit true positives and true negatives while minimizing false positives and false negatives to discern between these cases given the bad results of the confusion matrix for the binary classification problem we explore the results of the continuous experiments 
0	153	10118	all model parameters are tuned using the validation set we use accuracy as our performance evaluation in the binary classification model and mean squared error in the continuous target variable model the resulting parameters are detailed in there is however an important caveat when interpreting the results 	economic performance a model with very successful statistical performance of long short signals does not imply positive economic implications 
1	153	10119	all model parameters are tuned using the validation set we use accuracy as our performance evaluation in the binary classification model and mean squared error in the continuous target variable model the resulting parameters are detailed in there is however an important caveat when interpreting the results 	this is an inherent problem in directional forecasts a profitable investment strategy requires algorithms that correctly predict the direction of very large movements in the price of the asset 
1	153	10120	all model parameters are tuned using the validation set we use accuracy as our performance evaluation in the binary classification model and mean squared error in the continuous target variable model the resulting parameters are detailed in there is however an important caveat when interpreting the results 	in our case if an algorithm correctly predicts most small changes but misses large jumps in the exchange rate it is very likely that it will produce negative economic performance upon execution 
0	153	10121	all model parameters are tuned using the validation set we use accuracy as our performance evaluation in the binary classification model and mean squared error in the continuous target variable model the resulting parameters are detailed in there is however an important caveat when interpreting the results 	this issue has been previously assessed in the literature by at the end of every period the position is closed profits are cashed in and the strategy is repeated 
0	153	10122	all model parameters are tuned using the validation set we use accuracy as our performance evaluation in the binary classification model and mean squared error in the continuous target variable model the resulting parameters are detailed in there is however an important caveat when interpreting the results 	finally we use a longonly strategy as our benchmark for economic performance a 
0	153	10123	all model parameters are tuned using the validation set we use accuracy as our performance evaluation in the binary classification model and mean squared error in the continuous target variable model the resulting parameters are detailed in there is however an important caveat when interpreting the results 	binary classification the statistically best performing model corresponds to the economically most profitable specification 
0	153	10124	all model parameters are tuned using the validation set we use accuracy as our performance evaluation in the binary classification model and mean squared error in the continuous target variable model the resulting parameters are detailed in there is however an important caveat when interpreting the results 	however it is important to notice that this positive result is mostly driven by a single correct bet made between weeks 725 and 750 
1	153	10125	all model parameters are tuned using the validation set we use accuracy as our performance evaluation in the binary classification model and mean squared error in the continuous target variable model the resulting parameters are detailed in there is however an important caveat when interpreting the results 	all other strategies produce profits that are equal to or worse than the long only benchmark these results can be explained by the bad performance of the models in terms of the confusion matrix 
0	153	10126	all model parameters are tuned using the validation set we use accuracy as our performance evaluation in the binary classification model and mean squared error in the continuous target variable model the resulting parameters are detailed in there is however an important caveat when interpreting the results 	due to the very low true negative rate of most models all specifications are close to the long only benchmark and the departures are a consequence of few correct or incorrect short bets 
0	153	10127	all model parameters are tuned using the validation set we use accuracy as our performance evaluation in the binary classification model and mean squared error in the continuous target variable model the resulting parameters are detailed in there is however an important caveat when interpreting the results 	the differences with respect to the binary classification results are once again significant 
1	153	10128	all model parameters are tuned using the validation set we use accuracy as our performance evaluation in the binary classification model and mean squared error in the continuous target variable model the resulting parameters are detailed in there is however an important caveat when interpreting the results 	the final cumulative return in the continuous target variable framework is around 15 higher than under the binary classification framework 
0	153	10129	all model parameters are tuned using the validation set we use accuracy as our performance evaluation in the binary classification model and mean squared error in the continuous target variable model the resulting parameters are detailed in there is however an important caveat when interpreting the results 	furthermore all strategies outperform the long only benchmark with the best strategy being ridge regression 
0	153	10130	in addition the economic effect of an improved true negative rate is considerable unlike the binary classification case the outperformance of all strategies with respect to the benchmark is not driven by few correct short positions moreover the reduction in the true positive rate observed for the continuous target variable framework does not significantly penalize cumulative profits 	in addition the economic effect of an improved true negative rate is considerable 
1	153	10131	in addition the economic effect of an improved true negative rate is considerable unlike the binary classification case the outperformance of all strategies with respect to the benchmark is not driven by few correct short positions moreover the reduction in the true positive rate observed for the continuous target variable framework does not significantly penalize cumulative profits 	unlike the binary classification case the outperformance of all strategies with respect to the benchmark is not driven by few correct short positions 
0	153	10132	in addition the economic effect of an improved true negative rate is considerable unlike the binary classification case the outperformance of all strategies with respect to the benchmark is not driven by few correct short positions moreover the reduction in the true positive rate observed for the continuous target variable framework does not significantly penalize cumulative profits 	moreover the reduction in the true positive rate observed for the continuous target variable framework does not significantly penalize cumulative profits 
1	153	10133	in addition the economic effect of an improved true negative rate is considerable unlike the binary classification case the outperformance of all strategies with respect to the benchmark is not driven by few correct short positions moreover the reduction in the true positive rate observed for the continuous target variable framework does not significantly penalize cumulative profits 	the gains of a high specificity outweigh any losses derived from the reduction in sensitivity a natural question to address is which variables explain exchange rate forecasts the most 
1	153	10134	in addition the economic effect of an improved true negative rate is considerable unlike the binary classification case the outperformance of all strategies with respect to the benchmark is not driven by few correct short positions moreover the reduction in the true positive rate observed for the continuous target variable framework does not significantly penalize cumulative profits 	finally another interesting insight is that the usdmxn reacts strongly to global and emerging market em fixed income indicators 
0	153	10135	in addition the economic effect of an improved true negative rate is considerable unlike the binary classification case the outperformance of all strategies with respect to the benchmark is not driven by few correct short positions moreover the reduction in the true positive rate observed for the continuous target variable framework does not significantly penalize cumulative profits 	in theory the bilateral exchange rate should react strongly to the interest rate differential between the two countries 
0	153	10136	in addition the economic effect of an improved true negative rate is considerable unlike the binary classification case the outperformance of all strategies with respect to the benchmark is not driven by few correct short positions moreover the reduction in the true positive rate observed for the continuous target variable framework does not significantly penalize cumulative profits 	we believe the observed result provides evidence of investor behavior 
0	153	10137	in addition the economic effect of an improved true negative rate is considerable unlike the binary classification case the outperformance of all strategies with respect to the benchmark is not driven by few correct short positions moreover the reduction in the true positive rate observed for the continuous target variable framework does not significantly penalize cumulative profits 	as documented in recent years by
1	153	10138	this paper makes use of machine learning methods to forecast the us dollar against mexican peso exchange rate we use an innovative framework to find the best possible performance first we consider a market variables dataset and a fundamentals dataset on which we train ml algorithms 	this paper makes use of machine learning methods to forecast the us dollar against mexican peso exchange rate 
0	153	10139	this paper makes use of machine learning methods to forecast the us dollar against mexican peso exchange rate we use an innovative framework to find the best possible performance first we consider a market variables dataset and a fundamentals dataset on which we train ml algorithms 	we use an innovative framework to find the best possible performance 
1	153	10140	this paper makes use of machine learning methods to forecast the us dollar against mexican peso exchange rate we use an innovative framework to find the best possible performance first we consider a market variables dataset and a fundamentals dataset on which we train ml algorithms 	first we consider a market variables dataset and a fundamentals dataset on which we train ml algorithms 
1	153	10141	this paper makes use of machine learning methods to forecast the us dollar against mexican peso exchange rate we use an innovative framework to find the best possible performance first we consider a market variables dataset and a fundamentals dataset on which we train ml algorithms 	second we conduct binary classification experiments and continuous target experiments to produce the same output a binary long short signal on which we are able to execute a simple trading strategy our results suggest that continuous target prediction outperforms binary classification not only in terms of accuracy but also in terms of specificity and sensitivity 
0	153	10142	this paper makes use of machine learning methods to forecast the us dollar against mexican peso exchange rate we use an innovative framework to find the best possible performance first we consider a market variables dataset and a fundamentals dataset on which we train ml algorithms 	the economic results are in line with this finding with all algorithms outperforming a long only benchmark 
1	153	10143	this paper makes use of machine learning methods to forecast the us dollar against mexican peso exchange rate we use an innovative framework to find the best possible performance first we consider a market variables dataset and a fundamentals dataset on which we train ml algorithms 	the best results are produced by svm in the binary classification case and ridge regression in the continuous target case both in terms of accuracy and cumulative profits 
0	153	10144	this paper makes use of machine learning methods to forecast the us dollar against mexican peso exchange rate we use an innovative framework to find the best possible performance first we consider a market variables dataset and a fundamentals dataset on which we train ml algorithms 	last we find that the fundamentals dataset yields poor results future work could focus in several areas 
0	153	10145	this paper makes use of machine learning methods to forecast the us dollar against mexican peso exchange rate we use an innovative framework to find the best possible performance first we consider a market variables dataset and a fundamentals dataset on which we train ml algorithms 	first the recursive validation procedure proposed in
0	153	10146	the team worked on the same problem but used different datasets the contribution to this work was as follows christian gonz lez rojas was in charge of data collecting data processing algorithm selection and algorithm implementation on the market variables dataset for both the continuous and the binary framework he decided to consider gbc gbr as an additional model to further test the value of nonlinear relationships 	the team worked on the same problem but used different datasets 
1	153	10147	the team worked on the same problem but used different datasets the contribution to this work was as follows christian gonz lez rojas was in charge of data collecting data processing algorithm selection and algorithm implementation on the market variables dataset for both the continuous and the binary framework he decided to consider gbc gbr as an additional model to further test the value of nonlinear relationships 	the contribution to this work was as follows christian gonz lez rojas was in charge of data collecting data processing algorithm selection and algorithm implementation on the market variables dataset for both the continuous and the binary framework 
0	153	10148	the team worked on the same problem but used different datasets the contribution to this work was as follows christian gonz lez rojas was in charge of data collecting data processing algorithm selection and algorithm implementation on the market variables dataset for both the continuous and the binary framework he decided to consider gbc gbr as an additional model to further test the value of nonlinear relationships 	he decided to consider gbc gbr as an additional model to further test the value of nonlinear relationships 
0	153	10149	the team worked on the same problem but used different datasets the contribution to this work was as follows christian gonz lez rojas was in charge of data collecting data processing algorithm selection and algorithm implementation on the market variables dataset for both the continuous and the binary framework he decided to consider gbc gbr as an additional model to further test the value of nonlinear relationships 	he was also responsible for writing the cs229 poster and the cs229 final report 
0	153	10150	the team worked on the same problem but used different datasets the contribution to this work was as follows christian gonz lez rojas was in charge of data collecting data processing algorithm selection and algorithm implementation on the market variables dataset for both the continuous and the binary framework he decided to consider gbc gbr as an additional model to further test the value of nonlinear relationships 	his data and code can be found at this link molly herman worked on data collection data processing and algorithms for the fundamentals dataset 
0	153	10151	the team worked on the same problem but used different datasets the contribution to this work was as follows christian gonz lez rojas was in charge of data collecting data processing algorithm selection and algorithm implementation on the market variables dataset for both the continuous and the binary framework he decided to consider gbc gbr as an additional model to further test the value of nonlinear relationships 	she was responsible for modifying the cs229 poster to create an alternative version for the cs229a presentation and was in charge of writing her own final report for cs229a the division of work for the poster and the final report was done to provide deeper insight on the results to which each author contributed the most 
0	154	10152	the 2sigma competition at kaggle aims at advancing our understanding of how the content of news analytics might influence the performance of stock prices for this purpose a large set of daily market and news data is provided for a subset of us listed financial instruments this data shall be used to train any kind of learning algorithm deemed useful in order to predict future stock market returns the competition comprises two stages with two different evaluation periods 	the 2sigma competition at kaggle aims at advancing our understanding of how the content of news analytics might influence the performance of stock prices 
0	154	10153	the 2sigma competition at kaggle aims at advancing our understanding of how the content of news analytics might influence the performance of stock prices for this purpose a large set of daily market and news data is provided for a subset of us listed financial instruments this data shall be used to train any kind of learning algorithm deemed useful in order to predict future stock market returns the competition comprises two stages with two different evaluation periods 	for this purpose a large set of daily market and news data is provided for a subset of us listed financial instruments 
0	154	10154	the 2sigma competition at kaggle aims at advancing our understanding of how the content of news analytics might influence the performance of stock prices for this purpose a large set of daily market and news data is provided for a subset of us listed financial instruments this data shall be used to train any kind of learning algorithm deemed useful in order to predict future stock market returns the competition comprises two stages with two different evaluation periods 	this data shall be used to train any kind of learning algorithm deemed useful in order to predict future stock market returns the competition comprises two stages with two different evaluation periods 
0	154	10155	the 2sigma competition at kaggle aims at advancing our understanding of how the content of news analytics might influence the performance of stock prices for this purpose a large set of daily market and news data is provided for a subset of us listed financial instruments this data shall be used to train any kind of learning algorithm deemed useful in order to predict future stock market returns the competition comprises two stages with two different evaluation periods 	in the first stage the predictions are tested against historical data of the period 1 1 2017 to 7 31 2018 
0	154	10156	the 2sigma competition at kaggle aims at advancing our understanding of how the content of news analytics might influence the performance of stock prices for this purpose a large set of daily market and news data is provided for a subset of us listed financial instruments this data shall be used to train any kind of learning algorithm deemed useful in order to predict future stock market returns the competition comprises two stages with two different evaluation periods 	this stage will be terminated early next year at which time the final submissions of the participating teams must be handed in 
0	154	10157	the 2sigma competition at kaggle aims at advancing our understanding of how the content of news analytics might influence the performance of stock prices for this purpose a large set of daily market and news data is provided for a subset of us listed financial instruments this data shall be used to train any kind of learning algorithm deemed useful in order to predict future stock market returns the competition comprises two stages with two different evaluation periods 	the latter will then be evaluated against future data for about six months to identify the best performing submission which will be disclosed 7 15 2019 the objective function for this machine learning task is set the same for all participants in the competition and constructed as follows for each day t within the evaluation period the value x t is calculated aswhere for any financial asset i 1 m the term ti 1 1 stands for the predicted confidence value that it s ten day market adjusted leading return r ti r is either positive or negative 
0	154	10158	the 2sigma competition at kaggle aims at advancing our understanding of how the content of news analytics might influence the performance of stock prices for this purpose a large set of daily market and news data is provided for a subset of us listed financial instruments this data shall be used to train any kind of learning algorithm deemed useful in order to predict future stock market returns the competition comprises two stages with two different evaluation periods 	the universe variable u ti 0 1 controls whether the asset i is included in the evaluation at the particular evaluation day t finally the score which determines the position in the competition is composed of the mean and the standard deviation of the daily value x t with score 0 for x t 0 we apply three different algorithms to this problem logistic regression neural network and gradient boosting tree 
0	154	10159	there have been multiple attempts looking into the popular topic of forecasting stock price with techniques of machine learning based on the works we find the focus of these research projects vary mainly in three ways 1 the text information used in prediction ranges from public news economy trend to exclusive information about the characteristics of the company 	there have been multiple attempts looking into the popular topic of forecasting stock price with techniques of machine learning 
0	154	10160	there have been multiple attempts looking into the popular topic of forecasting stock price with techniques of machine learning based on the works we find the focus of these research projects vary mainly in three ways 1 the text information used in prediction ranges from public news economy trend to exclusive information about the characteristics of the company 	based on the works we find the focus of these research projects vary mainly in three ways 
0	154	10161	there have been multiple attempts looking into the popular topic of forecasting stock price with techniques of machine learning based on the works we find the focus of these research projects vary mainly in three ways 1 the text information used in prediction ranges from public news economy trend to exclusive information about the characteristics of the company 	 1 the text information used in prediction ranges from public news economy trend to exclusive information about the characteristics of the company 
0	154	10162	there have been multiple attempts looking into the popular topic of forecasting stock price with techniques of machine learning based on the works we find the focus of these research projects vary mainly in three ways 1 the text information used in prediction ranges from public news economy trend to exclusive information about the characteristics of the company 	 2 the targeting price change can be near term high frequency less than a minute short term tomorrow to a few days later and long term months later 
0	154	10163	there have been multiple attempts looking into the popular topic of forecasting stock price with techniques of machine learning based on the works we find the focus of these research projects vary mainly in three ways 1 the text information used in prediction ranges from public news economy trend to exclusive information about the characteristics of the company 	3 datasets and features
0	154	10164	all the data used in the project is provided by kaggle two sources of data are provided one for market data and one for news data both spanning from 2007 to the end of 2016 the market data contains various financial market information for 3511 us listed instruments 	all the data used in the project is provided by kaggle 
0	154	10165	all the data used in the project is provided by kaggle two sources of data are provided one for market data and one for news data both spanning from 2007 to the end of 2016 the market data contains various financial market information for 3511 us listed instruments 	two sources of data are provided one for market data and one for news data both spanning from 2007 to the end of 2016 
0	154	10166	all the data used in the project is provided by kaggle two sources of data are provided one for market data and one for news data both spanning from 2007 to the end of 2016 the market data contains various financial market information for 3511 us listed instruments 	the market data contains various financial market information for 3511 us listed instruments 
0	154	10167	all the data used in the project is provided by kaggle two sources of data are provided one for market data and one for news data both spanning from 2007 to the end of 2016 the market data contains various financial market information for 3511 us listed instruments 	it is comprised of more than 4 million samples and 16 features the returnsopennextmktres10 column indicates the market normalized return for the next 10 days and thus serves as the ground truth value for the prediction task 
0	154	10168	all the data used in the project is provided by kaggle two sources of data are provided one for market data and one for news data both spanning from 2007 to the end of 2016 the market data contains various financial market information for 3511 us listed instruments 	the news data contains information at both article level and asset level 
0	154	10169	all the data used in the project is provided by kaggle two sources of data are provided one for market data and one for news data both spanning from 2007 to the end of 2016 the market data contains various financial market information for 3511 us listed instruments 	there are more 9 million samples and 35 features 
0	154	10170	all the data used in the project is provided by kaggle two sources of data are provided one for market data and one for news data both spanning from 2007 to the end of 2016 the market data contains various financial market information for 3511 us listed instruments 	most of the news features are either numerical or type indicators except the headline feature which contains text 
0	154	10171	all the data used in the project is provided by kaggle two sources of data are provided one for market data and one for news data both spanning from 2007 to the end of 2016 the market data contains various financial market information for 3511 us listed instruments 	the news data provided is intentionally not normalized both data sets can be joined by using either the time stamp asset code or asset name 
0	154	10172	as shown in	as shown in
0	154	10173	we chose logistic regression as a starting point for establishing a baseline score the logistic regression takes in all the features as is such that it does not include higher degree terms because of the large size of the training data small regularization is used 	we chose logistic regression as a starting point for establishing a baseline score 
0	154	10174	we chose logistic regression as a starting point for establishing a baseline score the logistic regression takes in all the features as is such that it does not include higher degree terms because of the large size of the training data small regularization is used 	the logistic regression takes in all the features as is such that it does not include higher degree terms 
0	154	10175	we chose logistic regression as a starting point for establishing a baseline score the logistic regression takes in all the features as is such that it does not include higher degree terms because of the large size of the training data small regularization is used 	because of the large size of the training data small regularization is used 
0	154	10176	we chose logistic regression as a starting point for establishing a baseline score the logistic regression takes in all the features as is such that it does not include higher degree terms because of the large size of the training data small regularization is used 	the log likely hood is
0	154	10177	we implement a fully connected neural network with two inputs into the first input branch we feed all numerical values of the preprocessed dataset while the second input branch encodes the categorical data asset code for each sample in a trainable embedding layer after batch normalisation and two fully connected layers for the numerical part and one fully connected layer for the categorical part both branches of the network are concatenated 	we implement a fully connected neural network with two inputs 
0	154	10178	we implement a fully connected neural network with two inputs into the first input branch we feed all numerical values of the preprocessed dataset while the second input branch encodes the categorical data asset code for each sample in a trainable embedding layer after batch normalisation and two fully connected layers for the numerical part and one fully connected layer for the categorical part both branches of the network are concatenated 	into the first input branch we feed all numerical values of the preprocessed dataset while the second input branch encodes the categorical data asset code for each sample in a trainable embedding layer 
0	154	10179	we implement a fully connected neural network with two inputs into the first input branch we feed all numerical values of the preprocessed dataset while the second input branch encodes the categorical data asset code for each sample in a trainable embedding layer after batch normalisation and two fully connected layers for the numerical part and one fully connected layer for the categorical part both branches of the network are concatenated 	after batch normalisation and two fully connected layers for the numerical part and one fully connected layer for the categorical part both branches of the network are concatenated 
0	154	10180	we implement a fully connected neural network with two inputs into the first input branch we feed all numerical values of the preprocessed dataset while the second input branch encodes the categorical data asset code for each sample in a trainable embedding layer after batch normalisation and two fully connected layers for the numerical part and one fully connected layer for the categorical part both branches of the network are concatenated 	the concatenated data is finally fed into one more fully connected layer followed by the output layer 
0	154	10181	we implement a fully connected neural network with two inputs into the first input branch we feed all numerical values of the preprocessed dataset while the second input branch encodes the categorical data asset code for each sample in a trainable embedding layer after batch normalisation and two fully connected layers for the numerical part and one fully connected layer for the categorical part both branches of the network are concatenated 	all fully connected layers use relu activation except the output layer which has a sigmoid activation function 
0	154	10182	gradient boosting is a technique that combines weak predicting models into an ensemble to produce a much stronger one it is typically implemented on decision trees like other boosting algorithms gradient boosting is an iterative operation 	gradient boosting is a technique that combines weak predicting models into an ensemble to produce a much stronger one 
0	154	10183	gradient boosting is a technique that combines weak predicting models into an ensemble to produce a much stronger one it is typically implemented on decision trees like other boosting algorithms gradient boosting is an iterative operation 	it is typically implemented on decision trees 
0	154	10184	gradient boosting is a technique that combines weak predicting models into an ensemble to produce a much stronger one it is typically implemented on decision trees like other boosting algorithms gradient boosting is an iterative operation 	like other boosting algorithms gradient boosting is an iterative operation 
0	154	10185	gradient boosting is a technique that combines weak predicting models into an ensemble to produce a much stronger one it is typically implemented on decision trees like other boosting algorithms gradient boosting is an iterative operation 	at each iteration the algorithm creates a new estimator that minimizes the loss with respect to the current model 
0	154	10186	gradient boosting is a technique that combines weak predicting models into an ensemble to produce a much stronger one it is typically implemented on decision trees like other boosting algorithms gradient boosting is an iterative operation 	this minimization can be approximated by fitting the new estimator to the gradient of loss such that where f k is the ensemble model at kth iteration r ik is the gradient residual of the loss function with respect to f k 1 for ith data h k is the new model that fits r ik for i 1 m l is the loss function binary log loss function for this project it is similar to the normal gradient descent except that the gradient descent is performed on the output of the model instead of the parameters of each weak model 
0	154	10187	gradient boosting is a technique that combines weak predicting models into an ensemble to produce a much stronger one it is typically implemented on decision trees like other boosting algorithms gradient boosting is an iterative operation 	the regularization is achieved through several ways by slowly decreasing the learning rate setting the number of minimum samples in a tree leaf limiting number of leaves or penalizing the complexity of the tree model such as l2 regularization 
0	154	10188	gradient boosting is a technique that combines weak predicting models into an ensemble to produce a much stronger one it is typically implemented on decision trees like other boosting algorithms gradient boosting is an iterative operation 	lightgbm library is used to implement this algorithm in this project 
0	154	10189	gradient boosting is a technique that combines weak predicting models into an ensemble to produce a much stronger one it is typically implemented on decision trees like other boosting algorithms gradient boosting is an iterative operation 	it converts continuous features into bins which reduces memory and boosts speed and grows each tree with the priority given to the leaf with maximum delta loss leading to lower overall loss 
0	154	10190	a auc curves the auc score for the logistic regression the fully connected neural network model and the light gbm model is 0 5 0 5799 and 0 5753 respectively as shown by	a auc curves the auc score for the logistic regression the fully connected neural network model and the light gbm model is 0 5 0 5799 and 0 5753 respectively as shown by
0	154	10191	out of the three attempted algorithms the neural network performs the best followed closely by the gradient boosting tree while the logistic regression behaves almost like a random guess as the logistic regression is fairly a simple algorithm with linear mappings to each feature dimension its incapability to capture the complex relationship is expected on the other hand both the neural network and gradient boosting tree are powerful non linear algorithms with a large degree of flexibility and control making them competent to model complex situations for future work deeper dive into the feature engineering is needed 	out of the three attempted algorithms the neural network performs the best followed closely by the gradient boosting tree while the logistic regression behaves almost like a random guess 
0	154	10192	out of the three attempted algorithms the neural network performs the best followed closely by the gradient boosting tree while the logistic regression behaves almost like a random guess as the logistic regression is fairly a simple algorithm with linear mappings to each feature dimension its incapability to capture the complex relationship is expected on the other hand both the neural network and gradient boosting tree are powerful non linear algorithms with a large degree of flexibility and control making them competent to model complex situations for future work deeper dive into the feature engineering is needed 	as the logistic regression is fairly a simple algorithm with linear mappings to each feature dimension its incapability to capture the complex relationship is expected 
0	154	10193	out of the three attempted algorithms the neural network performs the best followed closely by the gradient boosting tree while the logistic regression behaves almost like a random guess as the logistic regression is fairly a simple algorithm with linear mappings to each feature dimension its incapability to capture the complex relationship is expected on the other hand both the neural network and gradient boosting tree are powerful non linear algorithms with a large degree of flexibility and control making them competent to model complex situations for future work deeper dive into the feature engineering is needed 	on the other hand both the neural network and gradient boosting tree are powerful non linear algorithms with a large degree of flexibility and control making them competent to model complex situations for future work deeper dive into the feature engineering is needed 
0	154	10194	out of the three attempted algorithms the neural network performs the best followed closely by the gradient boosting tree while the logistic regression behaves almost like a random guess as the logistic regression is fairly a simple algorithm with linear mappings to each feature dimension its incapability to capture the complex relationship is expected on the other hand both the neural network and gradient boosting tree are powerful non linear algorithms with a large degree of flexibility and control making them competent to model complex situations for future work deeper dive into the feature engineering is needed 	it is also worth exploring to combine neural network and gradient boosting tree in an ensemble fashion to produce a stronger model 
0	154	10195	out of the three attempted algorithms the neural network performs the best followed closely by the gradient boosting tree while the logistic regression behaves almost like a random guess as the logistic regression is fairly a simple algorithm with linear mappings to each feature dimension its incapability to capture the complex relationship is expected on the other hand both the neural network and gradient boosting tree are powerful non linear algorithms with a large degree of flexibility and control making them competent to model complex situations for future work deeper dive into the feature engineering is needed 	one of the news features is text based thus natural language processing can be implemented to extract useful information from it 
0	154	10196	out of the three attempted algorithms the neural network performs the best followed closely by the gradient boosting tree while the logistic regression behaves almost like a random guess as the logistic regression is fairly a simple algorithm with linear mappings to each feature dimension its incapability to capture the complex relationship is expected on the other hand both the neural network and gradient boosting tree are powerful non linear algorithms with a large degree of flexibility and control making them competent to model complex situations for future work deeper dive into the feature engineering is needed 	given the large parameter sets for the neural network and the gradient boosting tree achieve the optimum parameters is both difficult and time consuming 
0	154	10197	out of the three attempted algorithms the neural network performs the best followed closely by the gradient boosting tree while the logistic regression behaves almost like a random guess as the logistic regression is fairly a simple algorithm with linear mappings to each feature dimension its incapability to capture the complex relationship is expected on the other hand both the neural network and gradient boosting tree are powerful non linear algorithms with a large degree of flexibility and control making them competent to model complex situations for future work deeper dive into the feature engineering is needed 	however there is still possible room to make improvement by further tuning the parameters 
0	154	10198	out of the three attempted algorithms the neural network performs the best followed closely by the gradient boosting tree while the logistic regression behaves almost like a random guess as the logistic regression is fairly a simple algorithm with linear mappings to each feature dimension its incapability to capture the complex relationship is expected on the other hand both the neural network and gradient boosting tree are powerful non linear algorithms with a large degree of flexibility and control making them competent to model complex situations for future work deeper dive into the feature engineering is needed 	lastly choosing a more powerful baseline such as the support vector machine instead of the simple logistic regression should be considered 
0	154	10199	as a group working on this collaborated project we contributed equally overall barthold albrecht has additional contribution on establishment of the logistic regression model and the fully connected neural network model yanzhuo wang has additional contribution on establishment of the logistic regression model and the lgbm model 	as a group working on this collaborated project we contributed equally overall 
0	154	10200	as a group working on this collaborated project we contributed equally overall barthold albrecht has additional contribution on establishment of the logistic regression model and the fully connected neural network model yanzhuo wang has additional contribution on establishment of the logistic regression model and the lgbm model 	barthold albrecht has additional contribution on establishment of the logistic regression model and the fully connected neural network model 
0	154	10201	as a group working on this collaborated project we contributed equally overall barthold albrecht has additional contribution on establishment of the logistic regression model and the fully connected neural network model yanzhuo wang has additional contribution on establishment of the logistic regression model and the lgbm model 	yanzhuo wang has additional contribution on establishment of the logistic regression model and the lgbm model 
0	154	10202	as a group working on this collaborated project we contributed equally overall barthold albrecht has additional contribution on establishment of the logistic regression model and the fully connected neural network model yanzhuo wang has additional contribution on establishment of the logistic regression model and the lgbm model 	xiaofang zhu has additional contribution on establishing the fully connected neural network model 
0	154	10203	https drive google com open id 1mnf5opuzbdvotkxl6sjkglpqabcn8v9x	https drive google com open id 1mnf5opuzbdvotkxl6sjkglpqabcn8v9x
0	155	10204	the real estate market in large metropolitan areas across usa and canada is characterized by high volatility home prices in popular technological and cultural centers such as vancouver canada have been reportedly growing over the last decade owing to a constant influx of people including immigrants attracted by a combination of career opportunities and a superb geographical setting by the ocean bay and nearby mountains as a result predicting home prices has become a big challenge 	the real estate market in large metropolitan areas across usa and canada is characterized by high volatility 
0	155	10205	the real estate market in large metropolitan areas across usa and canada is characterized by high volatility home prices in popular technological and cultural centers such as vancouver canada have been reportedly growing over the last decade owing to a constant influx of people including immigrants attracted by a combination of career opportunities and a superb geographical setting by the ocean bay and nearby mountains as a result predicting home prices has become a big challenge 	home prices in popular technological and cultural centers such as vancouver canada have been reportedly growing over the last decade owing to a constant influx of people including immigrants attracted by a combination of career opportunities and a superb geographical setting by the ocean bay and nearby mountains 
0	155	10206	the real estate market in large metropolitan areas across usa and canada is characterized by high volatility home prices in popular technological and cultural centers such as vancouver canada have been reportedly growing over the last decade owing to a constant influx of people including immigrants attracted by a combination of career opportunities and a superb geographical setting by the ocean bay and nearby mountains as a result predicting home prices has become a big challenge 	as a result predicting home prices has become a big challenge 
0	155	10207	the real estate market in large metropolitan areas across usa and canada is characterized by high volatility home prices in popular technological and cultural centers such as vancouver canada have been reportedly growing over the last decade owing to a constant influx of people including immigrants attracted by a combination of career opportunities and a superb geographical setting by the ocean bay and nearby mountains as a result predicting home prices has become a big challenge 	real estate agents use their domain knowledge to estimate a home price aiding sellers and buyers in the transaction 
1	155	10208	the real estate market in large metropolitan areas across usa and canada is characterized by high volatility home prices in popular technological and cultural centers such as vancouver canada have been reportedly growing over the last decade owing to a constant influx of people including immigrants attracted by a combination of career opportunities and a superb geographical setting by the ocean bay and nearby mountains as a result predicting home prices has become a big challenge 	this estimate is often very subjective and facilitates bubbling the home prices especially in highly attractive areas like vancouver 
1	155	10209	the real estate market in large metropolitan areas across usa and canada is characterized by high volatility home prices in popular technological and cultural centers such as vancouver canada have been reportedly growing over the last decade owing to a constant influx of people including immigrants attracted by a combination of career opportunities and a superb geographical setting by the ocean bay and nearby mountains as a result predicting home prices has become a big challenge 	therefore our main study goal was to come up with an automated way of pricetagging a home based on its characteristics including floor area number of rooms location and others the input to our algorithm is a dataset of all condo listings under 2 5mm cad in downtown vancouver between january 2016 and october 2018 containing approximately 50 features after pre processing 
0	155	10210	the real estate market in large metropolitan areas across usa and canada is characterized by high volatility home prices in popular technological and cultural centers such as vancouver canada have been reportedly growing over the last decade owing to a constant influx of people including immigrants attracted by a combination of career opportunities and a superb geographical setting by the ocean bay and nearby mountains as a result predicting home prices has become a big challenge 	we then use linear regression neural networks and boosted tree models to predict the expected selling price of a condo 
0	155	10211	there is a good number of articles related to real estate pricing predictions in general it is difficult to compare the results given the diversity of features used to model the predictions and relevant error analysis however there are some common themes that we tried to reproduce and improve upon in our research 	there is a good number of articles related to real estate pricing predictions 
0	155	10212	there is a good number of articles related to real estate pricing predictions in general it is difficult to compare the results given the diversity of features used to model the predictions and relevant error analysis however there are some common themes that we tried to reproduce and improve upon in our research 	in general it is difficult to compare the results given the diversity of features used to model the predictions and relevant error analysis 
0	155	10213	there is a good number of articles related to real estate pricing predictions in general it is difficult to compare the results given the diversity of features used to model the predictions and relevant error analysis however there are some common themes that we tried to reproduce and improve upon in our research 	however there are some common themes that we tried to reproduce and improve upon in our research 
0	155	10214	there is a good number of articles related to real estate pricing predictions in general it is difficult to compare the results given the diversity of features used to model the predictions and relevant error analysis however there are some common themes that we tried to reproduce and improve upon in our research 	more often than not the authors attempt to use linear regression and boosted trees regression algorithms another author in another example related to bay area house pricing prediction
0	155	10215	the original dataset we received had exhaustive information about all condos listed for sale under 2 5mm cad in downtown vancouver between january 2016 and october 2018 the data was pulled from an official canadian real estate listings database called mls with the help of a local real estate agent and contained approximately 10 000 listings each listing had up to 237 features including immanent property characteristics like square footage number of bedrooms and bathrooms maintenance fees and relational characteristics like address vicinity to schools and public transportation and views 	the original dataset we received had exhaustive information about all condos listed for sale under 2 5mm cad in downtown vancouver between january 2016 and october 2018 
0	155	10216	the original dataset we received had exhaustive information about all condos listed for sale under 2 5mm cad in downtown vancouver between january 2016 and october 2018 the data was pulled from an official canadian real estate listings database called mls with the help of a local real estate agent and contained approximately 10 000 listings each listing had up to 237 features including immanent property characteristics like square footage number of bedrooms and bathrooms maintenance fees and relational characteristics like address vicinity to schools and public transportation and views 	the data was pulled from an official canadian real estate listings database called mls with the help of a local real estate agent and contained approximately 10 000 listings 
0	155	10217	the original dataset we received had exhaustive information about all condos listed for sale under 2 5mm cad in downtown vancouver between january 2016 and october 2018 the data was pulled from an official canadian real estate listings database called mls with the help of a local real estate agent and contained approximately 10 000 listings each listing had up to 237 features including immanent property characteristics like square footage number of bedrooms and bathrooms maintenance fees and relational characteristics like address vicinity to schools and public transportation and views 	each listing had up to 237 features including immanent property characteristics like square footage number of bedrooms and bathrooms maintenance fees and relational characteristics like address vicinity to schools and public transportation and views 
0	155	10218	the original dataset we received had exhaustive information about all condos listed for sale under 2 5mm cad in downtown vancouver between january 2016 and october 2018 the data was pulled from an official canadian real estate listings database called mls with the help of a local real estate agent and contained approximately 10 000 listings each listing had up to 237 features including immanent property characteristics like square footage number of bedrooms and bathrooms maintenance fees and relational characteristics like address vicinity to schools and public transportation and views 	the features can be classified into three categories structured data e g total floor area semistructured data e g address unstructured data e g listing agent comments 
0	155	10219	the original dataset we received had exhaustive information about all condos listed for sale under 2 5mm cad in downtown vancouver between january 2016 and october 2018 the data was pulled from an official canadian real estate listings database called mls with the help of a local real estate agent and contained approximately 10 000 listings each listing had up to 237 features including immanent property characteristics like square footage number of bedrooms and bathrooms maintenance fees and relational characteristics like address vicinity to schools and public transportation and views 	furthermore data can be categorized into various types interval scaled variables e g number of bedrooms year built etc 
0	155	10220	the original dataset we received had exhaustive information about all condos listed for sale under 2 5mm cad in downtown vancouver between january 2016 and october 2018 the data was pulled from an official canadian real estate listings database called mls with the help of a local real estate agent and contained approximately 10 000 listings each listing had up to 237 features including immanent property characteristics like square footage number of bedrooms and bathrooms maintenance fees and relational characteristics like address vicinity to schools and public transportation and views 	 temporal e g date property was sold rank e g floor number boolean e g fireplace yes no categorical e g dwelling type 
0	155	10221	our dataset did not have the geographical location of homes originally only the physical addresses since location is supposedly important for home value we used the google maps api 12 to geocode condo addresses to geographical coordinates latitude longitude in addition since the area of interest was relatively small only about 9 km 2 we approximated it with a flat rectangle and converted geographic to cartesian coordinates mapping all condos to a c feature engineering view scoring	our dataset did not have the geographical location of homes originally only the physical addresses 
0	155	10222	our dataset did not have the geographical location of homes originally only the physical addresses since location is supposedly important for home value we used the google maps api 12 to geocode condo addresses to geographical coordinates latitude longitude in addition since the area of interest was relatively small only about 9 km 2 we approximated it with a flat rectangle and converted geographic to cartesian coordinates mapping all condos to a c feature engineering view scoring	since location is supposedly important for home value we used the google maps api 12 to geocode condo addresses to geographical coordinates latitude longitude 
0	155	10223	our dataset did not have the geographical location of homes originally only the physical addresses since location is supposedly important for home value we used the google maps api 12 to geocode condo addresses to geographical coordinates latitude longitude in addition since the area of interest was relatively small only about 9 km 2 we approximated it with a flat rectangle and converted geographic to cartesian coordinates mapping all condos to a c feature engineering view scoring	in addition since the area of interest was relatively small only about 9 km 2 we approximated it with a flat rectangle and converted geographic to cartesian coordinates mapping all condos to a c feature engineering view scoring
0	155	10224	cleaning the data was essential to having an accurate model since it was originally input by real estate agents and was prone to mistakes we removed outliers for the numerical features using three sigma rule we then imputed data for features that had occasionally missing data less than 1 with medians 	cleaning the data was essential to having an accurate model since it was originally input by real estate agents and was prone to mistakes 
0	155	10225	cleaning the data was essential to having an accurate model since it was originally input by real estate agents and was prone to mistakes we removed outliers for the numerical features using three sigma rule we then imputed data for features that had occasionally missing data less than 1 with medians 	we removed outliers for the numerical features using three sigma rule 
0	155	10226	cleaning the data was essential to having an accurate model since it was originally input by real estate agents and was prone to mistakes we removed outliers for the numerical features using three sigma rule we then imputed data for features that had occasionally missing data less than 1 with medians 	we then imputed data for features that had occasionally missing data less than 1 with medians 
0	155	10227	cleaning the data was essential to having an accurate model since it was originally input by real estate agents and was prone to mistakes we removed outliers for the numerical features using three sigma rule we then imputed data for features that had occasionally missing data less than 1 with medians 	lastly we standardized the data 
0	155	10228	cleaning the data was essential to having an accurate model since it was originally input by real estate agents and was prone to mistakes we removed outliers for the numerical features using three sigma rule we then imputed data for features that had occasionally missing data less than 1 with medians 	the final feature set consisted of 48 features 
0	155	10229	to improve the quality of our predictions we performed error analysis with k fold cross validation cv we split our dataset randomly into a training and test set 80 20 the training set was used in cv setting where it was sequentially split into training and validation subsets k 5 times 	to improve the quality of our predictions we performed error analysis with k fold cross validation cv 
0	155	10230	to improve the quality of our predictions we performed error analysis with k fold cross validation cv we split our dataset randomly into a training and test set 80 20 the training set was used in cv setting where it was sequentially split into training and validation subsets k 5 times 	we split our dataset randomly into a training and test set 80 20 
0	155	10231	to improve the quality of our predictions we performed error analysis with k fold cross validation cv we split our dataset randomly into a training and test set 80 20 the training set was used in cv setting where it was sequentially split into training and validation subsets k 5 times 	the training set was used in cv setting where it was sequentially split into training and validation subsets k 5 times 
0	155	10232	to improve the quality of our predictions we performed error analysis with k fold cross validation cv we split our dataset randomly into a training and test set 80 20 the training set was used in cv setting where it was sequentially split into training and validation subsets k 5 times 	the training subset was used to fit the model to the data and validation subset was used to compute errors 
0	155	10233	to improve the quality of our predictions we performed error analysis with k fold cross validation cv we split our dataset randomly into a training and test set 80 20 the training set was used in cv setting where it was sequentially split into training and validation subsets k 5 times 	the average of k errors the cv error characterized how accurately our model performed we used two main metrics for calculating errors mean squared error and coefficient of determination r squared with y i predicted variable observations its mean x i vector of independent variables features f x the model mapping features x on y and n number of observations i 1 n 
0	155	10234	to improve the quality of our predictions we performed error analysis with k fold cross validation cv we split our dataset randomly into a training and test set 80 20 the training set was used in cv setting where it was sequentially split into training and validation subsets k 5 times 	mse characterizes the average of squared deviations of predictions from observations with mse 0 corresponding to an idealistic model exactly mimicking observations 
0	155	10235	to improve the quality of our predictions we performed error analysis with k fold cross validation cv we split our dataset randomly into a training and test set 80 20 the training set was used in cv setting where it was sequentially split into training and validation subsets k 5 times 	r 2 measures how well the model captures variability of observations given observed features with r 2 1 being an idealistic scenario 
0	155	10236	to improve the quality of our predictions we performed error analysis with k fold cross validation cv we split our dataset randomly into a training and test set 80 20 the training set was used in cv setting where it was sequentially split into training and validation subsets k 5 times 	one can show that these properties are closely related such thatgiving it the meaning of the fraction of explained variance in y we calculated mse and r 2 for both cv as explained above and training sets and also calculated the ratio of mse on cv and training sets to characterize how well our model generalized to new data model variance v ar m se cv m se train 
0	155	10237	to improve the quality of our predictions we performed error analysis with k fold cross validation cv we split our dataset randomly into a training and test set 80 20 the training set was used in cv setting where it was sequentially split into training and validation subsets k 5 times 	a good model would have this metric not much larger than 1 
0	155	10238	to improve the quality of our predictions we performed error analysis with k fold cross validation cv we split our dataset randomly into a training and test set 80 20 the training set was used in cv setting where it was sequentially split into training and validation subsets k 5 times 	a v ar 1 would mean we overfitted the data and our model would most likely perform badly on new data 
0	155	10239	to improve the quality of our predictions we performed error analysis with k fold cross validation cv we split our dataset randomly into a training and test set 80 20 the training set was used in cv setting where it was sequentially split into training and validation subsets k 5 times 	finally after tuning each model and obtaining best set of coefficients we calculated mse and r 2 for test set as the final unbiased accuracy characteristic 
0	155	10240	we used multiple multiple predictors linear regression as our benchmark model as the name suggests it assumes a linear relationship between the features and the predicted target variable and treats it as a linear combination of features f x t x b where x is the feature vector is the vector of model coefficients and b is the bias to train the linear regression model one needs to find the coefficients given data x and target variable observations y as approximation of the predictions of f x 	we used multiple multiple predictors linear regression as our benchmark model 
0	155	10241	we used multiple multiple predictors linear regression as our benchmark model as the name suggests it assumes a linear relationship between the features and the predicted target variable and treats it as a linear combination of features f x t x b where x is the feature vector is the vector of model coefficients and b is the bias to train the linear regression model one needs to find the coefficients given data x and target variable observations y as approximation of the predictions of f x 	as the name suggests it assumes a linear relationship between the features and the predicted target variable and treats it as a linear combination of features f x t x b where x is the feature vector is the vector of model coefficients and b is the bias 
0	155	10242	we used multiple multiple predictors linear regression as our benchmark model as the name suggests it assumes a linear relationship between the features and the predicted target variable and treats it as a linear combination of features f x t x b where x is the feature vector is the vector of model coefficients and b is the bias to train the linear regression model one needs to find the coefficients given data x and target variable observations y as approximation of the predictions of f x 	to train the linear regression model one needs to find the coefficients given data x and target variable observations y as approximation of the predictions of f x 
0	155	10243	we used multiple multiple predictors linear regression as our benchmark model as the name suggests it assumes a linear relationship between the features and the predicted target variable and treats it as a linear combination of features f x t x b where x is the feature vector is the vector of model coefficients and b is the bias to train the linear regression model one needs to find the coefficients given data x and target variable observations y as approximation of the predictions of f x 	by minimizing the least squares cost function w r t 
0	155	10244	we used multiple multiple predictors linear regression as our benchmark model as the name suggests it assumes a linear relationship between the features and the predicted target variable and treats it as a linear combination of features f x t x b where x is the feature vector is the vector of model coefficients and b is the bias to train the linear regression model one needs to find the coefficients given data x and target variable observations y as approximation of the predictions of f x 	coefficients they are found effectively using normal equation the added benefit of lasso regularization is that it sets coefficients of unimportant features to 0 and can be used as a feature selection technique for other methods 
0	155	10245	we used multiple multiple predictors linear regression as our benchmark model as the name suggests it assumes a linear relationship between the features and the predicted target variable and treats it as a linear combination of features f x t x b where x is the feature vector is the vector of model coefficients and b is the bias to train the linear regression model one needs to find the coefficients given data x and target variable observations y as approximation of the predictions of f x 	this was precisely the reason we used lasso in our research 
0	155	10246	one property of neural networks that makes them a popular ml method is their ability to perform end to end learning given some input features x a network is able to determine the appropriate intermediary features and weights of those features on its own a neural network s ability to model non linear data stems from its use of activation functions in between its neuron layers one example of a commonly used activation function is the rectified linear unit relu function its main advantage is that it has a very simple gradient and doesn t suffer from vanishing gradients at extreme values although it can cause dead neurons when the product of the weights and inputs skews negative the leaky relu activation function addresses this shortcoming where is a small number e g 0 1 dropout layers are commonly used to address overfitting in neural networks this is implemented by randomly dropping a small percentage of nodes in a layer during each update iteration preventing the network from over relying on any individual neuron 	one property of neural networks that makes them a popular ml method is their ability to perform end to end learning given some input features x a network is able to determine the appropriate intermediary features and weights of those features on its own a neural network s ability to model non linear data stems from its use of activation functions in between its neuron layers 
0	155	10247	one property of neural networks that makes them a popular ml method is their ability to perform end to end learning given some input features x a network is able to determine the appropriate intermediary features and weights of those features on its own a neural network s ability to model non linear data stems from its use of activation functions in between its neuron layers one example of a commonly used activation function is the rectified linear unit relu function its main advantage is that it has a very simple gradient and doesn t suffer from vanishing gradients at extreme values although it can cause dead neurons when the product of the weights and inputs skews negative the leaky relu activation function addresses this shortcoming where is a small number e g 0 1 dropout layers are commonly used to address overfitting in neural networks this is implemented by randomly dropping a small percentage of nodes in a layer during each update iteration preventing the network from over relying on any individual neuron 	one example of a commonly used activation function is the rectified linear unit relu function its main advantage is that it has a very simple gradient and doesn t suffer from vanishing gradients at extreme values although it can cause dead neurons when the product of the weights and inputs skews negative the leaky relu activation function addresses this shortcoming where is a small number e g 0 1 dropout layers are commonly used to address overfitting in neural networks 
0	155	10248	one property of neural networks that makes them a popular ml method is their ability to perform end to end learning given some input features x a network is able to determine the appropriate intermediary features and weights of those features on its own a neural network s ability to model non linear data stems from its use of activation functions in between its neuron layers one example of a commonly used activation function is the rectified linear unit relu function its main advantage is that it has a very simple gradient and doesn t suffer from vanishing gradients at extreme values although it can cause dead neurons when the product of the weights and inputs skews negative the leaky relu activation function addresses this shortcoming where is a small number e g 0 1 dropout layers are commonly used to address overfitting in neural networks this is implemented by randomly dropping a small percentage of nodes in a layer during each update iteration preventing the network from over relying on any individual neuron 	this is implemented by randomly dropping a small percentage of nodes in a layer during each update iteration preventing the network from over relying on any individual neuron 
0	155	10249	one property of neural networks that makes them a popular ml method is their ability to perform end to end learning given some input features x a network is able to determine the appropriate intermediary features and weights of those features on its own a neural network s ability to model non linear data stems from its use of activation functions in between its neuron layers one example of a commonly used activation function is the rectified linear unit relu function its main advantage is that it has a very simple gradient and doesn t suffer from vanishing gradients at extreme values although it can cause dead neurons when the product of the weights and inputs skews negative the leaky relu activation function addresses this shortcoming where is a small number e g 0 1 dropout layers are commonly used to address overfitting in neural networks this is implemented by randomly dropping a small percentage of nodes in a layer during each update iteration preventing the network from over relying on any individual neuron 	neural networks learn through the backpropagation of error gradients and the weights w l at a layer l are updated by where is the learning rate l i is the cost function and l i is a loss function least squares in our case for an i th example 
0	155	10250	as alternate methods accounting for nonlinear relationship between features and target variable we use tree based techniques random forest and gradient boosting the random forest rf is a special case of ensemble ml methods when individual models are combined together to produce balanced model that have higher generalization power than separate models in the case of rf individual learners are regression trees termed as where feature space is partitioned into m regions r 1 r m and the c m ave y i x i r m at each tree node the binary partition is performed on one variable 	as alternate methods accounting for nonlinear relationship between features and target variable we use tree based techniques random forest and gradient boosting 
0	155	10251	as alternate methods accounting for nonlinear relationship between features and target variable we use tree based techniques random forest and gradient boosting the random forest rf is a special case of ensemble ml methods when individual models are combined together to produce balanced model that have higher generalization power than separate models in the case of rf individual learners are regression trees termed as where feature space is partitioned into m regions r 1 r m and the c m ave y i x i r m at each tree node the binary partition is performed on one variable 	the random forest rf is a special case of ensemble ml methods when individual models are combined together to produce balanced model that have higher generalization power than separate models in the case of rf individual learners are regression trees termed as where feature space is partitioned into m regions r 1 r m and the c m ave y i x i r m 
0	155	10252	as alternate methods accounting for nonlinear relationship between features and target variable we use tree based techniques random forest and gradient boosting the random forest rf is a special case of ensemble ml methods when individual models are combined together to produce balanced model that have higher generalization power than separate models in the case of rf individual learners are regression trees termed as where feature space is partitioned into m regions r 1 r m and the c m ave y i x i r m at each tree node the binary partition is performed on one variable 	at each tree node the binary partition is performed on one variable 
0	155	10253	as alternate methods accounting for nonlinear relationship between features and target variable we use tree based techniques random forest and gradient boosting the random forest rf is a special case of ensemble ml methods when individual models are combined together to produce balanced model that have higher generalization power than separate models in the case of rf individual learners are regression trees termed as where feature space is partitioned into m regions r 1 r m and the c m ave y i x i r m at each tree node the binary partition is performed on one variable 	finding best binary partition in terms of split variable j and split point s by minimizing sum of squares is computationally infeasible and a greedy algorithm is used when decision is being made only one step forward 
0	155	10254	as alternate methods accounting for nonlinear relationship between features and target variable we use tree based techniques random forest and gradient boosting the random forest rf is a special case of ensemble ml methods when individual models are combined together to produce balanced model that have higher generalization power than separate models in the case of rf individual learners are regression trees termed as where feature space is partitioned into m regions r 1 r m and the c m ave y i x i r m at each tree node the binary partition is performed on one variable 	starting with all the data a pair of half planes is defined then j and s are found by solving after finding the split the data is partitioned into two regions and the splitting process is repeated on both regions and all subsequent regions until some stopping criterion is met 
0	155	10255	as alternate methods accounting for nonlinear relationship between features and target variable we use tree based techniques random forest and gradient boosting the random forest rf is a special case of ensemble ml methods when individual models are combined together to produce balanced model that have higher generalization power than separate models in the case of rf individual learners are regression trees termed as where feature space is partitioned into m regions r 1 r m and the c m ave y i x i r m at each tree node the binary partition is performed on one variable 	among different criteria most popular is stopping growing a tree when minimum node size is reached 
0	155	10256	as alternate methods accounting for nonlinear relationship between features and target variable we use tree based techniques random forest and gradient boosting the random forest rf is a special case of ensemble ml methods when individual models are combined together to produce balanced model that have higher generalization power than separate models in the case of rf individual learners are regression trees termed as where feature space is partitioned into m regions r 1 r m and the c m ave y i x i r m at each tree node the binary partition is performed on one variable 	individual trees are prone to overfitting and rf method overcomes this problem by combining multiple trees grown on separate data subsets 
0	155	10257	as alternate methods accounting for nonlinear relationship between features and target variable we use tree based techniques random forest and gradient boosting the random forest rf is a special case of ensemble ml methods when individual models are combined together to produce balanced model that have higher generalization power than separate models in the case of rf individual learners are regression trees termed as where feature space is partitioned into m regions r 1 r m and the c m ave y i x i r m at each tree node the binary partition is performed on one variable 	the default approach to forming subsets in rf is bootstrap sampling with replacement when dataset size remains the same but its composition varies among samples 
0	155	10258	as alternate methods accounting for nonlinear relationship between features and target variable we use tree based techniques random forest and gradient boosting the random forest rf is a special case of ensemble ml methods when individual models are combined together to produce balanced model that have higher generalization power than separate models in the case of rf individual learners are regression trees termed as where feature space is partitioned into m regions r 1 r m and the c m ave y i x i r m at each tree node the binary partition is performed on one variable 	this way overfitting is decreased as each individual tree is learning from a different subset of data 
0	155	10259	as alternate methods accounting for nonlinear relationship between features and target variable we use tree based techniques random forest and gradient boosting the random forest rf is a special case of ensemble ml methods when individual models are combined together to produce balanced model that have higher generalization power than separate models in the case of rf individual learners are regression trees termed as where feature space is partitioned into m regions r 1 r m and the c m ave y i x i r m at each tree node the binary partition is performed on one variable 	moreover a random subset k rather than the whole list of features m is considered at each split where usually m k this way if few features dominate the rest in their contribution to the target variable their contribution to the final model is decreased as now the chances for them to be selected for a split are reduced another tree based ensemble technique is called boosting when power of combining weak learners is leveraged 
0	155	10260	we start off with the linear regression lr using all features the reported cv mse is 0 19 this made us try nonlinear learning methods and the first one was neural networks nn there is no formula for building the perfect nn architecture 	we start off with the linear regression lr using all features 
0	155	10261	we start off with the linear regression lr using all features the reported cv mse is 0 19 this made us try nonlinear learning methods and the first one was neural networks nn there is no formula for building the perfect nn architecture 	the reported cv mse is 0 19 this made us try nonlinear learning methods and the first one was neural networks nn 
0	155	10262	we start off with the linear regression lr using all features the reported cv mse is 0 19 this made us try nonlinear learning methods and the first one was neural networks nn there is no formula for building the perfect nn architecture 	there is no formula for building the perfect nn architecture 
0	155	10263	we start off with the linear regression lr using all features the reported cv mse is 0 19 this made us try nonlinear learning methods and the first one was neural networks nn there is no formula for building the perfect nn architecture 	many design decisions are empirical and based on past experiences using them 
0	155	10264	we start off with the linear regression lr using all features the reported cv mse is 0 19 this made us try nonlinear learning methods and the first one was neural networks nn there is no formula for building the perfect nn architecture 	as a result we experimented with various layer depths neuron counts and activation functions 
0	155	10265	we start off with the linear regression lr using all features the reported cv mse is 0 19 this made us try nonlinear learning methods and the first one was neural networks nn there is no formula for building the perfect nn architecture 	we found that in general deeper networks with smaller layers performed better than shallow networks with larger layers 
0	155	10266	we start off with the linear regression lr using all features the reported cv mse is 0 19 this made us try nonlinear learning methods and the first one was neural networks nn there is no formula for building the perfect nn architecture 	this is supported by the nn initially suffered from significant overfitting cv mse was 10x of training mse 
0	155	10267	we start off with the linear regression lr using all features the reported cv mse is 0 19 this made us try nonlinear learning methods and the first one was neural networks nn there is no formula for building the perfect nn architecture 	this issue was addressed by adding dropout layers to the network and choosing the best configuration 5 
0	155	10268	we start off with the linear regression lr using all features the reported cv mse is 0 19 this made us try nonlinear learning methods and the first one was neural networks nn there is no formula for building the perfect nn architecture 	as a result the train cv mse difference dropped to below 15 gradient boosting regression gb finally provided most robust and accurate model leveraging non linear nature of interaction between condo features and target variable reporting cv set mse 0 1 and test mse 0 09 with m se cv m se train being 1 25
0	155	10269	in addition to testing the three models we looked at the top features highly correlated with selling price of a condo they were in order 1 number of bedrooms 2 number of bathrooms 3 total floor area 4 number of parking spaces 5 maintenance fees we also noticed that the more expensive the condo was the more subjective the price appeared to be and the worse our models performed speaking about listing price that we intentionally excluded from the model we note that it is highly correlated with our target variable the selling price 	in addition to testing the three models we looked at the top features highly correlated with selling price of a condo 
1	155	10270	in addition to testing the three models we looked at the top features highly correlated with selling price of a condo they were in order 1 number of bedrooms 2 number of bathrooms 3 total floor area 4 number of parking spaces 5 maintenance fees we also noticed that the more expensive the condo was the more subjective the price appeared to be and the worse our models performed speaking about listing price that we intentionally excluded from the model we note that it is highly correlated with our target variable the selling price 	they were in order 1 number of bedrooms 2 number of bathrooms 3 total floor area 4 number of parking spaces 5 maintenance fees we also noticed that the more expensive the condo was the more subjective the price appeared to be and the worse our models performed 
1	155	10271	in addition to testing the three models we looked at the top features highly correlated with selling price of a condo they were in order 1 number of bedrooms 2 number of bathrooms 3 total floor area 4 number of parking spaces 5 maintenance fees we also noticed that the more expensive the condo was the more subjective the price appeared to be and the worse our models performed speaking about listing price that we intentionally excluded from the model we note that it is highly correlated with our target variable the selling price 	speaking about listing price that we intentionally excluded from the model we note that it is highly correlated with our target variable the selling price 
0	155	10272	in addition to testing the three models we looked at the top features highly correlated with selling price of a condo they were in order 1 number of bedrooms 2 number of bathrooms 3 total floor area 4 number of parking spaces 5 maintenance fees we also noticed that the more expensive the condo was the more subjective the price appeared to be and the worse our models performed speaking about listing price that we intentionally excluded from the model we note that it is highly correlated with our target variable the selling price 	the mse between them is about 6 times smaller than our best mse 
1	155	10273	in addition to testing the three models we looked at the top features highly correlated with selling price of a condo they were in order 1 number of bedrooms 2 number of bathrooms 3 total floor area 4 number of parking spaces 5 maintenance fees we also noticed that the more expensive the condo was the more subjective the price appeared to be and the worse our models performed speaking about listing price that we intentionally excluded from the model we note that it is highly correlated with our target variable the selling price 	one might think that the domain knowledge of real estate agents is very thorough in estimating home value but there is a paradox 
1	155	10274	in addition to testing the three models we looked at the top features highly correlated with selling price of a condo they were in order 1 number of bedrooms 2 number of bathrooms 3 total floor area 4 number of parking spaces 5 maintenance fees we also noticed that the more expensive the condo was the more subjective the price appeared to be and the worse our models performed speaking about listing price that we intentionally excluded from the model we note that it is highly correlated with our target variable the selling price 	when people want to sell or buy a home they first look at the listing price and therefore the resulting selling price often is very close to the listing price with listing price being a major driving factor 
0	155	10275	in addition to testing the three models we looked at the top features highly correlated with selling price of a condo they were in order 1 number of bedrooms 2 number of bathrooms 3 total floor area 4 number of parking spaces 5 maintenance fees we also noticed that the more expensive the condo was the more subjective the price appeared to be and the worse our models performed speaking about listing price that we intentionally excluded from the model we note that it is highly correlated with our target variable the selling price 	our goal on the other hand was to come up with the emotionfree algorithm that uses only bare facts about property itself and external factors 
0	155	10276	in addition to testing the three models we looked at the top features highly correlated with selling price of a condo they were in order 1 number of bedrooms 2 number of bathrooms 3 total floor area 4 number of parking spaces 5 maintenance fees we also noticed that the more expensive the condo was the more subjective the price appeared to be and the worse our models performed speaking about listing price that we intentionally excluded from the model we note that it is highly correlated with our target variable the selling price 	comparing metrics of our model to those achieved by others we see that theres still room for improvement 
0	155	10277	our findings show that gradient boosting had the best performance followed closely by random forest and neural networks this makes sense because both algorithms are useful for non linear modeling the small data scale in our project lends itself more favorably to trees whereas it makes it more likely for a neural network to overfit resulting in slightly lower overall performance the models described in this paper can be further improved in future work through usage of additional training datasets and additional feature engineering 	our findings show that gradient boosting had the best performance followed closely by random forest and neural networks 
0	155	10278	our findings show that gradient boosting had the best performance followed closely by random forest and neural networks this makes sense because both algorithms are useful for non linear modeling the small data scale in our project lends itself more favorably to trees whereas it makes it more likely for a neural network to overfit resulting in slightly lower overall performance the models described in this paper can be further improved in future work through usage of additional training datasets and additional feature engineering 	this makes sense because both algorithms are useful for non linear modeling 
1	155	10279	our findings show that gradient boosting had the best performance followed closely by random forest and neural networks this makes sense because both algorithms are useful for non linear modeling the small data scale in our project lends itself more favorably to trees whereas it makes it more likely for a neural network to overfit resulting in slightly lower overall performance the models described in this paper can be further improved in future work through usage of additional training datasets and additional feature engineering 	the small data scale in our project lends itself more favorably to trees whereas it makes it more likely for a neural network to overfit resulting in slightly lower overall performance the models described in this paper can be further improved in future work through usage of additional training datasets and additional feature engineering 
0	155	10280	our findings show that gradient boosting had the best performance followed closely by random forest and neural networks this makes sense because both algorithms are useful for non linear modeling the small data scale in our project lends itself more favorably to trees whereas it makes it more likely for a neural network to overfit resulting in slightly lower overall performance the models described in this paper can be further improved in future work through usage of additional training datasets and additional feature engineering 	the federal interest rate has a direct effect on the supply of money and affordability of housing which can affect the selling price 
1	155	10281	our findings show that gradient boosting had the best performance followed closely by random forest and neural networks this makes sense because both algorithms are useful for non linear modeling the small data scale in our project lends itself more favorably to trees whereas it makes it more likely for a neural network to overfit resulting in slightly lower overall performance the models described in this paper can be further improved in future work through usage of additional training datasets and additional feature engineering 	over the timespan of the training dataset used in this study the federal interest rate changed from a low of 0 5 up to a high of 1 75 which could significantly affect the selling price of a condo 
0	155	10282	our findings show that gradient boosting had the best performance followed closely by random forest and neural networks this makes sense because both algorithms are useful for non linear modeling the small data scale in our project lends itself more favorably to trees whereas it makes it more likely for a neural network to overfit resulting in slightly lower overall performance the models described in this paper can be further improved in future work through usage of additional training datasets and additional feature engineering 	an additional dataset that would improve the model is upcoming new condo developments 
0	155	10283	our findings show that gradient boosting had the best performance followed closely by random forest and neural networks this makes sense because both algorithms are useful for non linear modeling the small data scale in our project lends itself more favorably to trees whereas it makes it more likely for a neural network to overfit resulting in slightly lower overall performance the models described in this paper can be further improved in future work through usage of additional training datasets and additional feature engineering 	large growing cities often have new real estate being built 
1	155	10284	our findings show that gradient boosting had the best performance followed closely by random forest and neural networks this makes sense because both algorithms are useful for non linear modeling the small data scale in our project lends itself more favorably to trees whereas it makes it more likely for a neural network to overfit resulting in slightly lower overall performance the models described in this paper can be further improved in future work through usage of additional training datasets and additional feature engineering 	additional inventory coming onto the market would affect existing condo prices negatively by increasing the available supply and alternatives for buyers lastly future work should focus on adding more temporal components to the model for example through features such as listing date number of condos sold in last n days and n day average sell price 
0	155	10285	we d like to thank adina dragasanu from re max crest realty for providing us the data that enabled our research 	we d like to thank adina dragasanu from re max crest realty for providing us the data that enabled our research 
0	156	10286	there are thousands of companies coming out worldwide each year over the past decades there has been a rapid growth in the formation of new companies both in the us and china thus it is an important and challenging task to understand what makes companies successful and to predict the success of a company 	there are thousands of companies coming out worldwide each year 
0	156	10287	there are thousands of companies coming out worldwide each year over the past decades there has been a rapid growth in the formation of new companies both in the us and china thus it is an important and challenging task to understand what makes companies successful and to predict the success of a company 	over the past decades there has been a rapid growth in the formation of new companies both in the us and china 
0	156	10288	there are thousands of companies coming out worldwide each year over the past decades there has been a rapid growth in the formation of new companies both in the us and china thus it is an important and challenging task to understand what makes companies successful and to predict the success of a company 	thus it is an important and challenging task to understand what makes companies successful and to predict the success of a company 
1	156	10289	there are thousands of companies coming out worldwide each year over the past decades there has been a rapid growth in the formation of new companies both in the us and china thus it is an important and challenging task to understand what makes companies successful and to predict the success of a company 	in this project we used crunchbase data to build a predictive model through supervised learning to classify which start ups are successful and which aren t 
1	156	10290	there are thousands of companies coming out worldwide each year over the past decades there has been a rapid growth in the formation of new companies both in the us and china thus it is an important and challenging task to understand what makes companies successful and to predict the success of a company 	we explored k nearest neighbours knn model on this task and compared it with logistic regression lr and random forests rf model in previous work 
1	156	10291	there are thousands of companies coming out worldwide each year over the past decades there has been a rapid growth in the formation of new companies both in the us and china thus it is an important and challenging task to understand what makes companies successful and to predict the success of a company 	we used f1 score as the metric and found that knn model has a better performance on this task which achieves 44 45 of f1 score and 73 70 of accuracy 
0	156	10292	thousands of companies are emerging around the world each year among them some are merged and acquired m a or go to public ipo while others may vanish and disappear what makes this difference and leads to the different endings for each company 	thousands of companies are emerging around the world each year 
0	156	10293	thousands of companies are emerging around the world each year among them some are merged and acquired m a or go to public ipo while others may vanish and disappear what makes this difference and leads to the different endings for each company 	among them some are merged and acquired m a or go to public ipo while others may vanish and disappear 
0	156	10294	thousands of companies are emerging around the world each year among them some are merged and acquired m a or go to public ipo while others may vanish and disappear what makes this difference and leads to the different endings for each company 	what makes this difference and leads to the different endings for each company 
0	156	10295	thousands of companies are emerging around the world each year among them some are merged and acquired m a or go to public ipo while others may vanish and disappear what makes this difference and leads to the different endings for each company 	how to predict the success of companies 
0	156	10296	thousands of companies are emerging around the world each year among them some are merged and acquired m a or go to public ipo while others may vanish and disappear what makes this difference and leads to the different endings for each company 	if the investors can know how likely the company will achieve success given their current information they can make a better decision on the investments 
1	156	10297	thousands of companies are emerging around the world each year among them some are merged and acquired m a or go to public ipo while others may vanish and disappear what makes this difference and leads to the different endings for each company 	therefore in this project given some key features of a company we want to predict the probability of its success 
1	156	10298	thousands of companies are emerging around the world each year among them some are merged and acquired m a or go to public ipo while others may vanish and disappear what makes this difference and leads to the different endings for each company 	more specifically the input features are of two types text features such as industry category list and location and numerical features such as the amount of money a company already raised 
0	156	10299	thousands of companies are emerging around the world each year among them some are merged and acquired m a or go to public ipo while others may vanish and disappear what makes this difference and leads to the different endings for each company 	we then use logistic regression random forests and k nearest neighbours to output a predicted probability of success 
0	156	10300	thousands of companies are emerging around the world each year among them some are merged and acquired m a or go to public ipo while others may vanish and disappear what makes this difference and leads to the different endings for each company 	here we define the company success as the event that gives a large sum of money to the company s founders investors and early employees specifically through a process of m a merger and acquisition or an ipo initial public offering 
1	156	10301	as machine learning becomes a more and more popular tool for researchers to utilize in the field of finance and investment we have found some related work to predict companies business success with machine learning and crunchbase bento lisin and nesterenko indeed these works propose a variety of efficient methods that we can use to predict the success of company however we notice that none of them implement k nearest neighbours model in this project we aim to apply knn model to solving this problem 	as machine learning becomes a more and more popular tool for researchers to utilize in the field of finance and investment we have found some related work to predict companies business success with machine learning and crunchbase bento lisin and nesterenko indeed these works propose a variety of efficient methods that we can use to predict the success of company 
0	156	10302	as machine learning becomes a more and more popular tool for researchers to utilize in the field of finance and investment we have found some related work to predict companies business success with machine learning and crunchbase bento lisin and nesterenko indeed these works propose a variety of efficient methods that we can use to predict the success of company however we notice that none of them implement k nearest neighbours model in this project we aim to apply knn model to solving this problem 	however we notice that none of them implement k nearest neighbours model 
0	156	10303	as machine learning becomes a more and more popular tool for researchers to utilize in the field of finance and investment we have found some related work to predict companies business success with machine learning and crunchbase bento lisin and nesterenko indeed these works propose a variety of efficient methods that we can use to predict the success of company however we notice that none of them implement k nearest neighbours model in this project we aim to apply knn model to solving this problem 	in this project we aim to apply knn model to solving this problem 
1	156	10304	the dataset we used was extracted from crunchbase data export containing 60k companies information updated to december 2015 there were four data files named company investments rounds and acquisition the company file contains most comprehensive information of the companies while other files contains more detailed information regarding the investment operations 	the dataset we used was extracted from crunchbase data export containing 60k companies information updated to december 2015 
0	156	10305	the dataset we used was extracted from crunchbase data export containing 60k companies information updated to december 2015 there were four data files named company investments rounds and acquisition the company file contains most comprehensive information of the companies while other files contains more detailed information regarding the investment operations 	there were four data files named company investments rounds and acquisition 
0	156	10306	the dataset we used was extracted from crunchbase data export containing 60k companies information updated to december 2015 there were four data files named company investments rounds and acquisition the company file contains most comprehensive information of the companies while other files contains more detailed information regarding the investment operations 	the company file contains most comprehensive information of the companies while other files contains more detailed information regarding the investment operations 
0	156	10307	the dataset we used was extracted from crunchbase data export containing 60k companies information updated to december 2015 there were four data files named company investments rounds and acquisition the company file contains most comprehensive information of the companies while other files contains more detailed information regarding the investment operations 	thus we chose the file company as the base and extracted meaningful features from other files to add into it 
0	156	10308	the company dataset consists the following columns name company s name	the company dataset consists the following columns name company s name
0	156	10309	we labeled the company that has m a with 1 otherwise 0 we plotted the amount of the 0 or 1 labeled data as we noticed some skewness regarding the distribution of date of funding events in this dataset as shown in	we labeled the company that has m a with 1 otherwise 0 
1	156	10310	we labeled the company that has m a with 1 otherwise 0 we plotted the amount of the 0 or 1 labeled data as we noticed some skewness regarding the distribution of date of funding events in this dataset as shown in	we plotted the amount of the 0 or 1 labeled data as we noticed some skewness regarding the distribution of date of funding events in this dataset as shown in
1	156	10311	we selected the most essential features to companies business success and end up with input features as category country funding rounds funding total usd and the difference between when first funding at and last funding at the training set is composed of two parts the first part of data is the numerical data number of funding rounds and total funding the second part of data is the date in string format such as first funding at final funding at and funded at columns 	we selected the most essential features to companies business success and end up with input features as category country funding rounds funding total usd and the difference between when first funding at and last funding at the training set is composed of two parts 
1	156	10312	we selected the most essential features to companies business success and end up with input features as category country funding rounds funding total usd and the difference between when first funding at and last funding at the training set is composed of two parts the first part of data is the numerical data number of funding rounds and total funding the second part of data is the date in string format such as first funding at final funding at and funded at columns 	the first part of data is the numerical data number of funding rounds and total funding 
0	156	10313	we selected the most essential features to companies business success and end up with input features as category country funding rounds funding total usd and the difference between when first funding at and last funding at the training set is composed of two parts the first part of data is the numerical data number of funding rounds and total funding the second part of data is the date in string format such as first funding at final funding at and funded at columns 	the second part of data is the date in string format such as first funding at final funding at and funded at columns 
1	156	10314	we selected the most essential features to companies business success and end up with input features as category country funding rounds funding total usd and the difference between when first funding at and last funding at the training set is composed of two parts the first part of data is the numerical data number of funding rounds and total funding the second part of data is the date in string format such as first funding at final funding at and funded at columns 	as there are too many missing data for funded at we finally chose first funding at and final funding at columns converted them from timestamp to numerical utc format and calculated a duration column with the subtracted data 
1	156	10315	the goal of this project is to make a binary prediction on the status of start ups whether they have gone through m a or ipo in this project we explored logistic regression random forests and k nearest neighbors 	the goal of this project is to make a binary prediction on the status of start ups whether they have gone through m a or ipo 
1	156	10316	the goal of this project is to make a binary prediction on the status of start ups whether they have gone through m a or ipo in this project we explored logistic regression random forests and k nearest neighbors 	in this project we explored logistic regression random forests and k nearest neighbors 
0	156	10317	logistic regression is a simple algorithm that is commonly used in binary classification due to its efficiency it is the first model we selected to do the classification the hypothesis of logistic regression algorithm is as follows the algorithm optimize by maximizing the following log likelihood function 	logistic regression is a simple algorithm that is commonly used in binary classification 
0	156	10318	logistic regression is a simple algorithm that is commonly used in binary classification due to its efficiency it is the first model we selected to do the classification the hypothesis of logistic regression algorithm is as follows the algorithm optimize by maximizing the following log likelihood function 	due to its efficiency it is the first model we selected to do the classification 
0	156	10319	logistic regression is a simple algorithm that is commonly used in binary classification due to its efficiency it is the first model we selected to do the classification the hypothesis of logistic regression algorithm is as follows the algorithm optimize by maximizing the following log likelihood function 	the hypothesis of logistic regression algorithm is as follows the algorithm optimize by maximizing the following log likelihood function 
1	156	10320	random forests construct a multitude of decision trees at training time and outputting the mode of the classification result of individual trees at each split point in the decision tree only a subset of features are selected to take into consideration by the algorithm the candidate features are generated using bootstrap 	random forests construct a multitude of decision trees at training time and outputting the mode of the classification result of individual trees 
1	156	10321	random forests construct a multitude of decision trees at training time and outputting the mode of the classification result of individual trees at each split point in the decision tree only a subset of features are selected to take into consideration by the algorithm the candidate features are generated using bootstrap 	at each split point in the decision tree only a subset of features are selected to take into consideration by the algorithm 
0	156	10322	random forests construct a multitude of decision trees at training time and outputting the mode of the classification result of individual trees at each split point in the decision tree only a subset of features are selected to take into consideration by the algorithm the candidate features are generated using bootstrap 	the candidate features are generated using bootstrap 
1	156	10323	random forests construct a multitude of decision trees at training time and outputting the mode of the classification result of individual trees at each split point in the decision tree only a subset of features are selected to take into consideration by the algorithm the candidate features are generated using bootstrap 	compared to an individual tree bootstrapping mitigates the variance by averaging the results of a large number of decision trees 
0	156	10324	an instance is classified by a majority vote of its k nearest neighbours the algorithm assigns class j to x i that maximizes 	an instance is classified by a majority vote of its k nearest neighbours 
0	156	10325	an instance is classified by a majority vote of its k nearest neighbours the algorithm assigns class j to x i that maximizes 	the algorithm assigns class j to x i that maximizes 
0	156	10326	in a confusion matrix we describe the performance of a classification model each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class vice versa there are four basic terms in a confusion matrix here we select three metrics accuracy f1 score and auc score accuracy the proportion we have predicted right 	in a confusion matrix we describe the performance of a classification model 
0	156	10327	in a confusion matrix we describe the performance of a classification model each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class vice versa there are four basic terms in a confusion matrix here we select three metrics accuracy f1 score and auc score accuracy the proportion we have predicted right 	each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class vice versa 
1	156	10328	in a confusion matrix we describe the performance of a classification model each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class vice versa there are four basic terms in a confusion matrix here we select three metrics accuracy f1 score and auc score accuracy the proportion we have predicted right 	there are four basic terms in a confusion matrix here we select three metrics accuracy f1 score and auc score accuracy the proportion we have predicted right 
1	156	10329	in a confusion matrix we describe the performance of a classification model each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class vice versa there are four basic terms in a confusion matrix here we select three metrics accuracy f1 score and auc score accuracy the proportion we have predicted right 	false positive rate fpr f p f p t nf1 score auc score area under the roc curve auc score area under roc curve total area
1	156	10330	to utilize more data in the training we split the dataset into three parts 95 data as training set 5 as cross validation set and 5 as test set since the dataset is quite imbalanced we up sample the minority class label 1 in the training set to balance the data but keep the cross validation set and test set untouched see we also normalize all the numerical features such as funding rounds and funding duration and use bag of words to encode the text features such as category list and country code 	to utilize more data in the training we split the dataset into three parts 95 data as training set 5 as cross validation set and 5 as test set 
1	156	10331	to utilize more data in the training we split the dataset into three parts 95 data as training set 5 as cross validation set and 5 as test set since the dataset is quite imbalanced we up sample the minority class label 1 in the training set to balance the data but keep the cross validation set and test set untouched see we also normalize all the numerical features such as funding rounds and funding duration and use bag of words to encode the text features such as category list and country code 	since the dataset is quite imbalanced we up sample the minority class label 1 in the training set to balance the data but keep the cross validation set and test set untouched see we also normalize all the numerical features such as funding rounds and funding duration and use bag of words to encode the text features such as category list and country code 
1	156	10332	after preprocessing the data we concatenate the two types of features and feed them to logistic regression model random forest model and k nearest neighbours model for random forest and k nearest neighbors model we used random search to tune the hyperparameters a list of hyperparameters and their associated range is summarized in the table below see	after preprocessing the data we concatenate the two types of features and feed them to logistic regression model random forest model and k nearest neighbours model 
1	156	10333	after preprocessing the data we concatenate the two types of features and feed them to logistic regression model random forest model and k nearest neighbours model for random forest and k nearest neighbors model we used random search to tune the hyperparameters a list of hyperparameters and their associated range is summarized in the table below see	for random forest and k nearest neighbors model we used random search to tune the hyperparameters 
0	156	10334	after preprocessing the data we concatenate the two types of features and feed them to logistic regression model random forest model and k nearest neighbours model for random forest and k nearest neighbors model we used random search to tune the hyperparameters a list of hyperparameters and their associated range is summarized in the table below see	a list of hyperparameters and their associated range is summarized in the table below see
1	156	10335	5 50 k number of neighbours 10 100 we use accuracy f1 score and auc score to compare the performance of different models but the f1 score is our primary metric the figure below summarize the results of each model on the validation set see	5 50 k number of neighbours 10 100 we use accuracy f1 score and auc score to compare the performance of different models but the f1 score is our primary metric 
1	156	10336	5 50 k number of neighbours 10 100 we use accuracy f1 score and auc score to compare the performance of different models but the f1 score is our primary metric the figure below summarize the results of each model on the validation set see	the figure below summarize the results of each model on the validation set see
0	156	10337	in the future we should include more features of the companies and examine which features are more significant than others also we will try more complex models such as neural network and pre trained word embedding using kernel method to move the data to higher dimensional space is also a good direction 	in the future we should include more features of the companies and examine which features are more significant than others 
1	156	10338	in the future we should include more features of the companies and examine which features are more significant than others also we will try more complex models such as neural network and pre trained word embedding using kernel method to move the data to higher dimensional space is also a good direction 	also we will try more complex models such as neural network and pre trained word embedding 
0	156	10339	in the future we should include more features of the companies and examine which features are more significant than others also we will try more complex models such as neural network and pre trained word embedding using kernel method to move the data to higher dimensional space is also a good direction 	using kernel method to move the data to higher dimensional space is also a good direction 
1	156	10340	in the future we should include more features of the companies and examine which features are more significant than others also we will try more complex models such as neural network and pre trained word embedding using kernel method to move the data to higher dimensional space is also a good direction 	in addition more new questions are to explore such as predicting the total funding size for a company regression problem 
0	156	10341	welcome to check our code here https github com chenchenpan predict success of startups	welcome to check our code here https github com chenchenpan predict success of startups
0	156	10342	thank you to the cs 229 teaching staff including prof andrew ng and the tas 	thank you to the cs 229 teaching staff including prof andrew ng and the tas 
1	157	10343	we construct using equation 1 the outcome variables the aud basis and the jpy basis interest rates are taken to be the fixed rate in overnight index swaps against central bank policy rates we further collect 32 data series for each of the three relevant currencies aud jpy and usd 	we construct using equation 1 the outcome variables the aud basis and the jpy basis 
0	157	10344	we construct using equation 1 the outcome variables the aud basis and the jpy basis interest rates are taken to be the fixed rate in overnight index swaps against central bank policy rates we further collect 32 data series for each of the three relevant currencies aud jpy and usd 	interest rates are taken to be the fixed rate in overnight index swaps against central bank policy rates 
0	157	10345	we construct using equation 1 the outcome variables the aud basis and the jpy basis interest rates are taken to be the fixed rate in overnight index swaps against central bank policy rates we further collect 32 data series for each of the three relevant currencies aud jpy and usd 	we further collect 32 data series for each of the three relevant currencies aud jpy and usd 
1	157	10346	we construct using equation 1 the outcome variables the aud basis and the jpy basis interest rates are taken to be the fixed rate in overnight index swaps against central bank policy rates we further collect 32 data series for each of the three relevant currencies aud jpy and usd 	our data capture activities in the financial markets conditions of the economy the state of international trade and the stance of economic policies see we first pre process the data by computing and using for most of the features the percentage change between two observations 
0	157	10347	we construct using equation 1 the outcome variables the aud basis and the jpy basis interest rates are taken to be the fixed rate in overnight index swaps against central bank policy rates we further collect 32 data series for each of the three relevant currencies aud jpy and usd 	this is done to both normalize and extract more meaningful information 
0	157	10348	we construct using equation 1 the outcome variables the aud basis and the jpy basis interest rates are taken to be the fixed rate in overnight index swaps against central bank policy rates we further collect 32 data series for each of the three relevant currencies aud jpy and usd 	we next augment the data in two ways 
0	157	10349	we construct using equation 1 the outcome variables the aud basis and the jpy basis interest rates are taken to be the fixed rate in overnight index swaps against central bank policy rates we further collect 32 data series for each of the three relevant currencies aud jpy and usd 	first while all financial markets data are reported daily other data are available only monthly or quarterly for low frequency series we impute daily observations based on the last available entry 
1	157	10350	we construct using equation 1 the outcome variables the aud basis and the jpy basis interest rates are taken to be the fixed rate in overnight index swaps against central bank policy rates we further collect 32 data series for each of the three relevant currencies aud jpy and usd 	second we interact and pairwise interact all features these additional features are used in the polynomial specification of regularized regressions 
1	157	10351	we construct using equation 1 the outcome variables the aud basis and the jpy basis interest rates are taken to be the fixed rate in overnight index swaps against central bank policy rates we further collect 32 data series for each of the three relevant currencies aud jpy and usd 	with the cleaned data set we construct two distinct samples that emphasize different aspects of the data 
0	157	10352	we construct using equation 1 the outcome variables the aud basis and the jpy basis interest rates are taken to be the fixed rate in overnight index swaps against central bank policy rates we further collect 32 data series for each of the three relevant currencies aud jpy and usd 	the complete sample retains the longest possible time horizon by including series that are available between 2004 and 2017 
0	157	10353	we construct using equation 1 the outcome variables the aud basis and the jpy basis interest rates are taken to be the fixed rate in overnight index swaps against central bank policy rates we further collect 32 data series for each of the three relevant currencies aud jpy and usd 	the post crisis finally we split each of our two samples into test vs training sets in two different ways 
1	157	10354	we construct using equation 1 the outcome variables the aud basis and the jpy basis interest rates are taken to be the fixed rate in overnight index swaps against central bank policy rates we further collect 32 data series for each of the three relevant currencies aud jpy and usd 	in both cases we arrive at a test set of 400 observations which is about a year and half in calendar days 
1	157	10355	we construct using equation 1 the outcome variables the aud basis and the jpy basis interest rates are taken to be the fixed rate in overnight index swaps against central bank policy rates we further collect 32 data series for each of the three relevant currencies aud jpy and usd 	the contiguous split uses as test set the last 200 observations and the middle 200 observations in the post crisis period this method emphasizes the time series nature of our outcome variable 
1	157	10356	we construct using equation 1 the outcome variables the aud basis and the jpy basis interest rates are taken to be the fixed rate in overnight index swaps against central bank policy rates we further collect 32 data series for each of the three relevant currencies aud jpy and usd 	the random split uses as test set 400 randomly chosen observations in the post crisis period which reflects more of a cross sectional test of the basis predictions ultimately each of our ml models is applied to 8 distinct sample split combination for each of the aud and jpy bases we have either the complete or the post crisis sample and within each sample we split train vs test using either a contiguous or a random approach 
1	157	10357	regularized linear regressions extend the ordinary least squares regression algorithm by allowing regularizations on the fitted model parameters regularization prevents the linear regression from overfitting especially when a large set of features are present any regularized linear regression solves the following optimization program where is a hyperparameter that determines the strength of regularization and determines the specific regularization function employed setting 0 corresponds to regularizating l 1 norm which is also known as lasso regression 	regularized linear regressions extend the ordinary least squares regression algorithm by allowing regularizations on the fitted model parameters 
1	157	10358	regularized linear regressions extend the ordinary least squares regression algorithm by allowing regularizations on the fitted model parameters regularization prevents the linear regression from overfitting especially when a large set of features are present any regularized linear regression solves the following optimization program where is a hyperparameter that determines the strength of regularization and determines the specific regularization function employed setting 0 corresponds to regularizating l 1 norm which is also known as lasso regression 	regularization prevents the linear regression from overfitting especially when a large set of features are present 
1	157	10359	regularized linear regressions extend the ordinary least squares regression algorithm by allowing regularizations on the fitted model parameters regularization prevents the linear regression from overfitting especially when a large set of features are present any regularized linear regression solves the following optimization program where is a hyperparameter that determines the strength of regularization and determines the specific regularization function employed setting 0 corresponds to regularizating l 1 norm which is also known as lasso regression 	any regularized linear regression solves the following optimization program where is a hyperparameter that determines the strength of regularization and determines the specific regularization function employed setting 0 corresponds to regularizating l 1 norm which is also known as lasso regression 
1	157	10360	regularized linear regressions extend the ordinary least squares regression algorithm by allowing regularizations on the fitted model parameters regularization prevents the linear regression from overfitting especially when a large set of features are present any regularized linear regression solves the following optimization program where is a hyperparameter that determines the strength of regularization and determines the specific regularization function employed setting 0 corresponds to regularizating l 1 norm which is also known as lasso regression 	this regularization will set less relevant s to zero thereby achieving model parsimony 
1	157	10361	regularized linear regressions extend the ordinary least squares regression algorithm by allowing regularizations on the fitted model parameters regularization prevents the linear regression from overfitting especially when a large set of features are present any regularized linear regression solves the following optimization program where is a hyperparameter that determines the strength of regularization and determines the specific regularization function employed setting 0 corresponds to regularizating l 1 norm which is also known as lasso regression 	an 1 corresponds to using l 2 regularization or ridge regression 
1	157	10362	regularized linear regressions extend the ordinary least squares regression algorithm by allowing regularizations on the fitted model parameters regularization prevents the linear regression from overfitting especially when a large set of features are present any regularized linear regression solves the following optimization program where is a hyperparameter that determines the strength of regularization and determines the specific regularization function employed setting 0 corresponds to regularizating l 1 norm which is also known as lasso regression 	it shrinks the coefficients of variables that are highly correlated 
1	157	10363	regularized linear regressions extend the ordinary least squares regression algorithm by allowing regularizations on the fitted model parameters regularization prevents the linear regression from overfitting especially when a large set of features are present any regularized linear regression solves the following optimization program where is a hyperparameter that determines the strength of regularization and determines the specific regularization function employed setting 0 corresponds to regularizating l 1 norm which is also known as lasso regression 	both of these algorithms also have a bayesian interpretation with lasso corresponding to a laplace prior and ridge to a normal prior over the regression coefficients 
1	157	10364	regularized linear regressions extend the ordinary least squares regression algorithm by allowing regularizations on the fitted model parameters regularization prevents the linear regression from overfitting especially when a large set of features are present any regularized linear regression solves the following optimization program where is a hyperparameter that determines the strength of regularization and determines the specific regularization function employed setting 0 corresponds to regularizating l 1 norm which is also known as lasso regression 	finally we also consider an elastic net algorithm with 1 2 
1	157	10365	regularized linear regressions extend the ordinary least squares regression algorithm by allowing regularizations on the fitted model parameters regularization prevents the linear regression from overfitting especially when a large set of features are present any regularized linear regression solves the following optimization program where is a hyperparameter that determines the strength of regularization and determines the specific regularization function employed setting 0 corresponds to regularizating l 1 norm which is also known as lasso regression 	this trades off the two previous regularization methods in addition to linear features we also consider a second degree polynomial of the features in these regularized regressions in order to capture non linearities 
1	157	10366	regularized linear regressions extend the ordinary least squares regression algorithm by allowing regularizations on the fitted model parameters regularization prevents the linear regression from overfitting especially when a large set of features are present any regularized linear regression solves the following optimization program where is a hyperparameter that determines the strength of regularization and determines the specific regularization function employed setting 0 corresponds to regularizating l 1 norm which is also known as lasso regression 	all features and the y are standardized when training the algorithm in order for the regularization to work as intended 
0	157	10367	regularized linear regressions extend the ordinary least squares regression algorithm by allowing regularizations on the fitted model parameters regularization prevents the linear regression from overfitting especially when a large set of features are present any regularized linear regression solves the following optimization program where is a hyperparameter that determines the strength of regularization and determines the specific regularization function employed setting 0 corresponds to regularizating l 1 norm which is also known as lasso regression 	we implement all algorithms in the statistical software r
1	157	10368	an algorithm might need to capture higher dimensional feature interactions in order to predict well a regression tree allows for considering such non linear interactions among features in each step the tree splits the data in one node the parent node into two subsets children nodes based on the value of one feature 	an algorithm might need to capture higher dimensional feature interactions in order to predict well 
1	157	10369	an algorithm might need to capture higher dimensional feature interactions in order to predict well a regression tree allows for considering such non linear interactions among features in each step the tree splits the data in one node the parent node into two subsets children nodes based on the value of one feature 	a regression tree allows for considering such non linear interactions among features 
1	157	10370	an algorithm might need to capture higher dimensional feature interactions in order to predict well a regression tree allows for considering such non linear interactions among features in each step the tree splits the data in one node the parent node into two subsets children nodes based on the value of one feature 	in each step the tree splits the data in one node the parent node into two subsets children nodes based on the value of one feature 
1	157	10371	an algorithm might need to capture higher dimensional feature interactions in order to predict well a regression tree allows for considering such non linear interactions among features in each step the tree splits the data in one node the parent node into two subsets children nodes based on the value of one feature 	the splitting rule is chosen to minimize the purity of the children nodes 
1	157	10372	an algorithm might need to capture higher dimensional feature interactions in order to predict well a regression tree allows for considering such non linear interactions among features in each step the tree splits the data in one node the parent node into two subsets children nodes based on the value of one feature 	the algorithm stops once the purity of the children nodes does not improve over the purity of the parent node 
1	157	10373	an algorithm might need to capture higher dimensional feature interactions in order to predict well a regression tree allows for considering such non linear interactions among features in each step the tree splits the data in one node the parent node into two subsets children nodes based on the value of one feature 	the prediction in each node j that contains m j training examples is j and the purity of a node d j is calculated with the deviance measure we implement the regression tree algorithm using the r package tree
0	157	10374	regression tree algorithms can exhibit high variance this problem can be remedied using a random forest the random forest uses bootstrap to grow multiple trees and returns the average prediction of those trees as its final prediction 	regression tree algorithms can exhibit high variance 
0	157	10375	regression tree algorithms can exhibit high variance this problem can be remedied using a random forest the random forest uses bootstrap to grow multiple trees and returns the average prediction of those trees as its final prediction 	this problem can be remedied using a random forest 
1	157	10376	regression tree algorithms can exhibit high variance this problem can be remedied using a random forest the random forest uses bootstrap to grow multiple trees and returns the average prediction of those trees as its final prediction 	the random forest uses bootstrap to grow multiple trees and returns the average prediction of those trees as its final prediction 
1	157	10377	regression tree algorithms can exhibit high variance this problem can be remedied using a random forest the random forest uses bootstrap to grow multiple trees and returns the average prediction of those trees as its final prediction 	it can be shown that the variance of a random forest containing n trees each with variance 2 and correlation is hence the overall variance can be decreased by choosing a high number of trees n we choose to grow 2000 and decorrelating the trees to achieve a low 
0	157	10378	regression tree algorithms can exhibit high variance this problem can be remedied using a random forest the random forest uses bootstrap to grow multiple trees and returns the average prediction of those trees as its final prediction 	the forest decorrelates its trees in two ways 
1	157	10379	regression tree algorithms can exhibit high variance this problem can be remedied using a random forest the random forest uses bootstrap to grow multiple trees and returns the average prediction of those trees as its final prediction 	first each tree is grown out of a bootstrapped sample which is different for each tree 
1	157	10380	regression tree algorithms can exhibit high variance this problem can be remedied using a random forest the random forest uses bootstrap to grow multiple trees and returns the average prediction of those trees as its final prediction 	moreover at each node the algorithm only considers splitting on a random sub sample of all available features 
1	157	10381	regression tree algorithms can exhibit high variance this problem can be remedied using a random forest the random forest uses bootstrap to grow multiple trees and returns the average prediction of those trees as its final prediction 	the size of this sub sample is mtry and is a hyperparameter that we will tune 
0	157	10382	regression tree algorithms can exhibit high variance this problem can be remedied using a random forest the random forest uses bootstrap to grow multiple trees and returns the average prediction of those trees as its final prediction 	each tree in the forest grows until it reaches a minimum number of terminal nodes leaves and that is set to five in our case 
1	157	10383	regression tree algorithms can exhibit high variance this problem can be remedied using a random forest the random forest uses bootstrap to grow multiple trees and returns the average prediction of those trees as its final prediction 	these measures contribute to less correlated trees and a lower overall variance of the random forest 
1	157	10384	regression tree algorithms can exhibit high variance this problem can be remedied using a random forest the random forest uses bootstrap to grow multiple trees and returns the average prediction of those trees as its final prediction 	we implement the random forest algorithm using the r package ranger
1	157	10385	before presenting the ml results we discuss the tuning of two key hyper parameters in regularized regressions and mtry in random forest across lasso ridge and elastic net we tune the strength of our regularization penalty via the one standard error approach that is the glmnet package supplies 100 s we choose the optimal as the value that s one standard error away from the that minimizes the cross validation error so as to reflect estimation errors inherent in calculating mses random forest has many degrees of freedom we focus on optimizing mtry or the number of randomly chosen features a node could split on 	before presenting the ml results we discuss the tuning of two key hyper parameters in regularized regressions and mtry in random forest across lasso ridge and elastic net we tune the strength of our regularization penalty via the one standard error approach 
1	157	10386	before presenting the ml results we discuss the tuning of two key hyper parameters in regularized regressions and mtry in random forest across lasso ridge and elastic net we tune the strength of our regularization penalty via the one standard error approach that is the glmnet package supplies 100 s we choose the optimal as the value that s one standard error away from the that minimizes the cross validation error so as to reflect estimation errors inherent in calculating mses random forest has many degrees of freedom we focus on optimizing mtry or the number of randomly chosen features a node could split on 	that is the glmnet package supplies 100 s we choose the optimal as the value that s one standard error away from the that minimizes the cross validation error so as to reflect estimation errors inherent in calculating mses random forest has many degrees of freedom 
1	157	10387	before presenting the ml results we discuss the tuning of two key hyper parameters in regularized regressions and mtry in random forest across lasso ridge and elastic net we tune the strength of our regularization penalty via the one standard error approach that is the glmnet package supplies 100 s we choose the optimal as the value that s one standard error away from the that minimizes the cross validation error so as to reflect estimation errors inherent in calculating mses random forest has many degrees of freedom we focus on optimizing mtry or the number of randomly chosen features a node could split on 	we focus on optimizing mtry or the number of randomly chosen features a node could split on 
1	157	10388	before presenting the ml results we discuss the tuning of two key hyper parameters in regularized regressions and mtry in random forest across lasso ridge and elastic net we tune the strength of our regularization penalty via the one standard error approach that is the glmnet package supplies 100 s we choose the optimal as the value that s one standard error away from the that minimizes the cross validation error so as to reflect estimation errors inherent in calculating mses random forest has many degrees of freedom we focus on optimizing mtry or the number of randomly chosen features a node could split on 	splitting only on a subset of features at each node reduces correlation among trees and drives down variance of the overall model 
0	157	10389	before presenting the ml results we discuss the tuning of two key hyper parameters in regularized regressions and mtry in random forest across lasso ridge and elastic net we tune the strength of our regularization penalty via the one standard error approach that is the glmnet package supplies 100 s we choose the optimal as the value that s one standard error away from the that minimizes the cross validation error so as to reflect estimation errors inherent in calculating mses random forest has many degrees of freedom we focus on optimizing mtry or the number of randomly chosen features a node could split on 	for each of the 8 random forests one on each sample split we ran ranger with mtry 5 6 7 15 
1	157	10390	before presenting the ml results we discuss the tuning of two key hyper parameters in regularized regressions and mtry in random forest across lasso ridge and elastic net we tune the strength of our regularization penalty via the one standard error approach that is the glmnet package supplies 100 s we choose the optimal as the value that s one standard error away from the that minimizes the cross validation error so as to reflect estimation errors inherent in calculating mses random forest has many degrees of freedom we focus on optimizing mtry or the number of randomly chosen features a node could split on 	the heuristic is to set mtry equal to the square root of feature dimension which would be 7 or 8 depending on the sample split 
1	157	10391	before presenting the ml results we discuss the tuning of two key hyper parameters in regularized regressions and mtry in random forest across lasso ridge and elastic net we tune the strength of our regularization penalty via the one standard error approach that is the glmnet package supplies 100 s we choose the optimal as the value that s one standard error away from the that minimizes the cross validation error so as to reflect estimation errors inherent in calculating mses random forest has many degrees of freedom we focus on optimizing mtry or the number of randomly chosen features a node could split on 	we choose the optimal mtry to be the number that minimizes the out of bag mse in the training set 
0	157	10392	before presenting the ml results we discuss the tuning of two key hyper parameters in regularized regressions and mtry in random forest across lasso ridge and elastic net we tune the strength of our regularization penalty via the one standard error approach that is the glmnet package supplies 100 s we choose the optimal as the value that s one standard error away from the that minimizes the cross validation error so as to reflect estimation errors inherent in calculating mses random forest has many degrees of freedom we focus on optimizing mtry or the number of randomly chosen features a node could split on 	our final mtry s include four 12 s four 14 s one 10 and one 15 
1	157	10393	before presenting the ml results we discuss the tuning of two key hyper parameters in regularized regressions and mtry in random forest across lasso ridge and elastic net we tune the strength of our regularization penalty via the one standard error approach that is the glmnet package supplies 100 s we choose the optimal as the value that s one standard error away from the that minimizes the cross validation error so as to reflect estimation errors inherent in calculating mses random forest has many degrees of freedom we focus on optimizing mtry or the number of randomly chosen features a node could split on 	we set the other parameters of the forest to 2000 trees and minimum of 5 leaves per node the performance of all eight of our ml models on the four contiguous splits are summarized in the mse on the random splits are not shown due to space constraint 
1	157	10394	before presenting the ml results we discuss the tuning of two key hyper parameters in regularized regressions and mtry in random forest across lasso ridge and elastic net we tune the strength of our regularization penalty via the one standard error approach that is the glmnet package supplies 100 s we choose the optimal as the value that s one standard error away from the that minimizes the cross validation error so as to reflect estimation errors inherent in calculating mses random forest has many degrees of freedom we focus on optimizing mtry or the number of randomly chosen features a node could split on 	the performance on the random splits are stunningly good the random forests generate mses on the test set of between 10 to 20 which is incredibly small compared to the 10 to 25 standard deviation of the outcome variables 
1	157	10395	before presenting the ml results we discuss the tuning of two key hyper parameters in regularized regressions and mtry in random forest across lasso ridge and elastic net we tune the strength of our regularization penalty via the one standard error approach that is the glmnet package supplies 100 s we choose the optimal as the value that s one standard error away from the that minimizes the cross validation error so as to reflect estimation errors inherent in calculating mses random forest has many degrees of freedom we focus on optimizing mtry or the number of randomly chosen features a node could split on 	however we embrace this success with reservation as it is difficult to interpret randomly selected observations with imputed values focusing our analysis on the performance on the contiguous splits we highlight three takeaways 
0	157	10396	before presenting the ml results we discuss the tuning of two key hyper parameters in regularized regressions and mtry in random forest across lasso ridge and elastic net we tune the strength of our regularization penalty via the one standard error approach that is the glmnet package supplies 100 s we choose the optimal as the value that s one standard error away from the that minimizes the cross validation error so as to reflect estimation errors inherent in calculating mses random forest has many degrees of freedom we focus on optimizing mtry or the number of randomly chosen features a node could split on 	first random forests achieve strong prediction performance 
1	157	10397	before presenting the ml results we discuss the tuning of two key hyper parameters in regularized regressions and mtry in random forest across lasso ridge and elastic net we tune the strength of our regularization penalty via the one standard error approach that is the glmnet package supplies 100 s we choose the optimal as the value that s one standard error away from the that minimizes the cross validation error so as to reflect estimation errors inherent in calculating mses random forest has many degrees of freedom we focus on optimizing mtry or the number of randomly chosen features a node could split on 	forests not only have the lowest mse in test sets in all but one sample but do so with a substantial margin 
1	157	10398	before presenting the ml results we discuss the tuning of two key hyper parameters in regularized regressions and mtry in random forest across lasso ridge and elastic net we tune the strength of our regularization penalty via the one standard error approach that is the glmnet package supplies 100 s we choose the optimal as the value that s one standard error away from the that minimizes the cross validation error so as to reflect estimation errors inherent in calculating mses random forest has many degrees of freedom we focus on optimizing mtry or the number of randomly chosen features a node could split on 	comparing to regularized regressions forests allow non linear effects and compare to regression trees forests lower variance by bagging and splitting on only a subset of features at each node 
1	157	10399	before presenting the ml results we discuss the tuning of two key hyper parameters in regularized regressions and mtry in random forest across lasso ridge and elastic net we tune the strength of our regularization penalty via the one standard error approach that is the glmnet package supplies 100 s we choose the optimal as the value that s one standard error away from the that minimizes the cross validation error so as to reflect estimation errors inherent in calculating mses random forest has many degrees of freedom we focus on optimizing mtry or the number of randomly chosen features a node could split on 	the superior performance of forests suggest that these are two important considerations we plot in second regularized regressions are informative about bias vs variance in the prediction 
1	157	10400	before presenting the ml results we discuss the tuning of two key hyper parameters in regularized regressions and mtry in random forest across lasso ridge and elastic net we tune the strength of our regularization penalty via the one standard error approach that is the glmnet package supplies 100 s we choose the optimal as the value that s one standard error away from the that minimizes the cross validation error so as to reflect estimation errors inherent in calculating mses random forest has many degrees of freedom we focus on optimizing mtry or the number of randomly chosen features a node could split on 	looking at the mses in the training vs test sets across the linear vs polynomial specifications of regularized regressions we note that the prediction error in the aud basis is likely caused by a bias problem as the mse decreases with the inclusion of the higher dimensional features in both the training and the test set 
1	157	10401	before presenting the ml results we discuss the tuning of two key hyper parameters in regularized regressions and mtry in random forest across lasso ridge and elastic net we tune the strength of our regularization penalty via the one standard error approach that is the glmnet package supplies 100 s we choose the optimal as the value that s one standard error away from the that minimizes the cross validation error so as to reflect estimation errors inherent in calculating mses random forest has many degrees of freedom we focus on optimizing mtry or the number of randomly chosen features a node could split on 	in contrast the results in jpy basis indicates a variance problem i e 
1	157	10402	before presenting the ml results we discuss the tuning of two key hyper parameters in regularized regressions and mtry in random forest across lasso ridge and elastic net we tune the strength of our regularization penalty via the one standard error approach that is the glmnet package supplies 100 s we choose the optimal as the value that s one standard error away from the that minimizes the cross validation error so as to reflect estimation errors inherent in calculating mses random forest has many degrees of freedom we focus on optimizing mtry or the number of randomly chosen features a node could split on 	overfitting as the polynomial improves the training error but increases the test error finally performance differ dramatically in the middle vs end test blocks 
1	157	10403	before presenting the ml results we discuss the tuning of two key hyper parameters in regularized regressions and mtry in random forest across lasso ridge and elastic net we tune the strength of our regularization penalty via the one standard error approach that is the glmnet package supplies 100 s we choose the optimal as the value that s one standard error away from the that minimizes the cross validation error so as to reflect estimation errors inherent in calculating mses random forest has many degrees of freedom we focus on optimizing mtry or the number of randomly chosen features a node could split on 	in results not shown due to space constraint we note that all models have respectable performance on the test block taken from the middle of the post crisis period 
0	157	10404	before presenting the ml results we discuss the tuning of two key hyper parameters in regularized regressions and mtry in random forest across lasso ridge and elastic net we tune the strength of our regularization penalty via the one standard error approach that is the glmnet package supplies 100 s we choose the optimal as the value that s one standard error away from the that minimizes the cross validation error so as to reflect estimation errors inherent in calculating mses random forest has many degrees of freedom we focus on optimizing mtry or the number of randomly chosen features a node could split on 	yet most models struggle with predictions in the last 200 observations 
1	157	10405	before presenting the ml results we discuss the tuning of two key hyper parameters in regularized regressions and mtry in random forest across lasso ridge and elastic net we tune the strength of our regularization penalty via the one standard error approach that is the glmnet package supplies 100 s we choose the optimal as the value that s one standard error away from the that minimizes the cross validation error so as to reflect estimation errors inherent in calculating mses random forest has many degrees of freedom we focus on optimizing mtry or the number of randomly chosen features a node could split on 	one potential reason is that outcome variables in this period exhibit patterns that have hitherto not been observed low variance elevated level and are thus difficult to predict via a supervised learning algorithm 
0	157	10406	violations of the covered interest rate parity condition are important phenomena in the global foreign exchange market and a better understanding of the cross currency basis can have profound implications on the theory of asset pricing in this project we take a step toward this understanding by predicting bases using machine learning techniques we find that random forests achieve fairly good predictions as measured by mse on test sets that encompass two separate blocks of observations in the post crisis period 	violations of the covered interest rate parity condition are important phenomena in the global foreign exchange market and a better understanding of the cross currency basis can have profound implications on the theory of asset pricing 
1	157	10407	violations of the covered interest rate parity condition are important phenomena in the global foreign exchange market and a better understanding of the cross currency basis can have profound implications on the theory of asset pricing in this project we take a step toward this understanding by predicting bases using machine learning techniques we find that random forests achieve fairly good predictions as measured by mse on test sets that encompass two separate blocks of observations in the post crisis period 	in this project we take a step toward this understanding by predicting bases using machine learning techniques 
1	157	10408	violations of the covered interest rate parity condition are important phenomena in the global foreign exchange market and a better understanding of the cross currency basis can have profound implications on the theory of asset pricing in this project we take a step toward this understanding by predicting bases using machine learning techniques we find that random forests achieve fairly good predictions as measured by mse on test sets that encompass two separate blocks of observations in the post crisis period 	we find that random forests achieve fairly good predictions as measured by mse on test sets that encompass two separate blocks of observations in the post crisis period 
1	157	10409	violations of the covered interest rate parity condition are important phenomena in the global foreign exchange market and a better understanding of the cross currency basis can have profound implications on the theory of asset pricing in this project we take a step toward this understanding by predicting bases using machine learning techniques we find that random forests achieve fairly good predictions as measured by mse on test sets that encompass two separate blocks of observations in the post crisis period 	this performance likely owes to random forest s ability to flexibly introduce non linear feature effects and strike a balance between bias and variance minimization in the future we would collect more economic features and use higher order polynomial features to improve the regularized linear regressions of aud basis given the observed bias issue with the aud data 
1	157	10410	violations of the covered interest rate parity condition are important phenomena in the global foreign exchange market and a better understanding of the cross currency basis can have profound implications on the theory of asset pricing in this project we take a step toward this understanding by predicting bases using machine learning techniques we find that random forests achieve fairly good predictions as measured by mse on test sets that encompass two separate blocks of observations in the post crisis period 	we will expand the set of algorithms employed to improve the performance on the jpy data as most algorithms seem to suffer from a variance problem 
1	157	10411	violations of the covered interest rate parity condition are important phenomena in the global foreign exchange market and a better understanding of the cross currency basis can have profound implications on the theory of asset pricing in this project we take a step toward this understanding by predicting bases using machine learning techniques we find that random forests achieve fairly good predictions as measured by mse on test sets that encompass two separate blocks of observations in the post crisis period 	specifically we will apply the boosting technique and we will consider training a neural network overall we are encouraged to see that we found models that perform reasonably well 
0	157	10412	violations of the covered interest rate parity condition are important phenomena in the global foreign exchange market and a better understanding of the cross currency basis can have profound implications on the theory of asset pricing in this project we take a step toward this understanding by predicting bases using machine learning techniques we find that random forests achieve fairly good predictions as measured by mse on test sets that encompass two separate blocks of observations in the post crisis period 	importantly the features selected as important by our various models are intuitive and sensible 
1	157	10413	violations of the covered interest rate parity condition are important phenomena in the global foreign exchange market and a better understanding of the cross currency basis can have profound implications on the theory of asset pricing in this project we take a step toward this understanding by predicting bases using machine learning techniques we find that random forests achieve fairly good predictions as measured by mse on test sets that encompass two separate blocks of observations in the post crisis period 	we hope to more closely examine the contribution of these features in the future and extend this analysis to a larger set of currency bases 
0	157	10414	violations of the covered interest rate parity condition are important phenomena in the global foreign exchange market and a better understanding of the cross currency basis can have profound implications on the theory of asset pricing in this project we take a step toward this understanding by predicting bases using machine learning techniques we find that random forests achieve fairly good predictions as measured by mse on test sets that encompass two separate blocks of observations in the post crisis period 	all tasks were performed by amy and stefan in equal parts 
0	158	10415	there are several studies evaluating the impact of increase in government spending for specific sectors in quality indexes for example the cited works by baldacci et al sutherland et al and gupta et al in the first example different variations of the least mean squared error regression model are tested as well as a covariance structure model based on latent variables the models are used to find the relation between public spending and quality indexes 	there are several studies evaluating the impact of increase in government spending for specific sectors in quality indexes for example the cited works by baldacci et al sutherland et al and gupta et al 
0	158	10416	there are several studies evaluating the impact of increase in government spending for specific sectors in quality indexes for example the cited works by baldacci et al sutherland et al and gupta et al in the first example different variations of the least mean squared error regression model are tested as well as a covariance structure model based on latent variables the models are used to find the relation between public spending and quality indexes 	in the first example different variations of the least mean squared error regression model are tested as well as a covariance structure model based on latent variables 
0	158	10417	there are several studies evaluating the impact of increase in government spending for specific sectors in quality indexes for example the cited works by baldacci et al sutherland et al and gupta et al in the first example different variations of the least mean squared error regression model are tested as well as a covariance structure model based on latent variables the models are used to find the relation between public spending and quality indexes 	the models are used to find the relation between public spending and quality indexes 
0	158	10418	there are several studies evaluating the impact of increase in government spending for specific sectors in quality indexes for example the cited works by baldacci et al sutherland et al and gupta et al in the first example different variations of the least mean squared error regression model are tested as well as a covariance structure model based on latent variables the models are used to find the relation between public spending and quality indexes 	in all the studies there was statistical evidence of a correlation between increase in spending and increase in the indexes 
0	158	10419	there are several studies evaluating the impact of increase in government spending for specific sectors in quality indexes for example the cited works by baldacci et al sutherland et al and gupta et al in the first example different variations of the least mean squared error regression model are tested as well as a covariance structure model based on latent variables the models are used to find the relation between public spending and quality indexes 	the present project aims to explore this relation to create a tool for budget planning 
1	158	10420	the target variable as explained previously is the ideb score of each school this data can be found on the page http ideb inep gov br it contains the evolution of the in brazilian public schools for the years of 2013 2015 and 2017 	the target variable as explained previously is the ideb score of each school 
0	158	10421	the target variable as explained previously is the ideb score of each school this data can be found on the page http ideb inep gov br it contains the evolution of the in brazilian public schools for the years of 2013 2015 and 2017 	this data can be found on the page http ideb inep gov br 
0	158	10422	the target variable as explained previously is the ideb score of each school this data can be found on the page http ideb inep gov br it contains the evolution of the in brazilian public schools for the years of 2013 2015 and 2017 	it contains the evolution of the in brazilian public schools for the years of 2013 2015 and 2017 
0	158	10423	the target variable as explained previously is the ideb score of each school this data can be found on the page http ideb inep gov br it contains the evolution of the in brazilian public schools for the years of 2013 2015 and 2017 	this index combines the scores of students in a national mandatory exam with data provided by each school describing rate approvals to assess the quality of basic education in public schools 
1	158	10424	the target variable as explained previously is the ideb score of each school this data can be found on the page http ideb inep gov br it contains the evolution of the in brazilian public schools for the years of 2013 2015 and 2017 	in the year of 2017 the goals for the 2019 and 2021 were established the second data source is the brazilian school census of 2013 http portal inep gov br microdados that has survey data on every public school in brazil 
0	158	10425	the target variable as explained previously is the ideb score of each school this data can be found on the page http ideb inep gov br it contains the evolution of the in brazilian public schools for the years of 2013 2015 and 2017 	this dataset contains information describing many aspects of the infrastructure of the school the qualification of teachers and the profile of students 
1	158	10426	the target variable as explained previously is the ideb score of each school this data can be found on the page http ideb inep gov br it contains the evolution of the in brazilian public schools for the years of 2013 2015 and 2017 	some examples of features include total number of students number of professors by level and area of education number of laboratories computers and offices number of students per race number of classes per subject total amount of time spent by students with extracurricular activities in total after the preparation of data the dataset includes 353 features per school 
0	158	10427	the target variable as explained previously is the ideb score of each school this data can be found on the page http ideb inep gov br it contains the evolution of the in brazilian public schools for the years of 2013 2015 and 2017 	most of them are count variables as the number of professors from each educational background number of different equipment etc 
0	158	10428	the target variable as explained previously is the ideb score of each school this data can be found on the page http ideb inep gov br it contains the evolution of the in brazilian public schools for the years of 2013 2015 and 2017 	a big part of transforming the data included counting different categories in categorical variables 
1	158	10429	the target variable as explained previously is the ideb score of each school this data can be found on the page http ideb inep gov br it contains the evolution of the in brazilian public schools for the years of 2013 2015 and 2017 	for example there is one entry in the original data for each student and teacher in the school in the final dataset there is only counts for the number of male female students mathematics biology chemistry teachers etc the last data source is the website https www fazenda sp gov br 
0	158	10430	the target variable as explained previously is the ideb score of each school this data can be found on the page http ideb inep gov br it contains the evolution of the in brazilian public schools for the years of 2013 2015 and 2017 	it has data detailing all of the disbursement made by the state government of sao paulo since 2010 
0	158	10431	the target variable as explained previously is the ideb score of each school this data can be found on the page http ideb inep gov br it contains the evolution of the in brazilian public schools for the years of 2013 2015 and 2017 	the database contains information on the targeted sector education health transportation etc 
0	158	10432	the target variable as explained previously is the ideb score of each school this data can be found on the page http ideb inep gov br it contains the evolution of the in brazilian public schools for the years of 2013 2015 and 2017 	 the subarea primary secondary higher education etc 
0	158	10433	the target variable as explained previously is the ideb score of each school this data can be found on the page http ideb inep gov br it contains the evolution of the in brazilian public schools for the years of 2013 2015 and 2017 	as well as a more detailed classification of the purpose of the spending scholarship for poor students construction of new schools purchase of food or transportation for students etc 
0	158	10434	the target variable as explained previously is the ideb score of each school this data can be found on the page http ideb inep gov br it contains the evolution of the in brazilian public schools for the years of 2013 2015 and 2017 	some of the expenditure categories include transportation for students food for students and workers
0	158	10435	the final project includes three different models that process data in different phases first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 the purpose of this model is to reduce the number of features from the census considered in the next phase 	the final project includes three different models that process data in different phases 
1	158	10436	the final project includes three different models that process data in different phases first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 the purpose of this model is to reduce the number of features from the census considered in the next phase 	first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 
1	158	10437	the final project includes three different models that process data in different phases first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 the purpose of this model is to reduce the number of features from the census considered in the next phase 	the purpose of this model is to reduce the number of features from the census considered in the next phase 
0	158	10438	the final project includes three different models that process data in different phases first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 the purpose of this model is to reduce the number of features from the census considered in the next phase 	only the most important variables detected by the algorithm in this phase continue in the dataset 
1	158	10439	the final project includes three different models that process data in different phases first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 the purpose of this model is to reduce the number of features from the census considered in the next phase 	the final set of variables have an accumulated feature importance of 0 99 in the model 
0	158	10440	the final project includes three different models that process data in different phases first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 the purpose of this model is to reduce the number of features from the census considered in the next phase 	in this phase 3 different algorithms are tested svm gradient boosted trees gbt and ridge regression 
0	158	10441	the final project includes three different models that process data in different phases first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 the purpose of this model is to reduce the number of features from the census considered in the next phase 	for the gbt two different implementations are evaluated scikit learn and lightgbm 
1	158	10442	the final project includes three different models that process data in different phases first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 the purpose of this model is to reduce the number of features from the census considered in the next phase 	the evaluation metric chosen is the r defined as the model chosen is the one that presents greatest r in the test set 638 schools 
0	158	10443	the final project includes three different models that process data in different phases first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 the purpose of this model is to reduce the number of features from the census considered in the next phase 	after the selection of variables 129 features continued to the next phase the clustering algorithm 
1	158	10444	the final project includes three different models that process data in different phases first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 the purpose of this model is to reduce the number of features from the census considered in the next phase 	this second model uses the descriptive variables to separate schools in groups with similar needs 
1	158	10445	the final project includes three different models that process data in different phases first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 the purpose of this model is to reduce the number of features from the census considered in the next phase 	since the final goal of the project is to define the budget and its optimal distribution for each school there is the need to isolate the effect of other variables not related to expenditure that are correlated to the ideb 
1	158	10446	the final project includes three different models that process data in different phases first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 the purpose of this model is to reduce the number of features from the census considered in the next phase 	this is the purpose of the second stage in the data processing framework the evaluation metric for the clusters is the mean silhouette coefficient 
1	158	10447	the final project includes three different models that process data in different phases first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 the purpose of this model is to reduce the number of features from the census considered in the next phase 	for one sample in the train set the silhouette is given by where a is the mean intra cluster euclidean distance to the considered point and b is the euclidean distance to the nearest point in other cluster 
0	158	10448	the final project includes three different models that process data in different phases first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 the purpose of this model is to reduce the number of features from the census considered in the next phase 	two different approaches are tested both of them use k means as the main algorithm 
0	158	10449	the final project includes three different models that process data in different phases first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 the purpose of this model is to reduce the number of features from the census considered in the next phase 	in one of them however the original data is first transformed with principal component analysis pca in order to reduce the dimensionality of the dataset 
1	158	10450	the final project includes three different models that process data in different phases first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 the purpose of this model is to reduce the number of features from the census considered in the next phase 	the model used data from 9837 schools this phase did not consider only schools administered by the state government but also those ran by the federal and city governments the final phase is a combination of multiple classifiers one for each cluster 
0	158	10451	the final project includes three different models that process data in different phases first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 the purpose of this model is to reduce the number of features from the census considered in the next phase 	each model predicts whether the school achieved its goal for the 2017 ideb 
0	158	10452	the final project includes three different models that process data in different phases first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 the purpose of this model is to reduce the number of features from the census considered in the next phase 	the input variables are the expenditure data for each school there are 3152 schools and 711 features 
1	158	10453	the final project includes three different models that process data in different phases first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 the purpose of this model is to reduce the number of features from the census considered in the next phase 	the assumption in this phase is that after isolating the effects of descriptive variables in the ideb it is possible to find an expenditure distribution that will minimize the total sum of investments per school while allowing it to achieve its goal 
1	158	10454	the final project includes three different models that process data in different phases first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 the purpose of this model is to reduce the number of features from the census considered in the next phase 	this distribution will be equal for all schools in the same cluster the evaluation metric is the f1 score that combines both precision and recall in order to guarantee that the model do not present good performance only for the most common class 
0	158	10455	the final project includes three different models that process data in different phases first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 the purpose of this model is to reduce the number of features from the census considered in the next phase 	the f1 score is given by in this phase only one algorithm was implemented derived from the first part the gbt implementation in scikit learn 
1	158	10456	the final project includes three different models that process data in different phases first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 the purpose of this model is to reduce the number of features from the census considered in the next phase 	the final tool can be applied in the estimation of the budget for each school in the chosen approach first all the schools that achieved their goals and for which the model presented correct predictions are selected 
1	158	10457	the final project includes three different models that process data in different phases first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 the purpose of this model is to reduce the number of features from the census considered in the next phase 	the initial budget estimate for each category is the minimum value greater than 0 if there is one found for that category in this group of schools 
1	158	10458	the final project includes three different models that process data in different phases first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 the purpose of this model is to reduce the number of features from the census considered in the next phase 	if the model predicts success in goal achievement with this expenditure distribution it is considered as the final budget if the model predicts fail in goal achievement one of the categories is chosen to be increased 
1	158	10459	the final project includes three different models that process data in different phases first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 the purpose of this model is to reduce the number of features from the census considered in the next phase 	the probability of selecting a specific category is equal to the normalized feature importance of the variable that represents this category according to the final model 
0	158	10460	the final project includes three different models that process data in different phases first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 the purpose of this model is to reduce the number of features from the census considered in the next phase 	the budget for the selected category than assumes the value of the second lowest expenditure for this category in the selected subset of schools 
0	158	10461	the final project includes three different models that process data in different phases first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 the purpose of this model is to reduce the number of features from the census considered in the next phase 	this process is repeated until the model predicts success in goal achievement 
0	158	10462	the final project includes three different models that process data in different phases first a regression model uses the descriptive data from the school census 353 features to predict the ideb of each school 3190 in 2013 the purpose of this model is to reduce the number of features from the census considered in the next phase 	if a specific category achieves its maximum possible value its probability of being selected in the following iterations goes to 0 
0	158	10463	the results for the each model tested is in	the results for the each model tested is in
1	158	10464	the 129 most important features in the previous model are then used in the clustering algorithm to separate schools in groups the results of the two models tested are in table 2 the number of clusters varied from 4 to 20 and in the final model consisted of 10 clusters in a tradeoff between increase in the silhouette and guaranteeing a reasonable number of schools in each cluster 	the 129 most important features in the previous model are then used in the clustering algorithm to separate schools in groups 
0	158	10465	the 129 most important features in the previous model are then used in the clustering algorithm to separate schools in groups the results of the two models tested are in table 2 the number of clusters varied from 4 to 20 and in the final model consisted of 10 clusters in a tradeoff between increase in the silhouette and guaranteeing a reasonable number of schools in each cluster 	the results of the two models tested are in table 2 
1	158	10466	the 129 most important features in the previous model are then used in the clustering algorithm to separate schools in groups the results of the two models tested are in table 2 the number of clusters varied from 4 to 20 and in the final model consisted of 10 clusters in a tradeoff between increase in the silhouette and guaranteeing a reasonable number of schools in each cluster 	the number of clusters varied from 4 to 20 and in the final model consisted of 10 clusters in a tradeoff between increase in the silhouette and guaranteeing a reasonable number of schools in each cluster 
0	158	10467	the 129 most important features in the previous model are then used in the clustering algorithm to separate schools in groups the results of the two models tested are in table 2 the number of clusters varied from 4 to 20 and in the final model consisted of 10 clusters in a tradeoff between increase in the silhouette and guaranteeing a reasonable number of schools in each cluster 	still some clusters ended up with few schools to the minimum of one 
0	158	10468	the 129 most important features in the previous model are then used in the clustering algorithm to separate schools in groups the results of the two models tested are in table 2 the number of clusters varied from 4 to 20 and in the final model consisted of 10 clusters in a tradeoff between increase in the silhouette and guaranteeing a reasonable number of schools in each cluster 	these consist of outliers and these clusters did not enter in the next phase 
0	158	10469	silhouette k means 0 767	silhouette k means 0 767
0	158	10470	although the silhouette for the model with pca preprocessing was higher both algorithms presented a very similar result with clusters almost identical the output of both cases were tested in the classification phase and the clusters from the model with pca presented a better weighted average f1 score for the classifiers for this reason this was the selected model 	although the silhouette for the model with pca preprocessing was higher both algorithms presented a very similar result with clusters almost identical 
1	158	10471	although the silhouette for the model with pca preprocessing was higher both algorithms presented a very similar result with clusters almost identical the output of both cases were tested in the classification phase and the clusters from the model with pca presented a better weighted average f1 score for the classifiers for this reason this was the selected model 	the output of both cases were tested in the classification phase and the clusters from the model with pca presented a better weighted average f1 score for the classifiers 
0	158	10472	although the silhouette for the model with pca preprocessing was higher both algorithms presented a very similar result with clusters almost identical the output of both cases were tested in the classification phase and the clusters from the model with pca presented a better weighted average f1 score for the classifiers for this reason this was the selected model 	for this reason this was the selected model 
1	158	10473	although the silhouette for the model with pca preprocessing was higher both algorithms presented a very similar result with clusters almost identical the output of both cases were tested in the classification phase and the clusters from the model with pca presented a better weighted average f1 score for the classifiers for this reason this was the selected model 	as expected by separating schools into clusters the performance of the classifiers increase 
1	158	10474	although the silhouette for the model with pca preprocessing was higher both algorithms presented a very similar result with clusters almost identical the output of both cases were tested in the classification phase and the clusters from the model with pca presented a better weighted average f1 score for the classifiers for this reason this was the selected model 	this means that it is easier for the model to find patterns in expenditure data when schools with similar descriptive features are grouped together and isolated from other groups 
0	158	10475	although the silhouette for the model with pca preprocessing was higher both algorithms presented a very similar result with clusters almost identical the output of both cases were tested in the classification phase and the clusters from the model with pca presented a better weighted average f1 score for the classifiers for this reason this was the selected model 	this supports the initial hypothesis 
1	158	10476	although the silhouette for the model with pca preprocessing was higher both algorithms presented a very similar result with clusters almost identical the output of both cases were tested in the classification phase and the clusters from the model with pca presented a better weighted average f1 score for the classifiers for this reason this was the selected model 	however it is also possible to observe that some clusters as number 0 presented a test f1 score lower than the model with all schools together 
0	158	10477	although the silhouette for the model with pca preprocessing was higher both algorithms presented a very similar result with clusters almost identical the output of both cases were tested in the classification phase and the clusters from the model with pca presented a better weighted average f1 score for the classifiers for this reason this was the selected model 	this might be an indication that this cluster is not homogeneous in terms of characteristics that might affect the ideb 
1	158	10478	although the silhouette for the model with pca preprocessing was higher both algorithms presented a very similar result with clusters almost identical the output of both cases were tested in the classification phase and the clusters from the model with pca presented a better weighted average f1 score for the classifiers for this reason this was the selected model 	in addition clusters as number 5 had problems with overfitting due to the small sample of schools it represents 
1	158	10479	cluster number 3 had excellent performance which indicates that this cluster is homogeneous and that it is possible to find a common expenditure distribution for these schools that will allow them to achieve their goals for this cluster the method of budget estimation described previously was implemented the prediction for the initial budget estimation minimum values for each category was 1 therefore there was no need to iteratively search for the expenditure distribution applying the minimum estimated budget for each school there is a reduction of r 314 972 841 00 in the total spending of the government of sao paulo with the schools in cluster 3 	cluster number 3 had excellent performance which indicates that this cluster is homogeneous and that it is possible to find a common expenditure distribution for these schools that will allow them to achieve their goals 
0	158	10480	cluster number 3 had excellent performance which indicates that this cluster is homogeneous and that it is possible to find a common expenditure distribution for these schools that will allow them to achieve their goals for this cluster the method of budget estimation described previously was implemented the prediction for the initial budget estimation minimum values for each category was 1 therefore there was no need to iteratively search for the expenditure distribution applying the minimum estimated budget for each school there is a reduction of r 314 972 841 00 in the total spending of the government of sao paulo with the schools in cluster 3 	for this cluster the method of budget estimation described previously was implemented 
1	158	10481	cluster number 3 had excellent performance which indicates that this cluster is homogeneous and that it is possible to find a common expenditure distribution for these schools that will allow them to achieve their goals for this cluster the method of budget estimation described previously was implemented the prediction for the initial budget estimation minimum values for each category was 1 therefore there was no need to iteratively search for the expenditure distribution applying the minimum estimated budget for each school there is a reduction of r 314 972 841 00 in the total spending of the government of sao paulo with the schools in cluster 3 	the prediction for the initial budget estimation minimum values for each category was 1 therefore there was no need to iteratively search for the expenditure distribution applying the minimum estimated budget for each school there is a reduction of r 314 972 841 00 in the total spending of the government of sao paulo with the schools in cluster 3 
0	158	10482	cluster number 3 had excellent performance which indicates that this cluster is homogeneous and that it is possible to find a common expenditure distribution for these schools that will allow them to achieve their goals for this cluster the method of budget estimation described previously was implemented the prediction for the initial budget estimation minimum values for each category was 1 therefore there was no need to iteratively search for the expenditure distribution applying the minimum estimated budget for each school there is a reduction of r 314 972 841 00 in the total spending of the government of sao paulo with the schools in cluster 3 	in addition according to the model all schools would have achieved their goals using the estimated expenditure distribution while with the current budget approximately 30 of schools in this cluster did not achieve their goals 
0	158	10483	the usefulness of the tool developed in this study depends heavily on the quality of the clustering algorithm for the initial assumption about the clusters to hold all the relevant factors associated with the schools that do not relate to their spending and that affect the ideb must be represented in the clustering variables when this is the case the separation of schools in groups will be able to isolate the effect of this variables and all variation observed in the ideb will be explained solely by differences in the expenditure distribution in the present project the assumption was valid for some clusters mainly number 3 	the usefulness of the tool developed in this study depends heavily on the quality of the clustering algorithm 
1	158	10484	the usefulness of the tool developed in this study depends heavily on the quality of the clustering algorithm for the initial assumption about the clusters to hold all the relevant factors associated with the schools that do not relate to their spending and that affect the ideb must be represented in the clustering variables when this is the case the separation of schools in groups will be able to isolate the effect of this variables and all variation observed in the ideb will be explained solely by differences in the expenditure distribution in the present project the assumption was valid for some clusters mainly number 3 	for the initial assumption about the clusters to hold all the relevant factors associated with the schools that do not relate to their spending and that affect the ideb must be represented in the clustering variables 
1	158	10485	the usefulness of the tool developed in this study depends heavily on the quality of the clustering algorithm for the initial assumption about the clusters to hold all the relevant factors associated with the schools that do not relate to their spending and that affect the ideb must be represented in the clustering variables when this is the case the separation of schools in groups will be able to isolate the effect of this variables and all variation observed in the ideb will be explained solely by differences in the expenditure distribution in the present project the assumption was valid for some clusters mainly number 3 	when this is the case the separation of schools in groups will be able to isolate the effect of this variables and all variation observed in the ideb will be explained solely by differences in the expenditure distribution in the present project the assumption was valid for some clusters mainly number 3 
1	158	10486	the usefulness of the tool developed in this study depends heavily on the quality of the clustering algorithm for the initial assumption about the clusters to hold all the relevant factors associated with the schools that do not relate to their spending and that affect the ideb must be represented in the clustering variables when this is the case the separation of schools in groups will be able to isolate the effect of this variables and all variation observed in the ideb will be explained solely by differences in the expenditure distribution in the present project the assumption was valid for some clusters mainly number 3 	for this cluster it was possible to create a good predictor of goal achievement only with expenditure features 
1	158	10487	the usefulness of the tool developed in this study depends heavily on the quality of the clustering algorithm for the initial assumption about the clusters to hold all the relevant factors associated with the schools that do not relate to their spending and that affect the ideb must be represented in the clustering variables when this is the case the separation of schools in groups will be able to isolate the effect of this variables and all variation observed in the ideb will be explained solely by differences in the expenditure distribution in the present project the assumption was valid for some clusters mainly number 3 	when this condition is present this tool can be very useful in minimizing the budget of the schools while guaranteeing they will achieve their goals however other clusters mainly number 0 are too heterogeneous to have the ideb explained only with spending data 
0	158	10488	the usefulness of the tool developed in this study depends heavily on the quality of the clustering algorithm for the initial assumption about the clusters to hold all the relevant factors associated with the schools that do not relate to their spending and that affect the ideb must be represented in the clustering variables when this is the case the separation of schools in groups will be able to isolate the effect of this variables and all variation observed in the ideb will be explained solely by differences in the expenditure distribution in the present project the assumption was valid for some clusters mainly number 3 	it means that to create a good predictor for goal achievement more variables are needed 
1	158	10489	the usefulness of the tool developed in this study depends heavily on the quality of the clustering algorithm for the initial assumption about the clusters to hold all the relevant factors associated with the schools that do not relate to their spending and that affect the ideb must be represented in the clustering variables when this is the case the separation of schools in groups will be able to isolate the effect of this variables and all variation observed in the ideb will be explained solely by differences in the expenditure distribution in the present project the assumption was valid for some clusters mainly number 3 	therefore for this clusters it is not possible to explain goal achievement only as a function of spending distribution to solve the problem described above the first step would be to incorporate new variables in the clustering phase 
1	158	10490	the usefulness of the tool developed in this study depends heavily on the quality of the clustering algorithm for the initial assumption about the clusters to hold all the relevant factors associated with the schools that do not relate to their spending and that affect the ideb must be represented in the clustering variables when this is the case the separation of schools in groups will be able to isolate the effect of this variables and all variation observed in the ideb will be explained solely by differences in the expenditure distribution in the present project the assumption was valid for some clusters mainly number 3 	for example sociodemographic variables of the region where the school is located are probably highly correlated with its ideb also 
1	158	10491	the usefulness of the tool developed in this study depends heavily on the quality of the clustering algorithm for the initial assumption about the clusters to hold all the relevant factors associated with the schools that do not relate to their spending and that affect the ideb must be represented in the clustering variables when this is the case the separation of schools in groups will be able to isolate the effect of this variables and all variation observed in the ideb will be explained solely by differences in the expenditure distribution in the present project the assumption was valid for some clusters mainly number 3 	features as the average income of residents average number of people in one house and distance from the center of the city are not present in the school census data used in the first two phases of the project the brazilian census have this type of sociodemographic data 
0	158	10492	the usefulness of the tool developed in this study depends heavily on the quality of the clustering algorithm for the initial assumption about the clusters to hold all the relevant factors associated with the schools that do not relate to their spending and that affect the ideb must be represented in the clustering variables when this is the case the separation of schools in groups will be able to isolate the effect of this variables and all variation observed in the ideb will be explained solely by differences in the expenditure distribution in the present project the assumption was valid for some clusters mainly number 3 	however in this dataset locations are described as sectors and each sector has its own code 
0	158	10493	the usefulness of the tool developed in this study depends heavily on the quality of the clustering algorithm for the initial assumption about the clusters to hold all the relevant factors associated with the schools that do not relate to their spending and that affect the ideb must be represented in the clustering variables when this is the case the separation of schools in groups will be able to isolate the effect of this variables and all variation observed in the ideb will be explained solely by differences in the expenditure distribution in the present project the assumption was valid for some clusters mainly number 3 	the problem when linking this dataset to the school census is that the last one does not have information on the code of the sector where the school is 
1	158	10494	the usefulness of the tool developed in this study depends heavily on the quality of the clustering algorithm for the initial assumption about the clusters to hold all the relevant factors associated with the schools that do not relate to their spending and that affect the ideb must be represented in the clustering variables when this is the case the separation of schools in groups will be able to isolate the effect of this variables and all variation observed in the ideb will be explained solely by differences in the expenditure distribution in the present project the assumption was valid for some clusters mainly number 3 	this needs to be solved in order to include data from the brazilian census in the clustering algorithm a second point of improvement is aggregating redundant categories of spending 
0	158	10495	the usefulness of the tool developed in this study depends heavily on the quality of the clustering algorithm for the initial assumption about the clusters to hold all the relevant factors associated with the schools that do not relate to their spending and that affect the ideb must be represented in the clustering variables when this is the case the separation of schools in groups will be able to isolate the effect of this variables and all variation observed in the ideb will be explained solely by differences in the expenditure distribution in the present project the assumption was valid for some clusters mainly number 3 	this problem was detailed previously 
1	158	10496	the usefulness of the tool developed in this study depends heavily on the quality of the clustering algorithm for the initial assumption about the clusters to hold all the relevant factors associated with the schools that do not relate to their spending and that affect the ideb must be represented in the clustering variables when this is the case the separation of schools in groups will be able to isolate the effect of this variables and all variation observed in the ideb will be explained solely by differences in the expenditure distribution in the present project the assumption was valid for some clusters mainly number 3 	because of these redundancies it might be difficult for the last model to identify the real impact of each subarea of investment on the ideb other limitation also explained previously is that the data provided by the government of sao paulo does not have detailed spending for each school 
1	158	10497	the usefulness of the tool developed in this study depends heavily on the quality of the clustering algorithm for the initial assumption about the clusters to hold all the relevant factors associated with the schools that do not relate to their spending and that affect the ideb must be represented in the clustering variables when this is the case the separation of schools in groups will be able to isolate the effect of this variables and all variation observed in the ideb will be explained solely by differences in the expenditure distribution in the present project the assumption was valid for some clusters mainly number 3 	there certainly is in the government database this type of data however it is not open to the public finally this project did not explicitly try to find a causal relation between the input features and the target variable which is a necessary step in the design of public policies 
0	158	10498	the usefulness of the tool developed in this study depends heavily on the quality of the clustering algorithm for the initial assumption about the clusters to hold all the relevant factors associated with the schools that do not relate to their spending and that affect the ideb must be represented in the clustering variables when this is the case the separation of schools in groups will be able to isolate the effect of this variables and all variation observed in the ideb will be explained solely by differences in the expenditure distribution in the present project the assumption was valid for some clusters mainly number 3 	a qualitative evaluation of the importance of the features in the first and third models as well as the impact they have on the target variable needs to be conducted 
0	158	10499	the usefulness of the tool developed in this study depends heavily on the quality of the clustering algorithm for the initial assumption about the clusters to hold all the relevant factors associated with the schools that do not relate to their spending and that affect the ideb must be represented in the clustering variables when this is the case the separation of schools in groups will be able to isolate the effect of this variables and all variation observed in the ideb will be explained solely by differences in the expenditure distribution in the present project the assumption was valid for some clusters mainly number 3 	this would be better performed with the assistance of specialists in the area this tool however is a good starting point for the government to explore quantitative tools in the design of public policies 
1	158	10500	the usefulness of the tool developed in this study depends heavily on the quality of the clustering algorithm for the initial assumption about the clusters to hold all the relevant factors associated with the schools that do not relate to their spending and that affect the ideb must be represented in the clustering variables when this is the case the separation of schools in groups will be able to isolate the effect of this variables and all variation observed in the ideb will be explained solely by differences in the expenditure distribution in the present project the assumption was valid for some clusters mainly number 3 	in a real implementation there would be an evaluation period when the tool would suggest the expenditure distribution and after its implementation the results would be reevaluated and incorporated in the model 
1	159	10501	pricing a rental property on airbnb is a challenging task for the owner as it determines the number of customers for the place on the other hand customers have to evaluate an offered price with minimal knowledge of an optimal value for the property this project aims to develop a price prediction model using a range of methods from linear regression to tree based models support vector regression svr k means clustering kmc and neural networks nns to tackle this challenge 	pricing a rental property on airbnb is a challenging task for the owner as it determines the number of customers for the place 
1	159	10502	pricing a rental property on airbnb is a challenging task for the owner as it determines the number of customers for the place on the other hand customers have to evaluate an offered price with minimal knowledge of an optimal value for the property this project aims to develop a price prediction model using a range of methods from linear regression to tree based models support vector regression svr k means clustering kmc and neural networks nns to tackle this challenge 	on the other hand customers have to evaluate an offered price with minimal knowledge of an optimal value for the property 
1	159	10503	pricing a rental property on airbnb is a challenging task for the owner as it determines the number of customers for the place on the other hand customers have to evaluate an offered price with minimal knowledge of an optimal value for the property this project aims to develop a price prediction model using a range of methods from linear regression to tree based models support vector regression svr k means clustering kmc and neural networks nns to tackle this challenge 	this project aims to develop a price prediction model using a range of methods from linear regression to tree based models support vector regression svr k means clustering kmc and neural networks nns to tackle this challenge 
1	159	10504	pricing a rental property on airbnb is a challenging task for the owner as it determines the number of customers for the place on the other hand customers have to evaluate an offered price with minimal knowledge of an optimal value for the property this project aims to develop a price prediction model using a range of methods from linear regression to tree based models support vector regression svr k means clustering kmc and neural networks nns to tackle this challenge 	features of the rentals owner characteristics and the customer reviews will be used to predict the price of the listing 
0	159	10505	existing literature shows that some studies focus on non shared property purchase or rental price predictions in a cs229 project yu and wu this project has tried to further the experimented methods from the literature by focusing on a variety of feature selection techniques implementing neural networks and leveraging the customer reviews through sentiment analysis the authors were unable to find the last two mentioned undertakings in the existing literature 	existing literature shows that some studies focus on non shared property purchase or rental price predictions 
1	159	10506	existing literature shows that some studies focus on non shared property purchase or rental price predictions in a cs229 project yu and wu this project has tried to further the experimented methods from the literature by focusing on a variety of feature selection techniques implementing neural networks and leveraging the customer reviews through sentiment analysis the authors were unable to find the last two mentioned undertakings in the existing literature 	in a cs229 project yu and wu this project has tried to further the experimented methods from the literature by focusing on a variety of feature selection techniques implementing neural networks and leveraging the customer reviews through sentiment analysis 
0	159	10507	existing literature shows that some studies focus on non shared property purchase or rental price predictions in a cs229 project yu and wu this project has tried to further the experimented methods from the literature by focusing on a variety of feature selection techniques implementing neural networks and leveraging the customer reviews through sentiment analysis the authors were unable to find the last two mentioned undertakings in the existing literature 	the authors were unable to find the last two mentioned undertakings in the existing literature 
0	159	10508	the main data source for this study is the public airbnb dataset for new york city 1 the dataset includes 50 221 entries each with 96 features see	the main data source for this study is the public airbnb dataset for new york city 1 
0	159	10509	the main data source for this study is the public airbnb dataset for new york city 1 the dataset includes 50 221 entries each with 96 features see	the dataset includes 50 221 entries each with 96 features 
0	159	10510	the reviews for each listing were analyzed using textblob 2 sentiment analysis module this method assigns a score between 1 and 1 to each review and the scores are averaged across each listing the final scores for each listing was included as a new feature in the model 	the reviews for each listing were analyzed using textblob 2 sentiment analysis module 
0	159	10511	the reviews for each listing were analyzed using textblob 2 sentiment analysis module this method assigns a score between 1 and 1 to each review and the scores are averaged across each listing the final scores for each listing was included as a new feature in the model 	this method assigns a score between 1 and 1 to each review and the scores are averaged across each listing 
0	159	10512	the reviews for each listing were analyzed using textblob 2 sentiment analysis module this method assigns a score between 1 and 1 to each review and the scores are averaged across each listing the final scores for each listing was included as a new feature in the model 	the final scores for each listing was included as a new feature in the model 
1	159	10513	after data preprocessing the feature vector contained 764 elements which was deemed excessive and when fed to models resulted in a high variance of error consequently several feature selection techniques were used to find the features with the most predictive values to both reduce the model variances and reduce the computation time based on prior experience with housing price estimation the first effort was manual selection of features to create a baseline for the selection process the second method was tuning the coefficient of linear regression model with lasso regularization trained on the train split from which the model with the best performance over validation split was selected 	after data preprocessing the feature vector contained 764 elements which was deemed excessive and when fed to models resulted in a high variance of error 
1	159	10514	after data preprocessing the feature vector contained 764 elements which was deemed excessive and when fed to models resulted in a high variance of error consequently several feature selection techniques were used to find the features with the most predictive values to both reduce the model variances and reduce the computation time based on prior experience with housing price estimation the first effort was manual selection of features to create a baseline for the selection process the second method was tuning the coefficient of linear regression model with lasso regularization trained on the train split from which the model with the best performance over validation split was selected 	consequently several feature selection techniques were used to find the features with the most predictive values to both reduce the model variances and reduce the computation time 
1	159	10515	after data preprocessing the feature vector contained 764 elements which was deemed excessive and when fed to models resulted in a high variance of error consequently several feature selection techniques were used to find the features with the most predictive values to both reduce the model variances and reduce the computation time based on prior experience with housing price estimation the first effort was manual selection of features to create a baseline for the selection process the second method was tuning the coefficient of linear regression model with lasso regularization trained on the train split from which the model with the best performance over validation split was selected 	based on prior experience with housing price estimation the first effort was manual selection of features to create a baseline for the selection process the second method was tuning the coefficient of linear regression model with lasso regularization trained on the train split from which the model with the best performance over validation split was selected 
1	159	10516	after data preprocessing the feature vector contained 764 elements which was deemed excessive and when fed to models resulted in a high variance of error consequently several feature selection techniques were used to find the features with the most predictive values to both reduce the model variances and reduce the computation time based on prior experience with housing price estimation the first effort was manual selection of features to create a baseline for the selection process the second method was tuning the coefficient of linear regression model with lasso regularization trained on the train split from which the model with the best performance over validation split was selected 	second set of features consisted of 78 features with non zero values based on this method finally lowest p values of regular linear regression model trained on train split were used to choose the third set of features 
0	159	10517	after data preprocessing the feature vector contained 764 elements which was deemed excessive and when fed to models resulted in a high variance of error consequently several feature selection techniques were used to find the features with the most predictive values to both reduce the model variances and reduce the computation time based on prior experience with housing price estimation the first effort was manual selection of features to create a baseline for the selection process the second method was tuning the coefficient of linear regression model with lasso regularization trained on the train split from which the model with the best performance over validation split was selected 	selection was bound by the total number of features to remain less than 100 
1	159	10518	after data preprocessing the feature vector contained 764 elements which was deemed excessive and when fed to models resulted in a high variance of error consequently several feature selection techniques were used to find the features with the most predictive values to both reduce the model variances and reduce the computation time based on prior experience with housing price estimation the first effort was manual selection of features to create a baseline for the selection process the second method was tuning the coefficient of linear regression model with lasso regularization trained on the train split from which the model with the best performance over validation split was selected 	the final set of features were those for which linear regression model performed best on validation split the performance of manually selected features as well as p value and lasso feature selection schemes were compared using the r 2 score of the linear regression models trained on the validation set 
1	159	10519	after data preprocessing the feature vector contained 764 elements which was deemed excessive and when fed to models resulted in a high variance of error consequently several feature selection techniques were used to find the features with the most predictive values to both reduce the model variances and reduce the computation time based on prior experience with housing price estimation the first effort was manual selection of features to create a baseline for the selection process the second method was tuning the coefficient of linear regression model with lasso regularization trained on the train split from which the model with the best performance over validation split was selected 	all models outperformed the baseline model which used the whole feature set and the second method lasso regularization yielded the highest r 2 score 
1	159	10520	linear regression was set as a baseline model on the dataset using all of the features as model inputs after selecting a set of features using lasso feature selection several machine learning models were considered in order to find the optimal one all of the models except neural networks were implemented using scikit learn library	linear regression was set as a baseline model on the dataset using all of the features as model inputs 
0	159	10521	linear regression was set as a baseline model on the dataset using all of the features as model inputs after selecting a set of features using lasso feature selection several machine learning models were considered in order to find the optimal one all of the models except neural networks were implemented using scikit learn library	after selecting a set of features using lasso feature selection several machine learning models were considered in order to find the optimal one 
0	159	10522	linear regression was set as a baseline model on the dataset using all of the features as model inputs after selecting a set of features using lasso feature selection several machine learning models were considered in order to find the optimal one all of the models except neural networks were implemented using scikit learn library	all of the models except neural networks were implemented using scikit learn library
1	159	10523	linear regression with l 2 regularization adds a penalizing term to the squared error cost function in order to help the algorithm converge for linearly separable data and reduce overfitting therefore ridge regression minimizes j y x 2 2 2 2 with respect to where x is a design matrix and is a hyperparameter since the baseline models were observed to have high variance ridge regression seemed to be an appropriate choice to solve the issue 	linear regression with l 2 regularization adds a penalizing term to the squared error cost function in order to help the algorithm converge for linearly separable data and reduce overfitting 
0	159	10524	linear regression with l 2 regularization adds a penalizing term to the squared error cost function in order to help the algorithm converge for linearly separable data and reduce overfitting therefore ridge regression minimizes j y x 2 2 2 2 with respect to where x is a design matrix and is a hyperparameter since the baseline models were observed to have high variance ridge regression seemed to be an appropriate choice to solve the issue 	therefore ridge regression minimizes j y x 2 2 2 2 with respect to where x is a design matrix and is a hyperparameter 
1	159	10525	linear regression with l 2 regularization adds a penalizing term to the squared error cost function in order to help the algorithm converge for linearly separable data and reduce overfitting therefore ridge regression minimizes j y x 2 2 2 2 with respect to where x is a design matrix and is a hyperparameter since the baseline models were observed to have high variance ridge regression seemed to be an appropriate choice to solve the issue 	since the baseline models were observed to have high variance ridge regression seemed to be an appropriate choice to solve the issue 
1	159	10526	in order to capture the non linearity of the data the training examples were split into different clusters using k means clustering on the features and the ridge regression was run on each of the individual clusters the data clusters were identified using the following algorithm 	in order to capture the non linearity of the data the training examples were split into different clusters using k means clustering on the features and the ridge regression was run on each of the individual clusters 
0	159	10527	in order to capture the non linearity of the data the training examples were split into different clusters using k means clustering on the features and the ridge regression was run on each of the individual clusters the data clusters were identified using the following algorithm 	the data clusters were identified using the following algorithm 
1	159	10528	initialize cluster centroids i k randomly repeat assgin each point to a cluster for each centroid calculate the loss function for the assignments and check for convergence until convergence	initialize cluster centroids i k randomly repeat assgin each point to a cluster for each centroid calculate the loss function for the assignments and check for convergence until convergence
1	159	10529	in order to model the non linear relationship between the covariates the team employed support vector regression with rbf kernel to identify a linear boundary in a high dimensional feature space using the implementation based on libsvm paper m where c 0 0 are given parameters this problem can be converted into a dual problem that does not involve x but involves k x z x z instead 	in order to model the non linear relationship between the covariates the team employed support vector regression with rbf kernel to identify a linear boundary in a high dimensional feature space 
0	159	10530	in order to model the non linear relationship between the covariates the team employed support vector regression with rbf kernel to identify a linear boundary in a high dimensional feature space using the implementation based on libsvm paper m where c 0 0 are given parameters this problem can be converted into a dual problem that does not involve x but involves k x z x z instead 	using the implementation based on libsvm paper m where c 0 0 are given parameters 
0	159	10531	in order to model the non linear relationship between the covariates the team employed support vector regression with rbf kernel to identify a linear boundary in a high dimensional feature space using the implementation based on libsvm paper m where c 0 0 are given parameters this problem can be converted into a dual problem that does not involve x but involves k x z x z instead 	this problem can be converted into a dual problem that does not involve x but involves k x z x z instead 
0	159	10532	in order to model the non linear relationship between the covariates the team employed support vector regression with rbf kernel to identify a linear boundary in a high dimensional feature space using the implementation based on libsvm paper m where c 0 0 are given parameters this problem can be converted into a dual problem that does not involve x but involves k x z x z instead 	since we are using rbf kernel k x z exp x z 2 2 2 
1	159	10533	neural network was used to build a model that combined the input features into high level predictors the architecture of the optimized network had 3 fully connected layers 20 neurons in the first hidden layer with relu activation function 5 neurons in the second hidden layer with relu activation function and 1 output neuron with a linear activation function 	neural network was used to build a model that combined the input features into high level predictors 
1	159	10534	neural network was used to build a model that combined the input features into high level predictors the architecture of the optimized network had 3 fully connected layers 20 neurons in the first hidden layer with relu activation function 5 neurons in the second hidden layer with relu activation function and 1 output neuron with a linear activation function 	the architecture of the optimized network had 3 fully connected layers 20 neurons in the first hidden layer with relu activation function 5 neurons in the second hidden layer with relu activation function and 1 output neuron with a linear activation function 
1	159	10535	since the relationship between the feature vector and price is non linear regression tree seemed like a proper model for this problem regression trees split the data points into regions according to the following formula where j is the feature the dataset is split on t is the threshold of the split r p is the parent region and r 1 and r 2 are the child regions squared error is used as the loss function since standalone regression trees have low predictive accuracies individually gradient boost tree ensemble was used to increase the models performance 	since the relationship between the feature vector and price is non linear regression tree seemed like a proper model for this problem 
1	159	10536	since the relationship between the feature vector and price is non linear regression tree seemed like a proper model for this problem regression trees split the data points into regions according to the following formula where j is the feature the dataset is split on t is the threshold of the split r p is the parent region and r 1 and r 2 are the child regions squared error is used as the loss function since standalone regression trees have low predictive accuracies individually gradient boost tree ensemble was used to increase the models performance 	regression trees split the data points into regions according to the following formula where j is the feature the dataset is split on t is the threshold of the split r p is the parent region and r 1 and r 2 are the child regions 
1	159	10537	since the relationship between the feature vector and price is non linear regression tree seemed like a proper model for this problem regression trees split the data points into regions according to the following formula where j is the feature the dataset is split on t is the threshold of the split r p is the parent region and r 1 and r 2 are the child regions squared error is used as the loss function since standalone regression trees have low predictive accuracies individually gradient boost tree ensemble was used to increase the models performance 	squared error is used as the loss function since standalone regression trees have low predictive accuracies individually gradient boost tree ensemble was used to increase the models performance 
0	159	10538	since the relationship between the feature vector and price is non linear regression tree seemed like a proper model for this problem regression trees split the data points into regions according to the following formula where j is the feature the dataset is split on t is the threshold of the split r p is the parent region and r 1 and r 2 are the child regions squared error is used as the loss function since standalone regression trees have low predictive accuracies individually gradient boost tree ensemble was used to increase the models performance 	the idea behind a gradient boost is to improve on a previous iteration of the model by correcting its predictions using another model based on the negative gradient of the loss 
0	159	10539	since the relationship between the feature vector and price is non linear regression tree seemed like a proper model for this problem regression trees split the data points into regions according to the following formula where j is the feature the dataset is split on t is the threshold of the split r p is the parent region and r 1 and r 2 are the child regions squared error is used as the loss function since standalone regression trees have low predictive accuracies individually gradient boost tree ensemble was used to increase the models performance 	the algorithm for the gradient boosting is the following
0	159	10540	initialize f 0 to be a constant model for m 1 number of iterations do for all training examples 	initialize f 0 to be a constant model for m 1 number of iterations do for all training examples 
1	159	10541	mean absolute error mae mean squared error mse and r 2 score were used to evaluate the trained models training 39 980 examples and validation 4 998 examples splits were used to choose the best performing models within each category the test set containing 4 998 examples was used to provide an unbiased estimate of error with the final models trained on both train and validation splits 	mean absolute error mae mean squared error mse and r 2 score were used to evaluate the trained models 
1	159	10542	mean absolute error mae mean squared error mse and r 2 score were used to evaluate the trained models training 39 980 examples and validation 4 998 examples splits were used to choose the best performing models within each category the test set containing 4 998 examples was used to provide an unbiased estimate of error with the final models trained on both train and validation splits 	training 39 980 examples and validation 4 998 examples splits were used to choose the best performing models within each category 
1	159	10543	mean absolute error mae mean squared error mse and r 2 score were used to evaluate the trained models training 39 980 examples and validation 4 998 examples splits were used to choose the best performing models within each category the test set containing 4 998 examples was used to provide an unbiased estimate of error with the final models trained on both train and validation splits 	the test set containing 4 998 examples was used to provide an unbiased estimate of error with the final models trained on both train and validation splits 
0	159	10544	mean absolute error mae mean squared error mse and r 2 score were used to evaluate the trained models training 39 980 examples and validation 4 998 examples splits were used to choose the best performing models within each category the test set containing 4 998 examples was used to provide an unbiased estimate of error with the final models trained on both train and validation splits 	results for the final models 3 are provided below 
1	159	10545	mean absolute error mae mean squared error mse and r 2 score were used to evaluate the trained models training 39 980 examples and validation 4 998 examples splits were used to choose the best performing models within each category the test set containing 4 998 examples was used to provide an unbiased estimate of error with the final models trained on both train and validation splits 	the outlined models had relatively similar r 2 scores which implicates that lasso feature importance analysis had made the most impact on improving the performance of the models by reducing the variance 
0	159	10546	mean absolute error mae mean squared error mse and r 2 score were used to evaluate the trained models training 39 980 examples and validation 4 998 examples splits were used to choose the best performing models within each category the test set containing 4 998 examples was used to provide an unbiased estimate of error with the final models trained on both train and validation splits 	even after the feature selection the resulting input vector was relatively large leaving room for model overfitting 
1	159	10547	mean absolute error mae mean squared error mse and r 2 score were used to evaluate the trained models training 39 980 examples and validation 4 998 examples splits were used to choose the best performing models within each category the test set containing 4 998 examples was used to provide an unbiased estimate of error with the final models trained on both train and validation splits 	this explains why gradient boost a tree based model prone to high varianceperformed worse than the rest of the models despite it not performing the worst on the training set 
1	159	10548	despite expanding the number of features in the feature vector svr with rbf kernel turned out to be the best performing model with the least mae and mse and the highest r 2 score on both train and test sets figure 2 rbf feature mapping was able to better model the prices of the apartments which have a non linear relationship with the apartment features since regularization is taken into account in the svr optimization problem parameter tuning ensured that the model was not overfitting 	despite expanding the number of features in the feature vector svr with rbf kernel turned out to be the best performing model with the least mae and mse and the highest r 2 score on both train and test sets figure 2 
1	159	10549	despite expanding the number of features in the feature vector svr with rbf kernel turned out to be the best performing model with the least mae and mse and the highest r 2 score on both train and test sets figure 2 rbf feature mapping was able to better model the prices of the apartments which have a non linear relationship with the apartment features since regularization is taken into account in the svr optimization problem parameter tuning ensured that the model was not overfitting 	rbf feature mapping was able to better model the prices of the apartments which have a non linear relationship with the apartment features 
0	159	10550	despite expanding the number of features in the feature vector svr with rbf kernel turned out to be the best performing model with the least mae and mse and the highest r 2 score on both train and test sets figure 2 rbf feature mapping was able to better model the prices of the apartments which have a non linear relationship with the apartment features since regularization is taken into account in the svr optimization problem parameter tuning ensured that the model was not overfitting 	since regularization is taken into account in the svr optimization problem parameter tuning ensured that the model was not overfitting 
1	159	10551	despite expanding the number of features in the feature vector svr with rbf kernel turned out to be the best performing model with the least mae and mse and the highest r 2 score on both train and test sets figure 2 rbf feature mapping was able to better model the prices of the apartments which have a non linear relationship with the apartment features since regularization is taken into account in the svr optimization problem parameter tuning ensured that the model was not overfitting 	ridge regression neural network k means ridge regression models had similar r 2 scores even though the last two models are more complex than ridge regression 
0	159	10552	despite expanding the number of features in the feature vector svr with rbf kernel turned out to be the best performing model with the least mae and mse and the highest r 2 score on both train and test sets figure 2 rbf feature mapping was able to better model the prices of the apartments which have a non linear relationship with the apartment features since regularization is taken into account in the svr optimization problem parameter tuning ensured that the model was not overfitting 	the architecture complexity of neural network was limited by the insufficient number of training examples for having too many unknown weights 
1	159	10553	despite expanding the number of features in the feature vector svr with rbf kernel turned out to be the best performing model with the least mae and mse and the highest r 2 score on both train and test sets figure 2 rbf feature mapping was able to better model the prices of the apartments which have a non linear relationship with the apartment features since regularization is taken into account in the svr optimization problem parameter tuning ensured that the model was not overfitting 	k means clustering model faced a similar issue since the frequency of some prices was greatly exceeding the frequency of others some clusters received too few training examples and drove down the overall model performance 
1	159	10554	this project attempts to come up with the best model for predicting the airbnb prices based on a set of features including property specifications owner information and customer reviews on the listings machine learning techniques including linear regression tree based models svr and neural networks along with feature importance analyses are employed to achieve the best results in terms of mean squared error mean absolute error and r 2 score the initial experimentation with the baseline model proved that the abundance of features leads to high variance and weak performance of the model on the validation set compared to the training set 	this project attempts to come up with the best model for predicting the airbnb prices based on a set of features including property specifications owner information and customer reviews on the listings 
1	159	10555	this project attempts to come up with the best model for predicting the airbnb prices based on a set of features including property specifications owner information and customer reviews on the listings machine learning techniques including linear regression tree based models svr and neural networks along with feature importance analyses are employed to achieve the best results in terms of mean squared error mean absolute error and r 2 score the initial experimentation with the baseline model proved that the abundance of features leads to high variance and weak performance of the model on the validation set compared to the training set 	machine learning techniques including linear regression tree based models svr and neural networks along with feature importance analyses are employed to achieve the best results in terms of mean squared error mean absolute error and r 2 score 
1	159	10556	this project attempts to come up with the best model for predicting the airbnb prices based on a set of features including property specifications owner information and customer reviews on the listings machine learning techniques including linear regression tree based models svr and neural networks along with feature importance analyses are employed to achieve the best results in terms of mean squared error mean absolute error and r 2 score the initial experimentation with the baseline model proved that the abundance of features leads to high variance and weak performance of the model on the validation set compared to the training set 	the initial experimentation with the baseline model proved that the abundance of features leads to high variance and weak performance of the model on the validation set compared to the training set 
1	159	10557	this project attempts to come up with the best model for predicting the airbnb prices based on a set of features including property specifications owner information and customer reviews on the listings machine learning techniques including linear regression tree based models svr and neural networks along with feature importance analyses are employed to achieve the best results in terms of mean squared error mean absolute error and r 2 score the initial experimentation with the baseline model proved that the abundance of features leads to high variance and weak performance of the model on the validation set compared to the training set 	lasso cross validation feature importance analysis reduced the variance and using advanced models such as svr and neural networks resulted in higher r 2 score for both the validation and test sets 
1	159	10558	this project attempts to come up with the best model for predicting the airbnb prices based on a set of features including property specifications owner information and customer reviews on the listings machine learning techniques including linear regression tree based models svr and neural networks along with feature importance analyses are employed to achieve the best results in terms of mean squared error mean absolute error and r 2 score the initial experimentation with the baseline model proved that the abundance of features leads to high variance and weak performance of the model on the validation set compared to the training set 	among the models tested support vector regression svr performed the best and produced an r 2 score of 69 and a mse of 0 147 defined on ln price on the test set 
1	159	10559	this project attempts to come up with the best model for predicting the airbnb prices based on a set of features including property specifications owner information and customer reviews on the listings machine learning techniques including linear regression tree based models svr and neural networks along with feature importance analyses are employed to achieve the best results in terms of mean squared error mean absolute error and r 2 score the initial experimentation with the baseline model proved that the abundance of features leads to high variance and weak performance of the model on the validation set compared to the training set 	this level of accuracy is a promising outcome given the heterogeneity of the dataset and the involved hidden factors including the personal characteristics of the owners which were impossible to consider the future works on this project can include i studying other feature selection schemes such as random forest feature importance ii further experimentation with neural net architectures and iii getting more training examples from other hospitality services such as vrbo to boost the performance of k means clustering with ridge regression model in particular 
0	159	10560	 liubov nikolenko data cleaning splitting categorical features implementing sentiment analysis of the reviews initial neural network implementation svr implementation and tuning k means ridge tuning hoormazd rezaei implementation of linear regression and tree ensembles datapreprocessing implementation of the evaluation metrics feature selection methods implementation tuning of the neural network pouya rezazadeh data cleaning and auxiliary visualization splitting categorical features result visualizations tree ensembles tuning k means ridge implementation 	 liubov nikolenko data cleaning splitting categorical features implementing sentiment analysis of the reviews initial neural network implementation svr implementation and tuning k means ridge tuning hoormazd rezaei implementation of linear regression and tree ensembles datapreprocessing implementation of the evaluation metrics feature selection methods implementation tuning of the neural network pouya rezazadeh data cleaning and auxiliary visualization splitting categorical features result visualizations tree ensembles tuning k means ridge implementation 
0	160	10561	doodle recognition has important consequences in computer vision and pattern recognition especially in relation to the handling of noisy datasets in this paper we build a multi class classifier to assign hand drawn doodles from google s online game quick draw into 345 unique categories 	doodle recognition has important consequences in computer vision and pattern recognition especially in relation to the handling of noisy datasets 
0	160	10562	doodle recognition has important consequences in computer vision and pattern recognition especially in relation to the handling of noisy datasets in this paper we build a multi class classifier to assign hand drawn doodles from google s online game quick draw into 345 unique categories 	in this paper we build a multi class classifier to assign hand drawn doodles from google s online game quick draw 
0	160	10563	doodle recognition has important consequences in computer vision and pattern recognition especially in relation to the handling of noisy datasets in this paper we build a multi class classifier to assign hand drawn doodles from google s online game quick draw into 345 unique categories 	into 345 unique categories 
0	160	10564	doodle recognition has important consequences in computer vision and pattern recognition especially in relation to the handling of noisy datasets in this paper we build a multi class classifier to assign hand drawn doodles from google s online game quick draw into 345 unique categories 	to do so we implement and compare multiple variations of k nearest neighbors and a convolutional neural network which achieve 35 accuracy and 60 accuracy respectively 
0	160	10565	doodle recognition has important consequences in computer vision and pattern recognition especially in relation to the handling of noisy datasets in this paper we build a multi class classifier to assign hand drawn doodles from google s online game quick draw into 345 unique categories 	by evaluating the models performance and learned features we can identify distinct characteristics of the dataset that will prove important for future work 
0	160	10566	in november 2016 google released an online game titled quick draw that challenges players to draw a given object in under 20 seconds however this is no ordinary game while the user is drawing an advanced neural network attempts to guess the category of the object and its predictions evolve as the user adds more and more detail beyond just the scope of quick draw the ability to recognize and classify hand drawn doodles has important implications for the development of artificial intelligence at large 	in november 2016 google released an online game titled quick draw 
0	160	10567	in november 2016 google released an online game titled quick draw that challenges players to draw a given object in under 20 seconds however this is no ordinary game while the user is drawing an advanced neural network attempts to guess the category of the object and its predictions evolve as the user adds more and more detail beyond just the scope of quick draw the ability to recognize and classify hand drawn doodles has important implications for the development of artificial intelligence at large 	that challenges players to draw a given object in under 20 seconds 
0	160	10568	in november 2016 google released an online game titled quick draw that challenges players to draw a given object in under 20 seconds however this is no ordinary game while the user is drawing an advanced neural network attempts to guess the category of the object and its predictions evolve as the user adds more and more detail beyond just the scope of quick draw the ability to recognize and classify hand drawn doodles has important implications for the development of artificial intelligence at large 	however this is no ordinary game while the user is drawing an advanced neural network attempts to guess the category of the object and its predictions evolve as the user adds more and more detail beyond just the scope of quick draw the ability to recognize and classify hand drawn doodles has important implications for the development of artificial intelligence at large 
0	160	10569	in november 2016 google released an online game titled quick draw that challenges players to draw a given object in under 20 seconds however this is no ordinary game while the user is drawing an advanced neural network attempts to guess the category of the object and its predictions evolve as the user adds more and more detail beyond just the scope of quick draw the ability to recognize and classify hand drawn doodles has important implications for the development of artificial intelligence at large 	for example research in computer vision and pattern recognition especially in subfields such as optical character recognition ocr would benefit greatly from the advent of a robust classifier on high noise datasets for the purposes of this project we choose to focus on classification of the finished doodles in their entirety 
1	160	10570	in november 2016 google released an online game titled quick draw that challenges players to draw a given object in under 20 seconds however this is no ordinary game while the user is drawing an advanced neural network attempts to guess the category of the object and its predictions evolve as the user adds more and more detail beyond just the scope of quick draw the ability to recognize and classify hand drawn doodles has important implications for the development of artificial intelligence at large 	while a simpler premise than that of the original game s this task remains difficult due to the large number of categories 345 wide variation of doodles within even a single category and confusing similarity between doodles across multiple categories thus we create a multi class classifier whose input is a quick draw 
0	160	10571	in november 2016 google released an online game titled quick draw that challenges players to draw a given object in under 20 seconds however this is no ordinary game while the user is drawing an advanced neural network attempts to guess the category of the object and its predictions evolve as the user adds more and more detail beyond just the scope of quick draw the ability to recognize and classify hand drawn doodles has important implications for the development of artificial intelligence at large 	doodle and whose output is the predicted category for the depicted object 
0	160	10572	similar to our task google engineers ha and eck used the quick draw online dataset to train their recurrent neural network rnn to learn sketch abstractions kim and saverese experimented with svm and knn performance on image classification specifically on airplanes cars faces and motorbikes 	similar to our task google engineers ha and eck used the quick draw 
0	160	10573	similar to our task google engineers ha and eck used the quick draw online dataset to train their recurrent neural network rnn to learn sketch abstractions kim and saverese experimented with svm and knn performance on image classification specifically on airplanes cars faces and motorbikes 	online dataset to train their recurrent neural network rnn to learn sketch abstractions 
0	160	10574	similar to our task google engineers ha and eck used the quick draw online dataset to train their recurrent neural network rnn to learn sketch abstractions kim and saverese experimented with svm and knn performance on image classification specifically on airplanes cars faces and motorbikes 	kim and saverese experimented with svm and knn performance on image classification specifically on airplanes cars faces and motorbikes 
0	160	10575	similar to our task google engineers ha and eck used the quick draw online dataset to train their recurrent neural network rnn to learn sketch abstractions kim and saverese experimented with svm and knn performance on image classification specifically on airplanes cars faces and motorbikes 	lu and tran architected a convolutional neural network cnn to tackle sketch classification 
0	160	10576	similar to our task google engineers ha and eck used the quick draw online dataset to train their recurrent neural network rnn to learn sketch abstractions kim and saverese experimented with svm and knn performance on image classification specifically on airplanes cars faces and motorbikes 	the state of the art as of 2017 comes from a cnn developed by seddati et al 
0	160	10577	similar to our task google engineers ha and eck used the quick draw online dataset to train their recurrent neural network rnn to learn sketch abstractions kim and saverese experimented with svm and knn performance on image classification specifically on airplanes cars faces and motorbikes 	with their deepsketch 3 model for sketch classification 
0	160	10578	google publicly released a quick draw dataset containing over 50 million images across 345 categories there are multiple different representations for the images 	google publicly released a quick draw 
0	160	10579	google publicly released a quick draw dataset containing over 50 million images across 345 categories there are multiple different representations for the images 	dataset containing over 50 million images across 345 categories 
0	160	10580	google publicly released a quick draw dataset containing over 50 million images across 345 categories there are multiple different representations for the images 	there are multiple different representations for the images 
0	160	10581	google publicly released a quick draw dataset containing over 50 million images across 345 categories there are multiple different representations for the images 	one dataset represents each drawing as a series of line vectors and another contains each image in a 28x28 grayscale matrix 
0	160	10582	google publicly released a quick draw dataset containing over 50 million images across 345 categories there are multiple different representations for the images 	because we focus on classification of the entire doodle in this project we use the latter version of the dataset 
0	160	10583	google publicly released a quick draw dataset containing over 50 million images across 345 categories there are multiple different representations for the images 	we treat each 28x28 pixel image as a 784 dimensional vector 
0	160	10584	google publicly released a quick draw dataset containing over 50 million images across 345 categories there are multiple different representations for the images 	to test our models we split the data into three different folds 70 for training 15 for validation and 15 for testing 
1	160	10585	google publicly released a quick draw dataset containing over 50 million images across 345 categories there are multiple different representations for the images 	to reduce computation time and storage of the data we decided to create a smaller subset of the original dataset by randomly sampling 1 of the drawings from each category as a result we obtain approximately 350 000 examples for the training set and 75 000 examples each for the validation and testing set 
0	160	10586	google publicly released a quick draw dataset containing over 50 million images across 345 categories there are multiple different representations for the images 	furthermore the number of drawings in each category is balanced so this leaves approximately 1000 examples per category in the training dataset 
0	160	10587	for our baseline we intuitively assume that all the images in a particular category should look relatively similar based on this assumption one way we could determine which category a given drawing belongs to is by looking at which training examples are the most nearby to the doodle under test this intuition corresponds with the k nearest neighbors knn algorithm the vanilla knn algorithm computes the k training examples that are closest in l 1 or l 2 distance to our current drawing 	for our baseline we intuitively assume that all the images in a particular category should look relatively similar 
0	160	10588	for our baseline we intuitively assume that all the images in a particular category should look relatively similar based on this assumption one way we could determine which category a given drawing belongs to is by looking at which training examples are the most nearby to the doodle under test this intuition corresponds with the k nearest neighbors knn algorithm the vanilla knn algorithm computes the k training examples that are closest in l 1 or l 2 distance to our current drawing 	based on this assumption one way we could determine which category a given drawing belongs to is by looking at which training examples are the most nearby to the doodle under test this intuition corresponds with the k nearest neighbors knn algorithm 
0	160	10589	for our baseline we intuitively assume that all the images in a particular category should look relatively similar based on this assumption one way we could determine which category a given drawing belongs to is by looking at which training examples are the most nearby to the doodle under test this intuition corresponds with the k nearest neighbors knn algorithm the vanilla knn algorithm computes the k training examples that are closest in l 1 or l 2 distance to our current drawing 	the vanilla knn algorithm computes the k training examples that are closest in l 1 or l 2 distance to our current drawing 
0	160	10590	for our baseline we intuitively assume that all the images in a particular category should look relatively similar based on this assumption one way we could determine which category a given drawing belongs to is by looking at which training examples are the most nearby to the doodle under test this intuition corresponds with the k nearest neighbors knn algorithm the vanilla knn algorithm computes the k training examples that are closest in l 1 or l 2 distance to our current drawing 	then it predicts the category that occurs the greatest number of times among those k neighbors 
1	160	10591	for our baseline we intuitively assume that all the images in a particular category should look relatively similar based on this assumption one way we could determine which category a given drawing belongs to is by looking at which training examples are the most nearby to the doodle under test this intuition corresponds with the k nearest neighbors knn algorithm the vanilla knn algorithm computes the k training examples that are closest in l 1 or l 2 distance to our current drawing 	however because we have 350k training examples and 75k validation examples this algorithm requires at least 3 5 10 5 7 5 10 4 784 2 10 13 operations to evaluate the entire validation set which is too slow consequently we propose a less computationally expensive variant of knn which we call 1 closest centroid 1 cc 
1	160	10592	for our baseline we intuitively assume that all the images in a particular category should look relatively similar based on this assumption one way we could determine which category a given drawing belongs to is by looking at which training examples are the most nearby to the doodle under test this intuition corresponds with the k nearest neighbors knn algorithm the vanilla knn algorithm computes the k training examples that are closest in l 1 or l 2 distance to our current drawing 	at a high level 1 cc equivalent to supervised kmeans clustering in which we compute a centroid for each category c using the training dataset and classify test examples according to the closest categories in more detail for each category c we calculate a centroid vector v c by taking the average of all of the vectors belonging to category c then to classify a given vector u we compute arg min c u v c 2 which seeks to minimize the squared difference in pixel values between the two images 
0	160	10593	for our baseline we intuitively assume that all the images in a particular category should look relatively similar based on this assumption one way we could determine which category a given drawing belongs to is by looking at which training examples are the most nearby to the doodle under test this intuition corresponds with the k nearest neighbors knn algorithm the vanilla knn algorithm computes the k training examples that are closest in l 1 or l 2 distance to our current drawing 	effectively we are choosing the category whose mean representation vector is closest in euclidean distance to our given vector u 
0	160	10594	for our baseline we intuitively assume that all the images in a particular category should look relatively similar based on this assumption one way we could determine which category a given drawing belongs to is by looking at which training examples are the most nearby to the doodle under test this intuition corresponds with the k nearest neighbors knn algorithm the vanilla knn algorithm computes the k training examples that are closest in l 1 or l 2 distance to our current drawing 	this reduces the number of points we look at for each u to only 345 one per category 
0	160	10595	1 cc makes the simplifying assumption that all doodles in a category will be similar to each other however in reality there are many different ways to draw a given object for example bear can be drawn with multiple representations as seen in	1 cc makes the simplifying assumption that all doodles in a category will be similar to each other 
0	160	10596	1 cc makes the simplifying assumption that all doodles in a category will be similar to each other however in reality there are many different ways to draw a given object for example bear can be drawn with multiple representations as seen in	however in reality there are many different ways to draw a given object 
0	160	10597	1 cc makes the simplifying assumption that all doodles in a category will be similar to each other however in reality there are many different ways to draw a given object for example bear can be drawn with multiple representations as seen in	for example bear can be drawn with multiple representations as seen in
0	160	10598	we noticed that voting in knn often ended up with ties to mitigate ties we further extend knn to not only utilize multiple clusters per category but also to use a weighted voting schema when tallying for final predictions we name this method knn weighted for short intuitively we wish to count votes from closer centroids more than votes from more distance centroids 	we noticed that voting in knn often ended up with ties 
0	160	10599	we noticed that voting in knn often ended up with ties to mitigate ties we further extend knn to not only utilize multiple clusters per category but also to use a weighted voting schema when tallying for final predictions we name this method knn weighted for short intuitively we wish to count votes from closer centroids more than votes from more distance centroids 	to mitigate ties we further extend knn to not only utilize multiple clusters per category but also to use a weighted voting schema when tallying for final predictions 
0	160	10600	we noticed that voting in knn often ended up with ties to mitigate ties we further extend knn to not only utilize multiple clusters per category but also to use a weighted voting schema when tallying for final predictions we name this method knn weighted for short intuitively we wish to count votes from closer centroids more than votes from more distance centroids 	we name this method knn weighted for short intuitively we wish to count votes from closer centroids more than votes from more distance centroids 
0	160	10601	we noticed that voting in knn often ended up with ties to mitigate ties we further extend knn to not only utilize multiple clusters per category but also to use a weighted voting schema when tallying for final predictions we name this method knn weighted for short intuitively we wish to count votes from closer centroids more than votes from more distance centroids 	thus we experiment with two different weighting schemas distance weighting 
0	160	10602	we noticed that voting in knn often ended up with ties to mitigate ties we further extend knn to not only utilize multiple clusters per category but also to use a weighted voting schema when tallying for final predictions we name this method knn weighted for short intuitively we wish to count votes from closer centroids more than votes from more distance centroids 	with distance weighting each cen troid s c s weighted vote w i is equal towhere x i is the vector representation of the test example 
0	160	10603	we noticed that voting in knn often ended up with ties to mitigate ties we further extend knn to not only utilize multiple clusters per category but also to use a weighted voting schema when tallying for final predictions we name this method knn weighted for short intuitively we wish to count votes from closer centroids more than votes from more distance centroids 	with rank weighting we first sort all centroids by increasing distance to the test example 
0	160	10604	we noticed that voting in knn often ended up with ties to mitigate ties we further extend knn to not only utilize multiple clusters per category but also to use a weighted voting schema when tallying for final predictions we name this method knn weighted for short intuitively we wish to count votes from closer centroids more than votes from more distance centroids 	within this sorted order the centroid c i at rank i has a weighted vote equal to
1	160	10605	as a comparison against the above knn methods we implement a convolutional neural network cnn a stateof the art model known for being able to recognize and quickly learn local features within an image to achieve the best results we perform data preprocessing first we calculate the mean across all training examples as well as the standard deviation we then for each example training validation and test subtract and divide by 	as a comparison against the above knn methods we implement a convolutional neural network cnn a stateof the art model known for being able to recognize and quickly learn local features within an image to achieve the best results we perform data preprocessing 
0	160	10606	as a comparison against the above knn methods we implement a convolutional neural network cnn a stateof the art model known for being able to recognize and quickly learn local features within an image to achieve the best results we perform data preprocessing first we calculate the mean across all training examples as well as the standard deviation we then for each example training validation and test subtract and divide by 	first we calculate the mean across all training examples as well as the standard deviation 
0	160	10607	as a comparison against the above knn methods we implement a convolutional neural network cnn a stateof the art model known for being able to recognize and quickly learn local features within an image to achieve the best results we perform data preprocessing first we calculate the mean across all training examples as well as the standard deviation we then for each example training validation and test subtract and divide by 	we then for each example training validation and test subtract and divide by 
0	160	10608	as a comparison against the above knn methods we implement a convolutional neural network cnn a stateof the art model known for being able to recognize and quickly learn local features within an image to achieve the best results we perform data preprocessing first we calculate the mean across all training examples as well as the standard deviation we then for each example training validation and test subtract and divide by 	to account for division by zero errors when dividing by we add an offset of 10 to beforehand the model architecture is shown in
0	160	10609	while raw accuracy is a good measure of a model s performance it penalizes harshly for an incorrect prediction wrong predictions receive 0 points and right predictions receive 1 point since we have so many categories including some that are extremely similar such as cake and birthday cake we evaluate our methods not only with raw accuracy but also with a scoring metric that is more lenient of incorrect predictions thus predictions are evaluated using mean average precision 3 map 3 where u is the number of drawings in the test set p k is the precision at cutoff k and n is the number of predictions per drawing put more intuitively the equation considers the top 3 predictions p 1 p 2 p 3 that the model makes for a given drawing 	while raw accuracy is a good measure of a model s performance it penalizes harshly for an incorrect prediction wrong predictions receive 0 points and right predictions receive 1 point 
1	160	10610	while raw accuracy is a good measure of a model s performance it penalizes harshly for an incorrect prediction wrong predictions receive 0 points and right predictions receive 1 point since we have so many categories including some that are extremely similar such as cake and birthday cake we evaluate our methods not only with raw accuracy but also with a scoring metric that is more lenient of incorrect predictions thus predictions are evaluated using mean average precision 3 map 3 where u is the number of drawings in the test set p k is the precision at cutoff k and n is the number of predictions per drawing put more intuitively the equation considers the top 3 predictions p 1 p 2 p 3 that the model makes for a given drawing 	since we have so many categories including some that are extremely similar such as cake and birthday cake we evaluate our methods not only with raw accuracy but also with a scoring metric that is more lenient of incorrect predictions thus predictions are evaluated using mean average precision 3 map 3 where u is the number of drawings in the test set p k is the precision at cutoff k and n is the number of predictions per drawing 
0	160	10611	while raw accuracy is a good measure of a model s performance it penalizes harshly for an incorrect prediction wrong predictions receive 0 points and right predictions receive 1 point since we have so many categories including some that are extremely similar such as cake and birthday cake we evaluate our methods not only with raw accuracy but also with a scoring metric that is more lenient of incorrect predictions thus predictions are evaluated using mean average precision 3 map 3 where u is the number of drawings in the test set p k is the precision at cutoff k and n is the number of predictions per drawing put more intuitively the equation considers the top 3 predictions p 1 p 2 p 3 that the model makes for a given drawing 	put more intuitively the equation considers the top 3 predictions p 1 p 2 p 3 that the model makes for a given drawing 
0	160	10612	while raw accuracy is a good measure of a model s performance it penalizes harshly for an incorrect prediction wrong predictions receive 0 points and right predictions receive 1 point since we have so many categories including some that are extremely similar such as cake and birthday cake we evaluate our methods not only with raw accuracy but also with a scoring metric that is more lenient of incorrect predictions thus predictions are evaluated using mean average precision 3 map 3 where u is the number of drawings in the test set p k is the precision at cutoff k and n is the number of predictions per drawing put more intuitively the equation considers the top 3 predictions p 1 p 2 p 3 that the model makes for a given drawing 	it then assigns a score of 1 i if p i is the correct label for the image and a score of 0 if the correct label is not in the top 3 guesses 
0	160	10613	while raw accuracy is a good measure of a model s performance it penalizes harshly for an incorrect prediction wrong predictions receive 0 points and right predictions receive 1 point since we have so many categories including some that are extremely similar such as cake and birthday cake we evaluate our methods not only with raw accuracy but also with a scoring metric that is more lenient of incorrect predictions thus predictions are evaluated using mean average precision 3 map 3 where u is the number of drawings in the test set p k is the precision at cutoff k and n is the number of predictions per drawing put more intuitively the equation considers the top 3 predictions p 1 p 2 p 3 that the model makes for a given drawing 	note that map 1 is equivalent to singleprediction accuracy 
0	160	10614	as seen in 1 cc performed best on the categories stairs circle and door knn performed best on the categories stairs the eiffel tower and bowtie for these categories the centroids are either simplistic circle door or are distinct in shape stairs the eiffel tower bowtie which causes the doodles to have less variance 	as seen in 1 cc performed best on the categories stairs circle and door 
0	160	10615	as seen in 1 cc performed best on the categories stairs circle and door knn performed best on the categories stairs the eiffel tower and bowtie for these categories the centroids are either simplistic circle door or are distinct in shape stairs the eiffel tower bowtie which causes the doodles to have less variance 	knn performed best on the categories stairs the eiffel tower and bowtie 
0	160	10616	as seen in 1 cc performed best on the categories stairs circle and door knn performed best on the categories stairs the eiffel tower and bowtie for these categories the centroids are either simplistic circle door or are distinct in shape stairs the eiffel tower bowtie which causes the doodles to have less variance 	for these categories the centroids are either simplistic circle door or are distinct in shape stairs the eiffel tower bowtie which causes the doodles to have less variance 
1	160	10617	as seen in 1 cc performed best on the categories stairs circle and door knn performed best on the categories stairs the eiffel tower and bowtie for these categories the centroids are either simplistic circle door or are distinct in shape stairs the eiffel tower bowtie which causes the doodles to have less variance 	thus the centroids are generally contain a clear outline of the object on the other hand 1 cc performed worst on the categories flip flops garden hose and wrist watch and knn performed worst on dog string bean and peas 
0	160	10618	as seen in 1 cc performed best on the categories stairs circle and door knn performed best on the categories stairs the eiffel tower and bowtie for these categories the centroids are either simplistic circle door or are distinct in shape stairs the eiffel tower bowtie which causes the doodles to have less variance 	the centroids for these bottom 3 categories are much more vague 
0	160	10619	as seen in 1 cc performed best on the categories stairs circle and door knn performed best on the categories stairs the eiffel tower and bowtie for these categories the centroids are either simplistic circle door or are distinct in shape stairs the eiffel tower bowtie which causes the doodles to have less variance 	for example dog was often confused with other four legged animals such as horse and cow furthermore some categories produced nearly identical centroids such as circle and octagon in
1	160	10620	knn with weighted votes by rank produced the highest map 3 and map 1 scores out of all the knn models 	knn with weighted votes by rank produced the highest map 3 and map 1 scores out of all the knn models 
1	160	10621	to achieve the best performance for the cnn model we tuned various hyperparameters including the number of units in each dense layer dropout rate and learning rate overall we found that the model producing the best map 3 score on the validation set had three dense layers with 700 500 and 400 units with each layer having a dropout rate of 0 2 furthermore we trained our model with learning rate of 1 10 3 and batch size of 32 across 20 epochs 	to achieve the best performance for the cnn model we tuned various hyperparameters including the number of units in each dense layer dropout rate and learning rate 
1	160	10622	to achieve the best performance for the cnn model we tuned various hyperparameters including the number of units in each dense layer dropout rate and learning rate overall we found that the model producing the best map 3 score on the validation set had three dense layers with 700 500 and 400 units with each layer having a dropout rate of 0 2 furthermore we trained our model with learning rate of 1 10 3 and batch size of 32 across 20 epochs 	overall we found that the model producing the best map 3 score on the validation set had three dense layers with 700 500 and 400 units with each layer having a dropout rate of 0 2 
0	160	10623	to achieve the best performance for the cnn model we tuned various hyperparameters including the number of units in each dense layer dropout rate and learning rate overall we found that the model producing the best map 3 score on the validation set had three dense layers with 700 500 and 400 units with each layer having a dropout rate of 0 2 furthermore we trained our model with learning rate of 1 10 3 and batch size of 32 across 20 epochs 	furthermore we trained our model with learning rate of 1 10 3 and batch size of 32 across 20 epochs 
0	160	10624	to achieve the best performance for the cnn model we tuned various hyperparameters including the number of units in each dense layer dropout rate and learning rate overall we found that the model producing the best map 3 score on the validation set had three dense layers with 700 500 and 400 units with each layer having a dropout rate of 0 2 furthermore we trained our model with learning rate of 1 10 3 and batch size of 32 across 20 epochs 	the end architecture fits the data well as we see from the loss plot in inspecting the accuracy distribution across individual category we note from
1	160	10625	we found that our cnn outperformed our extended knn algorithm with map 3 values of 62 1 and 34 4 respectively although both algorithms perform much better than random guessing of 0 5 but lower than human guessing of 73 0 although knn was able to identify multiple representations of the same category which increased accuracy compared to 1 nn knn still came short compared to our cnn due to its inability to recognize features and distinguish between apples and blueberries due to the presence of a stem for future work we would like to experiment with advanced cnn architectures such as vgg net and resnet which have already reached state of the art levels of image classification performance although not for sketches in particular additionally we have only used approximately 1 of the total quick draw 	we found that our cnn outperformed our extended knn algorithm with map 3 values of 62 1 and 34 4 respectively although both algorithms perform much better than random guessing of 0 5 but lower than human guessing of 73 0 
1	160	10626	we found that our cnn outperformed our extended knn algorithm with map 3 values of 62 1 and 34 4 respectively although both algorithms perform much better than random guessing of 0 5 but lower than human guessing of 73 0 although knn was able to identify multiple representations of the same category which increased accuracy compared to 1 nn knn still came short compared to our cnn due to its inability to recognize features and distinguish between apples and blueberries due to the presence of a stem for future work we would like to experiment with advanced cnn architectures such as vgg net and resnet which have already reached state of the art levels of image classification performance although not for sketches in particular additionally we have only used approximately 1 of the total quick draw 	although knn was able to identify multiple representations of the same category which increased accuracy compared to 1 nn knn still came short compared to our cnn due to its inability to recognize features and distinguish between apples and blueberries due to the presence of a stem for future work we would like to experiment with advanced cnn architectures such as vgg net and resnet which have already reached state of the art levels of image classification performance although not for sketches in particular 
0	160	10627	we found that our cnn outperformed our extended knn algorithm with map 3 values of 62 1 and 34 4 respectively although both algorithms perform much better than random guessing of 0 5 but lower than human guessing of 73 0 although knn was able to identify multiple representations of the same category which increased accuracy compared to 1 nn knn still came short compared to our cnn due to its inability to recognize features and distinguish between apples and blueberries due to the presence of a stem for future work we would like to experiment with advanced cnn architectures such as vgg net and resnet which have already reached state of the art levels of image classification performance although not for sketches in particular additionally we have only used approximately 1 of the total quick draw 	additionally we have only used approximately 1 of the total quick draw 
1	160	10628	we found that our cnn outperformed our extended knn algorithm with map 3 values of 62 1 and 34 4 respectively although both algorithms perform much better than random guessing of 0 5 but lower than human guessing of 73 0 although knn was able to identify multiple representations of the same category which increased accuracy compared to 1 nn knn still came short compared to our cnn due to its inability to recognize features and distinguish between apples and blueberries due to the presence of a stem for future work we would like to experiment with advanced cnn architectures such as vgg net and resnet which have already reached state of the art levels of image classification performance although not for sketches in particular additionally we have only used approximately 1 of the total quick draw 	dataset and we believe training our models on the complete dataset would improve accuracy as well incorporating stroke order information and extract features such as velocity and acceleration 
0	160	10629	we found that our cnn outperformed our extended knn algorithm with map 3 values of 62 1 and 34 4 respectively although both algorithms perform much better than random guessing of 0 5 but lower than human guessing of 73 0 although knn was able to identify multiple representations of the same category which increased accuracy compared to 1 nn knn still came short compared to our cnn due to its inability to recognize features and distinguish between apples and blueberries due to the presence of a stem for future work we would like to experiment with advanced cnn architectures such as vgg net and resnet which have already reached state of the art levels of image classification performance although not for sketches in particular additionally we have only used approximately 1 of the total quick draw 	finally we believe that ensembling techniques are interesting particularly for lighterweight methods such as knn 
